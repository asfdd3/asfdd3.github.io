[{"title":"VFS虚拟文件系统","url":"/2025/05/19/VFS%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","content":"VFS(虚拟文件系统)1.什么是VFSLinux 需要支持多种不同的文件系统（因为不同的文件系统有不同的特点），同时还要为用户提供一组统一的接口，因此要实现这个目的，就要将对各种不同文件系统和管理纳入到一个统一的框架中，也就是同一组系统调用，对各种不同的文件系统进行操作，这就是存在VFS的目的。\n这样，就可以对用户程序隐去各种不同文件系统的细节，为用户程序提供一个统一的、抽象的、虚拟的文件系统，这就是所谓“虚拟文件系统” - VFS（Virtual Filesystem Switch）。这个抽象层由一组标准的、抽象的文件操作构成，以系统调用的形式提供于用户程序，如read（）、write（）、lseek（）等等。这样，用户程序就可以 把所有的文件都看作一致的、抽象的“VFS文件”，通过这些系统调用对文件进行操作，而无需关心具体的文件属于什么文件系统以及具体文件系统的设计和实现,也就是说VFS是一个内核软件层，使应用程序与具体的文件系统解耦。\n举一个例子：在编写应用程序时，会经常使用到write（）系统调用，也就是向一个文件中写入数据。函数的原型为 ssize_t write(int fd, const void *buf, size_t count); 用户程序调用write（f, &amp;buf, len）的含义为向文件描述符为f的文件中，写入len个字节数据，。下图为write（）将数据写入到设备上的宏观流程。我们看到首先通过虚拟文件系统VFS，然后根据不同文件系统的write（）方法将数据写入物理设备上，宏观的调用流程如下图所示：\n\n2.VFS 整体架构虚拟文件系统作为内核中的一个抽象层，起到一个中间层的作用，对上(应用程序)提供统一接口，应用程序只需使用标准的文件操作（如 open、read、write1），无需关心底层是哪种文件系统（EXT4、NTFS、FAT等），对下为各种具体的文件系统（如 ext4、XFS 等）提供了统一的接口（其实就是实现不同文件系统的ops集合），VFS在内核中的整体架构如下所示。\n\n\n\n上述图片为VFS整体架构图，图片中各个组件作用大概如下：\n\nAPP：用户程序通过系统调用读写文件\nPage Cache：缓存文件的数据内容，例如次读取文件时从磁盘加载到页缓存，后续直接读缓存，避免磁盘I&#x2F;O。\nDirectory cache：缓存文件路径到Dentry的映射，减少频繁解析路径的开销。\nInode缓存：缓存文件的元数据（权限、大小、数据块位置等）\nBuffer Cache：缓存磁盘块的原始数据（已逐步被Page Cache取代，但在某些场景仍用于块设备操作）\n磁盘文件系统（ext2&#x2F;ext3&#x2F;ext4）：\next2：早期非日志式文件系统，简单但易崩溃损坏。\next3：增加日志功能，提升崩溃恢复能力。\next4：支持更大文件&#x2F;分区、延迟分配等高级特性。\n\n\n伪文件系统：\nproc：虚拟文件系统，动态暴露内核状态（如 &#x2F;proc&#x2F;cpuinfo）\nsysfs：提供设备&#x2F;驱动信息的统一接口（如 &#x2F;sys&#x2F;class）\n\n\n\n3.VFS关键数据结构VFS中包含着向物理文件系统转换的一系列数据结构，Linux中VFS层依靠四个主要的数据结构来述其结构信息，分别为超级块、索引结点、目录项和文件对象。这四个数据结构作用如下：\n3.1 Superblock（超级块）\n功能：超级块(块指的是存储和管理数据的基本单位)对象由各自的文件系统实现，用来存储文件系统的信息，如块大小、块数量等。这个对象对应为文件系统超级块或者文件系统控制块，它存储在磁盘特定的扇区上。不是基于磁盘的文件系统临时生成超级块，并保存在内存中，注意：所有超级块对象都以双向循环链表的形式链接在一起被管理。\n\n用途： \n\n超级块与物理文件系统一一对应。\n在挂载时初始化，帮助管理文件系统。\n\n\n\n管理超级块的结构体如下所示：\nstruct super_block &#123;    struct list_head    s_list;               // 指向链表的指针    dev_t               s_dev;                // 设备标识符    unsigned long       s_blocksize;          // 以字节为单位的块大小    loff_t              s_maxbytes;           // 文件大小上限    struct file_system_type    *s_type;       // 文件系统类型    const struct super_operations    *s_op;   // SuperBlock 操作函数，write_inode、put_inode 等    const struct dquot_operations    *dq_op;  // 磁盘限额函数    struct dentry        *s_root;             // 根目录&#125;\n\n3.2 Inode(索引节点)\n功能：每个文件都有一个唯一的inode，存储了文件的元数据，如文件大小、权限、访问时间等。它是文件系统中文件的抽象表示，不包含文件名。\n特点：inode存储在磁盘中（伪文件系统除外），在需要的时候会被加载到内存中，具体情况如下：inode 是文件系统的元数据结构，直接存储在磁盘上，用于长期保存文件的元信息（如权限、大小、块位置等）例如在Ext4文件系统中，inode集中存放在磁盘的固定区域。当访问某个文件时，会根据具体的磁盘上的inode(也就是磁盘中的inode, 比如ext4_inode_info)，来填充VFS的创建的inode(用私有指针指一下)。\n\nVFS管理的inode结构如下所示：\nstruct inode &#123;    umode_t                 i_mode;          // 文件权限及类型    kuid_t                  i_uid;           // user id    kgid_t                  i_gid;           // group id    const struct inode_operations    *i_op;  // inode 操作函数，如 create，mkdir，lookup，rename 等    struct super_block      *i_sb;           // 所属的 SuperBlock    loff_t                  i_size;          // 文件大小    struct timespec         i_atime;         // 文件最后访问时间    struct timespec         i_mtime;         // 文件最后修改时间    struct timespec         i_ctime;         // 文件元数据最后修改时间（包括文件名称）    const struct file_operations    *i_fop;  // 文件操作函数，open、write 等    void                    *i_private;      // 文件系统的私有数据&#125;\n\n3.3 Dentry(目录项)Dentry的核心作用是在内存中建立文件名（路径）与 inode 之间的高效映射。每个 Dentry 代表路径中一个特定部分。对于“&#x2F;bin&#x2F;ls”、“&#x2F;”、“bin”和“ls”都是目录项对象。前面是两个目录，最后一个是普通文件。在路径中， 包括普通文件在内，每一个部分都是目录项对象。目录项是描述文件的逻辑属性，只存在于内存中，举个例子，当调用open()函数打开一个文件时，内核会第一时间根据文件路径到 DEntry Cache 里面寻找相应的 DEntry，找到了就直接构造一个file对象并返回。如果该文件不在缓存中，那么 VFS 会根据找到的最近目录一级一级地向下加载，直到找到相应的文件。期间 VFS 会缓存所有被加载生成的dentry。注意:一个 INode 可能被多个 DEntry 所关联，即相当于为某一文件创建了多个文件路径.\nVFS管理的Dentry结构如下所示：\nstruct dentry &#123;    struct dentry *d_parent;     // 父目录    struct qstr d_name;          // 文件名称    struct inode *d_inode;       // 关联的 inode    struct list_head d_child;    // 父目录中的子目录和文件    struct list_head d_subdirs;  // 当前目录中的子目录和文件&#125;\n\n3.4 file 文件对象虚拟文件系统最后一个主要对象是文件对象，文件对象表示进程已打开的文件，每个进程都持有一个fd[]数组，数组里面存放的是指向file结构体的指针，同一进程的不同fd可以指向同一个file对象，file是内核中的数据结构，表示一个被进程打开的文件，和进程相关联。当应用程序调用open()函数的时候，VFS 就会创建相应的file对象。注意： file会通过Dentry找到inode，file的ops集合（read，write等）其实就是inode的i_fops; 这样感觉就实现了进程和文件系统之间的解耦。\nfile结构如下所示：\nstruct file &#123;    struct path                   f_path;    struct inode                  *f_inode;    const struct file_operations  *f_op;    unsigned int                  f_flags;    fmode_t                       f_mode;    loff_t                        f_pos;    struct fown_struct            f_owner;&#125;\n\n下图为上述四个关键数据结构的关系图：\n\n4.挂载4.1 什么叫挂载挂载（Mounting） 是将存储设备（如硬盘、U盘）或文件系统（如Ext4、NTFS）关联到Linux目录树中某个目录（称为挂载点）的过程。挂载后，访问该目录实际指向目标设备或文件系统的内容，而原目录下的文件会被临时隐藏。例如，将U盘挂载到&#x2F;mnt&#x2F;usb后，访问此目录即访问U盘数据，卸载后恢复原目录内容。内核通过虚拟文件系统（VFS）管理挂载表，动态路由路径解析，实现对多文件系统的统一访问。简言之，挂载是让外部存储“接入”目录树的机制，用户通过目录操作文件，无需关心物理设备细节。\n挂载是在用户态发起mount命令，该命令执行的时候需要指定文件系统的类型（例如Ext2）和文件系统数据的位置（也就是dev）。通过这些关键信息，VFS就可以完成Ext2文件系统的初始化，并将其关联到当前已经存在的文件系统当中，也就是建立起下面所示的文件系统树。\n\n如上图所示，该系统根文件系统是Ext4文件系统，而在其&#x2F;mnt目录下面又分别挂载了Ext4文件系统和XFS文件系统。最后形成了一个由多个文件系统组成的文件系统树。\n4.2 挂载点 挂载点（Mount Point）是 Linux系统中用于将外部存储设备或文件系统接入到目录树的一个空目录。通过挂载操作，该目录会成为访问目标文件系统的入口，原有内容会被临时隐藏，转而显示被挂载设备或文件系统的内容。\n一个挂载点用一个vfsmount来表示，属于VFS层的一部分，在用户执行mount系统调用的时候会被创建，它记录了文件系统实例与目录树的关联关系，是挂载机制的核心实现，作用如下：\n\n关联挂载点与超级块：记录被挂载的超级块，其实就是知道被挂载的是哪个文件系统\n支持路径解析：当用户访问路径时，VFS 通过 vfsmount 确定目标文件系统的位置。例如，当访问&#x2F;mnt&#x2F;data&#x2F;file.txt时VFS 发现 &#x2F;mnt&#x2F;data 是挂载点（进而可以拿到超级块的信息）进而调用目标文件系统的方法继续查找要操作的文件。\n\nvfsmount结构如下所示：\nstruct vfsmount &#123;\tstruct dentry *mnt_root;\t//挂载的目录\tstruct super_block *mnt_sb;\t//指向超级块\tint mnt_flags;\tstruct mnt_idmap *mnt_idmap;&#125; __randomize_layout;\n","categories":["文件系统学习"],"tags":["VFS"]},{"title":"inet_sock 与 inet_connnection_sock","url":"/2025/07/14/inet_sock%E7%BB%93%E6%9E%84/","content":"inet_sock 是 Linux 内核中专门用于表示 IPv4 协议套接字的结构体，它继承自通用的 sock 结构，并扩展了诸如本地和远程 IP 地址、端口号、IP 头部字段（如 TTL、TOS）、MSS 以及 IP 选项等关键字段。在 TCP 和 UDP 协议的实现中，inet_sock 提供了协议栈处理 IPv4 网络连接所需的关键信息，是 IPv4 套接字在内核中的核心表示。\nstruct inet_sock &#123;\t/* sk and pinet6 has to be the first two members of inet_sock */\tstruct sock\t\tsk;  //#if IS_ENABLED(CONFIG_IPV6)\tstruct ipv6_pinfo\t*pinet6;#endif\t/* Socket demultiplex comparisons on incoming packets. */#define inet_daddr\t\tsk.__sk_common.skc_daddr //比如tcp/udp connect 或会设置（如果udp不connect好像不会设置）// bind中设置 //inet-&gt;inet_rcv_saddr = inet-&gt;inet_saddr = addr-&gt;sin_addr.s_addr;#define inet_rcv_saddr\t\tsk.__sk_common.skc_rcv_saddr #define inet_dport\t\tsk.__sk_common.skc_dport //目的port，比如建立连接的时候会设置#define inet_num\t\tsk.__sk_common.skc_num   //本地端口，比如bind的时候会设置\tunsigned long\t\tinet_flags;\t//源ip地址 tcp bind中会设置 ！！！如果没有调用bind 那就会在connect中查找路由设置\t__be32\t\t\tinet_saddr;    \t__s16\t\t\tuc_ttl;  //单播的ttl，系统默认是64,可以通过setsockopt设置\t__be16\t\t\tinet_sport;  //bind中设置，或者，autobind中设置\tstruct ip_options_rcu __rcu\t*inet_opt; //ip选项\tatomic_t\t\tinet_id; //ip id   //发送的时候根据4元组加hash生成\t__u8\t\t\ttos;   //dscp+ecn  可以通过setsockopt设置\t__u8\t\t\tmin_ttl;  //setsockopt设置。tcp收包的时候可以根据ttldrop掉\t__u8\t\t\tmc_ttl;\t//多播ttl\t__u8\t\t\tpmtudisc;  //根据系统配置决定是否开启mtu探测\t__u8\t\t\trcv_tos;  //接收的tos，用户可以通过setsockopt获取\t__u8\t\t\tconvert_csum;\tint\t\t\tuc_index;  \t//输出网络设备的索引，通过setsockopt设置\tint\t\t\tmc_index;\t//多播，同上\t__be32\t\t\tmc_addr; //发送多播的源ip地址，注意，这指的是单播地址\tstruct &#123;\t\t__u16 lo;\t\t__u16 hi;\t&#125;\t\t\tlocal_port_range; //本地port的范围\tstruct ip_mc_socklist __rcu\t*mc_list; //一个链表，管理的是当前sock加入的所有多播地址信息，以及模式\tstruct inet_cork_full\tcork;  //udp发包用到这个字段。存的是ip层待添字段的信息&#125;;\n\ninet_connection_sock 是 Linux 内核中用于表示基于连接的 IPv4 套接字（如 TCP）的结构体，它在 inet_sock 的基础上进一步扩展，添加了处理连接状态所需的字段，例如定时器、接收窗口控制、连接超时管理、状态回调函数等。该结构是 TCP 等面向连接协议在内核中的核心控制块，负责管理连接建立、维护和释放的各个阶段。\nstruct inet_connection_sock &#123;\t/* inet_sock has to be the first member! */\tstruct inet_sock\t  icsk_inet; \t\t//inet_sock\tstruct request_sock_queue icsk_accept_queue;  //服务端使用，管理全链接队列和半连接队列，在listen的时候会初始化\tstruct inet_bind_bucket\t  *icsk_bind_hash;\t\t//管理端口号\ttcp connectin的时候会用到\tstruct inet_bind2_bucket  *icsk_bind2_hash;\t\t//管理端口号的ip地址 \tunsigned long\t\t  icsk_timeout;\t\t\t\t//tcp定时器使用保存下次执行定时器任务的时间，比如计算下次重传的时间，退避计算 \tstruct timer_list\t  icsk_retransmit_timer;\t//重传定时器 \tstruct timer_list\t  icsk_delack_timer;\t\t//延迟ack定时器\t__u32\t\t\t  icsk_rto;\t\t\t\t\t\t//下一次重传的时间\t__u32                     icsk_rto_min;\t\t\t//rto的最小值 防止因为rtt过小，导致不必要的重传\t__u32                     icsk_delack_max;\t\t//发送延迟ack的最大时间\t__u32\t\t\t  icsk_pmtu_cookie;\t\t\t\t//缓存路径mtu 计算mss时候要用到，connect的时候会初始化，icsk_sync_mss会设置这个值\tconst struct tcp_congestion_ops *icsk_ca_ops;\t//拥塞控制算法ops集合\t////tcp层和ip层直接的接口的ops集合\tconst struct inet_connection_sock_af_ops *icsk_af_ops;  \tconst struct tcp_ulp_ops  *icsk_ulp_ops;\t\t//扩展的接口？？？\tvoid __rcu\t\t  *icsk_ulp_data;\tvoid (*icsk_clean_acked)(struct sock *sk, u32 acked_seq);\t\t//TLS相关\tunsigned int\t\t  (*icsk_sync_mss)(struct sock *sk, u32 pmtu); //icsk_sync_mss\t__u8\t\t\t  icsk_ca_state:5,\t\t\t  //拥塞状态机的几种状态\t\t\t\t  icsk_ca_initialized:1,\t\t//是否初始化了拥塞算法\t\t\t\t  icsk_ca_setsockopt:1,\t\t\t//用户是否通过setsockopt设置过拥塞算法\t\t\t\t  icsk_ca_dst_locked:1;\t\t\t//bfp相关，是否因为路由改变加锁？？\t__u8\t\t\t  icsk_retransmits;\t\t\t//重传次数\t__u8\t\t\t  icsk_pending;\t\t\t\t//标志位，是否有某种定时器事件待处理\t__u8\t\t\t  icsk_backoff;\t\t\t\t\t//重传次数，计算rto的时候会用到\t__u8\t\t\t  icsk_syn_retries;\t\t\t\t//重传syn包的最大次数\t__u8\t\t\t  icsk_probes_out;\t\t\t//零窗口探测的次数\t__u16\t\t\t  icsk_ext_hdr_len;\t\t\t//ip选项的长度\tstruct &#123;\t\t__u8\t\t  pending;\t /* ACK is pending\t\t\t   *///是否有带发送的ack\t\t__u8\t\t  quick;\t /* Scheduled number of quick acks\t   *///快速ack\t\t__u8\t\t  pingpong;\t /* The session is interactive\t\t   *///是否是pingpang模式，也就是交互模式\t\t__u8\t\t  retry;\t /* Number of attempts\t\t\t   */\t//延迟ack重传次数？\t\t__u32\t\t  ato;\t\t /* Predicted tick of soft clock\t   */\t//延迟发送ack的时间\t\tunsigned long\t  timeout;\t /* Currently scheduled timeout\t\t   *///超时时间，到期后发送ack\t\t__u32\t\t  lrcvtime;\t /* timestamp of last received data packet */\t//最后一个数据包收上来的时间\t\t__u16\t\t  last_seg_size; /* Size of last incoming segment\t   *///最后一个段的大小\t\t__u16\t\t  rcv_mss;\t /* MSS used for delayed ACK decisions\t   *///接收报文的mss\t&#125; icsk_ack;     //管理延迟ack的结构，\tstruct &#123;\t\t/* Range of MTUs to search */\t\tint\t\t  search_high;\t\t\t\t\t\t//MTU探测的范围,发送的时候会使用二分查找确定一个值\t\tint\t\t  search_low;\t\t/* Information on the current probe. */\t\tu32\t\t  probe_size:31,\t\t\t\t\t\t//上面high和low计算出来的一个值探测的size\t\t/* Is the MTUP feature enabled for this connection? */\t\t\t\t  enabled:1;\t\t\t\t\t\t\t\t\t//是否使能路径mtu探测\t\tu32\t\t  probe_timestamp;\t\t\t\t\t\t//探测的时间戳\t&#125; icsk_mtup;\tu32\t\t\t  icsk_probes_tstamp;\tu32\t\t\t  icsk_user_timeout;\tu64\t\t\t  icsk_ca_priv[104 / sizeof(u64)];#define ICSK_CA_PRIV_SIZE\t  sizeof_field(struct inet_connection_sock, icsk_ca_priv)&#125;;\n\n","categories":["网络协议栈源码学习"],"tags":["sock"]},{"title":"IPSec","url":"/2025/05/18/ipsec/","content":"IPSec1.IPSec 简介起源随着Internet的发展，越来越多的企业直接通过Internet进行互联，但由于IP协议未考虑安全性，而且Internet上有大量的不可靠用户和网络设备，所以用户业务数据要穿越这些未知网络，根本无法保证数据的安全性，数据易被伪造、篡改或窃取。因此，迫切需要一种兼容IP协议的通用的网络安全方案。为了解决上述问题，IPSec（Internet Protocol Security）应运而生。IPSec是对IP的安全性补充，其工作在IP层，为IP网络通信提供透明的安全服务。\n定义IPSec是IETF（Internet Engineering Task Force）制定的一组开放的网络安全协议。它并不是一个单独的协议，而是一系列为IP网络提供安全性的协议和服务的集合，包括认证头AH（Authentication Header）和封装安全载荷ESP（Encapsulating SecurityPayload）两个安全协议、密钥交换和用于验证及加密的一些算法等。通过这些协议，在两个设备之间建立一条IPSec隧道。数据通过IPSec隧道进行转发，实现保护数据的安全性。\n受益IPSec通过加密与验证等方式，从以下几个方面保障了用户业务数据在Internet中的安全传输：\n\n数据来源验证：接收方验证发送方身份是否合法。\n数据加密：发送方对数据进行加密，以密文的形式在Internet上传送，接收方对接收的加密数据进行解密后处理或直接转发。\n数据完整性：接收方对接收的数据进行验证，以判定报文是否被篡改。\n抗重放：接收方拒绝旧的或重复的数据包，防止恶意用户通过重复发送捕获到的数据包所进行的攻击。\n\n2.IPSec原理描述2.1IPSec 协议框架2.1.1安全联盟安全联盟SA（Security Association）是通信对等体间对某些要素的协定，它描述了对等体间如何利用安全服务（例如加密）进行安全的通信。这些要素包括对等体间使用何种安全协议、要保护的数据流特征、对等体间传输的数据的封装模式、协议采用的加密和验证算法，以及用于数据安全转换、传输的密钥和SA的生存周期等。IPSec安全传输数据的前提是在IPSec对等体（即运行IPSec协议的两个端点）之间成功建立安全联盟。IPSec安全联盟简称IPSec SA，由一个三元组来唯一标识，这个三元组包括安全参数索引SPI（Security Parameter Index）、目的IP地址和使用的安全协议号（AH或ESP）。其中，SPI是为唯一标识SA而生成的一个32位比特的数值，它被封装在AH和ESP头中。IPSec SA是单向的逻辑连接，通常成对建立（Inbound和Outbound）。因此两个IPSec对等体之间的双向通信，最少需要建立一对IPSec SA形成一个安全互通的IPSec隧道，分别对两个方向的数据流进行安全保护。另外，IPSec SA的个数还与安全协议相关。如果只使用AH或ESP来保护两个对等体之间的流量，则对等体之间就有两个SA，每个方向上一个。如果对等体同时使用了AH和ESP，那么对等体之间就需要四个SA，每个方向上两个，分别对应AH和ESP。建立IPSec SA有两种方式：手工方式和IKE方式。二者的主要差异如表所示。\n\n\n\n对比项\n手工方式建立IPSec SA\nIKE方式自动建立IPSec SA\n\n\n\n加密&#x2F;验证密钥管理\n手工配置、手动刷新，易出错\n通过DH算法动态生成并自动刷新\n\n\n密钥管理成本\n高（需人工维护所有节点密钥）\n低（自动协商和轮换）\n\n\nSPI（安全参数索引）\n手工配置\n随机生成\n\n\n生存周期\n无限制，SA永久存在（除非手动删除）\n由生存周期参数控制，SA自动过期和重建\n\n\n安全性\n低（静态密钥易被破解，无前向保密）\n高（动态密钥、支持PFS、抗重放攻击）\n\n\n适用场景\n小型网络、临时测试环境\n中小型至大型网络、生产环境\n\n\n2.1.2安全协议IPSec使用认证头AH（Authentication Header）和封装安全载荷ESP EncapsulatingSecurity Payload）两种IP传输层协议来提供认证或加密等安全服务。\n\nAH协议：AH仅支持认证功能，不支持加密功能。AH在每一个数据包的标准IP报头后面添加一个AH报文头。AH对数据包和认证密钥进行Hash计算，接收方收到带有计算结果的数据包后，执行同样的Hash计算并与原计算结果比较，传输过程中对数据的任何更改将使计算结果无效，这样就提供了数据来源认证和数据完整性校验。AH协议的完整性验证范围为整个IP报文。\n\nESP协议：ESP支持认证和加密功能。ESP在每一个数据包的标准IP报头后面添加一个ESP报文头，并在数据包后面追加一个ESP尾（ESP Trailer和ESP Auth data）。与AH不同的是，ESP将数据中的有效载荷进行加密后再封装到数据包中，以保证数据的机密性，但ESP没有对IP头的内容进行保护，除非IP头被封装在ESP内部（采用隧道模式）。\n\n\nAH协议与ESP协议的比较如下所示：\n\n\n\n安全特性\nAH (认证头)\nESP (封装安全载荷)\n\n\n\n协议号\n51\n50\n\n\n数据完整性校验\n支持（验证整个IP报文）\n支持（传输模式：不验证IP头；隧道模式：验证整个IP报文）\n\n\n数据源验证\n支持\n支持\n\n\n数据加密\n不支持\n支持\n\n\n防报文重放攻击\n支持\n支持\n\n\nNAT-T (NAT穿越)\n不支持\n支持\n\n\nAH报文头结构\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| 下一个头部 (8 bits) | 载荷长度 (8 bits) |  保留 (16 bits)         |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                   安全参数索引 (SPI, 32 bits)                   |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                   序列号 (Sequence Number, 32 bits)            |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                                                               ||                认证数据 (可变长度，32 bits的整数倍)               ||                                                               |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n AH 报文头字段含义\n\n\n\n字段名\n长度\n含义\n\n\n\n下一头部\n8 bits\n标识 AH 报文头后面的负载类型： - 传输模式：被保护的上层协议（TCP&#x2F;UDP）或 ESP 协议编号 - 隧道模式：IP 协议或 ESP 协议编号（当 AH 与 ESP 同时使用时，下一头部为 ESP 报文头）\n\n\n负载长度\n8 bits\n以 32 比特为单位的 AH 报文头长度减 2（缺省值：4）\n\n\n保留字段\n16 bits\n保留将来使用，缺省为 0\n\n\nSPI\n32 bits\nIPSec 安全参数索引，唯一标识安全联盟（SA）\n\n\n序列号\n32 bits\n从 1 开始的单向递增计数器，防止重放攻击\n\n\n认证数据\n变长字段（32 比特整数倍，通常 96 bits）\n包含完整性校验值（ICV），用于接收方校验数据完整性。认证算法：- ✅ 推荐：SHA2、SM3- ⚠️ 不安全：MD5、SHA1（存在安全隐患）\n\n\nESP 报文结构\n+-----------------------------------------------+ &lt;-- ESP头部| 安全参数索引（SPI）                              |+-----------------------------------------------+| 序列号                                         |+-----------------------------------------------+ &lt;-- 加密部分开始|                                               || 负载数据（Payload，变长）                        ||                                               |+-----------------------------------------------+| 填充字段（0～255字节 Padding）                   |+-----------------------------------------------+| 填充长度（1B） | 下一头部（1B）                   |+-----------------------------------------------+ &lt;-- ESP尾部（加密部分结束）|                                               || 认证数据（ICV，完整性校验值，变长                  ||                                               |+-----------------------------------------------+ &lt;-- ESP认证部分\n\n ESP 报文头字段含义\n\n\n\n字段名\n长度\n含义\n\n\n\nSPI\n32 bits\nIPSec 安全参数索引，唯一标识安全联盟（SA）\n\n\n序列号\n32 bits\n从 1 开始的单向递增计数器，防止重放攻击\n\n\n负载数据\n变长\n原始 IP 报文中的可变长度数据内容（保护内容类型由下一头部字段标识）\n\n\n填充字段\n0-255 字节\n用于补齐加密算法要求的块长度\n\n\n填充长度\n8 bits\n表示填充字段的字节数（0 表示无填充）\n\n\n下一头部\n8 bits\n标识下一个负载类型：- 传输模式：上层协议编号（如 TCP&#x3D;6&#x2F;UDP&#x3D;17）- 隧道模式：IP 协议（IPv4&#x3D;4&#x2F;IPv6&#x3D;41）\n\n\n认证数据\n变长\n完整性校验值（ICV），需 32 位对齐\n\n\n2.1.3封装模式封装模式是指将AH或ESP相关的字段插入到原始IP报文中，以实现对报文的认证和加密，封装模式有传输模式和隧道模式两种。\n传输模式\n在传输模式中，AH头或ESP头被插入到IP头与传输层协议头之间，保护TCP&#x2F;UDP&#x2F;ICMP负载。由于传输模式未添加额外的IP头，所以原始报文中的IP地址在加密后报文的IP头中可见。以TCP报文为例，原始报文经过传输模式封装后，报文格式如下所示。\n\n隧道模式\n在隧道模式下，AH头或ESP头被插到原始IP头之前，另外生成一个新的报文头放到AH头或ESP头之前，保护IP头和负载。以TCP报文为例，原始报文经隧道模式封装后的报文结构如下图所示。\n\n隧道模式下，与AH协议相比，ESP协议的完整性验证范围不包括新IP头，无法保证新IP头的安全。\n传输模式和隧道模式比较\n传输模式和隧道模式的区别在于：\n\n从安全性来讲，隧道模式优于传输模式。它可以完全地对原始IP数据包进行验证和加密。隧道模式下可以隐藏内部IP地址，协议类型和端口。\n从性能来讲，隧道模式因为有一个额外的IP头，所以它将比传输模式占用更多带宽。\n从场景来讲，传输模式主要应用于两台主机或一台主机和一台VPN网关之间通信；隧道模式主要应用于两台VPN网关之间或一台主机与一台VPN网关之间的通信。当安全协议同时采用AH和ESP时，AH和ESP协议必须采用相同的封装模式。\n\n2.1.4 加密和验证IPSec提供了两种安全机制：加密和验证。加密机制保证数据的机密性，防止数据在传输过程中被窃听；验证机制能保证数据真实可靠，防止数据在传输过程中被仿冒和篡改。\n加密\nIPSec采用对称加密算法对数据进行加密和解密。如下图所示，数据发送方和接收方使用相同的密钥进行加密、解密。\n用于加密和解密的对称密钥可以手工配置，也可以通过IKE协议自动协商生成。常用的对称加密算法包括：数据加密标准DES（Data Encryption Standard）、3DES（Triple Data Encryption Standard）、先进加密标准AES（Advanced EncryptionStandard）国密算法（SM1和SM4）。其中，DES和3DES算法安全性低，存在安全风险，不推荐使用。\n验证\nIPSec的加密功能，无法验证解密后的信息是否是原始发送的信息或完整。IPSec采用HMAC（Keyed-Hash Message Authentication Code）功能，比较完整性校验值ICV进行数据包完整性和真实性验证。通常情况下，加密和验证通常配合使用。如图所示，在IPSec发送方，加密后的报文通过验证算法和对称密钥生成完整性校验值ICV，IP报文和完整性校验值ICV同时发给对端；在IPSec接收方，使用相同的验证算法和对称密钥对加密报文进行处理，同样得到完整性校验值ICV，然后比较完整性校验值ICV进行数据完整性和真实性验证，验证不通过的报文直接丢弃，验证通过的报文再进行解密。\n\n同加密一样，用于验证的对称密钥也可以手工配置，或者通过IKE协议自动协商生成。常用的验证算法包括：消息摘要MD5（Message Digest 5）、安全散列算法SHA1（Secure Hash Algorithm 1）、SHA2、国密算法SM3（Senior Middle 3）。其中，MD5、SHA1算法安全性低，存在安全风险，不推荐使用。\n2.1.5  密钥交换使用对称密钥进行加密、验证时，如何安全地共享密钥是一个很重要的问题。有两种方法解决这个问题：\n\n带外共享密钥在发送、接收设备上手工配置静态的加密、验证密钥。双方通过带外共享的方式（例如通过电话或邮件方式）保证密钥一致性。这种方式的缺点是安全性低，可扩展性差，在点到多点组网中配置密钥的工作量成倍增加。另外，为提升网络安全性需要周期性修改密钥，这种方式下也很难实施。\n使用一个安全的密钥分发协议通过IKE协议自动协商密钥。IKE采用DH算法在不安全的网络上安全地分发密钥。这种方式配置简单，可扩展性好，特别是在大型动态的网络环境下此优点更加突出。同时，通信双方通过交换密钥交换材料来计算共享的密钥，即使第三方截获了双方用于计算密钥的所有交换数据，也无法计算出真正的密钥，这样极大地提高了安全性。\n\nIKE 协议因特网密钥交换IKE（Internet Key Exchange）协议建立在Internet安全联盟和密钥管理协议ISAKMP定义的框架上，是基于UDP（User Datagram Protocol）的应用层协议。它为IPSec提供了自动协商密钥、建立IPSec安全联盟的服务，能够简化IPSec的配置和维护工作。IKE与IPSec的关系如图所示，对等体之间建立一个IKE SA完成身份验证和密钥信息交换后，在IKE SA的保护下，根据配置的AH&#x2F;ESP安全协议等参数协商出一对IPSecSA。此后，对等体间的数据将在IPSec隧道中加密传输。IKE SA是一个双向的逻辑连接，两个对等体间只建立一个IKE SA。\n\nIKE安全机制\nIKE具有一套自保护机制，可以在网络上安全地认证身份、分发密钥、建立IPSec SA：\n\n身份认证身份认证确认通信双方的身份（对等体的IP地址或名称），包括预共享密钥PSK（pre-shared key）认证、数字证书RSA（rsa-signature）认证和数字信封认证。在预共享密钥认证中，通信双方采用共享的密钥对报文进行Hash计算，判断双方的计算结果是否相同。如果相同，则认证通过；否则认证失败。当有1个对等体对应多个对等体时，需要为每个对等体配置预共享的密钥。该方法在小型网络中容易建立，但安全性较低。在数字证书认证中，通信双方使用CA证书进行数字证书合法性验证，双方各有自己的公钥（网络上传输）和私钥（自己持有）。发送方对原始报文进行Hash计算，并用自己的私钥对报文计算结果进行加密，生成数字签名。接收方使用发送方的公钥对数字签名进行解密，并对报文进行Hash计算，判断计算结果与解密后的结果是否相同。如果相同，则认证通过；否则认证失败。使用数字证书安全性高，但需要CA来颁发数字证书，适合在大型网络中使用。在数字信封认证中，发送方首先随机产生一个对称密钥，使用接收方的公钥对此对称密钥进行加密（被公钥加密的对称密钥称为数字信封），发送方用对称密钥加密报文，同时用自己的私钥生成数字签名。接收方用自己的私钥解密数字信封得到对称密钥，再用对称密钥解密报文，同时根据发送方的公钥对数字签名进行解密，验证发送方的数字签名是否正确。如果正确，则认证通过；否则认证失败。数字信封认证用于设备需要符合国家密码管理局要求时使用，此认证方法只能在IKEv1的主模式协商过程中支持。IKE支持的认证算法有：MD5、SHA1、SHA2-256、SHA2-384、SHA2-512、SM3。\n\n身份保护身份数据在密钥产生之后加密传送，实现了对身份数据的保护。IKE支持的加密算法有：DES、3DES、AES-128、AES-192、AES-256、SM1和SM4。\n\nDHDH是一种公共密钥交换方法，它用于产生密钥材料，并通过ISAKMP消息在发送和接收设备之间进行密钥材料交换。然后，两端设备各自计算出完全相同的对称密钥。该对称密钥用于计算加密和验证的密钥。在任何时候，通信双方都不交换真正的密钥。DH密钥交换是IKE的精髓所在。\n\nPFS完善的前向安全性PFS（Perfect Forward Secrecy）通过执行一次额外的DH交换，确保即使IKE SA中使用的密钥被泄露，IPSec SA中使用的密钥也不会受到损害。\n\n\n2.2IPSec 基本原理IPSec通过在IPSec对等体间建立双向安全联盟形成一个安全互通的IPSec隧道，并通过定义IPSec保护的数据流将要保护的数据引入该IPSec隧道，然后对流经IPSec隧道的数据通过安全协议进行加密和验证，进而实现在Internet上安全传输指定的数据。IPSec安全联盟可以手工建立，也可以通过IKEv1或IKEv2协议自动协商建立。本文重点介绍如何定义IPSec保护的数据流、IKE自动协商建立安全联盟的过程。\n2.2.1 定义 IPSec 保护的数据流IPSec是基于定义的感兴趣流触发对特定数据的保护，至于什么样的数据是需要IPSec保护的，可以通过以下两种方式定义。其中IPSec感兴趣流即需要IPSec保护的数据流。\n\nACL方式手工方式和IKE自动协商方式建立的IPSec隧道是由ACL来指定要保护的数据流范围，筛选出需要进入IPSec隧道的报文，ACL规则允许（permit）的报文将被保护，未匹配任何permit规则的报文将不被保护。这种方式可以利用ACL的丰富配置功能，根据IP地址、端口、协议类型等对报文进行过滤进而灵活制定IPSec的保护方法。\n路由方式通过IPSec虚拟隧道接口建立IPSec隧道，将所有路由到IPSec虚拟隧道接口的报文都进行IPSec保护，根据该路由的目的地址确定哪些数据流需要IPSec保护。其中IPSec虚拟隧道接口是一种三层逻辑接口。路由方式具有以下优点：\n通过路由将需要IPSec保护的数据流引到虚拟隧道接口，不需使用ACL定义待\n加&#x2F;解密的流量特征，简化了IPSec配置的复杂性。\n支持动态路由协议。\n通过GRE over IPSec支持对组播流量的保护。\n\n\n\n2.2.2 IKEv1 协商安全联盟的过程IKEv1 协商阶段1\nIKEv1协商阶段1的目的是建立IKE SA。IKE SA建立后对等体间的所有ISAKMP（一个框架 IKE是一种实现）消息都将通过加密和验证，这条安全通道可以保证IKEv1第二阶段的协商能够安全进行。IKEv1协商阶段1支持两种协商模式：主模式（Main Mode）和野蛮模式（AggressiveMode）。主模式包含三次双向交换，用到了六条ISAKMP信息，协商过程如下图所示。这三次交换分别是：\n\n消息①和②用于提议交换发起方发送一个或多个IKE安全提议，响应方查找最先匹配的IKE安全提议，并将这个IKE安全提议回应给发起方。匹配的原则为协商双方具有相同的加密算法、认证算法、认证方法和DH组标识。\n消息③和④用于密钥信息交换\n双方交换DH(一种密钥交换算法，不暴露私钥的情况下，计算出一个共享密钥)公共值和nonce(一个随机值)值，用于IKE SA的认证和加密密钥在这个阶段产生。消息⑤和⑥用于身份和认证信息交换（双方使用生成的密钥发送信息），双方进行身份认证和对整个主模式交换内容的认证。\n\n野蛮模式只用到三条信息，前两条消息①和②用于协商IKE安全提议，交换DH公共值、必需的辅助信息以及身份信息并且消息②中还包括响应方发送身份信息供发起方认证，消息③用于响应方认证发起方。IKEv1协商阶段1的协商过程如下图所示。\n\n与主模式相比，野蛮模式减少了交换信息的数目，提高了协商的速度，但是没有对身份信息进行加密保护。\nIKEv1 协商阶段 2\nIKEv1协商阶段2的目的就是建立用来安全传输数据的IPSec SA，并为数据传输衍生出密钥。这一阶段采用快速模式（Quick Mode）。该模式使用IKEv1协商阶段1中生成的密钥对ISAKMP消息的完整性和身份进行验证，并对ISAKMP消息进行加密，故保证了交换的安全性。IKEv1协商阶段2的协商过程如下图所示。\n\nIKEv1协商阶段2通过三条ISAKMP消息完成双方IPSec SA的建立：\n\n协商发起方发送本端的安全参数和身份认证信息。安全参数包括被保护的数据流和IPSec安全提议等需要协商的参数。身份认证信息包括第一阶段计算出的密钥和第二阶段产生的密钥材料等，可以再次认证对等体。\n协商响应方发送确认的安全参数和身份认证信息并生成新的密钥。IPSec SA数据传输需要的加密、验证密钥由第一阶段产生的密钥、SPI、协议等参数衍生得出，以保证每个IPSec SA都有自己独一无二的密钥。如果启用PFS，则需要再次应用DH算法计算出一个共享密钥，然后参与上述计算，因此在参数协商时要为PFS协商DH密钥组。\n发送方发送确认信息，确认与响应方可以通信，协商结束。\n\n2.2.3 IKEv2 协商安全联盟的过程采用IKEv2协商安全联盟比IKEv1协商过程要简化的多。要建立一对IPSec SA，IKEv1需要经历两个阶段：“主模式＋快速模式”或者“野蛮模式＋快速模式”，前者至少需要交换9条消息，后者也至少需要6条消息。而IKEv2正常情况使用2次交换共4条消息就可以完成一对IPSec SA的建立，如果要求建立的IPSec SA大于一对时，每一对IPSec SA只需额外增加1次创建子SA交换，也就是2条消息就可以完成。IKEv2定义了三种交换：初始交换（Initial Exchanges）、创建子SA交换（Create_Child_SA Exchange）以及通知交换（Informational Exchange）。\n初始交换\n正常情况下，IKEv2通过初始交换就可以完成第一对IPSec SA的协商建立。IKEv2初始交换对应IKEv1的第一阶段，初始交换包含两次交换四条消息，如下图所示。\n消息①和②属于第一次交换（称为IKE_SA_INIT交换），以明文方式完成IKE SA的参数协商，包括协商加密和验证算法，交换临时随机数和DH交换。IKE_SA_INIT交换后生成一个共享密钥材料，通过这个共享密钥材料可以衍生出IPSec SA的所有密钥。消息③和④属于第二次交换（称为IKE_AUTH交换），以加密方式完成身份认证、对前两条信息的认证和IPSec SA的参数协商。IKEv2支持RSA签名认证、预共享密钥认证以及扩展认证方法EAP（Extensible Authentication Protocol）。发起者通过在消息3中省去认证载荷来表明需要使用EAP认证。\n创建子 SA 交换\n当一个IKE SA需要创建多对IPSec SA时，需要使用创建子SA交换来协商多于一对的IPSec SA。另外，创建子SA交换还可以用于IKE SA的重协商。创建子SA交换包含一个交换两条消息，对应IKEv1协商阶段2，交换的发起者可以是初始交换的协商发起方，也可以是初始交换的协商响应方。创建子SA交换必须在初始交换完成后进行，交换消息由初始交换协商的密钥进行保护。类似于IKEv1，如果启用PFS，创建子SA交换需要额外进行一次DH交换，生成新的密钥材料。生成密钥材料后，子SA的所有密钥都从这个密钥材料衍生出来。\n通知交换\n运行IKE协商的两端有时会传递一些控制信息，例如错误信息或者通告信息，这些信息在IKEv2中是通过通知交换完成的，如下图所示。通知交换必须在IKE SA保护下进行，也就是说通知交换只能发生在初始交换之后。控制信息可能是IKE SA的，那么通知交换必须由该IKE SA来保护进行；也可能是某子SA的，那么该通知交换必须由生成该子SA的IKE SA来保护进行。\n\n","categories":["网络协议学习"],"tags":["IPSec"]},{"title":"moudle_init()原理学习","url":"/2025/05/28/module_init%E5%8E%9F%E7%90%86%E5%AD%A6%E4%B9%A0/","content":"moudle_init()原理学习1.静态加载与动态加载内核模块代码的例子#include &lt;linux/module.h&gt;#include &lt;linux/init.h&gt; static int hello_init(void)&#123;    printk(KERN_ALERT &quot;Hello World\\n&quot;);    return 0;&#125; static void hello_exit(void)&#123;    printk(KERN_ALERT &quot;Bye Bye World\\n&quot;);&#125; module_init(hello_init);module_exit(hello_exit);\n上述编写的内核模块有两种运行方式，一是静态编译链接进内核，在系统启动过程中进行初始化；一是编译成可动态加载的module，通过insmod动态加载重定位到内核。这两种方式可以在Makefile中通过obj-y或obj-m选项进行选择。\n动态加载：\n\n可根据系统需要运行动态加载模块，以扩充内核功能，不需要时将其卸载，以释放内存空间；\n当需要修改内核功能时，只需编译相应模块，而不必重新编译整个内核。\n\n静态加载但是有些模块必须要编译到内核，随内核一起运行，从不卸载，如 vfs等\n2.动态加载与静态加载实现原理同样的内核代码通过对moudle_init这个宏的不同展开来实现动态加载与卸载下面看一下moudle_init这个宏的实现：\n#ifndef MODULE#define module_init(x)\t__initcall(x);#define module_exit(x)\t__exitcall(x);#else /* MODULE */.../* Each module must use one module_init(). */#define module_init(initfn)\t\t\t\t\t\\\tstatic inline initcall_t __maybe_unused __inittest(void)\t\t\\\t&#123; return initfn; &#125;\t\t\t\t\t\\\tint init_module(void) __copy(initfn)\t\t\t\\\t\t__attribute__((alias(#initfn)));\t\t\\\t___ADDRESSABLE(init_module, __initdata);/* This is only required if you want to be unloadable. */#define module_exit(exitfn)\t\t\t\t\t\\\tstatic inline exitcall_t __maybe_unused __exittest(void)\t\t\\\t&#123; return exitfn; &#125;\t\t\t\t\t\\\tvoid cleanup_module(void) __copy(exitfn)\t\t\\\t\t__attribute__((alias(#exitfn)));\t\t\\\t___ADDRESSABLE(cleanup_module, __exitdata);#endif\n\n从上面代码可以发现moudle_init宏的不同展开方式取决于MODULE是否被定义，也就是通过Makefile控制的，上面部分用于将模块静态编译连接进内核，下面部分用于编译可动态加载的模块。\n2.1静态加载静态加载moudle_init宏经过一系列宏展开后如下所示：\n// 静态断言（确保函数类型匹配）static_assert(__same_type(initcall_t, &amp;hello_init));// 汇编部分：将函数地址存入特定节asm volatile (    &quot;.section    \\&quot;.initcall6.init\\&quot;, \\&quot;a\\&quot;        \\n&quot;  // 定义节    &quot;__initcall_hello__0_10_hello_init6:          \\n&quot;  // 标签    &quot;.long       hello_init - .                   \\n&quot;  //函数地址与标签地址的偏移量    &quot;.previous                                   \\n&quot;    // 恢复默认节);\n\n上述这段汇编代码的作用就是在 .initcall6.init 节中定义一个条目（entry），条目内容就是 hello_init 函数的相对偏移量。具体逻辑如下：\n切换到名为 .initcall6.init 的 ELF 段（Section）。.initcall6.init 是内核用于存储初始化函数指针的特殊段，数字 6 表示优先级（数字越小优先级越高）。内核启动时，会按优先级顺序遍历这些段。__initcall_hello__0_10_hello_init6 ：定义一个标签，用于标识当前初始化函数的位置。.long       hello_init - .存储 hello_init 函数地址相对于当前标签的偏移量。\n上述存放于 .initcall6.init 段中的__initcall_hello__0_10_hello_init6 是在start_kernel(系统调用的第一个c语言程序)中调用。 具体调用流程如下所示：\nstart_kernel|--&gt; rest_init    |    --&gt; kernel_thread        |        --&gt; kernel_init            |            --&gt; kernel_init_freeable                |                --&gt; do_basic_setup                    |                    --&gt; do_initcalls                        |                        --&gt; do_initcall_level(level)                            |                            --&gt; do_one_initcall(initcall_t fn)\n\n其中do_initcalls可以理解为处理module_init的一个入口函数具体定义如下：\nstatic void __init do_initcalls(void)&#123;\tint level;//初始化级别，0-7,优先级由高到低\tsize_t len = saved_command_line_len + 1;\tchar *command_line;\tcommand_line = kzalloc(len, GFP_KERNEL);\tif (!command_line)\t\tpanic(&quot;%s: Failed to allocate %zu bytes\\n&quot;, __func__, len);\t//遍历不同的优先级\tfor (level = 0; level &lt; ARRAY_SIZE(initcall_levels) - 1; level++) &#123;\t\t/* Parser modifies command_line, restore it each time */\t\tstrcpy(command_line, saved_command_line);\t\t//执行单个级别的初始化函数\t\tdo_initcall_level(level, command_line);\t&#125;\tkfree(command_line);&#125;\ndo_initcalls 中调用do_initcall_level来​​执行某个特定级别的所有初始化函数，具体函数如下所示：\nstatic void __init do_initcall_level(int level, char *command_line)&#123;\tinitcall_entry_t *fn;\tparse_args(initcall_level_names[level],\t\t   command_line, __start___param,\t\t   __stop___param - __start___param,\t\t   level, level,\t\t   NULL, ignore_unknown_bootoption);\ttrace_initcall_level(initcall_level_names[level]);\tfor (fn = initcall_levels[level]; fn &lt; initcall_levels[level+1]; fn++)\t\tdo_one_initcall(initcall_from_entry(fn));&#125;\ndo_one_initcall中根据一个具体的level依次调用do_one_initcall执行某一个level中的所有初始化函数\n上面的initcall_levels[]是一个指针数组定义如下所示：\nstatic initcall_t *initcall_levels[] __initdata = &#123;    __initcall0_start,    __initcall1_start,    __initcall2_start,    __initcall3_start,    __initcall4_start,    __initcall5_start,    __initcall6_start,    __initcall7_start,    __initcall_end,&#125;\n上述指针数组中每个元素定义如下所示：\nextern initcall_entry_t __initcall_start[];extern initcall_entry_t __initcall0_start[];extern initcall_entry_t __initcall1_start[];extern initcall_entry_t __initcall2_start[];extern initcall_entry_t __initcall3_start[];extern initcall_entry_t __initcall4_start[];extern initcall_entry_t __initcall5_start[];extern initcall_entry_t __initcall6_start[];extern initcall_entry_t __initcall7_start[];extern initcall_entry_t __initcall_end[];\n这里 __initcallX_start 符号不是传统意义上的变量。它们的值是在内核链接时由链接器自动计算的 可以理解也就是module_init注册的函数的偏移量就存在上述的__initcall6_start[]数组中\n这里举个例子：1.驱动通过 module_init 注册\nstatic int __init usb_init(void) &#123; /* ... */ &#125;module_init(usb_init);  // 默认对应级别6\n2.译器会将其转换为：\n.section &quot;.initcall6.init&quot;, &quot;a&quot;  ; 放入级别6的段__initcall_usb_init:    .long usb_init - .  ; 存储偏移量（非直接地址）\n\n3.链接脚本（vmlinux.lds）将所有 .initcall6.init 段合并？？？\n.initcall6.init : &#123;    __initcall6_start = .;    *(.initcall6.init)  ; 包含所有级别6的驱动初始化条目    __initcall7_start = .;&#125;\n4.内核通过 initcall_levels 访问\n可以看到上面的do_initcall_level中参数为initcall_from_entry 这个函数实现了将存储在内核初始化表中的​​相对偏移量​​转换为​​实际的函数地址​，​具体定义如下：\nstatic inline initcall_t initcall_from_entry(initcall_entry_t *entry)&#123;\treturn offset_to_ptr(entry);&#125;\n\n最终的do_one_initcall完成了针对某一个level中的一个entry的初始化：\nint __init_or_module do_one_initcall(initcall_t fn)&#123;\tint count = preempt_count();\tchar msgbuf[64];\tint ret;\tif (initcall_blacklisted(fn))\t\treturn -EPERM;\tdo_trace_initcall_start(fn);    //这里执行具体的函数调用！！！\tret = fn();\tdo_trace_initcall_finish(fn, ret);\tmsgbuf[0] = 0;\tif (preempt_count() != count) &#123;\t\tsprintf(msgbuf, &quot;preemption imbalance &quot;);\t\tpreempt_count_set(count);\t&#125;\tif (irqs_disabled()) &#123;\t\tstrlcat(msgbuf, &quot;disabled interrupts &quot;, sizeof(msgbuf));\t\tlocal_irq_enable();\t&#125;\tWARN(msgbuf[0], &quot;initcall %pS returned with %s\\n&quot;, fn, msgbuf);\tadd_latent_entropy();\treturn ret;&#125;\n\n\n2.2动态加载动态加载通常通过insmod加载内核模块，原理就是执行了init_module这个系统调用。如果是动态加载，moudle_init宏展开如下所示：\nstatic inline initcall_t __maybe_unused __inittest(void) &#123;     return hello_init; &#125;int init_module(void) __attribute__((alias(&quot;hello_init&quot;)));___ADDRESSABLE(init_module, __initdata);\n\n上述__inittest 函数确保 hello_init 符合 initcall_t 类型（返回 int 且无参数）\n上述的alias 属性是 gcc 的特有属性，将定义 init_module 为函数 initfn 的别名。所以 module_init(hello_init) 的作用就是定义一个变量名 init_module，其地址和 hello_init 是一样的\n上述例子编译可动态加载模块过程中，会自动产生 HelloWorld.mod.c 文件(最终会合并所有.o链接生成一个.ko)，内容如下：\n#include &lt;linux/module.h&gt;#include &lt;linux/vermagic.h&gt;#include &lt;linux/compiler.h&gt; MODULE_INFO(vermagic, VERMAGIC_STRING); struct module __this_module__attribute__((section(&quot;.gnu.linkonce.this_module&quot;))) = &#123;    .name = KBUILD_MODNAME,    .init = init_module,#ifdef CONFIG_MODULE_UNLOAD    .exit = cleanup_module,#endif    .arch = MODULE_ARCH_INIT,&#125;; static const char __module_depends[]__used__attribute__((section(&quot;.modinfo&quot;))) =&quot;depends=&quot;;\n\n由上述代码可知，定义了一个类型为 module 的全局变量 __this_module，成员 init 为 init_module（即 hello_init），且该变量链接到 .gnu.linkonce.this_module 段中\n最终，通过执行insmod这个可执行程序会通过系统调用加载内核模块，具体流程如下：\nSYSCALL_DEFINE3(init_module, ...)|--&gt;load_module    |    --&gt; do_init_module(mod)        |        --&gt; do_one_initcall(mod-&gt;init);\n\n可以看到动态加载的方式最终也是调用到了do_one_initcall只不过传入的参数实际上就是hello_init\n","categories":["其他"],"tags":["linux内核"]},{"title":"skb操作函数(一)","url":"/2025/06/15/skb%E7%9A%84%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0/","content":"skb预留和对齐数据的预留和对齐主要skb_reserve(),skb_put(),skb_push(),skb_pull这个几个函数来完成\nskb_reserve()skb_reserve() 是用于预留头部空间（headroom）的函数，它的作用是在 sk_buff 结构中提前空出一部分头部空间或者为了对齐，以便后续协议栈可以在这个位置添加协议头（如 MAC&#x2F;IP&#x2F;TCP 头）。\n注意skb_reserve()只能用于空的SKB，通常在分配SKB之后就会调用这个函数，注意此时的data和tail指针指向最初的位置。当skb在协议栈中向下传递的时候，data指针是向上移动的，然后复制需要封装的协议头部，最后更新skb的len字段，具体如下图：\n\nskb_reserve() 函数实现如下所示：\nstatic inline void skb_reserve(struct sk_buff *skb, int len)&#123;\tskb-&gt;data += len;\tskb-&gt;tail += len;&#125;\n\nskb_push()将数据指针 skb-&gt;data 向前移动（减少），为报文头腾出空间，并返回新位置的指针\n它通常用于：\n\n在已有 payload（有效负载）前添加协议头（如 IP、TCP 头）；\n构造完整报文时，按从上层到下层的顺序组装头部。\n\n函数原型如下：\nvoid *skb_push(struct sk_buff *skb, unsigned int len)&#123;\tskb-&gt;data -= len;\tskb-&gt;len  += len;\tif (unlikely(skb-&gt;data &lt; skb-&gt;head))\t\tskb_under_panic(skb, len, __builtin_return_address(0));\treturn skb-&gt;data;&#125;\n\n举个发送tcp数据包的例子，具体步骤如下：\n\n当发送TCP数据时，会根据一些条件，比如MSS申请一个skb\nTCP需要在申请的缓冲区头部预留足够的空间，用来填充各层首部，由于不知道各层首部的长度，比如是否存在ip选项等，会预留一个最大长度\n把TCP的payload复制到数据缓存区\n负载构建完成后封装TCP的首部\n交付给IP层，封装IP首部\n交付给链路层，封装链路层首部\n\n上述封装各层头之前，都需要用skb_push移动data指针来开辟一段空间，上述具体流程如下图所示：\n\nskb_put()在 sk_buff 缓冲区末尾“添加数据空间”，即将 skb-&gt;tail 向后移动，并增加 skb-&gt;len，为你写入数据腾出空间。\nskb-&gt;tail 表示当前写入数据的末尾；\nskb_put() 把 tail 向后移动 len 字节，表示“打算添加 len 字节数据”；\n返回原始 tail 地址，你可以在这个地址处填入数据（如 payload）；\n函数原型如下：\nvoid *skb_put(struct sk_buff *skb, unsigned int len)&#123;\tvoid *tmp = skb_tail_pointer(skb);\tSKB_LINEAR_ASSERT(skb);\tskb-&gt;tail += len;\tskb-&gt;len  += len;\tif (unlikely(skb-&gt;tail &gt; skb-&gt;end))\t\tskb_over_panic(skb, len, __builtin_return_address(0));\treturn tmp;&#125;\n\n调用skb_put()前后skb结构变化如下所示：\n\nskb_pull()将 skb-&gt;data 指针向后移动（跳过前面的数据），并减少 skb-&gt;len，通常用于跳过协议头或处理完某一层协议之后。\n函数原型如下：\nvoid *skb_pull(struct sk_buff *skb, unsigned int len)&#123;\treturn skb_pull_inline(skb, len);&#125;static inline void *skb_pull_inline(struct sk_buff *skb, unsigned int len)&#123;\treturn unlikely(len &gt; skb-&gt;len) ? NULL : __skb_pull(skb, len);&#125;static inline void *__skb_pull(struct sk_buff *skb, unsigned int len)&#123;\tskb-&gt;len -= len;\tBUG_ON(skb-&gt;len &lt; skb-&gt;data_len);\treturn skb-&gt;data += len;&#125;\n\n假设我们收到一个 IP 报文，skb-&gt;data 指向 IP 头，现在我们要把 IP 头跳过去，交给 TCP 层：\nstruct iphdr *iph = ip_hdr(skb);  // 当前 data 指向 IP 头skb_pull(skb, iph-&gt;ihl * 4);      // 跳过 IP 头struct tcphdr *tcph = (struct tcphdr *)skb-&gt;data;  // 现在 data 指向 TCP 头\n\n条用skb_pull前后skb结构变化如下所示：\n\n链表管理函数skb_queue_head_init()初始化 skb 队列（链表头）\nstatic inline void __skb_queue_head_init(struct sk_buff_head *list)&#123;\tlist-&gt;prev = list-&gt;next = (struct sk_buff *)list;\tlist-&gt;qlen = 0;&#125;static inline void skb_queue_head_init(struct sk_buff_head *list)&#123;\tspin_lock_init(&amp;list-&gt;lock);\t__skb_queue_head_init(list);&#125;\n\nskb_queue_head()将一个 sk_buff（网络数据包）添加到指定 sk_buff_head 队列的头部，并加锁保护\nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk)&#123;\tunsigned long flags;\tspin_lock_irqsave(&amp;list-&gt;lock, flags);\t__skb_queue_head(list, newsk);\tspin_unlock_irqrestore(&amp;list-&gt;lock, flags);&#125;static inline void __skb_queue_head(struct sk_buff_head *list,\t\t\t\t    struct sk_buff *newsk)&#123;\t__skb_queue_after(list, (struct sk_buff *)list, newsk);&#125;static inline void __skb_queue_after(struct sk_buff_head *list,\t\t\t\t     struct sk_buff *prev,\t\t\t\t     struct sk_buff *newsk)&#123;\t__skb_insert(newsk, prev, prev-&gt;next, list);&#125;\t\t\t\tstruct sk_buff *prev, struct sk_buff *next,\t\t\t\tstruct sk_buff_head *list)&#123;\t/* See skb_queue_empty_lockless() and skb_peek_tail()\t * for the opposite READ_ONCE()\t */\tWRITE_ONCE(newsk-&gt;next, next);\tWRITE_ONCE(newsk-&gt;prev, prev);\tWRITE_ONCE(next-&gt;prev, newsk);\tWRITE_ONCE(prev-&gt;next, newsk);\tlist-&gt;qlen++;&#125;\n\nskb_dequeue从一个 skb 队列（struct sk_buff_head）头部取出并移除一个 sk_buff（网络数据包）。\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list)&#123;\tunsigned long flags;\tstruct sk_buff *result;\tspin_lock_irqsave(&amp;list-&gt;lock, flags);\tresult = __skb_dequeue(list);\tspin_unlock_irqrestore(&amp;list-&gt;lock, flags);\treturn result;&#125;static inline struct sk_buff *__skb_dequeue(struct sk_buff_head *list)&#123;\tstruct sk_buff *skb = skb_peek(list);\tif (skb)\t\t__skb_unlink(skb, list);\treturn skb;&#125;static inline void __skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)&#123;\tstruct sk_buff *next, *prev;\tWRITE_ONCE(list-&gt;qlen, list-&gt;qlen - 1);\tnext\t   = skb-&gt;next;\tprev\t   = skb-&gt;prev;\tskb-&gt;next  = skb-&gt;prev = NULL;\tWRITE_ONCE(next-&gt;prev, prev);\tWRITE_ONCE(prev-&gt;next, next);&#125;\n\nskb_queue_purge释放并清空该 skb 队列中的所有 skb，其实就是出队+kfree skb\nvoid skb_queue_purge(struct sk_buff_head *list)&#123;\tstruct sk_buff *skb;\twhile ((skb = skb_dequeue(list)) != NULL)\t\tkfree_skb(skb);&#125;\n\nskb_queue_walk一个宏，遍历skb链表中每一个元素\n#define skb_queue_walk(queue, skb) \\\t\tfor (skb = (queue)-&gt;next;\t\t\t\t\t\\\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\t\t     skb = skb-&gt;next)\n\n","categories":["网络协议栈源码学习"],"tags":["skb"]},{"title":"skb操作函数(二)","url":"/2025/06/17/skb%E7%9A%84%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E4%BA%8C/","content":"删除skb尾部数据skb_trim()skb_trim() 是 Linux 内核中一个用于截断数据包长度的函数，主要用于收缩 skb 的数据长度（通常是从尾部截断）\nvoid skb_trim(struct sk_buff *skb, unsigned int len);\n\nvoid skb_trim(struct sk_buff *skb, unsigned int len)&#123;\tif (skb-&gt;len &gt; len)\t\t__skb_trim(skb, len);&#125;static inline void __skb_trim(struct sk_buff *skb, unsigned int len)&#123;\t__skb_set_length(skb, len);&#125;static inline void __skb_set_length(struct sk_buff *skb, unsigned int len)&#123;\tif (WARN_ON(skb_is_nonlinear(skb)))\t\treturn;\tskb-&gt;len = len;\tskb_set_tail_pointer(skb, len);&#125;static inline void skb_set_tail_pointer(struct sk_buff *skb, const int offset)&#123;\t//注意这里是先将tail指向data，然后在偏移offset\tskb_reset_tail_pointer(skb);\tskb-&gt;tail += offset;&#125;\n\nskb_trim操作前后如下图所示：\n\npskb_trim()pskb_trim() 是 Linux 内核中用于“安全地裁剪 skb 长度”的函数，类似于 skb_trim()，但它支持非线性（分页）skb，是一个更强大、更通用的版本。\nstatic inline int pskb_trim(struct sk_buff *skb, unsigned int len)&#123;\treturn (len &lt; skb-&gt;len) ? __pskb_trim(skb, len) : 0;&#125;static inline int __pskb_trim(struct sk_buff *skb, unsigned int len)&#123;\t//这里判断了是否存在了非线性部分的数据\tif (skb-&gt;data_len)\t\treturn ___pskb_trim(skb, len);\t__skb_trim(skb, len);\treturn 0;&#125;/* Trims skb to length len. It can change skb pointers. */int ___pskb_trim(struct sk_buff *skb, unsigned int len)&#123;\tstruct sk_buff **fragp;\tstruct sk_buff *frag;\tint offset = skb_headlen(skb); //线性的长度\tint nfrags = skb_shinfo(skb)-&gt;nr_frags; //有几个页\tint i;\tint err;\t//如果有别人在使用这个skb 调用pskb_expand_head 复制skb\tif (skb_cloned(skb) &amp;&amp;\t    unlikely((err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC))))\t\treturn err;\ti = 0;\t//线性长度大于要阶段的部分，那就直接drop page就可以了\tif (offset &gt;= len)\t\tgoto drop_pages;\tfor (; i &lt; nfrags; i++) &#123;\t\tint end = offset + skb_frag_size(&amp;skb_shinfo(skb)-&gt;frags[i]);\t\t//没到na&#x27;gepage就continue\t\tif (end &lt; len) &#123;\t\t\toffset = end;\t\t\tcontinue;\t\t&#125;\t\tskb_frag_size_set(&amp;skb_shinfo(skb)-&gt;frags[i++], len - offset);drop_pages:\t\tskb_shinfo(skb)-&gt;nr_frags = i;\t\tfor (; i &lt; nfrags; i++)\t\t\tskb_frag_unref(skb, i);\t\t//如果有fraglist非线性部分，释放fraglist\t\tif (skb_has_frag_list(skb))\t\t\tskb_drop_fraglist(skb);\t\tgoto done;\t&#125;\t//处理没有page只有fraglist的情况\tfor (fragp = &amp;skb_shinfo(skb)-&gt;frag_list; (frag = *fragp);\t     fragp = &amp;frag-&gt;next) &#123;\t\tint end = offset + frag-&gt;len;\t\tif (skb_shared(frag)) &#123;\t\t\tstruct sk_buff *nfrag;\t\t\tnfrag = skb_clone(frag, GFP_ATOMIC);\t\t\tif (unlikely(!nfrag))\t\t\t\treturn -ENOMEM;\t\t\tnfrag-&gt;next = frag-&gt;next;\t\t\tconsume_skb(frag);\t\t\tfrag = nfrag;\t\t\t*fragp = frag;\t\t&#125;\t\tif (end &lt; len) &#123;\t\t\toffset = end;\t\t\tcontinue;\t\t&#125;\t\tif (end &gt; len &amp;&amp;\t\t    unlikely((err = pskb_trim(frag, len - offset))))\t\t\treturn err;\t\tif (frag-&gt;next)\t\t\tskb_drop_list(&amp;frag-&gt;next);\t\tbreak;\t&#125;done://设置skb的len相关的字段\tif (len &gt; skb_headlen(skb)) &#123;\t\tskb-&gt;data_len -= skb-&gt;len - len;\t\tskb-&gt;len       = len;\t&#125; else &#123;\t\tskb-&gt;len       = len;\t\tskb-&gt;data_len  = 0;\t\tskb_set_tail_pointer(skb, len);\t&#125;\tif (!skb-&gt;sk || skb-&gt;destructor == sock_edemux)\t\tskb_condense(skb);\treturn 0;&#125;\n\npskb_trim操作前后如下所示：\n\n\n分割skbskb_split把一个大的 skb 分裂成两个 skb：保留前半段在原 skb 中，后半段移到新的 skb 中返回。这里具体可以分为两种情况，第一种，被拆分的数据包的线性部分就够用了。第二种，被拆分的数据包线性部分不够用，那就需要额外处理非线性部分\n注意:tcp分段中会用到这个函数，问题，为什么没考虑fraglist呢？跟如何调用这个函数有关？？\n**skb_split**() 函数原型如下：\nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len)&#123;\tint pos = skb_headlen(skb);\t//拷贝原本数据包的tx_flag\tskb_shinfo(skb1)-&gt;tx_flags |= skb_shinfo(skb)-&gt;tx_flags &amp;\t\t\t\t      SKBTX_SHARED_FRAG;\tskb_zerocopy_clone(skb1, skb, 0);//tcp zc 或者xdp 会有相应的处理\tif (len &lt; pos)\t/* Split line is inside header. *///\t//第一种情况， len的长度小于非线性部分的长度\t\tskb_split_inside_header(skb, skb1, len, pos);\telse\t\t/* Second chunk has no header, nothing to copy. */\t//第二种情况，需要处理非线性部分\t\tskb_split_no_header(skb, skb1, len, pos);&#125;\n\n上面的skb和skb1说明：\n\n原始大包在 skb 里；\n“拆出后半部分”放到新的 skb —— 就是 skb1；\n\nskb_split_inside_header 逻辑就是，把原本skb中除了要拆分出去的数据，都交友skb1去管理\nstatic inline void skb_split_inside_header(struct sk_buff *skb,\t\t\t\t\t   struct sk_buff* skb1,\t\t\t\t\t   const u32 len, const int pos)&#123;\tint i;\t//从原始 skb 的线性区中，从偏移len开始的位置，拷贝 pos - len 字节的数据到 新 skb1 的线性区尾部。\tskb_copy_from_linear_data_offset(skb, len, skb_put(skb1, pos - len),\t\t\t\t\t pos - len);\t/* And move data appendix as is. */\tfor (i = 0; i &lt; skb_shinfo(skb)-&gt;nr_frags; i++)\t\tskb_shinfo(skb1)-&gt;frags[i] = skb_shinfo(skb)-&gt;frags[i];\t//这里其实就是把原本skb的非线性部分，被新的skb1给引用了\tskb_shinfo(skb1)-&gt;nr_frags = skb_shinfo(skb)-&gt;nr_frags;\tskb_shinfo(skb)-&gt;nr_frags  = 0;\tskb1-&gt;data_len\t\t   = skb-&gt;data_len;\tskb1-&gt;len\t\t   += skb1-&gt;data_len;\tskb-&gt;data_len\t\t   = 0;\tskb-&gt;len\t\t   = len;\tskb_set_tail_pointer(skb, len);&#125;\n\n第二种情况skb_split_no_header() 用于将一个 skb 数据包在线性数据区 不需要拷贝 header 的情况下 进行拆分（split）的。此时拆分点 len 已经在非线性区域，所以只处理 frags。\nstatic inline void skb_split_no_header(struct sk_buff *skb,\t\t\t\t       struct sk_buff* skb1,\t\t\t\t       const u32 len, int pos)&#123;\tint i, k = 0;\tconst int nfrags = skb_shinfo(skb)-&gt;nr_frags;\tskb_shinfo(skb)-&gt;nr_frags = 0;\t//新的skb1的长度只有非线性部分了\tskb1-&gt;len\t\t  = skb1-&gt;data_len = skb-&gt;len - len;\t//原始的skb只保存len长度的数据\tskb-&gt;len\t\t  = len;\t//这里原始skb的非线性部分就是要保留的总长度减去一个头部 长度\tskb-&gt;data_len\t\t  = len - pos;\tfor (i = 0; i &lt; nfrags; i++) &#123;\t\t//这里的size是每一个page的长度\t\tint size = skb_frag_size(&amp;skb_shinfo(skb)-&gt;frags[i]);\t\t//非线性部分+size还没到截断的长度话\t\tif (pos + size &gt; len) &#123;\t\t\t//这里skb引用了skb\t\t\tskb_shinfo(skb1)-&gt;frags[k] = skb_shinfo(skb)-&gt;frags[i];\t\t\tif (pos &lt; len) &#123;\t\t\t\t/* Split frag.\t\t\t\t * We have two variants in this case:\t\t\t\t * 1. Move all the frag to the second\t\t\t\t *    part, if it is possible. F.e.\t\t\t\t *    this approach is mandatory for TUX,\t\t\t\t *    where splitting is expensive.\t\t\t\t * 2. Split is accurately. We make this.\t\t\t\t */\t\t\t\tskb_frag_ref(skb, i);\t\t\t\tskb_frag_off_add(&amp;skb_shinfo(skb1)-&gt;frags[0], len - pos);\t\t\t\tskb_frag_size_sub(&amp;skb_shinfo(skb1)-&gt;frags[0], len - pos);\t\t\t\tskb_frag_size_set(&amp;skb_shinfo(skb)-&gt;frags[i], len - pos);\t\t\t\tskb_shinfo(skb)-&gt;nr_frags++;\t\t\t&#125;\t\t\tk++;\t\t&#125; else\t\t\tskb_shinfo(skb)-&gt;nr_frags++;\t\t//这里更新了pos\t\tpos += size;\t&#125;\t//设置skb1 nr_frag数量\tskb_shinfo(skb1)-&gt;nr_frags = k;&#125;\n\n\n拆分后：\n\n\n拆分后：\n\n其他函数pskb_may_pull核心作用是确保 skb 的线性区域中至少有 len 字节的数据。如果不够，它会尝试从非线性区（页片 frags 或 frag_list）中 拉取（pull）数据进线性区域。\nstatic inline bool pskb_may_pull(struct sk_buff *skb, unsigned int len)&#123;\tif (likely(len &lt;= skb_headlen(skb)))\t\treturn true;\tif (unlikely(len &gt; skb-&gt;len))\t\treturn false;\treturn __pskb_pull_tail(skb, len - skb_headlen(skb)) != NULL;&#125;\n\n\n\nskb_queue_empty判断skb的队列是否为空\nstatic inline int skb_queue_empty(const struct sk_buff_head *list)&#123;\treturn list-&gt;next == (const struct sk_buff *) list;&#125;\n\nskb_get增加引用计数\nstatic inline struct sk_buff *skb_get(struct sk_buff *skb)&#123;\trefcount_inc(&amp;skb-&gt;users);\treturn skb;&#125;\n\nskb_shared判断是否有多个引用\nstatic inline int skb_shared(const struct sk_buff *skb)&#123;\treturn refcount_read(&amp;skb-&gt;users) != 1;&#125;\n\nskb_shared_check()如果skb被引用，则clone此skb并返回得到的skb\nstatic inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)&#123;\tmight_sleep_if(gfpflags_allow_blocking(pri));\tif (skb_shared(skb)) &#123;\t\tstruct sk_buff *nskb = skb_clone(skb, pri);\t\t//克隆后会对原来的skb引用计数-\t\tif (likely(nskb))\t\t\tconsume_skb(skb);\t\telse\t\t\tkfree_skb(skb);\t\tskb = nskb;\t&#125;\treturn skb;&#125;\n\nskb_unshare与skb_shared_check类似区别是一个是clone一个是copy\nstatic inline struct sk_buff *skb_unshare(struct sk_buff *skb,\t\t\t\t\t  gfp_t pri)&#123;\tmight_sleep_if(gfpflags_allow_blocking(pri));\tif (skb_cloned(skb)) &#123;\t\tstruct sk_buff *nskb = skb_copy(skb, pri);\t\t/* Free our shared copy */\t\tif (likely(nskb))\t\t\tconsume_skb(skb);\t\telse\t\t\tkfree_skb(skb);\t\tskb = nskb;\t&#125;\treturn skb;&#125;\n\nskb_orphan取消skb与sock结构的关联\nstatic inline void skb_orphan(struct sk_buff *skb)&#123;\tif (skb-&gt;destructor) &#123;\t\tskb-&gt;destructor(skb);\t\tskb-&gt;destructor = NULL;\t\tskb-&gt;sk\t\t= NULL;\t&#125; else &#123;\t\tBUG_ON(skb-&gt;sk);\t&#125;&#125;\n\nskb_cow(copy on wirte)确保skb有指定的头部空间，如果没有指定的头部空间，就重新分配。\nstatic inline int __skb_cow(struct sk_buff *skb, unsigned int headroom,\t\t\t    int cloned)&#123;\tint delta = 0;\t//不够的空间保存到delta中\tif (headroom &gt; skb_headroom(skb))\t\tdelta = headroom - skb_headroom(skb);\t//如果不够，或者skb是被clone过的\tif (delta || cloned)        //这里的第二个参数就是头部要扩充的空间，\t\treturn pskb_expand_head(skb, ALIGN(delta, NET_SKB_PAD), 0,\t\t\t\t\tGFP_ATOMIC);\treturn 0;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["skb"]},{"title":"skb_copy && skb_clone","url":"/2025/06/19/skb%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E4%B8%89/","content":"skb_clone()skb_clone() 创建一个新的 sk_buff 结构体，共享原始 skb 的数据 buffer，但结构体本身是独立的。\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask)&#123;\t//这里先拿到flones，后面可能快速clone\tstruct sk_buff_fclones *fclones = container_of(skb,\t\t\t\t\t\t       struct sk_buff_fclones,\t\t\t\t\t\t       skb1);\tstruct sk_buff *n;\tif (skb_orphan_frags(skb, gfp_mask))\t\treturn NULL;\t//这里判断skb alloc的时候是否有skb2，如果有有直接用skb2\tif (skb-&gt;fclone == SKB_FCLONE_ORIG &amp;&amp;\t    refcount_read(&amp;fclones-&gt;fclone_ref) == 1) &#123;\t\tn = &amp;fclones-&gt;skb2;\t\trefcount_set(&amp;fclones-&gt;fclone_ref, 2);\t&#125; else &#123;\t\tif (skb_pfmemalloc(skb))\t\t\tgfp_mask |= __GFP_MEMALLOC;\t\t//这里分配一个skbi二狗提\t\tn = kmem_cache_alloc(skbuff_head_cache, gfp_mask);\t\tif (!n)\t\t\treturn NULL;\t\tn-&gt;fclone = SKB_FCLONE_UNAVAILABLE;\t&#125;\t//设置skb的字段\treturn __skb_clone(n, skb);&#125;\n\n上述函数返回时调用__skb_clone 设置各个字段\nstatic struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)&#123;#define C(x) n-&gt;x = skb-&gt;x\t//断开链表\tn-&gt;next = n-&gt;prev = NULL;\tn-&gt;sk = NULL;\t//这里设置了原来的大部分元数据\t__copy_skb_header(n, skb);\tC(len);//总长度\tC(data_len);//非线性部分长度\tC(mac_len);//mac头长度\tn-&gt;hdr_len = skb-&gt;nohdr ? skb_headroom(skb) : skb-&gt;hdr_len;//如果是nohdr 就保存头部空间\tn-&gt;cloned = 1; //标识是clone出来的\tn-&gt;nohdr = 0;\tn-&gt;peeked = 0; //不是peek\tC(pfmemalloc);\tn-&gt;destructor = NULL;\t//直接指针指一下\tC(tail);\tC(end);\tC(head);\tC(head_frag);\tC(data);\tC(truesize);\trefcount_set(&amp;n-&gt;users, 1);\tatomic_inc(&amp;(skb_shinfo(skb)-&gt;dataref));//增加引用计数\tskb-&gt;cloned = 1;\treturn n;#undef C&#125;\n\n上述代码如下图所示：\n\npskb_copy复制 skb 的 线性部分（即 skb-&gt;data 到 skb-&gt;tail 的部分）， 对于非线性部分（如 frags[] 和 frag_list），只是保留引用，不复制数据。\nstatic inline struct sk_buff *pskb_copy(struct sk_buff *skb,\t\t\t\t\tgfp_t gfp_mask)&#123;\treturn __pskb_copy(skb, skb_headroom(skb), gfp_mask);&#125;static inline struct sk_buff *__pskb_copy(struct sk_buff *skb, int headroom,\t\t\t\t\t  gfp_t gfp_mask)&#123;\treturn __pskb_copy_fclone(skb, headroom, gfp_mask, false);&#125;\n\n上述__pskb_copy_fclone具体实现如下，主要就是申请一个skb，并复制线性部分的空间，非线性部分就是增加原来的的引用，之后复制原始skb的部分字段。\nstruct sk_buff *__pskb_copy_fclone(struct sk_buff *skb, int headroom,\t\t\t\t   gfp_t gfp_mask, bool fclone)&#123;\t//线性部分加原来数据包头部未使用的空间\tunsigned int size = skb_headlen(skb) + headroom;\t//是否需要fclone，如果需要，alloc的时候就申请两个skb\tint flags = skb_alloc_rx_flag(skb) | (fclone ? SKB_ALLOC_FCLONE : 0);\t//alloc一个skb\tstruct sk_buff *n = __alloc_skb(size, gfp_mask, flags, NUMA_NO_NODE);```cif (!n)\tgoto out;/* Set the data pointer *///设置data和tail指针skb_reserve(n, headroom);/* Set the tail pointer and length *///设置tail指针的位置skb_put(n, skb_headlen(skb));/* Copy the bytes *///memcpy一个线性部分skb_copy_from_linear_data(skb, n-&gt;data, n-&gt;len);n-&gt;truesize += skb-&gt;data_len;n-&gt;data_len  = skb-&gt;data_len;n-&gt;len\t     = skb-&gt;len;//处理非线性部分，就是加个引用if (skb_shinfo(skb)-&gt;nr_frags) &#123;\tint i;\t//zc相关大概率不会走\tif (skb_orphan_frags(skb, gfp_mask) ||\t    skb_zerocopy_clone(n, skb, gfp_mask)) &#123;\t\tkfree_skb(n);\t\tn = NULL;\t\tgoto out;\t&#125;\tfor (i = 0; i &lt; skb_shinfo(skb)-&gt;nr_frags; i++) &#123;\t\tskb_shinfo(n)-&gt;frags[i] = skb_shinfo(skb)-&gt;frags[i];\t\tskb_frag_ref(skb, i);\t&#125;\tskb_shinfo(n)-&gt;nr_frags = i;&#125;//如果哟fraglistif (skb_has_frag_list(skb)) &#123;\tskb_shinfo(n)-&gt;frag_list = skb_shinfo(skb)-&gt;frag_list;\t//加引用计数\tskb_clone_fraglist(n);&#125;//复制元数据skb_copy_header(n, skb);```out:\treturn n;&#125;\n\n拷贝后的skb如下图所示：\n\nskb_copyskb_copy() 是 Linux 内核中用于复制一个 sk_buff（socket buffer）的函数，作用是 深拷贝整个 skb 的内容 —— 包括线性部分（head 数据）和非线性部分（paged frags），注意这里拷贝后的skb就完全是一个线性的skb了，也就是没有非线性部分了。\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t gfp_mask)&#123;\t//原来数据包的头部剩余空间\tint headerlen = skb_headroom(skb);\t//线性长度加非线性长度\tunsigned int size = skb_end_offset(skb) + skb-&gt;data_len;\t//直接申请了线性长度加非线性长度的大小，注意：这里直接线性部分变成了非线性部分\tstruct sk_buff *n = __alloc_skb(size, gfp_mask,\t\t\t\t\tskb_alloc_rx_flag(skb), NUMA_NO_NODE);\tif (!n)\t\treturn NULL;\t/* Set the data pointer */\t//设置data指针\tskb_reserve(n, headerlen);\t/* Set the tail pointer and length */\t//设置tail指针\tskb_put(n, skb-&gt;len);\t//这里把非线性部分的数据也给拷贝了，偏\tBUG_ON(skb_copy_bits(skb, -headerlen, n-&gt;head, headerlen + skb-&gt;len));\t//拷贝skb的字段\tskb_copy_header(n, skb);\treturn n;&#125;\n\n上述代码在申请skb时直接申请了线性长度加非线性长度的大小，然后调用skb_copy_bits完成真正的copy\n//offset为从哪开始拷贝，to是拷贝到那里，len是拷贝多长int skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len)&#123;\tint start = skb_headlen(skb);//线性部分长度\tstruct sk_buff *frag_iter;\tint i, copy;\tif (offset &gt; (int)skb-&gt;len - len)\t\tgoto fault;\t/* Copy header. */\t//复制线性部分\tif ((copy = start - offset) &gt; 0) &#123;\t\tif (copy &gt; len)\t\t\tcopy = len;\t\t//memcpy一个头部长度\t\tskb_copy_from_linear_data_offset(skb, offset, to, copy);\t\tif ((len -= copy) == 0)\t\t\treturn 0;\t\toffset += copy;\t\tto     += copy;\t&#125;\tfor (i = 0; i &lt; skb_shinfo(skb)-&gt;nr_frags; i++) &#123;\t\tint end;\t\tskb_frag_t *f = &amp;skb_shinfo(skb)-&gt;frags[i];//每一个page的管理结构\t\tWARN_ON(start &gt; offset + len);\t\t//线性部分的长度加上frag的长度，表示当前结束的位置(数据包当前page+线性部分)\t\tend = start + skb_frag_size(f);\t\t//是否有要高倍的数据\t\tif ((copy = end - offset) &gt; 0) &#123;\t\t\tu32 p_off, p_len, copied;\t\t\tstruct page *p;\t\t\tu8 *vaddr;\t\t\tif (copy &gt; len)\t\t\t\tcopy = len;\t\t\t//拷贝每一个页的数据\t\t\tskb_frag_foreach_page(f,\t\t\t\t\t      skb_frag_off(f) + offset - start,\t\t\t\t\t      copy, p, p_off, p_len, copied) &#123;\t\t\t\tvaddr = kmap_atomic(p);//映射页到虚拟地址\t\t\t\t//memcpy\t\t\t\tmemcpy(to + copied, vaddr + p_off, p_len);\t\t\t\tkunmap_atomic(vaddr);\t\t\t&#125;\t\t\t//如果非线性部分都拷贝完了，也就是没有fraglist就直接return了\t\t\tif ((len -= copy) == 0)\t\t\t\treturn 0;\t\t\toffset += copy;//更新offset和同也就是更新从哪开始复制，复制到哪去\t\t\tto     += copy;\t\t&#125;\t\tstart = end;//变成下一个页对应数据包长度的起始地址。\t&#125;\t//有fraglist的情况\tskb_walk_frags(skb, frag_iter) &#123;\t\tint end;\t\tWARN_ON(start &gt; offset + len);\t\tend = start + frag_iter-&gt;len;\t\tif ((copy = end - offset) &gt; 0) &#123;\t\t\tif (copy &gt; len)\t\t\t\tcopy = len;\t\t\tif (skb_copy_bits(frag_iter, offset - start, to, copy))\t\t\t\tgoto fault;\t\t\tif ((len -= copy) == 0)\t\t\t\treturn 0;\t\t\toffset += copy;\t\t\tto     += copy;\t\t&#125;\t\tstart = end;\t&#125;\tif (!len)\t\treturn 0;fault:\treturn -EFAULT;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["skb"]},{"title":"struct skb && alloc_skb","url":"/2025/06/11/sockbuff%E7%BB%93%E6%9E%84/","content":"skb结构体及各个字段如下所示：\nstruct sk_buff &#123;\tunion &#123;\t\t//联合体第一部分\t\tstruct &#123;\t\t\t/* These two members must be first. */\t\t//头尾指针拉成链，例如tcp的接收发送队列\t\t\tstruct sk_buff\t\t*next;  \t\t\tstruct sk_buff\t\t*prev;\t\t\t\t\tunion &#123;\t\t\t\tstruct net_device\t*dev; //skb所属的netdev由驱动设置，比如指向bond设备\t\t\t\t/* Some protocols might use this space to store information,\t\t\t\t * while device pointer would be NULL.\t\t\t\t * UDP receive path is one user.\t\t\t\t */\t\t\t\tunsigned long\t\tdev_scratch; //udp会复用这部分空间，比如存校验和信息\t\t\t&#125;;\t\t&#125;;\t\tstruct rb_node\t\trbnode; //红黑数节点，tcp的重传队列，或者ip重组，或者tc会用到\t\tstruct list_head\tlist; //网卡收包没送到协议栈前会用到\t&#125;;\tunion &#123;\t\tstruct sock\t\t*sk; //所属的sock\t\tint\t\t\tip_defrag_offset; //保存分片报文的offset\t&#125;;\tunion &#123;\t\tktime_t\t\ttstamp; //接收发送时候会设置时间戳，例如dev-ximt 或者netif_receive_skb\t\tu64\t\tskb_mstamp_ns;/ //tcp的tx_delay会把值赋值到这个字段上pacing相关，tcp_repair也会用到 \t&#125;;\t/*\t * This is the control buffer. It is free to use for every\t * layer. Please put your private variables there. If you\t * want to keep them across layers you have to do a skb_clone()\t * first. This is owned by whoever has the skb queued ATM.\t */\tchar\t\t\tcb[48] __aligned(8); //不同协议层的私有字段，48B\tunion &#123;\t\tstruct &#123;\t\t\tunsigned long\t_skb_refdst; //查找路由后会 设置dst\t\t\tvoid\t\t(*destructor)(struct sk_buff *skb); //skb的析构函数\t\t&#125;;\t\tstruct list_head\ttcp_tsorted_anchor;  //repair模式使用会链在tpsock的一个队列中，清重传队列会移除\t&#125;;#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\tunsigned long\t\t _nfct;#endif\tunsigned int\t\tlen,   //数据包线性部分加非线性部分的总长度\t\t\t\tdata_len; //非线性部分的长度\t__u16\t\t\tmac_len, //mac头长度\t\t\t\thdr_len;  //头部长度，好像只有clone时候会用到\t/* Following fields are _not_ copied in __copy_skb_header()\t * Note that queue_mapping is here mostly to fill a hole.\t */\t__u16\t\t\tqueue_mapping; //对应的硬件队列，通常是hash算出来的/* if you move cloned around you also must adapt those constants */#ifdef __BIG_ENDIAN_BITFIELD#define CLONED_MASK\t(1 &lt;&lt; 7)#else#define CLONED_MASK\t1#endif#define CLONED_OFFSET()\t\toffsetof(struct sk_buff, __cloned_offset)\t/* private: */\t__u8\t\t\t__cloned_offset[0]; //实际没有，类似一个占位符\t/* public: */\t__u8\t\t\tcloned:1,   //是否被克隆过\t\t\t\t\tnohdr:1,\t//是否有头部空间，noheadroom为1，表示不能给头部添加长度了，tcp会置1\t\t\t\t\tfclone:2, \t//标识使用的那种克隆\t\t\t\tpeeked:1,   //udp的peek\t\t\t\thead_frag:1,  //网卡驱动申请page的时候就会设置这个字段，好像就是标识是从page上分配的\t\t\t\tpfmemalloc:1; //申请存放数据报道page如果pfmemalloc 池分配的页  就 为1 ，\t\t\t\t\t\t\t\t//表示存在内存压力，有这个标志后协议栈会有特殊的处理#ifdef CONFIG_SKB_EXTENSIONS\t__u8\t\t\tactive_extensions;#endif\t/* fields enclosed in headers_start/headers_end are copied\t * using a single memcpy() in __copy_skb_header()\t */\t/* private: */\t__u32\t\t\theaders_start[0];//占位符\t/* public: *//* if you move pkt_type around you also must adapt those constants */#ifdef __BIG_ENDIAN_BITFIELD#define PKT_TYPE_MAX\t(7 &lt;&lt; 5)#else#define PKT_TYPE_MAX\t7#endif#define PKT_TYPE_OFFSET()\toffsetof(struct sk_buff, __pkt_type_offset)\t/* private: */\t__u8\t\t\t__pkt_type_offset[0]; //占位符\t/* public: */\t__u8\t\t\tpkt_type:3;  //驱动设置的，本地广播还是other\t__u8\t\t\tignore_df:1;  //为1表示允许分片，用于隧道和桥接等处理\t__u8\t\t\tnf_trace:1; //netfilter相关，置1后netfllter可以trace\t__u8\t\t\tip_summed:2; //标识是否硬件卸载校验和\t__u8\t\t\tooo_okay:1; //多队列场景下，如果tcp队列中没有数据，可以将这位置为1，表示可以乱序发送\t\t\t\t\t\t\t\t//发送后，在pick_tx的时候就可能会换一个硬件队列，实现负载均衡\t__u8\t\t\tl4_hash:1;  //skb的hash字段是否是4元组的hash值\t__u8\t\t\tsw_hash:1; //skb的hash是否是硬件算的，会通过描述符带上来\t__u8\t\t\twifi_acked_valid:1;\t__u8\t\t\twifi_acked:1;\t__u8\t\t\tno_fcs:1;   //驱动使用，通知硬件是否需要计算校验和\t/* Indicates the inner headers are valid in the skbuff. */\t__u8\t\t\tencapsulation:1;  //标识是否是隧道报文，接收方向由驱动设置\t__u8\t\t\tencap_hdr_csum:1;  //隧道报文外层头部 1为需要计算校验和\t__u8\t\t\tcsum_valid:1;  //校验和是否有效#ifdef __BIG_ENDIAN_BITFIELD#define PKT_VLAN_PRESENT_BIT\t7#else#define PKT_VLAN_PRESENT_BIT\t0#endif#define PKT_VLAN_PRESENT_OFFSET()\toffsetof(struct sk_buff, __pkt_vlan_present_offset)\t/* private: */\t__u8\t\t\t__pkt_vlan_present_offset[0];\t/* public: */\t__u8\t\t\tvlan_present:1;  //是否存在vlan\t__u8\t\t\tcsum_complete_sw:1; //checksum是否是软件算的\t__u8\t\t\tcsum_level:2;\t__u8\t\t\tcsum_not_inet:1;  //非标准的校验和，比如sctp\t__u8\t\t\tdst_pending_confirm:1;  //标识路由是否是pending状态，tcptransmit 时候可能会设置#ifdef CONFIG_IPV6_NDISC_NODETYPE\t__u8\t\t\tndisc_nodetype:2;#endif\t__u8\t\t\tipvs_property:1;   //四层负载均衡相关\t__u8\t\t\tinner_protocol_type:1;  //标识隧道包报文的内层是以太网还是ip头\t__u8\t\t\tremcsum_offload:1; //远程校验和卸载，用于隧道报文处理#ifdef CONFIG_NET_SWITCHDEV\t__u8\t\t\toffload_fwd_mark:1;\t__u8\t\t\toffload_l3_fwd_mark:1;#endif#ifdef CONFIG_NET_CLS_ACT\t__u8\t\t\ttc_skip_classify:1;\t__u8\t\t\ttc_at_ingress:1;#endif#ifdef CONFIG_NET_REDIRECT\t__u8\t\t\tredirected:1;\t__u8\t\t\tfrom_ingress:1;#endif#ifdef CONFIG_TLS_DEVICE\t__u8\t\t\tdecrypted:1;#endif#ifdef CONFIG_NET_SCHED\t__u16\t\t\ttc_index;\t/* traffic control index */#endif\tunion &#123;\t\t__wsum\t\tcsum;   //各层的checksum\t\tstruct &#123;\t\t\t__u16\tcsum_start;  //发送端计算校验和开始的地方\t\t\t__u16\tcsum_offset; //发送端计算校验和结束的地方\t\t&#125;;\t&#125;;\t__u32\t\t\tpriority; //用户配置，用于选择网卡硬件队列\tint\t\t\tskb_iif;  //netfreciverskb中设置，输入网口的索引\t__u32\t\t\thash; //数据包hash值硬件可以通过描述符带上来\t__be16\t\t\tvlan_proto; //vlan的type  //网卡支持vlan卸载的话就会把vlan字段赋值\t__u16\t\t\tvlan_tci; //16位 vlan id和优先级#if defined(CONFIG_NET_RX_BUSY_POLL) || defined(CONFIG_XPS)  //绕过软中断 直接轮询收包\tunion &#123;\t\tunsigned int\tnapi_id;\t\tunsigned int\tsender_cpu;\t&#125;;#endif#ifdef CONFIG_NETWORK_SECMARK\t__u32\t\tsecmark;#endif\tunion &#123;\t\t__u32\t\tmark;        //用户配置的？ 很多地方用到 iptable tc ovs\t\t__u32\t\treserved_tailroom;  //申请数据包的时候end - tail -size 也就是保留的空间\t&#125;;\tunion &#123;\t\t__be16\t\tinner_protocol; //隧道报文内内存协议类型\t\t__u8\t\tinner_ipproto;  //隧道报文内层i \t&#125;;\t__u16\t\t\tinner_transport_header;  //隧道报文4层头的偏移，用来直接找4层头\t__u16\t\t\tinner_network_header; //隧道报文3层头偏移，找3层投\t__u16\t\t\tinner_mac_header;//找二层头\t__be16\t\t\tprotocol;\t__u16\t\t\ttransport_header;\t__u16\t\t\tnetwork_header;\t__u16\t\t\tmac_header; //同上\t/* private: */\t__u32\t\t\theaders_end[0];\t/* public: */\t/* These elements must be at the end, see alloc_skb() for details.  */\tsk_buff_data_t\t\ttail;//有效数据尾部\tsk_buff_data_t\t\tend;  //缓冲区结尾\tunsigned char\t\t*head, //缓冲区头\t\t\t\t*data;  //数据头\tunsigned int\t\ttruesize; //skb结构体大小加缓冲区大小\trefcount_t\t\tusers; //引用技术#ifdef CONFIG_SKB_EXTENSIONS\t/* only useable after checking -&gt;active_extensions != 0 */\tstruct skb_ext\t\t*extensions;#endif&#125;;\n\nskb的申请通常调用的是封装了__alloc_skb的函数，比如网卡驱动使用netdev_alloc_skb，napi_alloc_skb， 或者build_skb()+alloc_pages()数据包 tcp_sendmsg_locked中使用alloc_skb_fclone 申请数据包，其实最终都调用 到__alloc_skb只不过传递的参数不同\n这里直接介绍__alloc_skb的实现\n函数原型：\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask, int flags, int node)\n\n\n\n\n参数\n含义\n\n\n\nsize\n希望为 skb 分配的数据区大小\n\n\ngfp_mask\n内存分配标志，例如 GFP_KERNEL 或 GFP_ATOMIC\n\n\nflags\nskb 分配标志，比如是否是 RX 或是否要 FCLONE\n\n\nnode\nNUMA 节点编号，在哪个节点上分配内存\n\n\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,\t\t\t    int flags, int node)&#123;\tstruct kmem_cache *cache;\tstruct skb_shared_info *shinfo;\tstruct sk_buff *skb;\tu8 *data;\tbool pfmemalloc;\t//这个根据flag的类型，选择不同的cache来分配skb，tcp发送就设置了这个标志，\t//flcone是直接申请两个skb因为tcp发送路径经常需要clone\tcache = (flags &amp; SKB_ALLOC_FCLONE)\t\t? skbuff_fclone_cache : skbuff_head_cache;\t//是否是低内存情况下分配的skb\tif (sk_memalloc_socks() &amp;&amp; (flags &amp; SKB_ALLOC_RX))\t\tgfp_mask |= __GFP_MEMALLOC;\t/* Get the HEAD */\t//这里真正申请了一个skb       结构  \tskb = kmem_cache_alloc_node(cache, gfp_mask &amp; ~__GFP_DMA, node);\tif (!skb)\t\tgoto out;\t//预取\tprefetchw(skb);\t/* We do our best to align skb_shared_info on a separate cache\t * line. It usually works because kmalloc(X &gt; SMP_CACHE_BYTES) gives\t * aligned memory blocks, unless SLUB/SLAB debug is enabled.\t * Both skb-&gt;head and skb_shared_info are cache line aligned.\t */\t //对齐\tsize = SKB_DATA_ALIGN(size);\t//注意这里的size加上了管理非线性部分的结构体\tsize += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\t// 这里用kmalloc分配了数据包缓冲区\tdata = kmalloc_reserve(size, gfp_mask, node, &amp;pfmemalloc);\tif (!data)\t\tgoto nodata;\t/* kmalloc(size) might give us more room than requested.\t * Put skb_shared_info exactly at the end of allocated zone,\t * to allow max possible filling before reallocation.\t */\t //这里减去了管理非线性部分的大小\tsize = SKB_WITH_OVERHEAD(ksize(data));\t//预取\tprefetchw(data + size);\t/*\t * Only clear those fields we need to clear, not those that we will\t * actually initialise below. Hence, don&#x27;t put any more fields after\t * the tail pointer in struct sk_buff!\t */\t //这里memst了\tmemset(skb, 0, offsetof(struct sk_buff, tail));\t/* Account for allocated memory : skb + skb-&gt;head */\t//这里是skb结构体大小，加管理非线性部分结构体大小加申请数据缓冲区大小的三者之和\tskb-&gt;truesize = SKB_TRUESIZE(size);\tskb-&gt;pfmemalloc = pfmemalloc; //是否是内存压力的情况下分配的内存\trefcount_set(&amp;skb-&gt;users, 1); //skb引用计数\tskb-&gt;head = data; \tskb-&gt;data = data;\tskb_reset_tail_pointer(skb);\tskb-&gt;end = skb-&gt;tail + size; //end始终指向缓冲区结束\tskb-&gt;mac_header = (typeof(skb-&gt;mac_header))~0U;\tskb-&gt;transport_header = (typeof(skb-&gt;transport_header))~0U;\t/* make sure we initialize shinfo sequentially */\tshinfo = skb_shinfo(skb);\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\t//共享部分引用计数++\tatomic_set(&amp;shinfo-&gt;dataref, 1);\tif (flags &amp; SKB_ALLOC_FCLONE) &#123;\t\tstruct sk_buff_fclones *fclones;\t\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\t\tskb-&gt;fclone = SKB_FCLONE_ORIG; //标记是fastclone中的原始标记\t\trefcount_set(&amp;fclones-&gt;fclone_ref, 1);\t\tfclones-&gt;skb2.fclone = SKB_FCLONE_CLONE; //设置skb2的fclone标记\t&#125;out:\treturn skb;nodata:\tkmem_cache_free(cache, skb);\tskb = NULL;\tgoto out;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["skb"]},{"title":"套接字层socket、sock、文件系统之间的关系","url":"/2025/05/21/socket_sock_file_%E6%A6%82%E5%BF%B5/","content":"1.概念内核套接字层（socket layer）是 Linux 网络协议栈中承上启下的一层，负责将用户空间的 socket API（如 socket(), bind(), send(), recv() 等）与内核中的协议栈对接，其核心作用在于实现应用层与传输层协议之间的解耦，为应用程序提供一种统一且抽象的网络通信方式，套接字机制最初由 BSD UNIX 引入，现已广泛应用于各类网络编程环境中。\n套接字层（sockets）在整个网络协议栈中的位置如下图所示：\n\n2.关键数据结构套接字层使用的关键数据结构以及作用如下：\n2.1struct socket作用：是用户空间 socket 文件描述符在内核中的抽象\n核心字段如下所示：\nstruct socket &#123;    socket_state          state;     // 套接字状态    short                 type;      // SOCK_STREAM、SOCK_DGRAM 等    struct sock          *sk;        // 指向内核协议栈的 sock 结构    const struct proto_ops *ops;     // 指向协议操作函数表，如 inet_stream_ops    ...&#125;;\n\n\n\n2.2 struct sock作用：表示一个连接或一个套接字的协议控制块（protocol control block），协议相关逻辑都在这里实现\n核心字段（以 TCP 为例）：\nstruct sock &#123;    struct socket        *sk_socket;    // 回指到 struct socket    struct proto         *sk_prot;      // 协议操作（如 tcp_prot）    struct sk_buff_head   sk_receive_queue; // 接收队列    struct sk_buff_head   sk_write_queue;   // 发送队列    int                   sk_state;     // TCP 状态，如 ESTABLISHED 等    ...&#125;;\n\n2.3 、struct proto_ops作用：socket 操作函数表，对应 socket() 返回的文件描述符上的各种操作，如 send(), recv(), bind()\n核心字段：\nstruct proto_ops &#123;    int (*release)(struct socket *);    int (*bind)(struct socket *, struct sockaddr *, int);    int (*connect)(struct socket *, struct sockaddr *, int, int);    int (*sendmsg)(struct socket *, struct msghdr *, size_t);    int (*recvmsg)(struct socket *, struct msghdr *, size_t, int);    ...&#125;;\n\n2.4  struct proto作用：proto 是 面向传输层抽象设计的接口，把具体协议（TCP、UDP）与上层逻辑解耦，让上层只调用函数指针，而不用管协议细节\nstruct proto &#123;    struct sock *(*alloc)(struct net *, struct socket *, int, gfp_t);    void (*close)(struct sock *sk, long timeout);    int  (*connect)(struct sock *sk, struct sockaddr *uaddr, int addr_len);    int  (*sendmsg)(struct sock *sk, struct msghdr *msg, size_t len);    int  (*recvmsg)(struct sock *sk, struct msghdr *msg, size_t len, int noblock, int flags, int *addr_len);    ...&#125;;\n\n2.5struct file虽不专属于 socket 层，但与 socket 强相关。\n每个 socket 在内核中表现为一个文件，用户空间调用 socket() 后返回的文件描述符 fd 会指向一个 struct file，其 private_data 就是 &#96;struct socket\n2.6 整体关系图下图展示了上述结构体的关系图，其中task_struct对应一个进程，其files指向file_struct结构，该结构的主要功能是管理fd_arry 里面的每个fd对应一个打开的文件，其中的private指针指向的是I&#x2F;O对象的专有数据，对于socket层而言，就是socket结构，socket中的ops可以理解为用户态系统调用的实现。而sock的prot则是根据协议类型，进一步更为具体的实现。\n\n","categories":["网络协议栈源码学习"],"tags":["socket"]},{"title":"socket文件系统","url":"/2025/05/26/socket%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","content":"1.套接字与文件系统每一种文件都有各自的文件类型，例如设备文件包括字符设备文件和块设备文件等，而与套接字关联的文件类型为套接字文件。\n1.1套接字文件系统的注册为了能够让套接字与文件描述符相关联，并支持特殊套接字曾的节点分配和释放，系统中增加了sockfs文件系统类型sock_fs_type,具体定义如下：\nstatic struct file_system_type sock_fs_type = &#123;\t.name =\t\t&quot;sockfs&quot;, //文件系统名字\t//挂载的时候会被调用，在kern_mount()被调用\t.init_fs_context = sockfs_init_fs_context,//初始化文件系统的回调函数，\t.kill_sb =\tkill_anon_super,//卸载文件系统&#125;;\nsock_fs_type类型的文件系统注册发生在sock_init()在start_kernel中经过一些列调用会被调到.\n//start_kernel会调用到它static int __init sock_init(void)&#123;    ...\t//注册socket类型的文件系统\terr = register_filesystem(&amp;sock_fs_type);\tif (err)\t\tgoto out;    //挂载这个文件系统，返回一个超级块\tsock_mnt = kern_mount(&amp;sock_fs_type);\tif (IS_ERR(sock_mnt)) &#123;\t\terr = PTR_ERR(sock_mnt);\t\tgoto out_mount;\t&#125;    ....&#125;\n\n在上述代码中首先调用register_filesystem(&amp;sock_fs_type);来注册socket文件系统，主要工作判断是否有相同名字文件系统并插入到链表中。然后调用kern_mount完成挂载操作并返回一个超级块，在创建socket的时候创建inode就是用的这个超级块的alloc_node回调函数。kern_mount函数定义如下\nstruct vfsmount *kern_mount(struct file_system_type *type)&#123;\tstruct vfsmount *mnt;\t//实际的挂载操作\tmnt = vfs_kern_mount(type, SB_KERNMOUNT, type-&gt;name, NULL);\tif (!IS_ERR(mnt)) &#123;\t\t/*\t\t * it is a longterm mount, don&#x27;t release mnt until\t\t * we unmount before file sys is unregistered\t\t*/\t\treal_mount(mnt)-&gt;mnt_ns = MNT_NS_INTERNAL;\t&#125;\treturn mnt;&#125;\n\n上述代码调用vfs_kern_mount中调用了vfs_kern_mount完成实际的挂载，返回的为一个挂载点，其中SB_KERNMOUNT表示表示该挂载属于内核内部命名空间。\n在vfs_kern_mount中的工作就是创建一个fs_context结构体，把具体文件系统的回调函数与ctx-&gt;ops相关联，，这里的ctx是fs的一个私有结构。然后通过fc_mount中的get_tree把sb与ctx-&gt;ops关联了起来\nstruct vfsmount *vfs_kern_mount(struct file_system_type *type,\t\t\t\tint flags, const char *name,\t\t\t\tvoid *data)&#123;\tstruct fs_context *fc;\t//mnt为返回的挂载点\tstruct vfsmount *mnt;\tint ret = 0;\tif (!type)\t\treturn ERR_PTR(-EINVAL);\t\t//分配一个fs结构，并初始化，这里把具体文件系统的回调函数与ctx-&gt;ops相关联\t//fs为ctx的一个私有结构\tfc = fs_context_for_mount(type, flags);\tif (IS_ERR(fc))\t\treturn ERR_CAST(fc);\tif (name)\t\tret = vfs_parse_fs_string(fc, &quot;source&quot;,\t\t\t\t\t  name, strlen(name));\tif (!ret)\t\tret = parse_monolithic_mount_data(fc, data);\tif (!ret)\t//分配并初始化超级块和挂载点\t//注意这里面的get_tree把超级块与ctx-&gt;ops关联了起来\t\tmnt = fc_mount(fc);\telse\t\tmnt = ERR_PTR(ret);\tput_fs_context(fc);\treturn mnt;&#125;\nfs_context_for_mount是一个包裹函数，最终实际调用的是alloc_fs_context 核心逻辑就是创建一个fs结构体并初始化然后调用文件系统注册的init_fs_context函数，将fs的ctx与文件系统的ops关联上。 函数原型如下：\nstatic struct fs_context *alloc_fs_context(struct file_system_type *fs_type,\t\t\t\t      struct dentry *reference,\t\t\t\t      unsigned int sb_flags,\t\t\t\t      unsigned int sb_flags_mask,\t\t\t\t      enum fs_context_purpose purpose)&#123;\tint (*init_fs_context)(struct fs_context *);\tstruct fs_context *fc;\tint ret = -ENOMEM;\t//分配一个fc\tfc = kzalloc(sizeof(struct fs_context), GFP_KERNEL_ACCOUNT);\tif (!fc)\t\treturn ERR_PTR(-ENOMEM);\tfc-&gt;purpose\t= purpose;\tfc-&gt;sb_flags\t= sb_flags;\tfc-&gt;sb_flags_mask = sb_flags_mask;\tfc-&gt;fs_type\t= get_filesystem(fs_type);\tfc-&gt;cred\t= get_current_cred();\tfc-&gt;net_ns\t= get_net(current-&gt;nsproxy-&gt;net_ns);\tfc-&gt;log.prefix\t= fs_type-&gt;name;\tmutex_init(&amp;fc-&gt;uapi_mutex);\tswitch (purpose) &#123;\tcase FS_CONTEXT_FOR_MOUNT:\t\tfc-&gt;user_ns = get_user_ns(fc-&gt;cred-&gt;user_ns);\t\tbreak;\tcase FS_CONTEXT_FOR_SUBMOUNT:\t\tfc-&gt;user_ns = get_user_ns(reference-&gt;d_sb-&gt;s_user_ns);\t\tbreak;\tcase FS_CONTEXT_FOR_RECONFIGURE:\t\tatomic_inc(&amp;reference-&gt;d_sb-&gt;s_active);\t\tfc-&gt;user_ns = get_user_ns(reference-&gt;d_sb-&gt;s_user_ns);\t\tfc-&gt;root = dget(reference);\t\tbreak;\t&#125;\t/* TODO: Make all filesystems support this unconditionally */\tinit_fs_context = fc-&gt;fs_type-&gt;init_fs_context;\tif (!init_fs_context)\t\tinit_fs_context = legacy_init_fs_context;\t//调用文件系统的init函数，将fs与文件系统的ops关联上。\tret = init_fs_context(fc);\tif (ret &lt; 0)\t\tgoto err_fc;\tfc-&gt;need_free = true;\treturn fc;err_fc:\tput_fs_context(fc);\treturn ERR_PTR(ret);&#125;\nret = init_fs_context(fc); 为上述文件系统注册的回调函数\nstatic int sockfs_init_fs_context(struct fs_context *fc)&#123;\tstruct pseudo_fs_context *ctx = init_pseudo(fc, SOCKFS_MAGIC);\tif (!ctx)\t\treturn -ENOMEM;\tctx-&gt;ops = &amp;sockfs_ops;//alloc_inode ,free_inode\tctx-&gt;dops = &amp;sockfs_dentry_operations;\tctx-&gt;xattr = sockfs_xattr_handlers;\treturn 0;&#125;\n\n上述init_pseudo()中主要zu了两件事，申请了ctx结构作为fs的私有结构，然后注册了get_tree回调函数。这个get_tree ，回调里面的逻辑就是创建一个超级块，然后将ctx的ops赋值给超级块的ops，在fc_mount中会调用这个回调函数，函数定义如下所示：\nstatic const struct fs_context_operations pseudo_fs_context_ops = &#123;\t.free\t\t= pseudo_fs_free,\t.get_tree\t= pseudo_fs_get_tree,//创建了一个超级块最终会调用err = fill_super(sb, fc);填充超级块&#125;;struct pseudo_fs_context *init_pseudo(struct fs_context *fc,\t\t\t\t\tunsigned long magic)&#123;\tstruct pseudo_fs_context *ctx;\tctx = kzalloc(sizeof(struct pseudo_fs_context), GFP_KERNEL);\tif (likely(ctx)) &#123;\t\tctx-&gt;magic = magic;\t\tfc-&gt;fs_private = ctx;//设置私有结构\t\tfc-&gt;ops = &amp;pseudo_fs_context_ops; //设置ops集合，如上述代码所示\t\tfc-&gt;sb_flags |= SB_NOUSER;\t\tfc-&gt;global = true;\t&#125;\treturn ctx;&#125;\n\n上述fs_context_for_mount函数在vfs_kern_mount中被调用后会调用fc_mount来申请一个超级块，并初始化相关字段\nstruct vfsmount *fc_mount(struct fs_context *fc)&#123;\t//创建并初始化超级块\tint err = vfs_get_tree(fc);\tif (!err) &#123;\t\tup_write(&amp;fc-&gt;root-&gt;d_sb-&gt;s_umount);\t\t//创建挂载点\t\treturn vfs_create_mount(fc);\t&#125;\treturn ERR_PTR(err);&#125;\nvfs_get_tree中调用fc-&gt;ops-&gt;get_tree(fc)创建并初始化了超级块，代码如下所示：\nint vfs_get_tree(struct fs_context *fc)&#123;\tstruct super_block *sb;\tint error;\tif (fc-&gt;root)\t\treturn -EBUSY;\t//这里创建了超级块\terror = fc-&gt;ops-&gt;get_tree(fc);\tif (error &lt; 0)\t\treturn error;\tif (!fc-&gt;root) &#123;\t\tpr_err(&quot;Filesystem %s get_tree() didn&#x27;t set fc-&gt;root\\n&quot;,\t\t       fc-&gt;fs_type-&gt;name);\t\t/* We don&#x27;t know what the locking state of the superblock is -\t\t * if there is a superblock.\t\t */\t\tBUG();\t&#125;\t//将申请的sb赋值给sb\tsb = fc-&gt;root-&gt;d_sb;\tWARN_ON(!sb-&gt;s_bdi);\tsuper_wake(sb, SB_BORN);\terror = security_sb_set_mnt_opts(sb, fc-&gt;security, 0, NULL);\tif (unlikely(error)) &#123;\t\tfc_drop_locked(fc);\t\treturn error;\t&#125;\tWARN((sb-&gt;s_maxbytes &lt; 0), &quot;%s set sb-&gt;s_maxbytes to &quot;\t\t&quot;negative value (%lld)\\n&quot;, fc-&gt;fs_type-&gt;name, sb-&gt;s_maxbytes);\treturn 0;&#125;","categories":["网络协议栈源码学习"],"tags":["VFS","socket"]},{"title":"socket与文件描述符的映射","url":"/2025/05/27/socket%E4%B8%8E%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%9A%84%E6%98%A0%E5%B0%84/","content":"socket与文件描述符的映射1.socket与fd的映射应用层是通过文件描述符来找到内核的socket的一系列结构，因此在调用socket系统调用创建socket的过程中，会将一个fd与一个套接字相关联，对应的函数为__sys_socket函数中的sock_map_fd在sock_map_fd中，主要做了如下几个事情，获取一个空闲的文件描述符，创建一个file实例，将fd与file实例绑定，然后将这个file实例加入到进程打开的文件指针数组中，然后再将套接字与file相关连。这样fd，file，进程，socket四者之间就紧密的联系在了一起。代码如下：\nstatic int sock_map_fd(struct socket *sock, int flags)&#123;\tstruct file *newfile;\t//从当前进程获取一个未使用的文件描述符fd\tint fd = get_unused_fd_flags(flags);\tif (unlikely(fd &lt; 0)) &#123;\t\tsock_release(sock);\t\treturn fd;\t&#125;\t//创建一个文件对象\tnewfile = sock_alloc_file(sock, flags, NULL);\t//关联fd和文件对象\tif (!IS_ERR(newfile)) &#123;\t\tfd_install(fd, newfile);\t\treturn fd;\t&#125;\t//有错误，将fd标记为未使用\tput_unused_fd(fd);\treturn PTR_ERR(newfile);&#125;\n\n上述代码调用get_unused_fd_flags是一个包裹函数最终调用return alloc_fd(0, nofile, flags);获取一个未使用的文件描述符。\nstatic int alloc_fd(unsigned start, unsigned end, unsigned flags)&#123;\t//获取当前进程的文件描述副列表\tstruct files_struct *files = current-&gt;files;\tunsigned int fd;\tint error;\t//fd表，用来指向files_struct的fdtable\tstruct fdtable *fdt;\tspin_lock(&amp;files-&gt;file_lock);repeat:\t//这里指了一下\tfdt = files_fdtable(files);\tfd = start;\t//跳过已分配的fd\tif (fd &lt; files-&gt;next_fd)\t\tfd = files-&gt;next_fd;\tif (fd &lt; fdt-&gt;max_fds)\t//返回一个没有被使用的fd\t\tfd = find_next_fd(fdt, fd);\t/*\t * N.B. For clone tasks sharing a files structure, this test\t * will limit the total number of files that can be opened.\t */\terror = -EMFILE;\tif (fd &gt;= end)\t\tgoto out;\t//这里面会判断当前的fd是否大于max_fd,如果大于可能需要对​​文件描述符表的扩容\terror = expand_files(files, fd);\tif (error &lt; 0)\t\tgoto out;\t/*\t * If we needed to expand the fs array we\t * might have blocked - try again.\t */\t//可能会阻塞在试一次\tif (error)\t\tgoto repeat;\t//这里修改一下下一个要分配的fd，加速一下\tif (start &lt;= files-&gt;next_fd)\t\tfiles-&gt;next_fd = fd + 1;\t//设置已经使用的标志\t__set_open_fd(fd, fdt);\tif (flags &amp; O_CLOEXEC)\t\t__set_close_on_exec(fd, fdt);\telse\t\t__clear_close_on_exec(fd, fdt);\terror = fd;#if 1\t/* Sanity check */\tif (rcu_access_pointer(fdt-&gt;fd[fd]) != NULL) &#123;\t\tprintk(KERN_WARNING &quot;alloc_fd: slot %d not NULL!\\n&quot;, fd);\t\trcu_assign_pointer(fdt-&gt;fd[fd], NULL);\t&#125;#endifout:\tspin_unlock(&amp;files-&gt;file_lock);\treturn error;&#125;\n\n上述代码参数中的start为0,end为系统配置一个进程最多持有的描述数量，默认通常是1024，最大是多少？？如果文件描述副的fd大于了end则会报错打开了太多文件描述符。申请到fd之后在sock_map_fd中会调用sock_alloc_file创建文件对象，具体代码如下：\nstruct file *sock_alloc_file(struct socket *sock, int flags, const char *dname)&#123;\tstruct file *file;\tif (!dname)\t\tdname = sock-&gt;sk ? sock-&gt;sk-&gt;sk_prot_creator-&gt;name : &quot;&quot;;\t//alloc一个file绑定了socket_file_ops 回调函数集合 sock_mnt为一个挂在点\tfile = alloc_file_pseudo(SOCK_INODE(sock), sock_mnt, dname,\t\t\t\tO_RDWR | (flags &amp; O_NONBLOCK),\t\t\t\t&amp;socket_file_ops);\tif (IS_ERR(file)) &#123;\t\tsock_release(sock);\t\treturn file;\t&#125;\tfile-&gt;f_mode |= FMODE_NOWAIT;\tsock-&gt;file = file;\t//注意：这里关联了socket和file\tfile-&gt;private_data = sock;\t//标记为流式文件，不支持lseek(随机访问)？\tstream_open(SOCK_INODE(sock), file);\treturn file;&#125;\n\n上述sock_alloc_file中调用alloc_file_pseudo创建了一个file结构和dentry结构，并把sock对应的inode和denry相关联，然后在alloc_file_pseudo中将file的私有指针指向socket。这一系列逻辑完成了file，socket，inode，dentry，之间的关联和绑定了文件的ops，之后就可以通过file可以找到socket，通过dentry可以快速找到inode通过inode也可以找到socket。alloc_file_pseudo实现如下:\nstruct file *alloc_file_pseudo(struct inode *inode, struct vfsmount *mnt,\t\t\t\tconst char *name, int flags,\t\t\t\tconst struct file_operations *fops)&#123;\tstatic const struct dentry_operations anon_ops = &#123;\t\t.d_dname = simple_dname\t&#125;;\tstruct qstr this = QSTR_INIT(name, strlen(name));\tstruct path path;\tstruct file *file;\t//创建一个dentry\tpath.dentry = d_alloc_pseudo(mnt-&gt;mnt_sb, &amp;this);\tif (!path.dentry)\t\treturn ERR_PTR(-ENOMEM);\tif (!mnt-&gt;mnt_sb-&gt;s_d_op)\t\td_set_d_op(path.dentry, &amp;anon_ops);\t//给挂载点加一个引用计数\tpath.mnt = mntget(mnt);\t//关联denry和socket的inode 之后可以通过这个dentry快速找到inode\td_instantiate(path.dentry, inode);\t//申请一个file，同时挂上ops (也就是read，write)\tfile = alloc_file(&amp;path, flags, fops);\tif (IS_ERR(file)) &#123;\t\tihold(inode);\t\tpath_put(&amp;path);\t&#125;\treturn file;&#125;\n\n当申请了fd和file结构之后，会调用fd_install 完成文件描述fd与file的关联，此后用户就可以通过这个fd找到file，通过file找到socket，等等一系列信息。\nvoid fd_install(unsigned int fd, struct file *file)&#123;\tstruct files_struct *files = current-&gt;files;\tstruct fdtable *fdt;\trcu_read_lock_sched();\tif (unlikely(files-&gt;resize_in_progress)) &#123;\t\trcu_read_unlock_sched();\t\tspin_lock(&amp;files-&gt;file_lock);\t\tfdt = files_fdtable(files);\t\tBUG_ON(fdt-&gt;fd[fd] != NULL);\t\t//将fdt表的fd元素指向外面申请的file，这个file的私有结构就是socket！\t\trcu_assign_pointer(fdt-&gt;fd[fd], file);\t\tspin_unlock(&amp;files-&gt;file_lock);\t\treturn;\t&#125;\t/* coupled with smp_wmb() in expand_fdtable() */\tsmp_rmb();\tfdt = rcu_dereference_sched(files-&gt;fdt);\tBUG_ON(fdt-&gt;fd[fd] != NULL);\trcu_assign_pointer(fdt-&gt;fd[fd], file);\trcu_read_unlock_sched();&#125;\n\n2.根据文件描述符获取套接字当用户创建socket返回fd之后，执行bind，listen，send，recv都要传入上述提到的fd，内核会根据用户传入的fd找到对应的文件file，进而通过私有指针找到socket，对应的函数接口为sockfd_lookup_light\n//返回值为socketstatic struct socket *sockfd_lookup_light(int fd, int *err, int *fput_needed)&#123;\t\t//根据fd从进程管理的fdtable中找到对应的file，这里struct fd结构体中的一个字段为file\tstruct fd f = fdget(fd);\tstruct socket *sock;\t*err = -EBADF;\tif (f.file) &#123;\t\t//从file的私有指针中拿到socket\t\tsock = sock_from_file(f.file);\t\tif (likely(sock)) &#123;\t\t\t*fput_needed = f.flags &amp; FDPUT_FPUT;\t\t\treturn sock;\t\t&#125;\t\t*err = -ENOTSOCK;\t\tfdput(f);\t&#125;\treturn NULL;&#125;\n\n上述fdget最终调用到__fget_light，具体代码如下：\nstatic unsigned long __fget_light(unsigned int fd, fmode_t mask)&#123;\t//通过current宏获取文件描述符表\tstruct files_struct *files = current-&gt;files;\tstruct file *file;\t/*\t * If another thread is concurrently calling close_fd() followed\t * by put_files_struct(), we must not observe the old table\t * entry combined with the new refcount - otherwise we could\t * return a file that is concurrently being freed.\t *\t * atomic_read_acquire() pairs with atomic_dec_and_test() in\t * put_files_struct().\t */\t//判断当前进程的文件描述符表是否被其他进程共享，如果没有被共享，直接无锁查找\tif (atomic_read_acquire(&amp;files-&gt;count) == 1) &#123;\t\t//逻辑很简单，直接根据fd取对应的files\t\tfile = files_lookup_fd_raw(files, fd);\t\tif (!file || unlikely(file-&gt;f_mode &amp; mask))\t\t\treturn 0;\t\treturn (unsigned long)file;\t&#125; else &#123;\t\t//加rcu锁查找，多线程可能会走这里吧？？？\t\tfile = __fget(fd, mask);\t\tif (!file)\t\t\treturn 0;\t\treturn FDPUT_FPUT | (unsigned long)file;\t&#125;&#125;\n\n上述查找file的过程可以分为快速路径和慢速路径，当文件描述符表存在被多个线程访问时，需要加锁访问，否则直接走快速路径无锁拿到file(atomic_read_acquire(&amp;files-&gt;count))\n","categories":["网络协议栈源码学习"],"tags":["socket"]},{"title":"struct sock","url":"/2025/06/22/sock%E7%BB%93%E6%9E%84%E4%BD%93/","content":"sock 结构体在 Linux 网络协议栈中的作用非常核心，它本质上是用于管理传输层连接状态和控制信息的内核数据结构\nsock 结构体的主要功能：1.  管理传输层连接状态\n记录 socket 状态，如 TCP 的 ESTABLISHED、LISTEN、SYN_SENT 等；\n维护连接生命周期。\n\n2. 数据收发管理\n接收队列：sk_receive_queue\n发送队列：sk_write_queue\nBacklog 队列：sk_backlog\n\n3. 缓冲区和流控\n接收缓冲区大小（sk_rcvbuf）\n发送缓冲区大小（sk_sndbuf）\n控制数据收发速率，避免内存耗尽。\n\n4. 定时器与超时管理\nsk_timer：重传、超时的定时器；\n如 TCP 的重传定时器（RTO）、保活定时器（keepalive）。\n\n5. 协议特定操作**\n每个协议（TCP&#x2F;UDP）定义自己的操作集，绑定到 sock 的 sk_prot 字段；\n例如 TCP 操作集合为 tcp_prot，UDP 为 udp_prot；\n提供 sendmsg()、recvmsg()、close() 等接口。\n\n6. 连接路由与缓存\n加速报文发送时的路由查询。\n\n7. 等待队列管理\nsk_wq：实现 select()、poll()、epoll() 等机制；\n用户空间程序等待 socket 可读&#x2F;可写事件时使用。\n\n具体字段如下所示：\nstruct sock &#123;\t/*\t * Now struct inet_timewait_sock also uses sock_common, so please just\t * don&#x27;t add nothing before this first member (__sk_common) --acme\t */\tstruct sock_common\t__sk_common;#define sk_node\t\t\t__sk_common.skc_node    //管理hash表的节点，比如udp绑定端口后会把它加入udp的hash表1#define sk_nulls_node\t\t__sk_common.skc_nulls_node //管理tcp hash表的节点#define sk_refcnt\t\t__sk_common.skc_refcnt\t\t\t\t\t//引用技术#define sk_tx_queue_mapping\t__sk_common.skc_tx_queue_mapping //选择网卡队列的时候会用到#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING#define sk_rx_queue_mapping\t__sk_common.skc_rx_queue_mapping  //选择网卡队列的时候会用到#endif#define sk_dontcopy_begin\t__sk_common.skc_dontcopy_begin  //不可以复制的起始地址#define sk_dontcopy_end\t\t__sk_common.skc_dontcopy_end \t//不可以复制的结束地址，比如tcp建联的时候会用到sock_copy#define sk_hash\t\t\t__sk_common.skc_hash  //五元组hash值，比如说插入ehash时候会用到#define sk_portpair\t\t__sk_common.skc_portpair//下面两个字段的联合体#define sk_num\t\t\t__sk_common.skc_num    //本地port，比如说绑定端口的时候会设置#define sk_dport\t\t__sk_common.skc_dport  //目的port，比如建立连接的时候会设置#define sk_addrpair\t\t__sk_common.skc_addrpair //用不到这个字段，用于联合体优化布局#define sk_daddr\t\t__sk_common.skc_daddr  //目的ip地址，比如connect的时候会设置。#define sk_rcv_saddr\t\t__sk_common.skc_rcv_saddr //本地ip地址，比如connect的时候会设置。#define sk_family\t\t__sk_common.skc_family   //协议族比如afinet#define sk_state\t\t__sk_common.skc_state \t//tcp三次握手四次挥手用到的状态#define sk_reuse\t\t__sk_common.skc_reuse  //timewait相关#define sk_reuseport\t\t__sk_common.skc_reuseport \t//重用ip端口#define sk_ipv6only\t\t__sk_common.skc_ipv6only#define sk_net_refcnt\t\t__sk_common.skc_net_refcnt  //sock正在被使用的引用计数#define sk_bound_dev_if\t\t__sk_common.skc_bound_dev_if //输出网络设备的索引，比如用户指定了输出设备#define sk_bind_node\t\t__sk_common.skc_bind_node //tcp收包bhash的节点？#define sk_prot\t\t\t__sk_common.skc_prot \t//不同协议的ops集合#define sk_net\t\t\t__sk_common.skc_net  //对应的网络命名空间#define sk_v6_daddr\t\t__sk_common.skc_v6_daddr //ipv6#define sk_v6_rcv_saddr\t__sk_common.skc_v6_rcv_saddr //ipv6#define sk_cookie\t\t__sk_common.skc_cookie#define sk_incoming_cpu\t\t__sk_common.skc_incoming_cpu //收包cpu索引#define sk_flags\t\t__sk_common.skc_flags\t//一些标志#define sk_rxhash\t\t__sk_common.skc_rxhash //接收端的hash值\t/* early demux fields */\tstruct dst_entry __rcu\t*sk_rx_dst; //接收方向查找路由获取的dst\tint\t\t\tsk_rx_dst_ifindex;\t//收包网络设备索引\tu32\t\t\tsk_rx_dst_cookie;\t//ipv6相关\t\tsocket_lock_t\t\tsk_lock;\tatomic_t\t\tsk_drops;\t\t//缓冲区溢出的丢包统计\tint\t\t\tsk_rcvlowat;\t\t//​套接字接收数据最小阈值​\tstruct sk_buff_head\tsk_error_queue;\t\t//错误队列，收到icmp报文错误报文的时候就会放到这个队列\tstruct sk_buff_head\tsk_receive_queue;  //软中断放包的接收队列\t/*\t * The backlog queue is special, it is always used with\t * the per-socket spinlock held and requires low latency\t * access. Therefore we special case it&#x27;s implementation.\t * Note : rmem_alloc is in this structure to fill a hole\t * on 64bit arches, not because its logically part of\t * backlog.\t */\tstruct &#123;\t\tatomic_t\trmem_alloc;\t\tint\t\tlen;\t\tstruct sk_buff\t*head;\t\tstruct sk_buff\t*tail;\t&#125; sk_backlog; //后备队列，tcprecv会用到////skb的turesize总和，如果这个值大于缓冲区大小，就会丢弃，收包的时候会把skb的truesize累加上#define sk_rmem_alloc sk_backlog.rmem_alloc\tint\t\t\tsk_forward_alloc; // 缓冲区大小的一个标志，在申请skb的时候会增加，放入发送队列后会减少\tu32\t\t\tsk_reserved_mem;   //setsockopt配置，保留缓冲区的大小 //tcp存在内存压力的时候会用到 #ifdef CONFIG_NET_RX_BUSY_POLL\tunsigned int\t\tsk_ll_usec;\t/* ===== mostly read cache line ===== */\tunsigned int\t\tsk_napi_id;#endif\tint\t\t\tsk_rcvbuf;       //接收缓冲区大小，skb的turesize大于sk_rcvbuf就直接丢弃，sock初始化的时候读取系统配置设置的\tint\t\t\tsk_disconnects;   //记录断开连接的次数\tstruct sk_filter __rcu\t*sk_filter;\tunion &#123;\t\tstruct socket_wq __rcu\t*sk_wq;   //等待队列，多种调度需要\t\t/* private: */\t\tstruct socket_wq\t*sk_wq_raw;\t\t/* public: */\t&#125;;#ifdef CONFIG_XFRM\tstruct xfrm_policy __rcu *sk_policy[2];#endif\tstruct dst_entry __rcu\t*sk_dst_cache;  //sock关联的路由结构，在tcp建立连接或者发包的时候会设置\tatomic_t\t\tsk_omem_alloc;\tint\t\t\tsk_sndbuf;     //发送缓冲区大小，和rcvbuf一样，当在整个协议栈中使用内存大于这个值的时候，就回返回nobuf\t\t/* ===== cache line for TX ===== */\t// //发送队列中缓存数据包的大小，入队的时候会加turesize, 这里可以理解为没有交付到第三层的内存使用\t//就是发送队列使用的内存总量，比如果说TCP清重传队列的时候就会移除了\tint\t\t\tsk_wmem_queued;   \t//这个与上面的队列类似，但是是记录方向三个部分的内存，分别为传输层，tc，网卡队列中存的包总量\t//是在发送方向释放数据包的时候减少的，也就是网卡驱动释放数据包的时候减少的\trefcount_t\t\tsk_wmem_alloc;\t////tcp小发送队列的的标志，设置标志位后当协议站中发送缓冲区的数据大于这个队列的数据时不会立即发送会\t//用taskelt来调度发送。\tunsigned long\t\tsk_tsq_flags; \tunion &#123;\t\tstruct sk_buff\t*sk_send_head;\t\tstruct rb_root\ttcp_rtx_queue; //二叉树，管理tcp的重传队列\t&#125;;\tstruct sk_buff_head\tsk_write_queue; //发送队列\t__s32\t\t\tsk_peek_off;       //peek的偏移\tint\t\t\tsk_write_pending;      //记录当前套接字写操作的阻塞的数量\t__u32\t\t\tsk_dst_pending_confirm;   //标识路由是否需要验证，比如查看邻居表项是否有效，收到tcpack可能会置位\tu32\t\t\tsk_pacing_status; /* see enum sk_pacing */   //是否是tcppacing 状态，比如是否启用tcp_pacing\tlong\t\t\tsk_sndtimeo;   //发送方向缓冲区满的时候，允许等待的时间，如果哦时间到了就会返回nobuf\tstruct timer_list\tsk_timer;   //sock的定时器，例如tcp保活用到这个定时器\t__u32\t\t\tsk_priority;     //sock的优先级，可以通过setsockopt设置，会影响tc的调度\t__u32\t\t\tsk_mark;     //会影响路由或者ovs的差表逻辑，通过setsockopt设置，只能通过setsockopt设置吗\tunsigned long\t\tsk_pacing_rate; /* bytes per second */ //tcppacing速率\tunsigned long\t\tsk_max_pacing_rate;   //tcppacing的最大速率\t//alloc page后会用这个结构管理page 例如在ip_append中会alloc page,与skb的非线性部分密切相关\tstruct page_frag\tsk_frag;\t\t\t\tnetdev_features_t\tsk_route_caps;  //这里是设备支持的能力\tint\t\t\tsk_gso_type;\t\t\t//GSO的类型\tunsigned int\t\tsk_gso_max_size; //GSO分段的最大长度\tgfp_t\t\t\tsk_allocation;   //分配\t//   //发送方向的hash值作用是赋值到skb的txhash字段中，比如在xps的时候会用到，这个hash值好像是随机值\t__u32\t\t\tsk_txhash; \t\t/*\t * Because of non atomicity rules, all\t * changes are protected by socket lock.\t */\tu8\t\t\tsk_gso_disabled : 1,  //是否使能gso\t\t\t\tsk_kern_sock : 1, \t\t//是否是内核创建的socket\t\t\t\tsk_no_check_tx : 1,    //setsockopt设置，是否不需要计算校验和\t\t\t\tsk_no_check_rx : 1,\t\t//rx方向\t\t\t\tsk_userlocks : 4;      //几个标志位，设置后防止修改某些参数，比如缓冲区大小\tu8\t\t\tsk_pacing_shift;\t\t//tsq相关\tu16\t\t\tsk_type;\t\t\t\t//用户创建socket时候设置的type比如DGRAM\tu16\t\t\tsk_protocol;\t\t\t//用户创建socket时候设置的协议类型 比如UDP\tu16\t\t\tsk_gso_max_segs;\t\t//分段最大数量\tunsigned long\t        sk_lingertime;  //用于tcp的关闭中，不为0的时候会等待配置的时间等数据包发送完后关闭\tstruct proto\t\t*sk_prot_creator;   //具体协议的ops集合\trwlock_t\t\tsk_callback_lock;\tint\t\t\tsk_err,  \t\t\t\t// 保存错误类型\t\t\t\tsk_err_soft;\tu32\t\t\tsk_ack_backlog;  \t\t//TCP中三次握手完成，但是没有被accept的数量\tu32\t\t\tsk_max_ack_backlog;\t\t//这个是listen系统调用配置的\tkuid_t\t\t\tsk_uid;\t\t\t\t//用户的id创建socket的时候赋值的\tu8\t\t\tsk_txrehash;\t\t\t//setsockopt设置，是否需要重新计算发送的txhash值从而影响队列的选择#ifdef CONFIG_NET_RX_BUSY_POLL\tu8\t\t\tsk_prefer_busy_poll;\tu16\t\t\tsk_busy_poll_budget;#endif\tspinlock_t\t\tsk_peer_lock;   //setsockopt设置peer相关的时候用到的锁\tint\t\t\tsk_bind_phc;\tstruct pid\t\t*sk_peer_pid;   //unix用的？ 进程间通信\tconst struct cred\t*sk_peer_cred;  //安全相关\tlong\t\t\tsk_rcvtimeo;  //接收的超时时间\tktime_t\t\t\tsk_stamp;\t//时间戳，用户调用setsockopt的时候会设置，把skb的时间戳保存到这里#if BITS_PER_LONG==32\tseqlock_t\t\tsk_stamp_seq;#endif\tatomic_t\t\tsk_tskey;\t\t\t//当通过set_sockopt设置时间戳信息的时候会把这个字段赋值tcp的序列号\tatomic_t\t\tsk_zckey;\t\t//零拷贝相关，几乎用不到吧\tu32\t\t\tsk_tsflags;\t\t\t//时间戳的标志位，通过setsockopt设置，会根据标志把时间戳信息放到msg中给用户\tu8\t\t\tsk_shutdown;\tu8\t\t\tsk_clockid;\tu8\t\t\tsk_txtime_deadline_mode : 1, //setsockopt设置的没发现哪里使用\t\t\t\tsk_txtime_report_errors : 1, ////setsockopt设置的没发现哪里使用\t\t\t\tsk_txtime_unused : 6;\t\t//未使用的\tbool\t\t\tsk_use_task_frag;\t\t//表示是否从当前进程获取一个page的一部分，initsock的时候设置为true，ip_append_data中会用到\t\tstruct socket\t\t*sk_socket;  //关联的socket\tvoid\t\t\t*sk_user_data;#ifdef CONFIG_SECURITY\tvoid\t\t\t*sk_security;#endif\tstruct sock_cgroup_data\tsk_cgrp_data;  //cgroup相关\tstruct mem_cgroup\t*sk_memcg;\t\t\t//  //cgroup相关\tvoid\t\t\t(*sk_state_change)(struct sock *sk); //唤醒睡眠的进程，比如tcp状态发生改变的时候调用\tvoid\t\t\t(*sk_data_ready)(struct sock *sk); //软中断收到数据包，唤醒睡眠的进程\tvoid\t\t\t(*sk_write_space)(struct sock *sk);//有写的空间，唤醒,好像几乎不会被调用\tvoid\t\t\t(*sk_error_report)(struct sock *sk);\tint\t\t\t(*sk_backlog_rcv)(struct sock *sk,   //当sock被用户访问的时候，会把报放到后备队列\t\t\t\t\t\t  struct sk_buff *skb);#ifdef CONFIG_SOCK_VALIDATE_XMIT\tstruct sk_buff*\t\t(*sk_validate_xmit_skb)(struct sock *sk,\t\t\t\t\t\t\tstruct net_device *dev,\t\t\t\t\t\t\tstruct sk_buff *skb);#endif\tvoid                    (*sk_destruct)(struct sock *sk); //销毁套接字的回调\tstruct sock_reuseport __rcu\t*sk_reuseport_cb;#ifdef CONFIG_BPF_SYSCALL\tstruct bpf_local_storage __rcu\t*sk_bpf_storage;#endif\tstruct rcu_head\t\tsk_rcu;\t\t//rcu锁，好像没用到？？\tnetns_tracker\t\tns_tracker;   //网络命名空间，好像跟trace有关\tstruct hlist_node\tsk_bind2_node;  //tcp使用，比如连接的时候会把这个node与bhash和bhash2关联起来&#125;;\n\n","categories":["网络协议栈源码学习"],"tags":["sock"]},{"title":"socket系统调用","url":"/2025/05/13/socket%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8/","content":"简介socket 是网络编程中最基本的系统调用之一，用于创建一个网络通信的“端点”（即套接字，socket）。\n一、函数原型int socket(int domain, int type, int protocol);\n\n\n\n\n参数\n说明\n\n\n\ndomain\n协议族，比如：AF_INET 表示 IPv4\n\n\ntype\n套接字类型，比如：SOCK_STREAM 表示 TCP\n\n\nprotocol\n指定使用的协议，通常填 0 让系统自动选择\n\n\n二、函数调用流程用户态程序调用 socket()        ↓glibc 中封装的socket接口        ↓内部通过 syscall 指令        ↓进入内核，执行 sys_socket()\n三、代码分析1. glibc层代码分析代码位置：glibc-2.40\\sysdeps\\unix\\sysv\\linux\\socket.c\nint __socket (int fd, int type, int domain)&#123;#ifdef __ASSUME_SOCKET_SYSCALL //是否支持单独的系统调用号，肯定支持  return INLINE_SYSCALL_CALL (socket, fd, type, domain);#else\t//32位的处理器会走这个分支  return SOCKETCALL (socket, fd, type, domain);#endif&#125;libc_hidden_def (__socket)//暴露给外部程序的符号为socket() 其实是__socket()的别名weak_alias (__socket, socket)\n\n用户应用程序调用socket()实际调用的的是上述glibc中的__socket，在__socket中 INLINE_SYSCALL_CALL宏经过一系列展开后变成宏__INLINE_SYSCALL4 ，这个宏会进一步再展开，如下所示：\n#define __INLINE_SYSCALL4(name, a1, a2, a3, a4) \\  INLINE_SYSCALL (name, 4, a1, a2, a3, a4)\n\n上述的INLINE_SYSCALL 展开，结果如下所示：\n#define INLINE_SYSCALL(name, nr, args...)\t\t\t\t\\  (&#123;\t//sc_ret为系统调用的返回值    long int sc_ret = INTERNAL_SYSCALL (name, nr, args);\t//对返回值好像要简单检查一下    __glibc_unlikely (INTERNAL_SYSCALL_ERROR_P (sc_ret))\t\t\\    ? SYSCALL_ERROR_LABEL (INTERNAL_SYSCALL_ERRNO (sc_ret))\t\t\\    : sc_ret;\t\t\t\t\t\t\t\t\\  &#125;)\n\nINTERNAL_SYSCALL 进一步在展开，会变成了internal_syscall3\n这里举个例子，例如，未展开前为INTERNAL_SYSCALL(socket, 3, AF_INET, SOCK_STREAM, 0) 展开后会变成\ninternal_syscall3(__NR_socket, AF_INET, SOCK_STREAM, 0)，如果在x86架构中宏__NR_socket为41\n在glibc-2.40\\sysdeps\\unix\\sysv\\linux\\x86_64\\64\\arch-syscall.h 中有定义\n#define __NR_socket 41\n\n总之，上述宏经过一系列展开后变成了如下函数，其中number为系统调用号，arg1, arg2, arg3 为传入的参数\n#define internal_syscall3(number, arg1, arg2, arg3)\t\t\t\\(&#123;\t\t\t\t\t\t\t\t\t\\    unsigned long int resultvar;\t\t\t\t\t\\    TYPEFY (arg3, __arg3) = ARGIFY (arg3);\t\t\t \t\\    TYPEFY (arg2, __arg2) = ARGIFY (arg2);\t\t\t \t\\    TYPEFY (arg1, __arg1) = ARGIFY (arg1);\t\t\t \t\\    register TYPEFY (arg3, _a3) asm (&quot;rdx&quot;) = __arg3;\t\t\t\\    register TYPEFY (arg2, _a2) asm (&quot;rsi&quot;) = __arg2;\t\t\t\\    register TYPEFY (arg1, _a1) asm (&quot;rdi&quot;) = __arg1;\t\t\t\\    asm volatile (\t\t\t\t\t\t\t\\    &quot;syscall\\n\\t&quot;\t\t\t\t\t\t\t\\    : &quot;=a&quot; (resultvar)\t\t\t\t\t\t\t\\    : &quot;0&quot; (number), &quot;r&quot; (_a1), &quot;r&quot; (_a2), &quot;r&quot; (_a3)\t\t\t\\    : &quot;memory&quot;, REGISTERS_CLOBBERED_BY_SYSCALL);\t\t\t\\    (long int) resultvar;\t\t\t\t\t\t\\&#125;)\n\n上述代码的作用是将系统调用号和 3 个参数分别放入规定的寄存器（RAX, RDI, RSI, RDX），然后执行 syscall 指令，并将返回值保存到 resultvar：\n“memory”, REGISTERS_CLOBBERED_BY_SYSCALL 的意思是告诉编译器不要优化这段代码\n下面附上x86-64 Linux 的 syscall 调用约定传参的寄存器\n\n\n\n参数\n寄存器\n\n\n\nsyscall 编号\nRAX\n\n\n参数1\nRDI\n\n\n参数2\nRSI\n\n\n参数3\nRDX\n\n\n参数4\nR10\n\n\n参数5\nR8\n\n\n参数6\nR9\n\n\n2.内核代码分析执行上述syscall指令后CPU 会根据 MSR 寄存器（Model Specific Registers）跳转到系统调用函数，例如，x86架构则会进入到系统调用的统一入口函数entry_SYSCALL_64\nentry_SYSCALL_64函数的注册（也就把地址写入MSR寄存器）在syscall_init中完成代码如下：\nvoid syscall_init(void)&#123;\twrmsr(MSR_STAR, 0, (__USER32_CS &lt;&lt; 16) | __KERNEL_CS);\t//把entry_SYSCALL_64 的地址写入MSR_LSTAR寄存器\twrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64); \t...&#125;\n\n上述syscall_init 函数在start_kernel（）中被调用\n接下里看一下entry_SYSCALL_64的逻辑，\nSYM_CODE_START(entry_SYSCALL_64)\tUNWIND_HINT_EMPTY\t//这个是切换GS寄存器，这个寄存器是访问per-cpu变量的基址task_struct结构就依赖这个寄存器间接获得\tswapgs\t/* tss.sp2 is scratch space. */\t//将当前栈指针保存\tmovq\t%rsp, PER_CPU_VAR(cpu_tss_rw + TSS_sp2)\t//切换页表\tSWITCH_TO_KERNEL_CR3 scratch_reg=%rsp\t//切换当前栈指针到当前 CPU 的内核栈顶指针\tmovq\tPER_CPU_VAR(cpu_current_top_of_stack), %rspSYM_INNER_LABEL(entry_SYSCALL_64_safe_stack, SYM_L_GLOBAL)\t/* Construct struct pt_regs on stack */\t//把用户态的一些信息入栈（貌似就是pt_regs）\tpushq\t$__USER_DS\t\t\t\t/* pt_regs-&gt;ss */\tpushq\tPER_CPU_VAR(cpu_tss_rw + TSS_sp2)\t/* pt_regs-&gt;sp */\tpushq\t%r11\t\t\t\t\t/* pt_regs-&gt;flags */\tpushq\t$__USER_CS\t\t\t\t/* pt_regs-&gt;cs */\tpushq\t%rcx\t\t\t\t\t/* pt_regs-&gt;ip */SYM_INNER_LABEL(entry_SYSCALL_64_after_hwframe, SYM_L_GLOBAL)\t//这里存了系统调用号\tpushq\t%rax\t\t\t\t\t/* pt_regs-&gt;orig_ax */\t//其他寄存器的信息继续保存到栈中\tPUSH_AND_CLEAR_REGS rax=$-ENOSYS\t/* IRQs are off. */\t//把系统调用号放入 rdi → 作为函数的第 1 个参数\tmovq\t%rax, %rdi\t//把 pt_regs 地址放入 rsi → 作为第 2 个参数\tmovq\t%rsp, %rsi\t//调用do_syscall_64\tcall\tdo_syscall_64\t\t/* returns with IRQs disabled */\t...\tSYM_CODE_END(entry_SYSCALL_64)\n\n在用户执行 syscall 指令后，CPU 跳转到entry_SYSCALL_64，完成从用户态到内核态的切换、栈构造、参数准备，并调用 C 函数 do_syscall_64() 来处理系统调用。\n接下来看一下上述汇编代码中调用do_syscall_64 代码位于arch\\x86\\entry\\common.c中\n#ifdef CONFIG_X86_64__visible noinstr void do_syscall_64(unsigned long nr, struct pt_regs *regs)&#123;\t//这里开启了中断（进入系统调用前要先关中断在哪里没找到，好像sycall后会自动关）追踪，安全相关\tnr = syscall_enter_from_user_mode(regs, nr);\t//禁止插桩，\tinstrumentation_begin();\t//检查系统调用号是否在合法范围\tif (likely(nr &lt; NR_syscalls)) &#123;\t\tnr = array_index_nospec(nr, NR_syscalls);\t\t//nr存的是系统调用好，从系统调用表中找到对应的处理函数，然后将\t\t//返回值存入到regs-&gt;ax寄存器\t\tregs-&gt;ax = sys_call_table[nr](regs);        //32位#ifdef CONFIG_X86_X32_ABI\t&#125; else if (likely((nr &amp; __X32_SYSCALL_BIT) &amp;&amp;\t\t\t  (nr &amp; ~__X32_SYSCALL_BIT) &lt; X32_NR_syscalls)) &#123;\t\tnr = array_index_nospec(nr &amp; ~__X32_SYSCALL_BIT,\t\t\t\t\tX32_NR_syscalls);\t\tregs-&gt;ax = x32_sys_call_table[nr](regs);#endif\t&#125;\tinstrumentation_end();\tsyscall_exit_to_user_mode(regs);&#125;#endif\n\n上述代码中最关键的就是regs-&gt;ax = sys_call_table[nr](regs)找到对应的系统调用函数，并传入pt_regs，pt_regs保存了参数和用户的一些信息。\nsys_call_table[]就是一个函数指针，每个元素指向具体的函数实现，定义如下所示:\nasmlinkage const sys_call_ptr_t sys_call_table[__NR_syscall_max+1] = &#123;\t/*\t * Smells like a compiler bug -- it doesn&#x27;t work\t * when the &amp; below is removed.\t */\t[0 ... __NR_syscall_max] = &amp;__x64_sys_ni_syscall,//下面这个头文件貌似是编译生成的，include进来后就替换了上面的默认值#include &lt;asm/syscalls_64.h&gt;&#125;;\n\n上述 &lt;asm/syscalls_64.h&gt;是在编译过程中生成的，具体的流程如下：\nsyscall_64.tbl   ↓（作为输入）syscalltbl.sh 脚本   ↓（生成）syscalls_64.h   ↓（#include）用于填充 sys_call_table[]\n上述流程在\\arch\\x86\\entry\\syscalls中有体现。下面展示了部分syscall_64.tbl中的内容\n0\tcommon\tread\t\t\tsys_read1\tcommon\twrite\t\t\tsys_write2\tcommon\topen\t\t\tsys_open3\tcommon\tclose\t\t\tsys_close·········41\tcommon\tsocket\t\t\tsys_socket·········440\tcommon\tprocess_madvise\t\tsys_process_madvise\n\n系统调用号 41 对应的函数指针在 sys_call_table[]被syscalltbl.sh 处理后，最终指向 __x64_sys_socket 这个函数，而这个函数和SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol) 宏展开后是一个函数！\n","categories":["网络协议栈源码学习"],"tags":["socket"]},{"title":"sock的创建与初始化","url":"/2025/05/24/sock%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E5%88%9D%E5%A7%8B%E5%8C%96/","content":"以IPv4协议族为例，当用户态执行socket系统调用后，会调用到inet_create(),在inet_create()中会创建与socket关联的sock结构体，具体代码如下：\nstatic int inet_create(struct net *net, struct socket *sock, int protocol,\t\t       int kern)&#123;    .....\t//注意： 这里申请一个sock结构，这个sock结构可以理解为传输层协议和socket之间的一个中间层\t//对上提供socket层的结构，\t//对下与具体的协议相关\t//kern 标识这个套接字是否是内核创建的\tsk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot, kern);\tif (!sk)\t\tgoto out;    .....\t//这里初始化了上面申请的sock结构体的各个字段\tsock_init_data(sock, sk);    .....&#125;\n\nsk_alloc()上述sk_alloc()其实就是使用slab分配其分配了prot-&gt;size大小的的内存，也就是说分配了一个比sock结构体size还要大的内存，举个例子，如果是TCP协议，则分配的大小为sizeof(tcp_sock),也就是说tcp_sock内嵌了sock结构体，类似继承的关系，关系如下图所示：\n\n接下来具体看一下sk_alloc()的实现：\nstruct sock *sk_alloc(struct net *net, int family, gfp_t priority,\t\t      struct proto *prot, int kern)&#123;\tstruct sock *sk;\t//调用slab 分配一个sk结构体，注意这个结构体的大小取决与prot参数的size字段\t//__GFP_ZERO表示为内存申请后需要清零的标志位\tsk = sk_prot_alloc(prot, priority | __GFP_ZERO, family);\tif (sk) &#123;\t\t//设置协议族\t\tsk-&gt;sk_family = family;\t\t/*\t\t * See comment in struct sock definition to understand\t\t * why we need sk_prot_creator -acme\t\t */\t\t//这里很关键，将具体协议的prot关联到了sock上\t\tsk-&gt;sk_prot = sk-&gt;sk_prot_creator = prot;\t\t//记录是否是内核创建的\t\tsk-&gt;sk_kern_sock = kern;\t\tsock_lock_init(sk);\t\t//如果是用户进程创建的，就增加网络命名空间的引用计数\t\tsk-&gt;sk_net_refcnt = kern ? 0 : 1;\t\t//更新当前core上所有活跃套接字的引用计数。\t\tif (likely(sk-&gt;sk_net_refcnt)) &#123;\t\t\tget_net_track(net, &amp;sk-&gt;ns_tracker, priority);\t\t\tsock_inuse_add(net, 1);\t\t&#125; else &#123;\t\t\t__netns_tracker_alloc(net, &amp;sk-&gt;ns_tracker,\t\t\t\t\t      false, priority);\t\t&#125;\t\t//将sock与网络命名空间关联\t\tsock_net_set(sk, net);\t\t//发送缓冲区引用计数加1\t\trefcount_set(&amp;sk-&gt;sk_wmem_alloc, 1);\t\tmem_cgroup_sk_alloc(sk);\t\tcgroup_sk_alloc(&amp;sk-&gt;sk_cgrp_data);\t\tsock_update_classid(&amp;sk-&gt;sk_cgrp_data);\t\tsock_update_netprioidx(&amp;sk-&gt;sk_cgrp_data);\t\tsk_tx_queue_clear(sk);\t&#125;\treturn sk;&#125;\n\n上述代码中调用sk_prot_alloc() 根据不同的prot(也就是不同的协议)申请sock，同时设置了__GFP_ZERO 标志，表示需要将申请的内存memset，sk_prot_alloc() 的具体实现如下：\nstatic struct sock *sk_prot_alloc(struct proto *prot, gfp_t priority,\t\tint family)&#123;\tstruct sock *sk;\tstruct kmem_cache *slab;\t//从slab分配器中拿一个结构体，这个slabchace是inet_init中初始化的\tslab = prot-&gt;slab;\tif (slab != NULL) &#123;\t\tsk = kmem_cache_alloc(slab, priority &amp; ~__GFP_ZERO);\t\tif (!sk)\t\t\treturn sk;\t\t//是否需要memset，可以看到这里的大小是objsize\t\tif (want_init_on_alloc(priority))\t\t\tsk_prot_clear_nulls(sk, prot-&gt;obj_size);\t&#125; else\t//如果没使用slab就用kmalloc，kamlloc不也是slab吗？\t\tsk = kmalloc(prot-&gt;obj_size, priority);\tif (sk != NULL) &#123;\t\t//安全相关\t\tif (security_sk_alloc(sk, family, priority))\t\t\tgoto out_free;\t\t//增加引用计数\t\tif (!try_module_get(prot-&gt;owner))\t\t\tgoto out_free_sec;\t&#125;\treturn sk;out_free_sec:\tsecurity_sk_free(sk);out_free:\tif (slab != NULL)\t\tkmem_cache_free(slab, sk);\telse\t\tkfree(sk);\treturn NULL;&#125;\n\n从上述代码可知是通过slab = prot-&gt;slab;获取了一个slab对象，这个slab管理的结构体大小是inet_init()中调用proto_register()创建slab时候确定的，具体函数如下所示：\nint proto_register(struct proto *prot, int alloc_slab)&#123;\t......\tif (alloc_slab) &#123;\t\tprot-&gt;slab = kmem_cache_create_usercopy(prot-&gt;name,\t\t\t\t\tprot-&gt;obj_size, 0,\t\t\t\t\tSLAB_HWCACHE_ALIGN | SLAB_ACCOUNT |\t\t\t\t\tprot-&gt;slab_flags,\t\t\t\t\tprot-&gt;useroffset, prot-&gt;usersize,\t\t\t\t\tNULL);\t&#125;\t.......&#125;\n\n可以看到在上面创建slab缓存时，指定的size大小为prot-&gt;obj_size，例如tcp协议的obj_size则为sizeof(tcp_sock)结构体的大小。注意到上述创建slab缓存使用的接口是kmem_cache_create_usercopy，这个usercopy的意义是允许指定对象拷贝到用户空间的内存区域。prot-&gt;useroffset指的是对象中允许拷贝到用户空间的数据区域的偏移。prot-&gt;usersize指的是允许拷贝到用户空间的数据区域的大小。（tcp等协议这个字段好像都是空）\nsock_init_data()上述创建了sock结构体后紧接着就会调用sock_init_data完成初始化，sock_init_data中先从socket结构体中关联的inode中获取uid然后调用sock_init_data_uid()完成初始化，上述两个函数的代码如下：\nvoid sock_init_data(struct socket *sock, struct sock *sk)&#123;\t//注意：这里的uid是在创建socket和inode的时候设置的\t//i_uid用于表示与该 inode 关联的文件或对象的所有者用户ID\tkuid_t uid = sock ?\t\tSOCK_INODE(sock)-&gt;i_uid :\t\tmake_kuid(sock_net(sk)-&gt;user_ns, 0);\tsock_init_data_uid(sock, sk, uid);&#125;\n\nvoid sock_init_data_uid(struct socket *sock, struct sock *sk, kuid_t uid)&#123;\tsk_init_common(sk);\tsk-&gt;sk_send_head\t=\tNULL;\ttimer_setup(&amp;sk-&gt;sk_timer, NULL, 0);\tsk-&gt;sk_allocation\t=\tGFP_KERNEL;\t//设置接收和发送默认缓冲区大小\tsk-&gt;sk_rcvbuf\t\t=\tREAD_ONCE(sysctl_rmem_default);\tsk-&gt;sk_sndbuf\t\t=\tREAD_ONCE(sysctl_wmem_default);\t//即使是udp也设置状态为TCP_CLOSE\tsk-&gt;sk_state\t\t=\tTCP_CLOSE;\t//好像跟内存分配相关\tsk-&gt;sk_use_task_frag\t=\ttrue;\t//这里关联了sock与socket\tsk_set_socket(sk, sock);\tsock_set_flag(sk, SOCK_ZAPPED);\tif (sock) &#123;\t\t//设置用户配置的type类型给sock\t\tsk-&gt;sk_type\t=\tsock-&gt;type;\t\t//初始化套接字的等待队列\t\tRCU_INIT_POINTER(sk-&gt;sk_wq, &amp;sock-&gt;wq);\t\tsock-&gt;sk\t=\tsk;\t&#125; else &#123;\t\tRCU_INIT_POINTER(sk-&gt;sk_wq, NULL);\t&#125;\t//设置sock的uid\tsk-&gt;sk_uid\t=\tuid;\t\t//锁相关，没看太懂\trwlock_init(&amp;sk-&gt;sk_callback_lock);\tif (sk-&gt;sk_kern_sock)\t\tlockdep_set_class_and_name(\t\t\t&amp;sk-&gt;sk_callback_lock,\t\t\taf_kern_callback_keys + sk-&gt;sk_family,\t\t\taf_family_kern_clock_key_strings[sk-&gt;sk_family]);\telse\t\tlockdep_set_class_and_name(\t\t\t&amp;sk-&gt;sk_callback_lock,\t\t\taf_callback_keys + sk-&gt;sk_family,\t\t\taf_family_clock_key_strings[sk-&gt;sk_family]);\tsk-&gt;sk_state_change\t=\tsock_def_wakeup; //唤醒睡眠的进程，比如tcp状态发生改变的时候调用\tsk-&gt;sk_data_ready\t=\tsock_def_readable; //软中断收到数据包，唤醒睡眠的进程\tsk-&gt;sk_write_space\t=\tsock_def_write_space;//有写的空间，唤醒,好像几乎不会被调用\tsk-&gt;sk_error_report\t=\tsock_def_error_report;\tsk-&gt;sk_destruct\t\t=\tsock_def_destruct; //销毁套接字的回调\tsk-&gt;sk_frag.page\t=\tNULL;\tsk-&gt;sk_frag.offset\t=\t0;\tsk-&gt;sk_peek_off\t\t=\t-1;  //peek的偏移量\tsk-&gt;sk_peer_pid \t=\tNULL; //对端的进程id，同一个主机上才有吧？\tsk-&gt;sk_peer_cred\t=\tNULL; //也是对端的信息\tspin_lock_init(&amp;sk-&gt;sk_peer_lock);\tsk-&gt;sk_write_pending\t=\t0;  //写缓存区没有空间了\tsk-&gt;sk_rcvlowat\t\t=\t1;  //唤醒相关的水位线？1表示一个字节也唤醒\tsk-&gt;sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT; //设置接收的超时时间 全F\tsk-&gt;sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT; ////设置发送的超时时间 全F\tsk-&gt;sk_stamp = SK_DEFAULT_STAMP;#if BITS_PER_LONG==32\tseqlock_init(&amp;sk-&gt;sk_stamp_seq);#endif\tatomic_set(&amp;sk-&gt;sk_zckey, 0);#ifdef CONFIG_NET_RX_BUSY_POLL\tsk-&gt;sk_napi_id\t\t=\t0;\tsk-&gt;sk_ll_usec\t\t=\tREAD_ONCE(sysctl_net_busy_read);#endif\tsk-&gt;sk_max_pacing_rate = ~0UL;  //发送速率相关，tcp拥塞控制的时候会用到bbr算法会用到\tsk-&gt;sk_pacing_rate = ~0UL;\tWRITE_ONCE(sk-&gt;sk_pacing_shift, 10);\tsk-&gt;sk_incoming_cpu = -1;   //记录属于哪个cpu\tsk_rx_queue_clear(sk);\t/*\t * Before updating sk_refcnt, we must commit prior changes to memory\t * (Documentation/RCU/rculist_nulls.rst for details)\t */\tsmp_wmb();\trefcount_set(&amp;sk-&gt;sk_refcnt, 1);\tatomic_set(&amp;sk-&gt;sk_drops, 0);&#125;EXPORT_SYMBOL(sock_init_data_uid);\n\n上述代码主要做了如下几个个事情，调用sk_init_common 初始化sock的接受队列和发送队列，这个错误队列好像是ip层收到icmp的错误报文，会放到这个错误队列中。\nstatic void sk_init_common(struct sock *sk)&#123;\t//初始化接收，发送和错误队列。\tskb_queue_head_init(&amp;sk-&gt;sk_receive_queue);\tskb_queue_head_init(&amp;sk-&gt;sk_write_queue);\tskb_queue_head_init(&amp;sk-&gt;sk_error_queue);\trwlock_init(&amp;sk-&gt;sk_callback_lock);\t//锁相关没太懂\tlockdep_set_class_and_name(&amp;sk-&gt;sk_receive_queue.lock,\t\t\taf_rlock_keys + sk-&gt;sk_family,\t\t\taf_family_rlock_key_strings[sk-&gt;sk_family]);\tlockdep_set_class_and_name(&amp;sk-&gt;sk_write_queue.lock,\t\t\taf_wlock_keys + sk-&gt;sk_family,\t\t\taf_family_wlock_key_strings[sk-&gt;sk_family]);\tlockdep_set_class_and_name(&amp;sk-&gt;sk_error_queue.lock,\t\t\taf_elock_keys + sk-&gt;sk_family,\t\t\taf_family_elock_key_strings[sk-&gt;sk_family]);\tlockdep_set_class_and_name(&amp;sk-&gt;sk_callback_lock,\t\t\taf_callback_keys + sk-&gt;sk_family,\t\t\taf_family_clock_key_strings[sk-&gt;sk_family]);&#125;\n\n然后在sock_init_data_uid()中初始化了接收缓冲区和发送缓冲区的大小，将socket的sock字段指向当前的sock结构，然后注册唤醒进程睡眠的相关函数，例如收包的唤醒函数，有空间可写的回调函数，tcp状态发生改变的回调函数。\n","categories":["网络协议栈源码学习"],"tags":["sock"]},{"title":"tcp_sock","url":"/2025/07/15/tcp_sock/","content":"struct tcp_sock &#123;\t/* inet_connection_sock has to be the first member of tcp_sock */\tstruct inet_connection_sock\tinet_conn;\t\t\t\t//inet_connection_sock\tu16\ttcp_header_len;\t/* Bytes of tcp header to send\t*/\t//tcp头部长度\tu16\tgso_segs;\t/* Max number of segs per GSO packet\t*///分段数量/* *\tHeader prediction flags *\t0x5?10 &lt;&lt; 16 + snd_wnd in net byte order */\t\t__be32\tpred_flags;\t\t\t\t\t\t\t\t\t//一个标志位根据首部长和ack还有窗口大小计算，用于预测是否能走快速路径收包/* *\tRFC793 variables by their proper names. This means you can *\tread the code and the spec side by side (and laugh ...) *\tSee RFC793 and RFC1122. The RFC writes these in capitals. */\tu64\tbytes_received;\t/* RFC4898 tcpEStatsAppHCThruOctetsReceived  //实际接收的字节数，也就是确认的字节数\t\t\t\t * sum(delta(rcv_nxt)), or how many bytes\t\t\t\t * were acked.\t\t\t\t */\tu32\tsegs_in;\t/* RFC4898 tcpEStatsPerfSegsIn             //接收了多少个段\t\t\t\t * total number of segments in.\t\t\t\t */\tu32\tdata_segs_in;\t/* RFC4898 tcpEStatsPerfDataSegsIn\t\t//与上面类似，表示接收了多少个有payload的段\t\t\t\t * total number of data segments in.\t\t\t\t */ \tu32\trcv_nxt;\t/* What we want to receive next \t*/       //下一个想要接收报文的序号\tu32\tcopied_seq;\t/* Head of yet unread data\t\t*/\t\t\t\t//用户进程已经读取到的位置\tu32\trcv_wup;\t/* rcv_nxt on last window update sent\t*/\t\t//上一次更新的rcv_nxt，延迟ack可能会用到 \tu32\tsnd_nxt;\t/* Next sequence we send\t\t*/\t\t\t\t//下一个待发送的序列号\tu32\tsegs_out;\t/* RFC4898 tcpEStatsPerfSegsOut\t\t\t\t\t//发出去的段数\t\t\t\t * The total number of segments sent.\t\t\t\t */\tu32\tdata_segs_out;\t/* RFC4898 tcpEStatsPerfDataSegsOut\t\t\t//发出去的段数，包括没有payload的\t\t\t\t * total number of data segments sent.\t\t\t\t */\tu64\tbytes_sent;\t/* RFC4898 tcpEStatsPerfHCDataOctetsOut\t\t\t//发送出去的字节数，不包括头部长度\t\t\t\t * total number of data bytes sent.\t\t\t\t */\tu64\tbytes_acked;\t/* RFC4898 tcpEStatsAppHCThruOctetsAcked\t//对端已经确认的字节总数\t\t\t\t * sum(delta(snd_una)), or how many bytes\t\t\t\t * were acked.\t\t\t\t */\tu32\tdsack_dups;\t/* RFC4898 tcpEStatsStackDSACKDups\t\t\t\t//重复ack的数量，可能是\t\t\t\t * total number of DSACK blocks received\t\t\t\t */ \tu32\tsnd_una;\t/* First byte we want an ack for\t*/\t\t\t//发送出去未确认的序号 \tu32\tsnd_sml;\t/* Last byte of the most recently transmitted small packet */  //发送小于mss的数据包的最后一个字节的序列号 ，nagle算法用到\tu32\trcv_tstamp;\t/* timestamp of last received ACK (for keepalives) */  //接收数据包的时间戳，tcpack中赋值\tu32\tlsndtime;\t/* timestamp of last sent data packet (for restart window) */\t\t//记录发包时间用于计算rtt\tu32\tlast_oow_ack_time;  /* timestamp of last out-of-window ACK */\t\t//收到了乱续的ack ，可以根据这个时间触发重传\t//延迟或合并 ACK 时，会将当前的 rcv_nxt（接收窗口的下一个期望序列号）暂存到 compressed_ack_rcv_nxt\tu32\tcompressed_ack_rcv_nxt;\t\t\t\t\t\t\t\t\t\t\t\tu32\ttsoffset;\t/* timestamp offset */ //tcp三次握手时间确定这个值，确保相对tcp的时间是单调递增的，计算时间戳的时候会用到\t//将 TCP Socket 挂载到全局的 tsq_tasklet 任务队列中，实现 异步批量释放发送队列内存\tstruct list_head tsq_node; /* anchor in tsq_tasklet.head list */ //把当前sock放入软中断中等待调度\tstruct list_head tsorted_sent_queue; /* time-sorted sent but un-SACKed skbs *///时间排序的已发送但未确认队列，发送的时候会将skb挂到这里\tu32\tsnd_wl1;\t/* Sequence for window update\t\t*/  //发送窗口更新时候的序列号\tu32\tsnd_wnd;\t/* The window we expect to receive\t*/\t\t//发送窗口大小\tu32\tmax_window;\t/* Maximal window ever seen from peer\t*/  //最大接收窗口值,从tcp的窗口字段找到\tu32\tmss_cache;\t/* Cached effective mss, not including SACKS */\tu32\twindow_clamp;\t/* Maximal window to advertise\t\t*/\t//最大缓冲区大小\tu32\trcv_ssthresh;\t/* Current window clamp\t\t\t*/   //慢启动阈值\tu8\tscaling_ratio;\t/* see tcp_win_from_space() */\t\t\t//窗口缩放因子\t/* Information of the most recently (s)acked skb */\tstruct tcp_rack &#123;\t\tu64 mstamp; /* (Re)sent time of the skb */ //记录数据包​​最近一次发送或重传的时间戳\t\tu32 rtt_us;  /* Associated RTT */ //通过 ACK 报文的确认时间与数据包发送时间的差值计算得到\t\tu32 end_seq; /* Ending TCP sequence of the skb */\t//​​最近被确认的数据包的结束序列号\t\tu32 last_delivered; /* tp-&gt;delivered at last reo_wnd adj */  //之前传输已被接受的数量\t\tu8 reo_wnd_steps;   /* Allowed reordering window */  //调整乱续窗口的因子，在收到dsack的时候会更新#define TCP_RACK_RECOVERY_THRESH 16\t\tu8 reo_wnd_persist:5, /* No. of recovery since last adj */ //误判丢包，增加这个值，真是丢包就减少这个值，目的是减少乱续容忍但是对丢包敏感\t\t   dsack_seen:1, /* Whether DSACK seen after last adj */\t\t\t//发送端收到重复选择ack的时候设置\t\t   advanced:1;\t /* mstamp advanced since last lost marking */  //表示更新了rack的字段？可能处罚重传？\t&#125; rack;\tu16\tadvmss;\t\t/* Advertised MSS\t\t\t*/\tu8\tcompressed_ack;\t//收到乱续包的时候会增加  不立即发送ack\tu8\tdup_ack_counter:2, \t//重复ack的数量\t\ttlp_retrans:1,\t/* TLP is a retransmission */ //Tail Loss Probe重传\t\tunused:5;\tu32\tchrono_start;\t/* Start time in jiffies of a TCP chrono */\t//测量 TCP 连接在不同状态下的耗时，例如接收窗口不足的时候\tu32\tchrono_stat[3];\t\t//当前的类型\tu8\tchrono_type:2,\t/* current chronograph type */\t//标记为应用层限速，应该是限制cwnd的增长，initsock的时候就会设这为1\t\trate_app_limited:1,  /* rate_&#123;delivered,interval_us&#125; limited? */\t//TFO 标志通过setsockopt设置，允许发syn包的时候携带数据\t\tfastopen_connect:1, /* FASTOPEN_CONNECT sockopt */\t//在没有cookie的情况下也允许syn包携带数据\t\tfastopen_no_cookie:1, /* Allow send/recv SYN+data without a cookie */\t\t/*接收端收到乱序数据包（如序列号25-30），暂存于out_of_order_queue队列，并通过SACK块（25-31）告知发送端。当接收缓冲区不足（sk_rmem_alloc超过sk_rcvbuf阈值），内核清理乱序队列（调用tcp_prune_ofo_queue），丢弃已SACK确认的数据。发送端后续收到ACK时，发现之前SACK确认的数据未被接收端最终确认（ACK未覆盖SACK范围），判定为SACK reneging，设置is_sack_reneg:1*/\t\tis_sack_reneg:1,    /* in recovery from loss with SACK reneg? */\t\tfastopen_client_fail:2; /* reason why fastopen failed */\t\t//用了3bit 禁用  cork(等待) 和push(立即发送)\tu8\tnonagle     : 4,/* Disable Nagle algorithm?             */\t//线性重传机制，而不是指数退避，setsockopt设置\t\tthin_lto    : 1,/* Use linear timeouts for thin streams */\t//用户获取msg的时候可以获取队列中剩余的字节数\t\trecvmsg_inq : 1,/* Indicate # of bytes in queue upon recvmsg */\t\t//热迁移场景发送数据包的时候会用到\t\trepair      : 1,\t\t//tcp_enter_loss中设置该标志位，用于判断虚假丢包，如果是虚假丢包，就快速撤销丢包状态\t\tfrto        : 1;/* F-RTO (RFC5682) activated in CA_Loss */\tu8\trepair_queue; //setsockopt设置，热迁移场景用到\tu8\tsave_syn:2,\t/* Save headers of SYN packet */ //setsocktopt设置，服务端收到syn包后，决定是否提取包头\t\tsyn_data:1,\t/* SYN includes data */ //syn包是否携带数据，fastopen相关\t\tsyn_fastopen:1,\t/* SYN includes Fast Open option */ //用户设置，是否启用fastopen\t\tsyn_fastopen_exp:1,/* SYN includes Fast Open exp. option */\t\tsyn_fastopen_ch:1, /* Active TFO re-enabling probe */\t\tsyn_data_acked:1,/* data in SYN is acked by SYN-ACK */ //标记syn包携带数据后，是否被对端确认\t\tis_cwnd_limited:1;/* forward progress limited by snd_cwnd? *///由于拥塞窗口无法发包\tu32\ttlp_high_seq;\t/* snd_nxt at the time of TLP */ //丢包探测报文的序号\tu32\ttcp_tx_delay;\t/* delay (in usec) added to TX packets *///setscokopt设置，transmit的时候会add_tx_dleay\tu64\ttcp_wstamp_ns;\t/* departure time for next sent data packet */ //pacing 会用到，每次发送一个数据包后会更新这个值\tu64\ttcp_clock_cache; /* cache last tcp_clock_ns() (see tcp_mstamp_refresh()) *///当前时间缓存从寄存器读出来的时间戳tcp_clock_ns，避免反复获取，就缓存起来了/* RTT measurement */ \tu64\ttcp_mstamp;\t/* most recent packet received/sent */ //收发包的时间戳也是tcp_mstamp_refresh设置的\t//计算rtt相关，清除重传队列的时候会用到下面一系列变量\tu32\tsrtt_us;\t/* smoothed round trip time &lt;&lt; 3 in usecs */ //平滑后的rtt ，会右移3位\tu32\tmdev_us;\t/* medium deviation\t\t\t*/\t//衡量rtt的波动程度\tu32\tmdev_max_us;\t/* maximal mdev for the last rtt period\t*/ //最近 1 个 RTT 周期内最大偏差\tu32\trttvar_us;\t/* smoothed mdev_max\t\t\t*/   //平滑的 RTT 偏差估值，用于最终计算 RTO\tu32\trtt_seq;\t/* sequence number to update rttvar\t*/ //计算rtt对应数据包的序列号\tstruct  minmax rtt_min;  //连接历史中的最小 RTT 值 tcp_ack中调用\tu32\tpackets_out;\t/* Packets which are &quot;in flight&quot;\t*/ //发送的时候会设置这个值， 表示发出去还没有收到ack的数量\tu32\tretrans_out;\t/* Retransmitted packets out\t\t*/ //重传的时候会加这个值，ack的处理中会减\tu32\tmax_packets_out;  /* max packets_out in last window */  //发送的时候会更新这个值，拥塞算法会用到\tu32\tcwnd_usage_seq;  /* right edge of cwnd usage tracking flight */ //同上tcp_cwnd_validate会用到\tu16\turg_data;\t/* Saved octet of OOB data and control flags */ //收到带外数据的时候会设置这个字段？\tu8\tecn_flags;\t/* ECN status bits.\t\t\t*/      //  发送syn包的时候开启ecn会设置这个字段，或者收到网络中的ecn包\tu8\tkeepalive_probes; /* num of allowed keep alive probes\t*/  //set设置，保活探测超过这个设置就直接error\tu32\treordering;\t/* Packet reordering metric.\t\t*/     //乱序容忍度，newreno会用到，其他地方还会用到吗？\tu32\treord_seen;\t/* number of data packet reordering events */   //乱序包的数量，rack会用到\tu32\tsnd_up;\t\t/* Urgent pointer\t\t*/    //发送方会设置，oob数据的序号/* *      Options received (usually on last packet, some only on SYN packets). */\tstruct tcp_options_received rx_opt; //存储接收到的tcp选项，时间戳，scak，mss/* *\tSlow start and congestion control (see also Nagle, and Karn &amp; Partridge) */ \tu32\tsnd_ssthresh;\t/* Slow start size threshold\t\t*/ //snd_ssthresh慢启动阈值 \tu32\tsnd_cwnd;\t/* Sending congestion window\t\t*/  //发送放能发多少个mss\tu32\tsnd_cwnd_cnt;\t/* Linear increase counter\t\t*/\t//tcpack中处理拥塞的时候会根据ack设置并使用该值如bic\tu32\tsnd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */  //拥塞窗口的最大值\tu32\tsnd_cwnd_used;\t\t\t\t\t\t\t\t\t\t// 等于packetout 为发送出去还没有确认的包数\tu32\tsnd_cwnd_stamp;\t\t\t\t\t\t\t\t\t\t// 每次调整拥塞窗口的时间戳，在发送方调整拥塞窗口的时候会用到\tu32\tprior_cwnd;\t/* cwnd right before starting loss recovery */ //enterloss 的时候记录丢包前的值\tu32\tprr_delivered;\t/* Number of newly delivered packets to  //影响cwnd sack的数量会决定这个值\t\t\t\t * receiver in Recovery. */\tu32\tprr_out;\t/* Total number of pkts sent during Recovery. *///快恢复状态中发包数量，也包括重传的\tu32\tdelivered;\t/* Total data packets delivered incl. rexmits */\t//被确认的ack总数，包括重传确认的，拥塞控制会用到\tu32\tdelivered_ce;\t/* Like the above but only ECE marked packets */\t\t//ack中带有ece的总数\tu32\tlost;\t\t/* Total data packets lost incl. rexmits */\t\t\t//丢包总数，多个地方会设置\tu32\tapp_limited;\t/* limited until &quot;delivered&quot; reaches this val */  //是否受限与应用程序？发送的时候也会设置 bbr算法会用到，初始化时不为0 收到数据包后会修改这个值\tu64\tfirst_tx_mstamp;  /* start of window send phase */\t\t\t//发送的时候记录的时间戳  ，ack中也会用到，供拥塞算法使用？\tu64\tdelivered_mstamp; /* time we reached &quot;delivered&quot; */\t\t\t\t\t//bbr算法使用发送的时候会设置，处理ack的时候也会设置\tu32\trate_delivered;    /* saved rate sample: packets delivered */\t\t//用于计算发速率 tcp_rate_gen 中被设置\tu32\trate_interval_us;  /* saved rate sample: time elapsed */\t\t\t\t//时间间隔 同上， 这两个字段都是为了计算tcp的实施传输速率 ss命令可以获取 \tu32\trcv_wnd;\t/* Current receiver window\t\t*/\t\t\t\t\t\t//接收窗口的大小\tu32\twrite_seq;\t/* Tail(+1) of data held in tcp send buffer */\t\t\t//发送缓冲区中最后一个字节的下一个序列号\tu32\tnotsent_lowat;\t/* TCP_NOTSENT_LOWAT */\t\t\t\t\t\t\t\t//通过set设置的一个低水位线，会和未发送的字节比较，如果小于，就表示内存不足，好像发送就会阻塞？\tu32\tpushed_seq;\t/* Last pushed seq, required to talk to windows */\t\t//设置push标志位的时候的序列号\tu32\tlost_out;\t/* Lost packets\t\t\t*/\t\t\t\t\t\t\t\t//丢包的数量\tu32\tsacked_out;\t/* SACK&#x27;d packets\t\t\t*/\t\t\t\t\t\t\t//sack确认包的数量\tstruct hrtimer\tpacing_timer;\t\t\t\t\t\t\t\t\t\t\t//tsq的定时器，init tcp_sock的时候会设置，注意：软中的也调用这个注册的函数在free的时候\tstruct hrtimer\tcompressed_ack_timer;\t\t\t\t\t\t\t\t\t//延迟sack定时器。\t/* from STCP, retrans queue hinting */\t\t\t\tstruct sk_buff* lost_skb_hint;\t\t\t\t\t\t\t\t\t\t\t//定位第一个丢失的包 ，优化性能？\tstruct sk_buff *retransmit_skb_hint;\t\t\t\t\t\t\t\t\t//下一个要重传的数据包\t/* OOO segments go in this rbtree. Socket lock must be held. */\t\t\t//tcp的乱续队列\tstruct rb_root\tout_of_order_queue;\t\t\t\t\t\t\t\t\t\t\tstruct sk_buff\t*ooo_last_skb; /* cache rb_last(out_of_order_queue) */\t//乱续队列中欧给你最后一个数据包\t/* SACKs data, these 2 need to be together (see tcp_options_write) */\tstruct tcp_sack_block duplicate_sack[1]; /* D-SACK block */\t\t\t\t\t\tstruct tcp_sack_block selective_acks[4]; /* The SACKS themselves*/\t\t\t//发送端收到sack\tstruct tcp_sack_block recv_sack_cache[4];\t\t\t\t\t\t\t\t\t//接收端生成sack\tstruct sk_buff *highest_sack;   /* skb just after the highest\t\t\t\t//已经被接收段确认的最高序列号的数据包？？？\t\t\t\t\t * skb with SACKed bit set\t\t\t\t\t * (validity guaranteed only if\t\t\t\t\t * sacked_out &gt; 0)\t\t\t\t\t */\tint     lost_cnt_hint;\t\t\t\t\t\t\t\t\t\t\t\t\t//处理重传队列用到，一个数量，是丢包标记前未处理的数据包数量\tu32\tprior_ssthresh; /* ssthresh saved at recovery start\t*/\t\t\t\t//保存原来的慢启动阈值\tu32\thigh_seq;\t/* snd_nxt at onset of congestion\t*/\t\t\t\t\t//进入拥塞控制状态后下一个待发送的序列号\tu32\tretrans_stamp;\t/* Timestamp of the last retransmit,\t\t\t\t\t//最后一个重传的时间戳\t\t\t\t * also used in SYN-SENT to remember stamp of\t\t\t\t * the first SYN. */\tu32\tundo_marker;\t/* snd_una upon a new recovery episode. */\t\t\t//记录的是序号，enterloss或者回复时候保存的una\tint\tundo_retrans;\t/* number of undoable retransmissions. */\t\t\t\t//记录进入恢复阶段时的重传包数，用于判断是否真是丢包？？未确认重传包数量\tu64\tbytes_retrans;\t/* RFC4898 tcpEStatsPerfOctetsRetrans\t\t\t\t\t//重传skb字节书\t\t\t\t * Total data bytes retransmitted\t\t\t\t */\tu32\ttotal_retrans;\t/* Total retransmits for entire connection */\t\t\t\t//总计重传数和上面的一样__tcp_retransmit_skb中++\tu32\turg_seq;\t/* Seq of received urgent pointer */\t\t\t\t\t\t\t//指向紧急数据的序列号\tunsigned int\t\tkeepalive_time;\t  /* time before keep alive takes place */\t//保活时间不活跃后多久探测一次\tunsigned int\t\tkeepalive_intvl;  /* time interval between keep alive probes */ //探测失败后多久在探测一次\tint\t\t\tlinger2;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//finwait2状态的保持时间/* Sock_ops bpf program related variables */#ifdef CONFIG_BPF\tu8\tbpf_sock_ops_cb_flags;  /* Control calling BPF programs\t\t\t\t\t * values defined in uapi/linux/tcp.h\t\t\t\t\t */\tu8\tbpf_chg_cc_inprogress:1; /* In the middle of\t\t\t\t\t  * bpf_setsockopt(TCP_CONGESTION),\t\t\t\t\t  * it is to avoid the bpf_tcp_cc-&gt;init()\t\t\t\t\t  * to recur itself by calling\t\t\t\t\t  * bpf_setsockopt(TCP_CONGESTION, &quot;itself&quot;).\t\t\t\t\t  */#define BPF_SOCK_OPS_TEST_FLAG(TP, ARG) (TP-&gt;bpf_sock_ops_cb_flags &amp; ARG)#else#define BPF_SOCK_OPS_TEST_FLAG(TP, ARG) 0#endif\tu16 timeout_rehash;\t/* Timeout-triggered rehash attempts */\t\t\t\t\t//超时处理中会重设hash值，表示重设hash值的次数\tu32 rcv_ooopack; /* Received out-of-order packets, for tcpinfo */          //乱续数据包总数/* Receiver side RTT estimation */\tu32 rcv_rtt_last_tsecr;\tstruct &#123;\t\tu32\trtt_us;\t\tu32\tseq;\t\tu64\ttime;\t&#125; rcv_rtt_est;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//接收方的rtt,处理ack的时候会用到/* Receiver queue space */\tstruct &#123;\t\tu32\tspace;\t\tu32\tseq;\t\tu64\ttime;\t&#125; rcvq_space;/* TCP-specific MTU probe information. */\tstruct &#123;\t\tu32\t\t  probe_seq_start;\t\tu32\t\t  probe_seq_end;\t&#125; mtu_probe;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//mtu探测的消息\tu32     plb_rehash;     /* PLB-triggered rehash attempts */\t\t\t\t\t\t//拥塞算法会用到\tu32\tmtu_info; /* We received an ICMP_FRAG_NEEDED / ICMPV6_PKT_TOOBIG\t\t\t//没发现哪里会用到\t\t\t   * while socket was owned by user.\t\t\t   */#if IS_ENABLED(CONFIG_MPTCP)\tbool\tis_mptcp;#endif#if IS_ENABLED(CONFIG_SMC)\tbool\t(*smc_hs_congested)(const struct sock *sk);\tbool\tsyn_smc;\t/* SYN includes SMC */#endif#ifdef CONFIG_TCP_MD5SIG/* TCP AF-Specific parts; only used by MD5 Signature support so far */\tconst struct tcp_sock_af_ops\t*af_specific;/* TCP MD5 Signature Option information */\tstruct tcp_md5sig_info\t__rcu *md5sig_info;#endif/* TCP fastopen related information */\tstruct tcp_fastopen_request *fastopen_req;\t\t\t\t\t\t\t\t//管理TFO的结构，sendmsgtfo会设置里面的字段\t/* fastopen_rsk points to request_sock that resulted in this big\t * socket. Used to retransmit SYNACKs etc.\t */\tstruct request_sock __rcu *fastopen_rsk;\t\t\t\t\t\t\t\t//requset sock\tstruct saved_syn *saved_syn;\t\t\t\t\t\t\t\t\t\t\t//服务端收到syn包时候保存的syn包信息&#125;;\n\n","categories":["网络协议栈源码学习"],"tags":["sock","TCP"]},{"title":"x86汇编学习(三)","url":"/2025/06/03/x86%E6%B1%87%E7%BC%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/","content":"函数调用函数调用是一种代码封装机制，他通过参数和返回值实现特定的功能，隐藏实现的细节，提供清晰的接口\n实现函数调用需要提供以下机制：\n\n控制转移：跳转到被调用过程并在结束后返回原处；\n参数传递：将调用方的数据传给被调用方；\n返回值传递：将结果从被调用方传回；\n局部变量管理：分配并释放被调用过程所需的内存\n\nx86-64 架构的过程调用依赖一套特定的寄存器和内存使用规范，以减少开销和提高效率。\n栈帧栈帧就是每次函数调用时，在栈上分配的一块内存区域,里面通常会包含：\n\n\n\n内容\n作用\n\n\n\n参数区\n用来保存传入函数的参数（如果没用寄存器传）\n\n\n返回地址\n函数执行完后，跳回调用者的地方\n\n\n保存的寄存器\n保存调用者的寄存器内容，防止被破坏\n\n\n局部变量\n函数内部使用的变量（如 int x = 5;）\n\n\n临时空间\n编译器优化、对齐等需求\n\n\n对应的布局大体如下：\n↑ 高地址││ 调用者保存的返回地址      ← call 指令自动压入（跳回调用点）││ 可选：旧的基址指针 (%rbp)  ← 如果使用帧指针（frame pointer）││ 被调用者保存的寄存器      ← 被调用函数保存如 rbx, rbp, r12 等││ 对齐填充                  ← 保证16字节对齐要求││ 被调用者的局部变量        ← 编译器通过 sub rsp, XXX 分配空间││ 可选：超过6个参数的栈参数  ← 前6个参数通过寄存器传，其余在栈上传↓低地址\n\n\n\n\n\n一个简单的例子演示函数 P 调用 Q 时，两者的栈帧结构分别是什么样的\n// 被调用函数 Qint Q(int a, int b) &#123;    int sum = a + b;   // 局部变量    return sum;&#125;// 调用者函数 Pint P() &#123;    int x = 3, y = 4;    int result = Q(x, y);  // 调用 Q    return result + 1;&#125;\n\n栈帧分析\n内存高地址↑|-----------------------------------------|| main → call P 压入的返回地址            | ←  P 的返回地址|-----------------------------------------|| P 保存的寄存器（如 rbx）                 || P 的局部变量 y = 4                      || P 的局部变量 x = 3                      || P 的局部变量 result                     ||-----------------------------------------|| P → call Q 压入的返回地址               | ←  Q 的返回地址|-----------------------------------------|| Q 参数 a = 3（由 rdi 传，但可写入栈）     || Q 参数 b = 4（由 rsi 传，但可写入栈）     || Q 保存的寄存器（如 rbx）                 || Q 的局部变量 sum                        |↓内存低地址\n\n函数调用函数调用其实就是“跳转”到另一个地址，把控制权从函数 P 跳转到函数 Q，就是把程序计数器（PC）设置为 Q 函数起始地址。\n但跳转之后，返回原来位置怎么办？\n\n跳到 Q 之后，执行完 Q 后，还得回到 P 原来的地方继续执行。\n所以：调用时必须“记住”返回地址。\n\n在 x86-64 中，是怎么记住这个返回地址的？\n使用 call 指令\ncall Q\n\n做了两件事：\n\n把“下一条指令地址”压入栈（这就是“返回地址”）；\n跳转到 Q 函数的入口。\n\n这个压入栈的地址（称为 地址 A），就是 call Q 指令执行完后，应该继续执行的下一条指令地址。\n使用 ret 指令：\nret\n\n\n\n\n指令形式\n描述\n\n\n\ncall Label\n调用固定地址的函数（如 call Q）\n\n\ncall *Operand\n调用函数指针（如 call *%rax）\n\n\nret\n返回上一函数\n\n\n举个例子\nc语言代码如下:\nlong swap_add(long *xp, long *yp)&#123;    long x = *xp;  // 取出 *xp    long y = *yp;  // 取出 *yp    *xp = y;       // 交换 *xp 和 *yp 的值    *yp = x;    return x + y;  // 返回原值之和&#125;long caller()&#123;    long arg1 = 534;    long arg2 = 1057;    long sum = swap_add(&amp;arg1, &amp;arg2);    long diff = arg1 - arg2;    return sum * diff;&#125;\n\n对应的汇编代码如下：\nsubq $16, %rsp                  # 为局部变量 arg1 和 arg2 分配 16 字节栈空间movq $534, (%rsp)              # 把常量 534 存入栈顶 → arg1movq $1057, 8(%rsp)            # 把常量 1057 存入 [rsp + 8] → arg2leaq 8(%rsp), %rsi             # 加载 &amp;arg2 的地址到 %rsi → 作为第二个参数movq %rsp, %rdi                # 加载 &amp;arg1 的地址到 %rdi → 作为第一个参数call swap_add                  # 调用 swap_add 函数（结果保存在 %rax，返回地址被压栈）movq (%rsp), %rdx             # 把交换后的 arg1 取到 %rdxsubq 8(%rsp), %rdx            # 计算 diff = arg1 - arg2，结果保存在 %rdximulq %rdx, %rax              # sum * diff，结果保存在 %rax（作为最终返回值）addq $16, %rsp                # 恢复栈指针，释放之前为 arg1/arg2 分配的栈空间ret                           # 从 caller 函数返回，ret 会从栈中弹出返回地址并跳回\n\n数组C语言把数组实现得非常简单直接 —— 它其实就是一段连续内存块\n举例：\nchar A[12];char *B[8];int C[6];double *D[5];\n\n\n\n\n数组\n元素大小（字节）\n总大小（字节）\n起始地址\n第 i 个元素地址\n\n\n\nA\n1\n12\nx_A\nx_A + i\n\n\nB\n8\n64\nx_B\nx_B + 8i\n\n\nC\n4\n24\nx_C\nx_C + 4i\n\n\nD\n8\n40\nx_D\nx_D + 8i\n\n\n举例：\n在 x86-64 架构中，内存引用指令可以用来简化数组访问的实现。比如，假设 E 是一个 int 类型的数组，而我们想要计算 E[i] 的值。\n此时，E 的起始地址被存储在寄存器 %rdx 中，索引 i 存储在寄存器 %rcx 中。那么下面这条指令\nmovl (%rdx, %rcx, 4), %eax\n\n会执行地址计算：x_E + 4*i，读取该地址处的内存内容，并将这个值存入寄存器 %eax 中。\n指针的的运算C语言允许对指针做加法运算；\n表达式 p + i 并不是简单的“地址 + 数字”，而是地址 + i × L，其中 L 是该指针类型 T 的大小；\n\n比如 int *p，每加1其实是加4字节；\n比如 double *p，每加1是加8字节；\n\n举例：\n以表达式 E[i]（访问第 i 个元素）为例：\n\n假设 E 是一个 int 数组，起始地址在 %rdx，下标 i 存在 %rcx；\n那么 E[i] 就是从 rdx + 4*i 所在的内存读取；\n对应汇编是：\n\nmovl (%rdx, %rcx, 4), %eax\n\n说明：int 是 4 字节，所以用 movl（加载 4 字节）和缩放因子 4\n结构体C语言的 struct 用来定义一种聚合类型，它可以把多个不同类型的变量“组合成一个整体”。\n\n在内存中，结构体的所有成员是连续存放的；\n结构体变量其实就是一个内存块；\n结构体指针指向结构体起始地址，访问成员时靠偏移量；\n编译器会记录每个成员相对结构体首地址的偏移，用来计算成员地址。\n\n举例：\n假设有如下结构体\nstruct rec &#123;    int i;    int j;    int a[2];    int *p;&#125;;\n\n对应的各个字段的偏移如下所示：\n\n\n\n偏移量 (byte)\n字段内容\n\n\n\n0\ni\n\n\n4\nj\n\n\n8\na[0]\n\n\n12\na[1]\n\n\n16\np\n\n\n如果访问结构体字段 r-&gt;i 并将其存入 r-&gt;j，对应的汇编如下所示：\n假设 struct rec *r 的地址保存在 %rdi 中：\n# Registers: r in %rdimovl (%rdi), %eax        # 将 r-&gt;i 的值读入 %eaxmovl %eax, 4(%rdi)       # 把 r-&gt;i 的值写入 r-&gt;j（偏移 4 字节）\n\n","categories":["《深入理解计算机系统》"],"tags":["汇编语言"]},{"title":"x86汇编学习(一)","url":"/2025/05/27/x86%E6%B1%87%E7%BC%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/","content":"汇编语言GCC（C语言编译器）可以将程序编译为汇编代码的形式进行输出。汇编语言是机器指令的文本表示形式，它详细地列出了程序中每一条指令。然后GCC 会调用汇编器（assembler）和链接器（linker），根据这些汇编代码生成最终的可执行机器代码。\n本文基于x86-64架构,\t 它是现在最常见处理器的机器语言，也是驱动大型数据中心和超级计算机的最常见处理器的机器语言。这种语言的历史悠久，开始 于 Intel 公司 1978 年的第一个 16 位处理器，然后扩展为 32 位，最近又扩展到 64 位。\n1.x86的发展过程8086(1978 年， 29K 个晶体管）。它是第一代单芯片、 16 位微处理器之一。 \ni386(1985 年， 275K 个晶体管）。将体系结构扩展到 32 位。这是 Intel 系列中第一台全面支持 Unix操作系统的机器。\nPentium 4E(2004 年， 125M 个品体管）。增加了超线程(hyperthreading) , 这种技术可以在一个处理器上同时运行两个程序；还增加了 EM64T, 它是 Intel 对AMD提出的对 IA32 的 64 位扩展的实现，我们称之为 x86-64。\n2.程序编码假设一个C程序，有两个文件p1.c 和 p2.c 我们用 Unix命令行编译这些代码：\nlinux&gt; gcc -Og -o p p1.c p2.c \n\n命令 gcc 指的就是 GCC C 编译器。因为这是 Linux上默认的编译器。编译选项-Og告诉编译器使用的优化等级。\n实际上 gcc 命令调用了一整套的程序，将源代码转化成可执行代码。首先， C预处理器扩展源代码，插入所有用#include命令指定的文件，并展开所有用#define 声明指定的宏。其次，编译器产生两个源文件的汇编代码，名字分别为 p1.s 和 p2.s。接下来，汇编器会将汇编代码转化成二进制目标代码文件p1.a 和 p2.o。 目标代码是机器代码的一种形式，它包含所有指令的二进制表示，但是还没有填入全局值的地址（此时全局变量的地址应该没有确定）。最后，链接器将两个目标代码文件与实现库函数（例如printf)的代码合并，并产生最终的可执行代码文件p。\n虽然 C语言提供了一种模型，可以在内存中声明和分配各种数据类型的对象**，但是机器码只是简单地将内存看成一个很大的、按字节寻址的数组**。 C语言中的数据类型，例如数组和结构体，在机器代码中用一组连续的字节来表示。\n代码示例：\n假设我们写了一个 C 语言代码文件 mstore.c，包含如下的函数定义：\nlong mult2(long, long);void multstore(long x, long y, long *dest) &#123;    long t = mult2(x, y);    *dest = t;&#125;\n\n在命令行上使用 -S 选项，就能看到 C 语言编译器产生的汇编代码\nlinux&gt; gcc -Og -S mstore.c\n\n这会使 GCC 运行编译器，产生一个汇编文件 mstore.s，但是不做其他进一步的工作。（通常情况下，它还会继续调用汇编器产生目标代码文件）。\n汇编代码文件包含各种声明，包括下面几行：\nmultstore:    pushq   %rbx //把寄存器 %rbx 的值压栈，保存现场，防止被后面修改    movq    %rdx, %rbx //把第三个参数（dest 指针）保存到 %rbx 中，因为调用 mult2 之后 %rdx 可能会被破坏    call    mult2 //调用 mult2(x, y) 函数（参数 x 和 y 存在 %rdi 和 %rsi 中）。    movq    %rax, (%rbx) //将 mult2 的返回值（在 %rax 中）存储到 %rbx 指向的地址中，也就是 *dest = result。    popq    %rbx //恢复之前保存的 %rbx 的值，保持寄存器一致性。    ret\n\n要查看机器代码文件的内容，有一类称为反汇编器（disassembler）的程序非常有用。这些程序根据机器代码产生一种类似于汇编代码的格式。在 Linux 系统中，带 -d 命令行标志的程序 objdump（表示 “object dump”）可以充当这个角色：\nlinux&gt; objdump -d mstore.o\n\n结果如下\nDisassembly of function multstore in binary file mstore.o0000000000000000 &lt;multstore&gt;: Offset  Bytes         Equivalent assembly language----------------------------------------------------  0:     53            push   %rbx                ; 保存 %rbx 到栈中  1:     48 89 d3      mov    %rdx, %rbx          ; 把第三个参数 dest 存入 %rbx  4:     e8 00 00 00 00 callq  9 &lt;multstore+0x9&gt;  ; 调用 mult2(x, y)  9:     48 89 03      mov    %rax, (%rbx)        ; 把结果保存到 *dest 中  c:     5b            pop    %rbx                ; 恢复 %rbx  d:     c3            retq                       ; 返回\n\n\n\n然而生成实际可执行的代码需要对一组目标代码文件运行链接器，而这一组目标代码文件中必须含有一个main 函数。假设在文件 main.c 中有下面这样的函数：\n#include &lt;stdio.h&gt;void multstore(long, long, long *);int main() &#123;    long d;    multstore(2, 3, &amp;d);    printf(&quot;2 * 3 --&gt; %ld\\n&quot;, d);    return 0;&#125;long mult2(long a, long b) &#123;    long s = a * b;    return s;&#125;\n\n用如下命令成可执行文件prog:\nlinux&gt; gcc -Og -o prog main.c mstore.c\n\n我们也可以反汇编 prog 文件\nDisassembly of function sum multstore binary file prog0000000000400540 &lt;multstore&gt;: Offset     Bytes             Equivalent assembly language---------------------------------------------------------- 400540:    53                push   %rbx 400541:    48 89 d3          mov    %rdx, %rbx 400544:    e8 42 00 00 00    callq  40058b &lt;mult2&gt; 400549:    48 89 03          mov    %rax, (%rbx) 40054c:    5b                pop    %rbx 40054d:    c3                retq 40054e:    90                nop 40054f:    90                nop\n\n这段代码与 mstore.c 反汇编产生的代码几乎完全一样。其中一个主要的区别是左边列出的地址不同——链接器将这段代码的地址移动到了一段不同的地址范围中。第二个不同之处在于链接器填上了 callq 指令调用函数 mult2 需要使用的地址（反汇编代码第 4 行）。链接器的任务之一就是为函数调用匹配到可执行代码中函数的地址.（这里的地址0000000000400540应该就是虚拟内存中的地址）\n3.汇编伪指令假设我们用如下命令生成文件 mstore.s完整的汇编文件代码如下所示：\n\t.file\t&quot;010-mstore.c&quot; //告诉汇编器这个文件来源于哪个源文件（调试信息用）\t.text  \t\t\t\t\t//告诉汇编器后面的内容是程序代码\t.globl\tmultstore \t\t//使函数对其他文件可见（链接时可调用）\t.type\tmultstore, @function //\t指定符号类型是函数\t供链接器识别该符号为函数multstore:\tpushq\t%rbx\tmovq\t%rdx, %rbx\tcall\tmult2\tmovq\t%rax, (%rbx)\tpopq\t%rbx\tret\t.size\tmultstore, .-multstore //告诉链接器这个函数占用了多少字节。在 ELF 格式中，每个符号都可以带有大小信息\t.ident\t&quot;GCC: (Ubuntu 4.8.1-2ubuntu1~12.04) 4.8.1&quot; //加入一条标识信息，说明是用什么编译器编译的。\t.section\t.note.GNU-stack,&quot;&quot;,@progbits\n\n所有以 . 开头的行都是指导汇编器和链接器工作的伪指令。\n4.C 类型与汇编指令后缀对应表 Intel 用术语”字(word)” 表示 16 位数据类型。因此，称 32 位数为“双字”, 称 64 位数为“四字” 下表为C语言基本数据类型对应的 x86-64 表示：\n\n\n\nC 声明\nIntel 数据类型\n汇编代码后缀\n大小（字节）\n\n\n\nchar\n字节\nb（byte）\n1 字节\n\n\nshort\n字\nw（word）\n2 字节\n\n\nint\n双字\nl（long）\n4 字节\n\n\nlong\n四字\nq（quad）\n8 字节\n\n\nchar*\n四字（指针）\nq\n8 字节\n\n\nfloat\n单精度\ns（single）\n4 字节\n\n\ndouble\n双精度\nl（long）\n8 字节\n\n\n 汇编后缀的意义：\n在 x86-64 汇编中，许多指令都有后缀来说明操作数的数据大小：\n\nmovb：移动 1 字节（byte）\nmovw：移动 2 字节（word）\nmovl：移动 4 字节（long word）\nmovq：移动 8 字节（quad word）\n\n5.x86-64 架构下的通用寄存器通用寄存器：临时存储运算数据、函数参数、返回值、地址、计数、内存地址等各种信息，能够加快数据处理速度，减少内存读写等\n每个寄存器的用途（按调用约定）如下表所示：\n\n\n\n64位寄存器\n32位\n16位\n8位\n用途说明\n\n\n\n%rax\n%eax\n%ax\n%al\n返回值寄存器\n\n\n%rbx\n%ebx\n%bx\n%bl\n被调用者保存\n\n\n%rcx\n%ecx\n%cx\n%cl\n第4个参数\n\n\n%rdx\n%edx\n%dx\n%dl\n第3个参数\n\n\n%rsi\n%esi\n%si\n%sil\n第2个参数\n\n\n%rdi\n%edi\n%di\n%dil\n第1个参数\n\n\n%rbp\n%ebp\n%bp\n%bpl\n被调用者保存\n\n\n%rsp\n%esp\n%sp\n%spl\n栈指针\n\n\n%r8\n%r8d\n%r8w\n%r8b\n第5个参数\n\n\n%r9\n%r9d\n%r9w\n%r9b\n第6个参数\n\n\n%r10\n%r10d\n%r10w\n%r10b\n调用者保存\n\n\n%r11\n%r11d\n%r11w\n%r11b\n调用者保存\n\n\n%r12\n%r12d\n%r12w\n%r12b\n被调用者保存\n\n\n%r13\n%r13d\n%r13w\n%r13b\n被调用者保存\n\n\n%r14\n%r14d\n%r14w\n%r14b\n被调用者保存\n\n\n%r15\n%r15d\n%r15w\n%r15b\n被调用者保存\n\n\n所有这些寄存器本质上都是 64 位的，但我们可以只访问其中的低 32、16、8 位（如上表第二，第三，第四列所示）\n与专用寄存器的区别：\n\n\n\n类型\n说明\n例子\n\n\n\n通用寄存器\n用于各种灵活数据操作\n%rax, %rdi\n\n\n专用寄存器\n有固定用途\n%rip（指令指针），%cr3（控制）\n\n\n段寄存器\n用于段地址（早期保护模式）\n%cs, %ds\n\n\n标志寄存器\n保存运算结果标志\n%eflags\n\n\n6.x86 汇编寻址方式表寻址（Addressing）就是确定数据所在地址的过程，具体汇编的格式和含义如下表所示：\n\n\n\n类型\n格式\n操作数值表示\n名称（寻址方式）\n\n\n\n立即数\n$Imm\nImm\n立即数寻址（Immediate Addressing）\n\n\n寄存器\nr_a\nR[r_a]\n寄存器寻址（Register Addressing）\n\n\n存储器\nImm\nM[Imm]\n绝对寻址（Absolute Addressing）\n\n\n存储器\n(r_a)\nM[R[r_a]]\n间接寻址（Indirect Addressing）\n\n\n存储器\nImm(r_b)\nM[Imm + R[r_b]]\n基址 + 偏移量（Base + Offset）寻址\n\n\n存储器\n(r_b, r_i)\nM[R[r_b] + R[r_i]]\n变址寻址（Indexed Addressing）\n\n\n存储器\nImm(r_b, r_i)\nM[Imm + R[r_b] + R[r_i]]\n变址 + 偏移量寻址\n\n\n存储器\n(r_i, s)\nM[R[r_i] * s]\n比例变址寻址（Scaled Index）\n\n\n存储器\nImm(,r_i,s)\nM[Imm + R[r_i] * s]\n比例变址 + 偏移量寻址\n\n\n存储器\n(r_b, r_i, s)\nM[R[r_b] + R[r_i] * s]\n比例变址 + 基址寻址\n\n\n存储器\nImm(r_b, r_i, s)\nM[Imm + R[r_b] + R[r_i] * s]\n比例变址 + 基址 + 偏移量寻址\n\n\n格式指的是在汇编语言中书写一个操作数（比如常量、寄存器或内存地址）时，它的语法结构长什么样，也就是“写法”。\n操作数表示是指汇编语言中那个操作数在执行时真正的数值含义，如下表所示：\n\n\n\n概念\n意义\n举例\n\n\n\n格式\n汇编里写法\n8(%rbp)\n\n\n操作数表示\n真正要访问的值的含义\nM[8 + R[rbp]]\n\n\n举例：\n\n立即数寻址（Immediate Addressing）\n\nmov $5, %eax      # 把常量 5 移到寄存器 eax 中\n\n\n寄存器寻址（Register Addressing）\n\nmov %ebx, %eax    # 把 %ebx 里的值复制到 %eax\n\n\n间接寻址（Indirect Addressing)\n\nmov (%rbx), %eax  # 把 %rbx 指向的内存地址中的值加载到 %eax\n\n\n基址 + 偏移寻址（Base + Offset)\n\nmov 8(%rbp), %eax # 取栈帧中偏移 8 字节的变量到 %eax\n\n","categories":["《深入理解计算机系统》"],"tags":["汇编语言"]},{"title":"x86汇编学习(四)","url":"/2025/06/04/x86%E6%B1%87%E7%BC%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"数据对齐数据按对齐要求存放，可以让处理器和内存系统更高效地访问数据\n对齐的要求比如 64 位系统（x86-64），一个 double 或 long 类型的变量通常要求8字节对齐，也就是说，变量的地址必须是8的倍数。\n下表格列出了常见类型的对齐要求（K）：\n\n\n\nK\n类型\n\n\n\n1\nchar\n\n\n2\nshort\n\n\n4\nint, float\n\n\n8\nlong, double, char*\n\n\n对齐的实现（以结构体为例）编译器会自动插入填充字节（padding），确保每个成员的起始地址都满足其类型的对齐要求，整个结构体的大小也是最大对齐数的倍数\n举例\nstruct S1 &#123;    int i;    char c;    int j;&#125;;\n\n\n这里 int 要求4字节对齐，char 要求1字节对齐。\n如果没有填充，i（4字节）后面紧跟c（1字节），接着就是j。这样j的起始地址不是4的倍数，不符合对齐要求。\n编译器会在c和j之间插入3字节填充，使得j从8的倍数地址开始。\n结构体总大小也要满足最大对齐数的倍数。\n\n偏移 0    4   5     8内容 i |  c | 填充 | j\n\n注意：结构体 S1 的最大对齐值是4，所以整个结构体的对齐值就是4。\n内存越界引用和缓冲区溢出C 语言对数组的访问没有边界检查，如果写操作越界，不仅会破坏数据本身，还可能破坏栈上的局部变量和保存的状态信息（比如返回地址、寄存器值等）。\n如果程序使用了被破坏的状态，再执行比如函数返回（ret 指令），就会出错，甚至导致程序劫持（如攻击者可控制返回地址）。\n举例：\n/* Implementation of library function gets() */char *gets(char *s)&#123;    int c;    char *dest = s;    while ((c = getchar()) != &#x27;\\n&#x27; &amp;&amp; c != EOF)        *dest++ = c;    if (c == EOF &amp;&amp; dest == s)        /* No characters read */        return NULL;    *dest++ = &#x27;\\0&#x27;; /* Terminate string */    return s;&#125;/* Read input line and write it back */void echo()&#123;    char buf[8];    /* Way too small! */    gets(buf);    puts(buf);&#125;\n\n前面的代码实现了标准库函数 gets，并用它来说明该函数存在的严重缺陷。gets 会从标准输入读取一整行内容，直到遇到回车换行符或发生错误为止，然后把读到的字符串复制到参数 s 指定的位置，并在末尾添加一个 null 字符。在 echo 这个例子中，我们调用了 gets，让它从标准输入读取一行内容并输出到标准输出。\ngets 最大的问题在于，它无法判断为存放整个字符串预留的空间是否足够。在 echo 的演示代码中，缓冲区长度被故意设置得很小，只有 8 个字节。因此，只要输入的字符串长度超过 7 个字符（最后一位要留给结束符），就会发生越界写入的问题。\n对应的汇编代码如下：\nvoid echo()1  echo:2      subq $24, %rsp      # Allocate 24 bytes on stack3      movq %rsp, %rdi     # Compute buf as %rsp4      call gets           # Call gets5      movq %rsp, %rdi     # Compute buf as %rsp6      call puts           # Call puts7      addq $24, %rsp      # Deallocate stack space8      ret                 # Return\n\n该程序在栈上分配了 24 个字节。字符数组 buf 位于栈顶，可以看到，%rsp 被复制到 %rdi 作为调用 gets 和 puts 的参数。这个调用的参数和存储的返回指针之间的 16 字节是未被使用的。只要用户输入不超过 7 个字符，gets 返回的字符串（包括结尾的 null）就能够放进为 buf 分配的空间里。不过，长一些的字符串就会导致 gets 覆盖栈上存储的某些信息。随着字符串变长，下面的信息可能会被破坏：\n\n\n\n输入的字符串数量\n附加的被破坏的状态\n\n\n\n0~7\n无\n\n\n8~23\n未被使用的栈空间\n\n\n24~31\n返回地址\n\n\n32+\ncaller 中保存的状态\n\n\n只要输入的字符串长度不超过 23 个字符，一般不会导致严重的问题。但如果输入再长一些，返回地址以及更多保存的状态信息就可能被覆盖和破坏。如果返回地址被覆盖，当执行 ret 指令（第 8 行）时，程序可能会跳转到一个完全不可预料的位置，导致异常行为。这种内存越界写的问题在看 C 语言源码时是无法直接发现的，只有通过研究底层机器级别的程序，才能真正理解像 gets 这种函数在内存越界写时带来的危害。\n","categories":["《深入理解计算机系统》"],"tags":["汇编语言"]},{"title":"x86汇编学习(二)","url":"/2025/05/28/x86%E6%B1%87%E7%BC%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"汇编指令学习MOV指令 下表列出的是最简单形式的数据传送指令-MOV类。这些指令把数据从源位置复制到目的位置，不做任何变化。 MOV 类由四条指令组成： movb、 movw、 movl 和 movq。这些指令都执行同样的操作；主要区别在千它们操作的数据大小不同：分别是 1、 2 、 4 和 8 字节。\n\n\n\n指令\n效果\n描述\n\n\n\nMOV S, D\nD ← S\n传送\n\n\nmovb\n\n传送字节\n\n\nmovw\n\n传送字\n\n\nmovl\n\n传送双字\n\n\nmovq\n\n传送四字\n\n\nmovabsq I, R\nR ← I\n传送绝对的四字\n\n\n源操作数指定的值是一个立即数，存储在寄存器中或者内存中。目的操作数指定一个 位置，要么是一个寄存器或者，要么是一个内存地址。 x86-64 加了一条限制，传送指令的 两个操作数不能都指向内存位置。将一个值从一个内存位置复制到另一个内存位置需要两条指令——第一条指令将源值加载到寄存器中，第二条将该寄存器值写入目的位置。\n下面的 MOV指令示例给出了源和目的类型的五种可能的组合。第一个是源操作数，第二个是目的操作数：\n\n\n\n序号\n指令\n源类型 → 目的类型\n字节数\n\n\n\n1\nmovl $0x4050, %eax\nImmediate → Register\n4 bytes\n\n\n2\nmovw %bp, %sp\nRegister → Register\n2 bytes\n\n\n3\nmovb (%rdi, %rcx), %al\nMemory → Register\n1 byte\n\n\n4\nmovb $-17, (%rsp)\nImmediate → Memory\n1 byte\n\n\n5\nmovq %rax, -12(%rbp)\nRegister → Memory\n8 bytes\n\n\n数据扩展传送指令当你把一个小的数据（如 1 字节）存入一个大的寄存器（如 4 字节）时，需要决定怎么填补高位。 这个过程叫“扩展（Extend）”，有两种方式：\n\n\n\n扩展方式\n含义\n\n\n\n零扩展（Zero-Extend）\n高位填 0，不改变原数\n\n\n符号扩展（Sign-Extend）\n高位按符号位（最高位）填充，保持数值含义\n\n\n零扩展传送指令\n\n\n\n指令\n效果\n描述\n\n\n\nmovzbw\n把字节零扩展到字\n高位填 0，目标是16位\n\n\nmovzbl\n把字节零扩展到双字\n高位填 0，目标是32位\n\n\nmovzwl\n把字零扩展到双字\n把16位数扩展为32位\n\n\nmovzbq\n把字节零扩展到四字\n把8位扩展为64位（x86-64用）\n\n\nmovzwq\n把字零扩展到四字\n把16位扩展为64位（x86-64用）\n\n\n符号扩展传送指令\n\n\n\n指令\n效果\n描述\n\n\n\nmovsbw\n把字节符号扩展到字\n符号位复制到高位\n\n\nmovsbl\n把字节符号扩展到双字\n8 → 32 位，保持正负性\n\n\nmovswl\n把字符号扩展到双字\n16 → 32 位\n\n\nmovsbq\n把字节符号扩展到四字\n8 → 64 位\n\n\nmovswq\n把字符号扩展到四字\n16 → 64 位\n\n\nmovslq\n把双字符号扩展到四字\n32 → 64 位\n\n\ncltq\nrax ← sign_extend(eax)\n把 eax 的符号扩展复制到 rax\n\n\n示例\nC语言代码如下:\nlong exchange(long *xp, long y)&#123;    long x = *xp;    *xp = y;    return x;&#125;\n\n对应的汇编代码如下：\nexchange:    movq    (%rdi), %rax      # Get x at xp. Set as return value.    movq    %rsi, (%rdi)      # Store y at xp.    ret                       # Return.\n\n寄存器约定：\n\nxp 存放在 %rdi\ny  存放在 %rsi\n\n当过程开始执行时，过程参数 xp 和 y分别存储在寄存器%rdi 和%rsi 中。然后，指令 2 从内存中读出 x, 把它存放到寄存器%rax 中，直接实现了 C程序中的操作 x=*xp。然后用寄存器%rax从这个函数返回一个值，因而返回值就是 x。将 y写入到寄存器%rdi 中的 xp 指向的内存位置，直接实现了操作*xp=y。这个例子说明了如何用 MOV 指令从内存中读值到寄存器（第 2 行），如何从寄存器写到内存（第 3 行）。\n有两点值得注意。首先，我们看到 C语言中所谓的”指针”其实就是地址。间接引用指针就是将该指针放在一个寄存器中，然后在内存引用中使用这个寄存器。其次，像 x这样的局部变量通常是保存在寄存器中，而不是内存中。访问寄存器比访问内存要快得多。\nPUSH &amp;&amp; POP 指令pushq 把一个值压入栈中，popq 从栈中弹出一个值\n栈的基本概念：\n\n栈（stack）是一种先进后出（LIFO）的数据结构\n在 x86-64 架构中，栈是向低地址增长的\n栈的“顶”是当前栈的最新元素，指针 %rsp 指向这个栈顶\n\npushq 指令详解（压栈）把一个四字（8字节）值压入栈中\n实现步骤：\n\n先将 %rsp 减 8（因为栈向下生长）\n再把值存入新地址 [rsp]\n\n\n\n\n指令\n效果\n描述\n\n\n\npushq S\nR[%rsp] ← R[%rsp] - 8``M[R[%rsp]] ← S\n将四字压入栈\n\n\n因此，指令pushq %rbp等价于：\nsubq $8, %rspmovq %rbp, (%rsp)\n\n\n\npopq 指令详解（弹栈）从栈顶取出一个四字（8字节）值赋给寄存器\n实现步骤：\n\n从 [rsp] 读取值赋给目标寄存器\n再将 %rsp 加 8，栈顶上移\n\n\n\n\n指令\n效果\n描述\n\n\n\npopq D\nD ← M[R[%rsp]]``R[%rsp] ← R[%rsp] + 8\n将四字弹出栈\n\n\n因此，指令popq %rbp等价于：\nmovq (%rsp), %rbp     ; 把栈顶的值写入 %rbpaddq $8, %rsp         ; 栈指针回退，栈“弹出”一个值\n\n算术与逻辑指令\n\n\n指令\n效果\n描述\n\n\n\nleaq S, D\nD ← &amp;S\n加载有效地址\n\n\nINC D\nD ← D + 1\n加1\n\n\nDEC D\nD ← D - 1\n减1\n\n\nNEG D\nD ← -D\n取负\n\n\nNOT D\nD ← ~D\n取补\n\n\nADD S, D\nD ← D + S\n加法\n\n\nSUB S, D\nD ← D - S\n减法\n\n\nIMUL S, D\nD ← D * S\n乘法\n\n\nXOR S, D\nD ← D ^ S\n异或\n\n\nOR S, D\n&#96;D ← D\nS&#96;\n\n\nAND S, D\nD ← D &amp; S\n与\n\n\nSAL k, D\nD ← D &lt;&lt; k\n左移（算术）\n\n\nSHL k, D\nD ← D &lt;&lt; k\n左移（与SAL相同）\n\n\nSAR k, D\nD ← D &gt;&gt;a k\n算术右移（保留符号）\n\n\nSHR k, D\nD ← D &gt;&gt;l k\n逻辑右移（高位补0）\n\n\n上述加载有效地址（load effective address）指令 leaq 实际上是 movq 指令的变形。它的指令形式是从内存读数据到寄存器，但实际上它根本就没有引用内存。它的第一个操作数看上去是一个内存引用，但该指令并不是从指定的位置读入数据，而是将有效地址写入到目的操作数。类似于 C 语言中的 &amp;x —— 不是取变量的值，而是取变量的“地址”。\n特殊的算术操作当两个 64 位的整数相乘时，结果可能超过 64 位，因此需要 128 位 来表示结果。\nx86-64 指令集中支持这种情况的指令并不多，专门提供了一些用于处理 128 位乘积和除法 的指令。\n\n\n\n指令\n效果\n描述\n\n\n\nimulq S\n%rdx:%rax ← S × %rax（符号扩展）\n有符号乘法\n\n\nmulq S\n%rdx:%rax ← S × %rax（零扩展）\n无符号乘法\n\n\nclto\n%rdx ← 符号扩展(%rax)\n转换为八字\n\n\nidivq S\n%rax ← (%rdx:%rax) ÷ S``%rdx ← (%rdx:%rax) mod S\n有符号除法\n\n\ndivq S\n%rax ← (%rdx:%rax) ÷ S``%rdx ← (%rdx:%rax) mod S\n无符号除法\n\n\nimulq S：\n有符号乘法，将 S 与 %rax 相乘，结果保存在 %rdx:%rax 中。\n有“符号扩展”的作用，适用于带正负号的整数。\n\nmulq S：\n无符号乘法（没有正负号），同样保存在 %rdx:%rax。\n\n举例：\nmovq $5, %raxmovq $6, %rbximulq %rbx   ; S = %rbx\n\nRFLAGS寄存器除了整数寄存器之外，CPU 还维护一个单个位的条件码寄存器（flag），用来记录最近的操作是否：\n\n有溢出\n结果是否为 0\n是否是负数\n是否进位\n\n最常用的条件码有：\n\n\n\n条件码\n含义\n说明\n\n\n\nCF\nCarry Flag\n进位标志：无符号加法进位、减法借位\n\n\nZF\nZero Flag\n零标志：结果为 0\n\n\nSF\nSign Flag\n符号标志：结果为负（最高位为 1）\n\n\nOF\nOverflow Flag\n溢出标志：有符号加法或减法结果溢出（例如正数+正数得到负数）\n\n\n举例1：\nmov $0xFFFFFFFFFFFFFFFF, %raxadd $1, %rax     ; 溢出了\n\n%rax 原本是最大无符号值，加 1 会溢出，导致：\n\nCF = 1（发生进位）\nZF = 1（结果变为 0）\n\n举例2：\ncmp %rax, %rbx   ; 实际做的是: %rbx - %rax，设置条件码je equal_label   ; 如果 ZF == 1，跳转（即相等）\n\ncmp %rax, %rbx 会执行：%rbx - %rax，但不保存结果，只是设置条件码（flags）\n如果 结果为 0（即 %rax == %rbx），那么 ZF（Zero Flag）会被置为 1\n紧接着 je equal_label 会检查 ZF 是否为 1：\n\n如果是，跳转到标签 equal_label 处继续执行\n否则，顺序执行下一条指令\n\nSET指令指令是用来“读取条件码的状态”，并把它转换成布尔值（0 或 1）存入一个字节中（只能设置一个字节）。\n举例：\ncmp %rax, %rbx     ; 比较 %rbx - %rax，设置条件码setl %al           ; 如果 %rbx &lt; %rax（有符号），%al = 1，否则 = 0\n\n上述通过读取 RFLAGS 寄存器中的条件码位（如 ZF、SF、OF、CF）来判断是否成立。\n\n\n\n指令\n同义名\n效果\n设置条件\n\n\n\nsete D\nsetz\nD ← ZF\n相等 &#x2F; 零\n\n\nsetne D\nsetnz\nD ← ¬ZF\n不等 &#x2F; 非零\n\n\nsets D\n\nD ← SF\n负数\n\n\nsetns D\n\nD ← ¬SF\n非负数\n\n\nsetg D\nsetnle\nD ← (SF = OF) ∧ ¬ZF\n大于（有符号）\n\n\nsetge D\nsetnl\nD ← (SF = OF)\n大于等于（有符号）\n\n\nsetl D\nsetnge\nD ← SF ≠ OF\n小于（有符号）\n\n\nsetle D\nsetng\nD ← (SF ≠ OF) ∨ ZF\n小于等于（有符号）\n\n\nseta D\nsetnbe\nD ← ¬CF ∧ ¬ZF\n大于（无符号）\n\n\nsetae D\nsetnb\nD ← ¬CF\n大于等于（无符号）\n\n\nsetb D\nsetnae\nD ← CF\n小于（无符号）\n\n\nsetbe D\nsetna\nD ← CF ∨ ZF\n小于等于（无符号）\n\n\n跳转指令跳转指令的作用包括，实现条件判断（if &#x2F; else），实现循环（while &#x2F; for &#x2F; do while），实现函数跳转 &#x2F; 返回， 实现无条件跳转（goto）\n\n\n\n指令\n同义名\n跳转条件\n描述\n\n\n\njmp Label\n\n1\n直接跳转\n\n\njmp *Operand\n\n\n间接跳转\n\n\nje Label\njz\nZF\n相等 &#x2F; 零\n\n\njne Label\njnz\n¬ZF\n不相等 &#x2F; 非零\n\n\njs Label\n\nSF\n负数\n\n\njns Label\n\n¬SF\n非负数\n\n\njg Label\njnle\n(SF &#x3D; OF) ∧ ¬ZF\n大于（有符号）\n\n\njge Label\njnl\nSF &#x3D; OF\n大于等于（有符号）\n\n\njl Label\njnge\nSF ≠ OF\n小于（有符号）\n\n\njle Label\njng\n(SF ≠ OF) ∨ ZF\n小于等于（有符号）\n\n\nja Label\njnbe\n¬CF ∧ ¬ZF\n超过（无符号）\n\n\njae Label\njnb\n¬CF\n超过或相等（无符号≥）\n\n\njb Label\njnae\nCF\n低于（无符号）\n\n\njbe Label\njna\nCF ∨ ZF\n低于或相等（无符号≤）\n\n\n","categories":["《深入理解计算机系统》"],"tags":["汇编语言"]},{"title":"内核socket套接字的创建","url":"/2025/05/21/%E5%86%85%E6%A0%B8socket%E5%88%9B%E5%BB%BA/","content":"内核socket创建用户程序执行syscall指令，系统会从用户态陷入内核态并根据传入的系统调用号（例如在x86架构下socket系统调用号为41）从系统调用表中找到对应的处理函数，socekt（）系统调用对应的处理函数如下所示：\nSYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)&#123; return __sys_socket(family, type, protocol);&#125;\n\n上述宏SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)经过一系列展开后其实就是系统调用表中系统调用号41对应的处理函数\n上述__sys_socket函数定义如下：\n1.sock_create（）int __sys_socket(int family, int type, int protocol)&#123; struct socket *sock; int flags; //根据传入的参数创建socket sock = __sys_socket_create(family, type,       update_socket_protocol(family, type, protocol)); if (IS_ERR(sock))  return PTR_ERR(sock); //不关心低4bit sock的type flags = type &amp; ~SOCK_TYPE_MASK; if (SOCK_NONBLOCK != O_NONBLOCK &amp;&amp; (flags &amp; SOCK_NONBLOCK))  //清位之后置位  flags = (flags &amp; ~SOCK_NONBLOCK) | O_NONBLOCK; //将socket映射一个文件描述符号 return sock_map_fd(sock, flags &amp; (O_CLOEXEC | O_NONBLOCK));&#125;\n\n该函数主要做了两个事情，创建socekt和映射描述符fd。\n上面创建socket函数__sys_socket_create定义如下：\n2.__sys_socket_create（）static struct socket *__sys_socket_create(int family, int type, int protocol)&#123; struct socket *sock; int retval; /* Check the SOCK_* constants for consistency.  */ BUILD_BUG_ON(SOCK_CLOEXEC != O_CLOEXEC); BUILD_BUG_ON((SOCK_MAX | SOCK_TYPE_MASK) != SOCK_TYPE_MASK); BUILD_BUG_ON(SOCK_CLOEXEC &amp; SOCK_TYPE_MASK); BUILD_BUG_ON(SOCK_NONBLOCK &amp; SOCK_TYPE_MASK); //用户参数合法性检查，是否有非法标志位 if ((type &amp; ~SOCK_TYPE_MASK) &amp; ~(SOCK_CLOEXEC | SOCK_NONBLOCK))  return ERR_PTR(-EINVAL); type &amp;= SOCK_TYPE_MASK; //创建套接字 retval = sock_create(family, type, protocol, &amp;sock); if (retval &lt; 0)  return ERR_PTR(retval); return sock;&#125;\n\n上述sock_create为实际创建socket的函数，它包裹了__sock_create()如下所示：\nint sock_create(int family, int type, int protocol, struct socket **res)&#123; return __sock_create(current-&gt;nsproxy-&gt;net_ns, family, type, protocol, res, 0);&#125;\n\n可以看到__sock_create多带了一个参数current-&gt;nsproxy-&gt;net_ns  这个current是一个宏，用于获取当前进程的task_struct指针\ncurrent宏定义如下:\nstatic __always_inline struct task_struct *get_current(void)&#123; //这个pcpu_hot中有一个字段就是current_task也就是task_struct //this_cpu_read_stable()就是读取per_cpu变量的一个宏 return this_cpu_read_stable(pcpu_hot.current_task);//从per-cpu变量中获取当前的task_sturct结构&#125;\n\n上述current宏其实等同于指向一个task_struct的指针，而__sock_create(current-&gt;nsproxy-&gt;net_ns, family, type, protocol, res, 0);中参数current-&gt;nsproxy-&gt;net_ns就是指向一个具体的网络命名空间，为什么要传入这个网络命令空间作为参数？举个例子，如果进程属于某个容器的网络命名空间，创建的套接字必须关联到该容器的网络栈，而非宿主机的默认命名空间， 比如在创建docker进程的时候，就会设置sproxy-&gt;net_ns所属的网络命令空间。最终的目的一定是为了流量隔离。\n接下来看一下真正创建socket的函数__sock_create\n3.__sock_create()int __sock_create(struct net *net, int family, int type, int protocol,\t\t\t struct socket **res, int kern)&#123;\tint err;\tstruct socket *sock;\tconst struct net_proto_family *pf;\t//合法性检查\tif (family &lt; 0 || family &gt;= NPROTO)\t\treturn -EAFNOSUPPORT;\tif (type &lt; 0 || type &gt;= SOCK_MAX)\t\treturn -EINVAL;\t//过时的 PF_INET + SOCK_PACKET 参数组合转换为现代支持的 PF_PACKET 协议族\tif (family == PF_INET &amp;&amp; type == SOCK_PACKET) &#123;\t\tpr_info_once(&quot;%s uses obsolete (PF_INET,SOCK_PACKET)\\n&quot;,\t\t\t     current-&gt;comm);\t\tfamily = PF_PACKET;\t&#125;\t//安全相关的钩子\terr = security_socket_create(family, type, protocol, kern);\tif (err)\t\treturn err;\t//分配并初始化一个套接字对应的 inode 和socket 结构\tsock = sock_alloc();\tif (!sock) &#123;\t\tnet_warn_ratelimited(&quot;socket: no more sockets\\n&quot;);\t\treturn -ENFILE;\t//这里的type也是用户创建socket的type\tsock-&gt;type = type;\trcu_read_lock();\t//从sock_register数组中找到一个元素pf，这个pf中有一个create()回调函数，\t//这个回调函数就是family类型(比如AF_INET)需要的create函数。\tpf = rcu_dereference(net_families[family]);\terr = -EAFNOSUPPORT;\tif (!pf)\t\tgoto out_release;\t//增加引用计数？有些family可能是以模块方式加载的？？？\tif (!try_module_get(pf-&gt;owner))\t\tgoto out_release;\t/* Now protected by module ref count */\trcu_read_unlock();\t//如果用户指定的family类型是AF_INIT,那这个函数就是调用的inet_create()\terr = pf-&gt;create(net, sock, protocol, kern);\tif (err &lt; 0)\t\tgoto out_module_put;\tif (!try_module_get(sock-&gt;ops-&gt;owner))\t\tgoto out_module_busy;\t//减引用计数\tmodule_put(pf-&gt;owner);\t//安全模块相关\terr = security_socket_post_create(sock, family, type, protocol, kern);\tif (err)\t\tgoto out_sock_release;\t*res = sock;\treturn 0;out_module_busy:\terr = -EAFNOSUPPORT;out_module_put:\tsock-&gt;ops = NULL;\tmodule_put(pf-&gt;owner);out_sock_release:\tsock_release(sock);\treturn err;out_release:\trcu_read_unlock();\tgoto out_sock_release;&#125;EXPORT_SYMBOL(__sock_create);\n\n\n上述代码中通过调用sock_alloc()分配了inode和socket结构体，并对inode结构体进行初始化，比如设置唯一的inode编号等，具体代码如下：\nstruct socket *sock_alloc(void)&#123;\tstruct inode *inode;\tstruct socket *sock;\t//调用socket文件系统的超级块的ops申请一个inode，注意：socket结构体也是在这里分配的\tinode = new_inode_pseudo(sock_mnt-&gt;mnt_sb);\tif (!inode)\t\treturn NULL;\t//通过container_of拿到socket结构体 \tsock = SOCKET_I(inode);\tinode-&gt;i_ino = get_next_ino();//分配唯一的inode编号\tinode-&gt;i_mode = S_IFSOCK | S_IRWXUGO; //文件类型\tinode-&gt;i_uid = current_fsuid();\tinode-&gt;i_gid = current_fsgid();\tinode-&gt;i_op = &amp;sockfs_inode_ops;//绑定ops\treturn sock;&#125;\n\n上述代码通过调用new_inode_pseudo()创建了inode和socket，socket的获取通过宏SOCKET_I（container_of）返回socket其中sock_mnt是一个vfsmount(可以理解为一个挂载点)结构mnt_sb为一个超级块，在sock_init()中被挂载，sock_init()在start_kernel中会最终被调用到。\npf = rcu_dereference(net_families[family]); 这一行作用是根据用户传入的不同的协议族（比如AF_INET）来选择具体的回调函数，然后会调用pf-&gt;create(net, sock, protocol, kern); 这个-&gt;create() 就取决于family的类型。对于AF_INET类型的family，就是调用inet_create()，注册的过程由sock_register()实现，该函数就是将不同的family类型，注册到一个数组中(这个数组叫net_families)。对应的函数如下：\nint sock_register(const struct net_proto_family *ops)&#123;\tint err;\tif (ops-&gt;family &gt;= NPROTO) &#123;\t\tpr_crit(&quot;protocol %d &gt;= NPROTO(%d)\\n&quot;, ops-&gt;family, NPROTO);\t\treturn -ENOBUFS;\t&#125;\tspin_lock(&amp;net_family_lock);\tif (rcu_dereference_protected(net_families[ops-&gt;family],\t\t\t\t      lockdep_is_held(&amp;net_family_lock)))\t\terr = -EEXIST;\telse &#123;\t\t//这里注册了不同family类型到net_families数组中！\t\trcu_assign_pointer(net_families[ops-&gt;family], ops);\t\terr = 0;\t&#125;\tspin_unlock(&amp;net_family_lock);\tpr_info(&quot;NET: Registered %s protocol family\\n&quot;, pf_family_names[ops-&gt;family]);\treturn err;&#125;\n\n对于AF_INET(ipv4)协议族，上述注册的函数为inet_create()，在inet_init()中被调用，同样inet_init()也是最终被start_kernel()调用到。\n也就是说err = pf-&gt;create(net, sock, protocol, kern);会根据协议族的类型调用不同的create函数，同时传入用户制定的类型(TYPE)和协议做为参数，下面默认使用ipv4协议族进行举例，待分析函数就是inet_create()函数实现如下所示：\n也就是说err = pf-&gt;create(net, sock, protocol, kern);会根据协议族的类型调用不同的create函数，同时传入用户制定的类型(TYPE)和协议做为参数，下面使用ipv4协议族进行举例，对应的函数就是inet_create()，该函数其实主要处理了三个逻辑：\n\n根据用户制定协议从inetsw找到socket和sock对应的ops\n创建sock结构，并进行一系列的初始化（例如绑定sock的ops，这里不同的协议对应不同的ops）\n调用sock的init函数，完成对具体协议的初始化inet_create函数定义如下：\n\nstatic int inet_create(struct net *net, struct socket *sock, int protocol,\t\t       int kern)&#123;\tstruct sock *sk;\tstruct inet_protosw *answer;\tstruct inet_sock *inet;\tstruct proto *answer_prot;\tunsigned char answer_flags;\tint try_loading_module = 0;\tint err;\t\t//参数合法性检查\tif (protocol &lt; 0 || protocol &gt;= IPPROTO_MAX)\t\treturn -EINVAL;\t//初始化socket的状态\tsock-&gt;state = SS_UNCONNECTED;\t/* Look for the requested type/protocol pair. */lookup_protocol:\terr = -ESOCKTNOSUPPORT;\trcu_read_lock();\t//遍历inetsw[sock-&gt;type]这个元素的链表，找到protocol相同的元素，\tlist_for_each_entry_rcu(answer, &amp;inetsw[sock-&gt;type], list) &#123;\t\terr = 0;\t\t/* Check the non-wild match. */\t\t//精确匹配，用户指定的protocol和链表中的某个元素相同。\t\tif (protocol == answer-&gt;protocol) &#123;\t\t\tif (protocol != IPPROTO_IP)\t\t\t\tbreak;\t\t&#125; else &#123;\t\t\t/* Check for the two wild cases. */\t\t\t//如果用户指定的proto是0那就走这个分支，\t\t\t//比如type是SOCK_STREAM，proto=0 那answer关联的就是TCP\t\t\tif (IPPROTO_IP == protocol) &#123;\t\t\t\tprotocol = answer-&gt;protocol;\t\t\t\tbreak;\t\t\t&#125;\t\t\tif (IPPROTO_IP == answer-&gt;protocol)\t\t\t\tbreak;\t\t&#125;\t\terr = -EPROTONOSUPPORT;\t&#125;\t//错误的处理\tif (unlikely(err)) &#123;\t\tif (try_loading_module &lt; 2) &#123;\t\t\trcu_read_unlock();\t\t\t/*\t\t\t * Be more specific, e.g. net-pf-2-proto-132-type-1\t\t\t * (net-pf-PF_INET-proto-IPPROTO_SCTP-type-SOCK_STREAM)\t\t\t */\t\t\tif (++try_loading_module == 1)\t\t\t\trequest_module(&quot;net-pf-%d-proto-%d-type-%d&quot;,\t\t\t\t\t       PF_INET, protocol, sock-&gt;type);\t\t\t/*\t\t\t * Fall back to generic, e.g. net-pf-2-proto-132\t\t\t * (net-pf-PF_INET-proto-IPPROTO_SCTP)\t\t\t */\t\t\telse\t\t\t\trequest_module(&quot;net-pf-%d-proto-%d&quot;,\t\t\t\t\t       PF_INET, protocol);\t\t\tgoto lookup_protocol;\t\t&#125; else\t\t\tgoto out_rcu_unlock;\t&#125;\terr = -EPERM;\t//用户有权限才能创建raw socket套接字\tif (sock-&gt;type == SOCK_RAW &amp;&amp; !kern &amp;&amp;\t    !ns_capable(net-&gt;user_ns, CAP_NET_RAW))\t\tgoto out_rcu_unlock;\t//将上述找到的answer-&gt;ops赋值给socket的ops\tsock-&gt;ops = answer-&gt;ops;\t//将上述找到的answer-&gt;ops赋值给answer_prot，下面创建sock结构的时候会用到\tanswer_prot = answer-&gt;prot;\tanswer_flags = answer-&gt;flags;\trcu_read_unlock();\tWARN_ON(!answer_prot-&gt;slab);\terr = -ENOMEM;\t//注意： 这里申请一个sock结构，这个sock结构可以理解为传输层协议和socket之间的一个中间层\t//对上提供socket层的结构，\t//对下与具体的协议相关\t//kern 标识这个套接字是否是内核创建的\tsk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot, kern);\tif (!sk)\t\tgoto out;\terr = 0;\t//标识端口是否可以重用 这里raw 和icmp是设置了INET_PROTOSW_REUSE 这个标志位。\tif (INET_PROTOSW_REUSE &amp; answer_flags)\t\tsk-&gt;sk_reuse = SK_CAN_REUSE;\tinet = inet_sk(sk);\t//是否是一个面向连接套接字，对于TCP是有这个标志位的\tinet_assign_bit(IS_ICSK, sk, INET_PROTOSW_ICSK &amp; answer_flags);\tinet_clear_bit(NODEFRAG, sk);\t//如果是rawsocket就指定了端口号？\tif (SOCK_RAW == sock-&gt;type) &#123;\t\tinet-&gt;inet_num = protocol;\t\tif (IPPROTO_RAW == protocol)\t\t\tinet_set_bit(HDRINCL, sk);\t&#125;\t//根据系统参数决定是否开启mtu探测\tif (READ_ONCE(net-&gt;ipv4.sysctl_ip_no_pmtu_disc))\t\tinet-&gt;pmtudisc = IP_PMTUDISC_DONT;\telse\t\tinet-&gt;pmtudisc = IP_PMTUDISC_WANT;\t\t//设置ip_id字段\tatomic_set(&amp;inet-&gt;inet_id, 0);\t//这里初始化了上面申请的sock结构体的各个字段\tsock_init_data(sock, sk);\tsk-&gt;sk_destruct\t   = inet_sock_destruct;\t//这里记录了用户指定的协议\tsk-&gt;sk_protocol\t   = protocol;\tsk-&gt;sk_backlog_rcv = sk-&gt;sk_prot-&gt;backlog_rcv;\tsk-&gt;sk_txrehash = READ_ONCE(net-&gt;core.sysctl_txrehash);\t//初始化inet_sock的一些字段 单播/多播ttl，tos，管理多播的mc_list\tinet-&gt;uc_ttl\t= -1;   \tinet_set_bit(MC_LOOP, sk);\tinet-&gt;mc_ttl\t= 1;\tinet_set_bit(MC_ALL, sk);\tinet-&gt;mc_index\t= 0;\tinet-&gt;mc_list\t= NULL;\tinet-&gt;rcv_tos\t= 0;\t//tcp或者udp 应该不会走这个逻辑，因为还没有调用bind，inet_num此时应该为0\tif (inet-&gt;inet_num) &#123;\t\tinet-&gt;inet_sport = htons(inet-&gt;inet_num);\t\t/* Add to protocol hash chains. */\t\terr = sk-&gt;sk_prot-&gt;hash(sk);\t\tif (err) &#123;\t\t\tsk_common_release(sk);\t\t\tgoto out;\t\t&#125;\t&#125;\t//这里是特定协议的初始化逻辑\tif (sk-&gt;sk_prot-&gt;init) &#123;\t\terr = sk-&gt;sk_prot-&gt;init(sk);\t\tif (err) &#123;\t\t\tsk_common_release(sk);\t\t\tgoto out;\t\t&#125;\t&#125;\tif (!kern) &#123;\t\terr = BPF_CGROUP_RUN_PROG_INET_SOCK(sk);\t\tif (err) &#123;\t\t\tsk_common_release(sk);\t\t\tgoto out;\t\t&#125;\t&#125;out:\treturn err;out_rcu_unlock:\trcu_read_unlock();\tgoto out;&#125;\n\n上述代码首先根据用户指定的type和protocol类型从inewsw[]中找到匹配的socket和sock的ops，注意这里inewsw[]是一个数组，数组中的每个元素又是一个链表，其实可以理解成一个hash表，hash表的key是type，而protocol是用来寻找某个桶中的的具体的一个元素。上述的inet_sw数组中的元素是由inetsw_array[]中填充进来的，填充的过程在inet_init()函数中实现。inetsw_array[]数组的定义和填充inetsw[]的代码如下：\n//这个数组的作用就是把数组中的元素注册到inet_sw[]中static struct inet_protosw inetsw_array[] =&#123;\t&#123;\t\t.type =       SOCK_STREAM,\t\t.protocol =   IPPROTO_TCP,\t\t.prot =       &amp;tcp_prot,\t\t.ops =        &amp;inet_stream_ops,\t\t.flags =      INET_PROTOSW_PERMANENT |\t\t\t      INET_PROTOSW_ICSK,\t&#125;,\t&#123;\t\t.type =       SOCK_DGRAM,\t\t.protocol =   IPPROTO_UDP,\t\t.prot =       &amp;udp_prot,\t\t.ops =        &amp;inet_dgram_ops,\t\t.flags =      INET_PROTOSW_PERMANENT,       &#125;,       &#123;\t\t.type =       SOCK_DGRAM,\t\t.protocol =   IPPROTO_ICMP,\t\t.prot =       &amp;ping_prot,\t\t.ops =        &amp;inet_sockraw_ops,\t\t.flags =      INET_PROTOSW_REUSE,       &#125;,       &#123;\t       .type =       SOCK_RAW,\t       .protocol =   IPPROTO_IP,\t/* wild card */\t       .prot =       &amp;raw_prot,\t       .ops =        &amp;inet_sockraw_ops,\t       .flags =      INET_PROTOSW_REUSE,       &#125;&#125;;\n\n上述代码为inetsw_array[]数组，其中prot为socket的ops，用户态不同的系统调用会调用到socket的不同ops上。prot则为具体协议的ops。也就是说ops是socket关联的回调函数，prot为sock关联的回调函数，两者其实是密切相关的，可以理解为ops是用户与内核的一个桥梁或者中间层，而prot则是具体的实现。\n注册inetsw_array到inet_sw[]数组中的代码在inet_init()中，代码如下：\n//遍历inetsw_array数组中的元素后调用inet_register_protosw函数将元素插入到inetsw[]中\tfor (q = inetsw_array; q &lt; &amp;inetsw_array[INETSW_ARRAY_LEN]; ++q)\t\tinet_register_protosw(q);void inet_register_protosw(struct inet_protosw *p)&#123;\tstruct list_head *lh;\tstruct inet_protosw *answer;\tint protocol = p-&gt;protocol;\tstruct list_head *last_perm;\tspin_lock_bh(&amp;inetsw_lock);\t//合法性检查\tif (p-&gt;type &gt;= SOCK_MAX)\t\tgoto out_illegal;\t//last_perm保存的是一个socket-&gt;type中最后一个永久协议的位置\tlast_perm = &amp;inetsw[p-&gt;type];\tlist_for_each(lh, &amp;inetsw[p-&gt;type]) &#123;\t\tanswer = list_entry(lh, struct inet_protosw, list);\t\t/* Check only the non-wild match. */\t\t//不是永久协议的情况（TCP/UDP为永久协议）\t\tif ((INET_PROTOSW_PERMANENT &amp; answer-&gt;flags) == 0)\t\t\tbreak;\t\t//和永久协议的protocol一样\t\tif (protocol == answer-&gt;protocol)\t\t\tgoto out_permanent;\t\t//走到这里给永久协议赋值\t\tlast_perm = lh;\t&#125;\t//将新的协议注册到协议之后。\tlist_add_rcu(&amp;p-&gt;list, last_perm);out:\tspin_unlock_bh(&amp;inetsw_lock);\treturn;out_permanent:\tpr_err(&quot;Attempt to override permanent protocol %d\\n&quot;, protocol);\tgoto out;out_illegal:\tpr_err(&quot;Ignoring attempt to register invalid socket type %d\\n&quot;,\t       p-&gt;type);\tgoto out;&#125;\n","categories":["网络协议栈源码学习"],"tags":["socket"]},{"title":"网卡硬件各组件","url":"/2025/05/23/%E7%BD%91%E5%8D%A1%E7%A1%AC%E4%BB%B6%E6%A8%A1%E5%9D%97/","content":"网卡核心硬件组成现代有线以太网卡中，MAC、PHY、DMA、PCIe 是网卡最核心的硬件模块，这些模块构成了数据通信的基础框架。以下是它们的详细分工和协作关系，具体的架构图如下所示：\nPHYPHY 层属于 OSI 物理层（Layer 1），主要负责 数字信号 ↔ 模拟信号 的转换，具体包括：\n\n链路管理\n\n\n自动协商​​：与对端设备协商速率（如10&#x2F;100&#x2F;1000 Mbps）和双工模式。\n链路检测​​：监测连接状态（如网线是否插入）。\n\n\n信号转换​\n\n\n数模转换​​：将MAC层生成的数字信号转换为适合线缆（如双绞线、光纤）传输的模拟信号（如电信号或光信号）。\n模数转换​​：将接收到的模拟信号还原为数字信号供上层处理。\n\n\n物理介质适配​\n\n支持不同介质标准（如以太网的RJ-45接口、光纤接口），适应电压、阻抗等物理特性,例如：100BASE-TX（双绞线）、1000BASE-SX（光纤）等。\n\n编码与解码\n\n使用特定编码方案（如曼彻斯特编码、PAM4）以提高抗干扰能力，确保信号完整性。\nMAC\n发送数据时​​，计算 ​​CRC（循环冗余校验）​​，确保数据完整性。\n接收数据时​​：\n从 PHY 层接收原始比特流，解析成以太网帧。\n检查​目标MAC地址​​（仅接收发给本机、广播或组播的帧）。\n校验 ​​FCS​​，丢弃损坏的帧。\n\n\n流量控制：使用 ​​PAUSE 帧（IEEE 802.3x）​​ 通知对端设备暂停发送，防止缓冲区溢出。\n\nDMA数据直接传输，网卡通过DMA引擎直接读写主机内存\nRSSRSS 是一种由 网卡硬件实现 的多队列技术，主要用于 提升多核 CPU 的网络数据包处理性能。网卡硬件将流量分散到多个接收队列（RX Queues），每个队列绑定不同CPU核心。\nTSOTSO 是一种由 网卡硬件实现 的优化技术，旨在 将TCP数据包的分片（Segmentation）任务从CPU转移到网卡，从而大幅降低CPU负载并提升网络吞吐量。\nPCIe\n提供网卡与CPU&#x2F;内存的物理通道，决定最大带宽（如100G需PCIe 4.0 x8）。\n支持DMA、MSI-X中断，优化响应速度\n\n","categories":["其他"],"tags":["网卡"]},{"title":"处理器体系结构","url":"/2025/06/05/%E5%A4%84%E7%90%86%E5%99%A8%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%80%EF%BC%89/","content":"处理器体系结构一个处理器支持的指令和指令的字节级编码称为它的指令集体系结构(Instruction-Set Architecture, ISA) \n本章将简要介绍处理器硬件的设计，学习一个硬件系统执行某种 ISA指令的方式，以更好地理解计算机是如何工作的。\n本章首先定义一个简单的指令集，作为处理器实现的运行示例。因为受 x86-64 指令集的启发，它被俗称为 “x86”, 所以我们称我们的指令集为 “Y86-64” 指令集。 与 x86-64 相比， Y86-64 指令集的数据类型、指令和寻址方式都要少一些。它的字节级编码也比较简单，虽然 Y86-64 指令集很简单，它仍然足够完整，能让我们写一些处理整数的程序。\nY86-64 的寄存器\n寄存器：Y86-64 有 15 个通用寄存器（与 x86-64 类似），如 %rax、%rsp、%rbp 等，每个存储 64 位数据。%rsp 主要用作栈指针。\n\n条件码：ZF、SF、OF 用于保存最近计算的结果信息。\n\n程序计数器（PC）：存储当前将要执行的指令地址。\n\n\nY86-64 指令\ny86-64 的 movq 指令分成了 4 个不同的指令：irmovq、rrmovq、mrmovq 和 rmmovq，分别显示地指明源和目的的格式。源可以是立即数（i）、寄存器（r）或内存（m）。指令名字的第一个字母就表明了源的类型，目前可以是寄存器（r）或内存（m）。指令名字的第二个字母指明了目的的类型。它决定如何实现数据传送时，显式地指明数据传送的路径。\n有 4 个整数操作指令，如图它们是 addq、subq、andq 和 xorq。它们只对寄存器数据进行操作，而 Y86-64 还允许对内存数据进行这些操作。这些指令会设置 3 个条件码 ZF、SF 和 OF（零、符号和溢出）。\n7 个跳转指令是 jmp、jle、jl、je、jne、jge 和 jg。根据分支指令的类型和条件代码的设置来选择分支。分支条件和 x86-64 的一样）。\n有 6 个条件传送指令：cmovle、cmovl、cmove、cmovne、cmovge 和 cmovg。这些指令的格式与寄存器-寄存器传送指令 rrmovq 一样，但是只有当条件码满足所需的要求时，才会更新目的寄存器的值。\ncall 指令将返回地址入栈，然后跳到目的地址。ret 指令从栈中调用出返回。\npushq 和 popq 指令实现了入栈和出栈，和 x86-64 中一样。\nY86-64 只有一条与主机操作相关的指令 halt。x86-64 的应用程序不允许使用这条指令，因为它会导致整个系统停止运行。对于 Y86-64 来说，执行 halt 指令会导致处理器终止，并将状态设置为 HLT。\n\n指令和对应的编码如下所示：\n\n\n\n指令\n字节 0\n字节 1\n字节 2\n字节 3\n字节 4\n字节 5\n字节 6\n字节 7\n字节 8\n字节 9\n\n\n\nhalt\n00\n\n\n\n\n\n\n\n\n\n\n\nnop\n10\n\n\n\n\n\n\n\n\n\n\n\nrrmovq rA, rB\n20\nrA rB\n\n\n\n\n\n\n\n\n\n\nirmovq V, rB\n30\nF rB\nV\nV\nV\nV\nV\nV\nV\nV\n\n\nrmmovq rA, D(rB)\n40\nrA rB\nD\nD\nD\nD\nD\nD\nD\nD\n\n\nmrmovq D(rB), rA\n50\nrA rB\nD\nD\nD\nD\nD\nD\nD\nD\n\n\nOPq rA, rB\n6 fn\nrA rB\n\n\n\n\n\n\n\n\n\n\njXX Dest\n7 fn\nDest\nDest\nDest\nDest\nDest\nDest\nDest\nDest\nDest\n\n\ncmovXX rA, rB\n2 fn\nrA rB\n\n\n\n\n\n\n\n\n\n\ncall Dest\n80\nDest\nDest\nDest\nDest\nDest\nDest\nDest\nDest\nDest\n\n\nret\n90\n\n\n\n\n\n\n\n\n\n\n\npushq rA\nA0\nrA F\n\n\n\n\n\n\n\n\n\n\npopq rA\nB0\nrA F\n\n\n\n\n\n\n\n\n\n\n说明：\n\nrA, rB 表示寄存器编号（各4位）。\nF 表示无效寄存器编号。\nfn 表示功能码&#x2F;条件码。\nV 表示立即数（8字节）。\nD 表示位移（8字节）。\nDest 表示目标地址（8字节）。\n\n有的指令只有一个字节长，而有的需要操作数的指令编码就更长一些。\nRISC 和 CISC 指令集\n\n\n特性\nCISC\n早期的 RISC\n\n\n\n指令数量\n指令数量很多。Intel 描述全套指令的文档有 1200 多页。\n指令数量少得多，通常少于 100 个。\n\n\n延迟执行的指令\n有些指令的延迟很长，如从内存复制块、复杂地址传递等。\n没有较长延迟的指令，有些早期 RISC 甚至没有整数乘除法指令，需用其他方式实现。\n\n\n指令长度\n编码是可变长度的，x86-64 的指令长度可以是 1~15 字节。**\n编码是固定长度的，通常所有指令都是 4 字节。\n\n\n地址操作方式\n地址操作方式丰富，支持多种寻址模式，如偏移+基址+变址寄存器+缩放因子组合。\n简单寻址方式，通常只有基址加偏移寻址。\n\n\n内存和寄存器操作\n支持对内存和寄存器数据进行算术和逻辑运算。\n通常只能对寄存器数据进行算术和逻辑运算。与内存交互需使用 load&#x2F;store 指令，属于 load&#x2F;store 架构。\n\n\n程序级细节可见性\n对机器级程序来说实现细节不可见，ISA 抽象了程序之间执行顺序的细节。\n机器级程序实现细节可见，有些 RISC 禁止特定指令序列，必须满足约束条件后才可执行。\n\n\n条件码\n有条件码作为副产品（如 ZF、SF、OF），用于条件分支检测。\n没有条件码，需要通过测试指令将结果放入普通寄存器，再基于此判断。\n\n\n过程链接\n使用栈传参和返回地址，栈被用来存储过程参数和返回地址。\n使用大量寄存器进行参数传递与返回，避免对内存引用。通常有 32 个以上寄存器。\n\n\nRISC 和 CISC 和核心区别如下:设计理念不同：\n\nRISC 追求“简单指令 + 快速流水线”\nCISC 追求“复杂指令 + 高表达能力 + 更节省代码空间”\n\n指令执行粒度不同：\n\nRISC 一条指令只做一件事（例如：加法只能寄存器之间）\nCISC 一条指令可以做多件事（例如：内存中的数+寄存器的数→再写回内存）\n\n是否 Load&#x2F;Store 架构：\n\nRISC：运算只能在寄存器之间，访问内存要用专门的 load/store\nCISC：可以直接对内存进行加减乘除等操作（如 add [eax], ebx）\n\n举个例子对比：\nRISC 指令：\nLDR r0, [r1]LDR r2, [r3]ADD r4, r0, r2STR r4, [r5]\n\n需要 4 条指令把内存中两个值相加后写回。\n\nCISC 指令（x86）：\nADD [eax], ebx\n\n一条指令就可以把内存中一个值和寄存器相加，并写回内存。\n\n\n指令处理**在处理一条指令时，可以将其操作组织为一系列固定的处理阶段，**每个阶段执行特定功能，有助于形成统一的执行顺序，充分利用硬件资源。各阶段及其简要说明如下：\n取指\n使用 PC（程序计数器）作为地址，从内存中读取指令字节。\n提取出指令的：\n操作码（icode）\n功能码（ifun）\n目标寄存器编号（rA, rB，如果指令需要）\n立即数（valC，如果指令包含立即数）\n\n\n\n译码根据 rA、rB 字段，从寄存器堆中读取源操作数：\n\nvalA = R[rA]\nvalB = R[rB]\n\n上面的valA 和valB是ALU的两个输入\n执行执行算术&#x2F;逻辑操作：\n\n加法、减法、位移、比较……\n\n计算地址（如内存访问地址）：\n\nvalE = valB + valC（例如内存偏移地址）\n\n计算条件分支是否成立（对条件跳转指令）\n访存\n如果是读（如 mrmovq，popq）：\nvalM = Mem[valE]\n\n\n如果是写（如 rmmovq，pushq）：\nMem[valE] = valA\n\n\n\n写回将结果写入目标寄存器\n根据指令类型，将 valE 或 valM 写入 rA 或 rB：\n\n如：R[rB] = valE 或 R[rA] = valM\n\n更新 PC更新程序计数器 PC，为下一条指令准备\n举例：\naddq %rax, %rbx 的执行流程如下：\n\n\n\n阶段\n操作说明\n\n\n\n取指\n从 PC 位置取出 addq 指令\n\n\n译码\n读出 rax → valA，rbx → valB\n\n\n执行\nALU 执行 valE &#x3D; valA + valB\n\n\n访存\n无操作\n\n\n写回\n将结果 valE 写入 rbx（目的寄存器）\n\n\n更新 PC\nPC &#x3D; valP\n\n\nmrmovq 8(%rbp), %rax 的执行流程如下\n\n\n\n阶段\n动作说明\n\n\n\n取指（Fetch）\n从内存中读取当前指令的字节内容：包括 icode（为 mrmovq）、ifun、rB（即 rbp）、rA（即 rax）和立即数 valC = 8，并计算下一条指令地址 valP = PC + 指令长度。\n\n\n译码（Decode）\n从寄存器文件中读出： - valB = R[rbp]（基地址） - valA 不使用（但可能会读取）\n\n\n执行（Execute）\n用 ALU 计算内存地址： - valE = valB + valC = R[rbp] + 8\n\n\n访存（Memory）\n从内存地址 valE 读取 8 字节数据： - valM = Mem[valE]\n\n\n写回（Write Back）\n将 valM 写入目的寄存器： - R[rax] = valM\n\n\n更新 PC（PC Update）\nPC = valP，准备执行下一条指令。\n\n\n流水线流水线（Pipeline）**是一种**提高指令吞吐率（吞吐量）*的技术，核心思想就是将指令的执行过程划分成若干个阶段，让*多条指令同时在不同阶段执行，从而**并行化处理流程，类似工业生产线。\n常见的流水线阶段（以经典的五级流水线为例）：\n\n\n\n阶段缩写\n阶段名称\n英文全称\n功能说明\n\n\n\nIF\n取指阶段\nInstruction Fetch\n从内存中读取指令\n\n\nID\n译码阶段\nInstruction Decode\n分析指令含义，读取寄存器操作数\n\n\nEX\n执行阶段\nExecute\n运算、地址计算或条件判断\n\n\nMEM\n访存阶段\nMemory Access\n对数据内存进行读&#x2F;写操作（如load&#x2F;store）\n\n\nWB\n写回阶段\nWrite Back\n将计算结果写回寄存器\n\n\n假设我们有三条指令：I1、I2、I3，它们的流水线执行如下：\n\n\n\n周期\nI1\nI2\nI3\n\n\n\n1\nIF\n\n\n\n\n2\nID\nIF\n\n\n\n3\nEX\nID\nIF\n\n\n4\nMEM\nEX\nID\n\n\n5\nWB\nMEM\nEX\n\n\n6\n\nWB\nMEM\n\n\n7\n\n\nWB\n\n\n流水线的局限性不一致划分：\n流水线各阶段所需时间不同，导致不能统一设定短时钟周期的问题。\n举例：\n设想设计一个 5 阶段流水线，划分如下：\n\n\n\n阶段\n功能\n延迟（ps）\n\n\n\nIF\n取指\n80\n\n\nID\n译码\n60\n\n\nEX\n执行（ALU）\n180\n\n\nMEM\n访存\n100\n\n\nWB\n写回\n40\n\n\n\n由于 EX 最慢，占用了 180ps；\n所以整个流水线的最短时钟周期 &#x3D; 180ps（必须以最慢阶段为限）；\n然而其他阶段（例如 WB 只要 40ps）却浪费了大量时间 → 这就是“不一致划分”。\n\n流水线太长，性能反而下降：\n分支惩罚严重扩大（Branch Penalty ↑）分支预测失败时，需要清空整个流水线，流水线越深，浪费越多。\n\n比如：\n5级流水线：预测失败，丢掉5条指令\n20级流水线：预测失败，丢掉20条指令\n\n\n\n带反馈的流水线系统在流水线系统中，指令从头到尾的执行并不总是严格按照顺序完成\n虽然每条指令在流水线中依次经过取指、译码、执行等阶段，但由于依赖关系或跳转，某些指令会被暂停、重排或撤销，不一定严格顺序完成\n比如：\n\n分支指令：需要执行阶段判断是否跳转 → 才知道下一条指令取哪里\n数据依赖：当前指令需要前一条指令的计算结果 → 必须等它先完成\n\n举例：\n1: add r1, r2, r3     ; r1 = r2 + r32: sub r4, r1, r5     ; r4 = r1 - r5\n\n\n第二条指令 依赖第一条指令的执行结果 r1；\n如果你流水线太快地推进，就可能在 r1 还没写回时，sub 就读到了旧值或错值；\n所以需要一种 机制把 r1 的新值反馈给第二条指令 → 这就是数据前递的一种反馈。\n\n\n\n\n周期\n指令1（add）\n指令2（sub）\n问题说明\n\n\n\n1\nIF\n\n\n\n\n2\nID\nIF\n\n\n\n3\nEX\nID\n❗r1 尚未计算出来，sub 读不到新值\n\n\n4\nMEM\nEX\n❌ sub 用错值执行\n\n\n5\nWB\nMEM\n\n\n\n带反馈的方法：\n\n在 add 的 EX 阶段算出 r1 的值；\n直接从 EX 阶段前递到 sub 的 EX 阶段使用，绕过寄存器文件；\n不用等到 WB 阶段再写回 → 提前“反馈”给下条指令。\n\n\n\n\n周期\n指令1（add）\n指令2（sub）\n说明\n\n\n\n1\nIF\n\n\n\n\n2\nID\nIF\n\n\n\n3\nEX\nID\n\n\n\n4\nMEM\nEX (用前递)\n✅ 从 add 的 EX 阶段取值\n\n\n5\nWB\nMEM\n\n\n\n流水线冒险*当多条指令在流水线中*同时执行时，由于它们之间存在某些冲突或依赖关系**，导致流水线无法按原计划推进，甚至必须暂停、插入空周期或重新执行。\n\n\n\n类型\n全称\n说明\n\n\n\n1️⃣ 数据冒险（Data Hazard）\n指令之间有数据依赖\n后面的指令使用前面指令尚未计算出的结果\n\n\n2️⃣ 控制冒险（Control Hazard）\n与跳转&#x2F;分支指令相关\n分支结果未确定时，无法决定取哪条指令\n\n\n\n\n\n\n\n数据冒险：\nadd r1, r2, r3    ; r1 = r2 + r3sub r4, r1, r5    ; r4 = r1 - r5  ← 依赖上面 r1 的结果\n\n\n如果 sub 提前执行，会用到错误的 r1 → 数据冒险\n解决方法：数据前递（Forwarding）、暂停（Stall）（好像就是插入气泡）\n\n控制冒险（Control Hazard）asm复制编辑beq r1, r2, label  ; 如果相等则跳转add r3, r4, r5      ; 跳 or 不跳？影响下一条指令是否执行\n\n\n如果跳转条件尚未判断完成 → 无法确定下一条指令 → 控制冒险\n解决方法：分支预测、分支延迟槽、清空流水线\n\n","categories":["《深入理解计算机系统》"],"tags":["处理器体系结构"]},{"title":"socket I/O","url":"/2025/06/03/%E5%A5%97%E6%8E%A5%E5%AD%97IO/","content":"socket I&#x2F;O输出系统调用send、sendto 和 sendmsg 都是用于发送数据的系统调用，面向连接的协议通常使用send(例如tcp或者已经connect的udp)，而无连接的协议通常使用send_to而sendmsg支持发送时设置多个缓存区，也就是说一次调用可以发送多个数据包。\nsend系统调用用户态调用send的函数原型\n#include &lt;sys/socket.h&gt;ssize_t send(int sockfd, const void *buf, size_t len, int flags);\n上述参数解释如下：\n\n\n\n参数名\n类型\n描述\n注意事项\n\n\n\nsockfd\nint\n已连接的 socket 文件描述符\n必须是通过 connect() 建立连接的 TCP socket 或已连接的 UDP socket\n\n\nbuf\nconst void *\n待发送数据的缓冲区地址\n用户态指针，指向要发送的数据（如字符串、二进制数据等）\n\n\nlen\nsize_t\n要发送的数据长度（字节数）\n实际发送的数据量可能小于此值（需检查返回值）\n\n\nflags\nint\n控制发送行为的标志位\n多个标志可通过按位或 | 组合（如 MSG_DONTWAIT | MSG_NOSIGNAL）\n\n\nsend系统调用内核实现\nSYSCALL_DEFINE4(send, int, fd, void __user *, buff, size_t, len,\t\tunsigned int, flags)&#123;\treturn __sys_sendto(fd, buff, len, flags, NULL, 0);&#125;\n__sys_sendto实现如下所示：\nint __sys_sendto(int fd, void __user *buff, size_t len, unsigned int flags,\t\t struct sockaddr __user *addr,  int addr_len)&#123;\tstruct socket *sock;\tstruct sockaddr_storage address;//这个叫通用地址容器，目的就是兼容用户态传进来的不同结构，例如sock_addrin或者ll\tint err;\tstruct msghdr msg;\tstruct iovec iov;\tint fput_needed;\t//这里吧用户态的地址和长度记录到msg.msg_iter这个结构体中，后续内核会操作这个结构体，ITER_SOURCE 表示写\terr = import_single_range(ITER_SOURCE, buff, len, &amp;iov, &amp;msg.msg_iter);\tif (unlikely(err))\t\treturn err;\t//根据fd找到socekt结构体 从fdt中找到file从file的私有字段找到socket\tsock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed);\tif (!sock)\t\tgoto out;\t//设置msg的其他字段\tmsg.msg_name = NULL;\tmsg.msg_control = NULL;\tmsg.msg_controllen = 0;\tmsg.msg_namelen = 0;\tmsg.msg_ubuf = NULL;\t//用户调用sendto时才走这个分支，因为addr不为空\tif (addr) &#123;\t\terr = move_addr_to_kernel(addr, addr_len, &amp;address);\t\tif (err &lt; 0)\t\t\tgoto out_put;\t\tmsg.msg_name = (struct sockaddr *)&amp;address;\t\tmsg.msg_namelen = addr_len;\t&#125;\tflags &amp;= ~MSG_INTERNAL_SENDMSG_FLAGS;\t//如果需要设置非阻塞\tif (sock-&gt;file-&gt;f_flags &amp; O_NONBLOCK)\t\tflags |= MSG_DONTWAIT;\tmsg.msg_flags = flags;\t//调用对应socekt的发函数\terr = __sock_sendmsg(sock, &amp;msg);out_put:\tfput_light(sock-&gt;file, fput_needed);out:\treturn err;&#125;\n\n\n上述代码主要工作就是申请了一个msg变量，并把用户态传进来的数据的地址和长度记录到msg变量的迭代器msg_iter中，并根据fd找到对应的socket套接字，然后调用__sock_sendmsg发送数据。import_single_range里面调用到iov_iter_ubuf来填充迭代器这个结构，填充这个结构的目的就是内核接下来会操作这个结构进行数据的拷贝。\nstatic inline void iov_iter_ubuf(struct iov_iter *i, unsigned int direction,\t\t\tvoid __user *buf, size_t count)&#123;\tWARN_ON(direction &amp; ~(READ | WRITE));\t*i = (struct iov_iter) &#123;\t\t.iter_type = ITER_UBUF, //标记为用户缓冲区类型\t\t.copy_mc = false,\t\t.user_backed = true, //标记为来自用户空间\t\t.data_source = direction,  //记录数据流向\t\t.ubuf = buf, //用户缓冲区指针\t\t.count = count, //数据长度\t\t.nr_segs = 1  //单段缓冲区，非聚合分散\t&#125;;&#125;\n\n__sock_sendmsg中的逻辑就是调用socket的ops函数，对于AF_INE类型的socket调用的send就是inet_sendmsg具体代码如下：\nstatic int __sock_sendmsg(struct socket *sock, struct msghdr *msg)&#123;\tint err = security_socket_sendmsg(sock, msg,\t\t\t\t\t  msg_data_left(msg));\treturn err ?: sock_sendmsg_nosec(sock, msg);&#125;//sock_sendmsg_nosec定义如下static inline int sock_sendmsg_nosec(struct socket *sock, struct msghdr *msg)&#123;\tint ret = INDIRECT_CALL_INET(READ_ONCE(sock-&gt;ops)-&gt;sendmsg, inet6_sendmsg,\t\t\t\t     inet_sendmsg, sock, msg,\t\t\t\t     msg_data_left(msg));\tBUG_ON(ret == -EIOCBQUEUED);\tif (trace_sock_send_length_enabled())\t\tcall_trace_sock_send_length(sock-&gt;sk, ret, 0);\treturn ret;&#125;\n在inet_sendmsg 中则会进一步调用socket所关联的sock结果的sendmsg，注意这里socket与sock的关联是在创建socket的时候根据协议和类型确定的，对于tcp调用的是tcp_sendmsg对于udp调用的是udp_sendmsg。\nsendto系统调用实现可以看到sendto系统调用与上面的send系统调用最终调用的都是__sys_sendto，区别就是sendto中的addr字段传入的不为空，在__sys_sendto会有由move_addr_to_kernel处理。\nSYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,\t\tunsigned int, flags, struct sockaddr __user *, addr,\t\tint, addr_len)&#123;\treturn __sys_sendto(fd, buff, len, flags, addr, addr_len);&#125;\n\n//传入了用户态的地址空间，和地址长度int move_addr_to_kernel(void __user *uaddr, int ulen, struct sockaddr_storage *kaddr)&#123;\tif (ulen &lt; 0 || ulen &gt; sizeof(struct sockaddr_storage))\t\treturn -EINVAL;\tif (ulen == 0)\t\treturn 0;\t//将用户态的结构体copy到内核，这里的kaddr可以兼容所有的sockaddr的结构体128字节\tif (copy_from_user(kaddr, uaddr, ulen))\t\treturn -EFAULT;\t//安全相关\treturn audit_sockaddr(ulen, kaddr);&#125;\n\nsendmsg系统调用实现sendmsg送复杂网络消息​​的系统调用，支持多块数据（iovec）、目标地址、控制信息（如 cmsg）等高级功能，对应内核部分代码如下：\nSYSCALL_DEFINE3(sendmsg, int, fd, struct user_msghdr __user *, msg, unsigned int, flags)&#123;\treturn __sys_sendmsg(fd, msg, flags, true);&#125;long __sys_sendmsg(int fd, struct user_msghdr __user *msg, unsigned int flags,\t\t   bool forbid_cmsg_compat)&#123;\tint fput_needed, err;\tstruct msghdr msg_sys;\tstruct socket *sock;\tif (forbid_cmsg_compat &amp;&amp; (flags &amp; MSG_CMSG_COMPAT))\t\treturn -EINVAL;\t//根据fd查找对应的socket\tsock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed);\tif (!sock)\t\tgoto out;\t//传入用户指定的msg和内核的msg\terr = ___sys_sendmsg(sock, msg, &amp;msg_sys, flags, NULL, 0);\tfput_light(sock-&gt;file, fput_needed);out:\treturn err;&#125;\n与sendto和send不同，sendmsg首先查找对应的socket，然后创建内核msg结构和传入用户的设置的msg结构，调用___sys_sendmsg发送：___sys_sendmsg函数中，主要就是把用户指定的msg信息，存到内核创建的msg_sys中\nstatic int ___sys_sendmsg(struct socket *sock, struct user_msghdr __user *msg,\t\t\t struct msghdr *msg_sys, unsigned int flags,\t\t\t struct used_address *used_address,\t\t\t unsigned int allowed_msghdr_flags)&#123;\tstruct sockaddr_storage address;\tstruct iovec iovstack[UIO_FASTIOV], *iov = iovstack;\tssize_t err;\tmsg_sys-&gt;msg_name = &amp;address;\t//将用户态的msg拷贝到内核态msg\terr = sendmsg_copy_msghdr(msg_sys, msg, flags, &amp;iov);\tif (err &lt; 0)\t\treturn err;\t//调用协议栈发送\terr = ____sys_sendmsg(sock, msg_sys, flags, used_address,\t\t\t\tallowed_msghdr_flags);\tkfree(iov);\treturn err;&#125;\n上述____sys_sendmsg中的逻辑为将用户设置的控制信息并保存到内核msg中，\nstatic int ____sys_sendmsg(struct socket *sock, struct msghdr *msg_sys,\t\t\t   unsigned int flags, struct used_address *used_address,\t\t\t   unsigned int allowed_msghdr_flags)&#123;\tunsigned char ctl[sizeof(struct cmsghdr) + 20]\t\t\t\t__aligned(sizeof(__kernel_size_t));\t/* 20 is size of ipv6_pktinfo */\tunsigned char *ctl_buf = ctl;\tint ctl_len;\tssize_t err;\terr = -ENOBUFS;\t//msg_controllen为用户msghdr控制字段的总长度\tif (msg_sys-&gt;msg_controllen &gt; INT_MAX)\t\tgoto out;\tflags |= (msg_sys-&gt;msg_flags &amp; allowed_msghdr_flags);\tctl_len = msg_sys-&gt;msg_controllen;\t//32位系统的处理逻辑\tif ((MSG_CMSG_COMPAT &amp; flags) &amp;&amp; ctl_len) &#123;\t\terr =\t\t    cmsghdr_from_user_compat_to_kern(msg_sys, sock-&gt;sk, ctl,\t\t\t\t\t\t     sizeof(ctl));\t\tif (err)\t\t\tgoto out;\t\tctl_buf = msg_sys-&gt;msg_control;\t\tctl_len = msg_sys-&gt;msg_controllen;\t&#125; else if (ctl_len) &#123;\t//64位的处理逻辑\t\tBUILD_BUG_ON(sizeof(struct cmsghdr) !=\t\t\t     CMSG_ALIGN(sizeof(struct cmsghdr)));\t\tif (ctl_len &gt; sizeof(ctl)) &#123;\t\t\tctl_buf = sock_kmalloc(sock-&gt;sk, ctl_len, GFP_KERNEL);\t\t\tif (ctl_buf == NULL)\t\t\t\tgoto out;\t\t&#125;\t\terr = -EFAULT;\t\t//这里的msg_control_user是用户态的指针所以用copy_from_user\t\tif (copy_from_user(ctl_buf, msg_sys-&gt;msg_control_user, ctl_len))\t\t\tgoto out_freectl;\t\tmsg_sys-&gt;msg_control = ctl_buf;\t\tmsg_sys-&gt;msg_control_is_user = false;\t&#125;\tflags &amp;= ~MSG_INTERNAL_SENDMSG_FLAGS;\tmsg_sys-&gt;msg_flags = flags;\t//设置非阻塞\tif (sock-&gt;file-&gt;f_flags &amp; O_NONBLOCK)\t\tmsg_sys-&gt;msg_flags |= MSG_DONTWAIT;\t/*\t * If this is sendmmsg() and current destination address is same as\t * previously succeeded address, omit asking LSM&#x27;s decision.\t * used_address-&gt;name_len is initialized to UINT_MAX so that the first\t * destination address never matches.\t */\tif (used_address &amp;&amp; msg_sys-&gt;msg_name &amp;&amp;\t    used_address-&gt;name_len == msg_sys-&gt;msg_namelen &amp;&amp;\t    !memcmp(&amp;used_address-&gt;name, msg_sys-&gt;msg_name,\t\t    used_address-&gt;name_len)) &#123;\t\terr = sock_sendmsg_nosec(sock, msg_sys);\t\tgoto out_freectl;\t&#125;\terr = __sock_sendmsg(sock, msg_sys);\t/*\t * If this is sendmmsg() and sending to current destination address was\t * successful, remember it.\t */\tif (used_address &amp;&amp; err &gt;= 0) &#123;\t\tused_address-&gt;name_len = msg_sys-&gt;msg_namelen;\t\tif (msg_sys-&gt;msg_name)\t\t\tmemcpy(&amp;used_address-&gt;name, msg_sys-&gt;msg_name,\t\t\t       used_address-&gt;name_len);\t&#125;out_freectl:\tif (ctl_buf != ctl)\t\tsock_kfree_s(sock-&gt;sk, ctl_buf, ctl_len);out:\treturn err;&#125;\n\n接收系统调用recv,recvfrom,recvmsg三个接收系统调用与发送系统调用类似，不同的地方就是数据流向是相反的。\n\n\n\n系统调用\n适用场景\n关键功能\n典型用途\n\n\n\nrecv\n已连接的套接字（如 TCP）\n- 从已建立连接的套接字接收数据- 不支持获取发送方地址\nTCP 数据接收\n\n\nrecvfrom\n无连接套接字（如 UDP）\n- 接收数据包- 可获取发送方地址（struct sockaddr）\nUDP 数据接收\n\n\nrecvmsg\n所有套接字（最通用）\n- 支持多缓冲区（struct iovec）- 支持控制信息（cmsg）- 可获取发送方地址\n高级场景：- 文件描述符传递- 接收 TTL&#x2F;接口信息\n\n\nrecv系统调用实现SYSCALL_DEFINE4(recv, int, fd, void __user *, ubuf, size_t, size,\t\tunsigned int, flags)&#123;\treturn __sys_recvfrom(fd, ubuf, size, flags, NULL, NULL);&#125;\nint __sys_recvfrom(int fd, void __user *ubuf, size_t size, unsigned int flags,\t\t   struct sockaddr __user *addr, int __user *addr_len)&#123;\tstruct sockaddr_storage address;\tstruct msghdr msg = &#123;\t\t/* Save some cycles and don&#x27;t copy the address if not needed */\t\t.msg_name = addr ? (struct sockaddr *)&amp;address : NULL,\t&#125;;\tstruct socket *sock;\tstruct iovec iov;\tint err, err2;\tint fput_needed;\t//将用户态指向的缓冲区地址，存到msg中\terr = import_single_range(ITER_DEST, ubuf, size, &amp;iov, &amp;msg.msg_iter);\tif (unlikely(err))\t\treturn err;\t//查找对应的socket\tsock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed);\tif (!sock)\t\tgoto out;\tif (sock-&gt;file-&gt;f_flags &amp; O_NONBLOCK)\t\tflags |= MSG_DONTWAIT;\t//调用套接字的ops收包\terr = sock_recvmsg(sock, &amp;msg, flags);\t//recv系统调用传入的addr为空,如果不为空，会记录源数据包的源ip地址\tif (err &gt;= 0 &amp;&amp; addr != NULL) &#123;\t\terr2 = move_addr_to_user(&amp;address,\t\t\t\t\t msg.msg_namelen, addr, addr_len);\t\tif (err2 &lt; 0)\t\t\terr = err2;\t&#125;\tfput_light(sock-&gt;file, fput_needed);out:\treturn err;&#125;\nrecvfrom系统调用实现可获取发送方的 ​​IP 地址和端口​​，适用于 UDP 等无连接协议（每个数据包可能来自不同发送方）\nSYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,\t\tunsigned int, flags, struct sockaddr __user *, addr,\t\tint __user *, addr_len)&#123;\treturn __sys_recvfrom(fd, ubuf, size, flags, addr, addr_len);&#125;\n与recv一样最终都调用到了__sys_recvfrom，不同的地方在于传入的add不为空，用户可以拿到数据包的源ip地址\nrecvmsg系统调用实现SYSCALL_DEFINE3(recvmsg, int, fd, struct user_msghdr __user *, msg,\t\tunsigned int, flags)&#123;\treturn __sys_recvmsg(fd, msg, flags, true);&#125;\n\n上述__sys_recvmsg与发送系统调用sendmsg类似，主要工作就是将数据包拷贝拷贝到用户的指定的缓冲区，以及数据包的源ip地址，和数据包的控制信息。\nstatic int ____sys_recvmsg(struct socket *sock, struct msghdr *msg_sys,\t\t\t   struct user_msghdr __user *msg,\t\t\t   struct sockaddr __user *uaddr,\t\t\t   unsigned int flags, int nosec)&#123;\tstruct compat_msghdr __user *msg_compat =\t\t\t\t\t(struct compat_msghdr __user *) msg;\tint __user *uaddr_len = COMPAT_NAMELEN(msg);\tstruct sockaddr_storage addr;\tunsigned long cmsg_ptr;\tint len;\tssize_t err;\tmsg_sys-&gt;msg_name = &amp;addr;\tcmsg_ptr = (unsigned long)msg_sys-&gt;msg_control;\tmsg_sys-&gt;msg_flags = flags &amp; (MSG_CMSG_CLOEXEC|MSG_CMSG_COMPAT);\t/* We assume all kernel code knows the size of sockaddr_storage */\tmsg_sys-&gt;msg_namelen = 0;\tif (sock-&gt;file-&gt;f_flags &amp; O_NONBLOCK)\t\tflags |= MSG_DONTWAIT;\tif (unlikely(nosec))\t\terr = sock_recvmsg_nosec(sock, msg_sys, flags);\telse\t//收包逻辑，调用收包函数\t\terr = sock_recvmsg(sock, msg_sys, flags);\tif (err &lt; 0)\t\tgoto out;\t//这里是接收数据包的长度\tlen = err;\t//拷贝源ip地址到用户态的uaddr(把下面的addr拷贝到了uaddr)\tif (uaddr != NULL) &#123;\t\terr = move_addr_to_user(&amp;addr,\t\t\t\t\tmsg_sys-&gt;msg_namelen, uaddr,\t\t\t\t\tuaddr_len);\t\tif (err &lt; 0)\t\t\tgoto out;\t&#125;\t//更新用户态的标志位，比如消息截断等标志（收一部分）\terr = __put_user((msg_sys-&gt;msg_flags &amp; ~MSG_CMSG_COMPAT),\t\t\t COMPAT_FLAGS(msg));\tif (err)\t\tgoto out;\tif (MSG_CMSG_COMPAT &amp; flags)\t\terr = __put_user((unsigned long)msg_sys-&gt;msg_control - cmsg_ptr,\t\t\t\t &amp;msg_compat-&gt;msg_controllen);\telse\t\terr = __put_user((unsigned long)msg_sys-&gt;msg_control - cmsg_ptr,\t\t\t\t &amp;msg-&gt;msg_controllen);\tif (err)\t\tgoto out;\terr = len;out:\treturn err;&#125;","categories":["网络协议栈源码学习"],"tags":["socket"]},{"title":"虚拟内存（一）","url":"/2025/06/28/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98(%E4%B8%80)/","content":"虚拟内存概念：为了更有效地管理内存并减少出错，现代系统提供了一种对主存的抽象，叫做 虚拟内存（VM）。  虚拟内存是硬件异常、地址翻译、主存、磁盘文件和内核软件的共同作用，为每个进程提供了一个大的一致且私有的地址空间。\n虚拟内存提供三大核心能力：\n\n高效利用主存将主存视为磁盘地址空间的高速缓存，仅保存活跃区域，按需在磁盘和主存之间调页，从而高效使用物理内存。\n\n一致的地址空间给每个进程提供一致的虚拟地址空间，简化内存管理和程序设计。\n\n地址空间隔离与保护防止进程间互相干扰，保障系统稳定性和安全性。\n\n\n物理寻址与虚拟寻址物理寻址（Physical Addressing） 主存由 M 个连续的字节单元组成，每个字节有唯一的物理地址（如 0, 1, 2, …）。 早期 PC、一些数字信号处理器、嵌入式微控制器和 Cray 超级计算机等直接使用物理地址，CPU 访问内存时直接生成物理地址，通过内存总线取出数据。\n虚拟寻址（Virtual Addressing） 现代处理器一般使用虚拟寻址。CPU 生成的地址是虚拟地址（VA），需要通过**地址翻译（Address Translation）转换成对应的物理地址（PA）。 地址翻译由 CPU 的内存管理单元（MMU）**和操作系统配合完成。MMU 使用操作系统维护的查询表（如页表），动态把虚拟地址映射到物理地址，实现灵活的内存管理和保护。\n地址空间就是一个有序的、非负整数地址的集合，用来给数据单元（如字节）编号。\n本质：就是一组连续的数字 0, 1, 2, ..., N-1。\n线性地址空间：如果是连续的整数集合，就叫做线性地址空间，现代 CPU 都是线性的。\n大小：由地址能表示的最大值决定，比如：\n\n32 位虚拟地址空间：最多表示 2^32 个不同的地址。\n64 位虚拟地址空间：最多表示 2^64 个不同的地址。\n\n虚拟内存作为缓存的工具在概念上，虚拟内存可以看作是一个由 N 个连续字节组成的大数组，这个数组存在磁盘上，每个字节都有唯一的虚拟地址作为索引。这个大数组里的数据不是全部都放在主存中，而是像其他缓存一样，把活跃部分缓存在主存里。 磁盘是存储层次结构中的较低层，主存是较高层，两者之间的数据传输是按块进行的。为此，虚拟内存系统把整个虚拟地址空间分割成固定大小的块，称为 虚拟页（Virtual Page, VP），每个页大小是 P 字节。同样，物理内存也被分成相同大小的 物理页（Physical Page, PP），也叫做 页帧（Page Frame）。\n在任何时刻，所有的虚拟页可以分为三个互不重叠的类别：\n\n未分配页：还没有被使用或创建的虚拟页，没有实际数据，对应的磁盘空间也不存在。\n已分配且已缓存页：已经分配，并且当前缓存在物理内存中的页。\n已分配但未缓存页：已经分配，但目前没有缓存在物理内存中的页，数据保存在磁盘上。\n\n\nDRAM 缓存的组织结构在存储层次结构里：\n\nSRAM 缓存 指的是 CPU 内部的 L1&#x2F;L2&#x2F;L3 缓存，用来缓存 DRAM。\nDRAM 缓存（虚拟内存缓存）指的是主存用来缓存磁盘上的虚拟页。\n\n位置决定开销差异\n\nDRAM 比 SRAM 慢大约 10 倍。\n磁盘比 DRAM 慢 100,000 倍以上。\n所以 DRAM 缓存的未命中（需要从磁盘换页） 代价远远比 SRAM 缓存的未命中高。\n\n映射方式：全相联\n\nDRAM 缓存是全相联的，任何虚拟页可以放到任何物理页帧。\n\n写策略：总是写回\n\n因为磁盘访问太慢，虚拟内存的写操作不会立刻写回磁盘。\n写会先落在物理内存页帧里，等到换出时才写回（写回，write-back），而不是像某些 SRAM 缓存可选直写（write-through）。\n\n主存（DRAM 缓存）是磁盘（虚拟页）的缓存层。\n当进程修改了某个虚拟页：\n\n改动先只写到物理内存里的页帧（Page Frame）\n页帧会被标记为 脏页（Dirty Page）\n\n等到这个物理页帧要被换出（被别的页替换掉）时，操作系统才把脏页写回磁盘。\n这样能减少对磁盘的频繁写操作，提高性能。\n页表同任何缓存一样，虚拟内存系统必须有某种方法来判定一个虚拟页是否缓存在DRAM 中的某个地方。 如果是，系统还必须确定这个虚拟页存放在哪个物理页中。如果不命中，系统必须判断这个虚拟页存放在磁盘的哪个位置，在物理内存中选择一个牺牲页，并将虚拟页从磁盘复制到 DRAM 中，替换这个牺牲页。 这些功能是由软硬件联合提供的，包括操作系统软件、 MMU(内存管理单元）中的地址翻译硬件和一个存放在物理内存中叫做页表(page table)的数据结构，页表将虚拟页映射到物理页。每次地址翻译硬件将一个虚拟地址转换为物理地址时，都会读取页表。 操作系统负责维护页表的内容，以及在磁盘与 DRAM之间来回传送页。\n页表（Page Table）\n\n本质是一个页表条目（Page Table Entry，PTE） 的数组。\n虚拟地址空间里的每个虚拟页都在页表里有且只有一个对应的 PTE，位置是固定的（根据虚拟页号直接找到）。\n\nPTE（页表条目）的内容\n\n每个 PTE 里至少包含：\n有效位（Valid Bit）：表明这个虚拟页是否当前缓存在物理内存（DRAM）里。\n地址字段：如果有效位是 1，表示这个字段保存了该虚拟页对应的物理页帧的起始位置。\n\n\n\n如下图所示：\n\n缺页缺页就是CPU 访问了一个虚拟地址，对应的页不在物理内存里。\n在虚拟内存的习惯说法中， DRAM 缓存不命中称为缺页(page fault)。 下图展示了在缺页之前页表的状态。 CPU 引用了 VP 3 中的一个字， VP 3 并未缓存在 DRAM 中。mmu从内存中读取 PTE 3, 从有效位推断出 VP 3 未被缓存，并且触发一个缺页异常。缺页异常调用内核中的缺页异常处理程序，该程序会选择一个牺牲页（这里不一定会发生换入换出吧？如果物理内存里有空闲页帧，就直接放进去，不需要换出）， 在此例中就是存放在 pp 3 中的 VP 4。\n\n接下来，内核从磁盘复制 VP 3 到内存中的 pp 3, 更新 PTE 3, 随后返回。当异常 理程序返回时**，它会重新启动导致缺页的指令**，该指令会把导致缺页的虚拟地址重发送到 地址翻译硬件。但是现在， VP 3 已经缓存在主存中了，那么页命中也能由地址翻译硬件正常处理了。\n\n","categories":["《深入理解计算机系统》"],"tags":["虚拟内存"]},{"title":"虚拟内存（二）","url":"/2025/06/29/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98(%E4%BA%8C)/","content":"虚拟内存简要总结虚拟内存的核心作用\n\n每个进程都有独立的虚拟地址空间和页表，实现地址空间隔离和内存保护。\n操作系统通过页表把进程的虚拟地址映射到物理内存，实现内存管理的灵活性。\n\n虚拟内存带来的好处\n\n简化链接：不同进程的代码和数据在虚拟空间中布局一致（比如代码总从同一个虚拟地址开始），链接器生成的可执行文件不依赖物理内存实际布局，简化了实现。\n简化加载：加载器只需分配虚拟页并做映射，实际数据只有在访问时才按需从磁盘加载到内存（即按需调页、懒加载）。\n简化共享：同一份物理内存（如操作系统内核、C标准库）可以被多个进程的虚拟空间同时映射，实现高效代码和数据共享。\n简化内存分配：进程申请新内存时，操作系统只需分配若干虚拟页，并将它们映射到任意空闲的物理页，不必保证物理上连续。\n\n共享机制\n\n默认情况下，每个进程的虚拟空间互不影响，各自私有。\n需要共享的内容（如内核代码、共享库）可被多个进程的虚拟空间指向同一份物理内存。\n\n内存映射 (memory mapping)\n\n操作系统允许把一组虚拟页映射到文件的任意位置，实现高效的文件访问与共享（如 mmap 系统调用）。\n\n虚拟内存实现内存保护的原理\n内存保护的目标\n防止用户进程修改只读的代码区。\n防止用户进程访问内核空间（内核代码和数据）。\n防止进程间相互访问对方的私有内存。\n防止进程随意修改共享内存，除非所有共享方都明确允许。\n\n\n独立地址空间与访问控制\n每个进程有自己的虚拟地址空间，彼此隔离，天然实现了基本的内存保护。\n\n\n页表权限位（PTE 许可位）\n在每个页表项（PTE）中增加了权限控制位（如 SUP、READ、WRITE）：\nSUP（超级用户&#x2F;内核模式）：此页是否只能被内核模式访问，用户模式下不允许访问。\nREAD：此页是否允许读。\nWRITE：此页是否允许写。\n\n\n通过这些权限位，操作系统可以精确控制每个页面的访问权限。\n比如：用户进程不能读写内核空间，只能访问本进程被允许的区域。\n\n\n\n\n访问权限检查的实现方式\n每次CPU访问内存，都会通过页表查找虚拟地址对应的物理页，同时检查权限位。\n如果当前进程没有足够权限（比如用户进程试图写只读区、读内核区），CPU会触发异常，即“段错误（segmentation fault）”，操作系统捕获并处理，比如终止该进程。\n\n\n\n\n地址翻译虚拟地址空间（VAS）和物理地址空间（PAS）是两个不同的地址集合。\n地址映射就是虚拟地址空间的每个元素映射到物理地址空间的元素，或者无效（不存在于物理内存中，产生缺页）\n虚拟地址结构\n虚拟地址由**虚拟页号（VPN）和虚拟页内偏移量（VPO）**两部分组成。\n假设虚拟地址是n位，其中低p位是VPO，高(n-p)位是VPN。\n例：32位地址空间，4KB页（12位偏移），则VPN占20位，VPO占12位。\n\n\n\n页表的作用\n页表基址寄存器（PTBR）：指向当前进程的页表起始地址。\n页表用来存储虚拟页号到物理页号（PPN）的映射关系。\n每个页表项包含：\n有效位：表示该虚拟页是否在物理内存中（1&#x3D;在，0&#x3D;不在）。\n物理页号（PPN）：虚拟页号实际对应的物理页号。\n\n\n\n\n虚拟内存系统中页面命中和缺页的流程如下图所示：\n页面命中：\n\nCPU生成虚拟地址（VA）\n程序执行时（如读写数据或取指令），CPU发出一个虚拟地址，传递给MMU。\n\n\nMMU生成页表项地址（PTEA）\nMMU用虚拟地址的页号部分和页表基址，算出页表项的物理地址（PTEA）。\n\n\n高速缓存&#x2F;主存返回PTE\nMMU访问cache或主存，取得这个PTE内容（主要是物理页号、有效位等）。\n\n\nMMU构造物理地址（PA）\nMMU把PTE中的物理页号（PPN）和虚拟地址的页内偏移拼起来，形成物理地址（PA），传递给cache&#x2F;主存。\n\n\n高速缓存&#x2F;主存返回数据\ncache&#x2F;主存用物理地址去存取数据，把结果送回CPU，继续指令执行\n\n\n\n\n缺页处理：\nCPU生成虚拟地址（VA）\nMMU生成PTEA，cache&#x2F;内存未找到有效PTE（有效位&#x3D;0）\n\n发生异常（缺页异常，step 4）\n\nCPU&#x2F;操作系统进入缺页异常处理程序\n\n操作系统查明缺页原因（比如页面还在磁盘上）。\n通过缺页异常处理程序，把所需页面调入内存。\n\n从磁盘读取页面\n\n需要的数据页面从磁盘调入物理内存（新页）。\n\n更新页表\n\n操作系统把新页面的物理页号写回页表，并把有效位置1。\n\n重新发起原指令\n\n程序恢复，被中断的指令重新执行，流程回到页面命中那一套。\n\nSRAM与虚拟内存在实际工作中，CPU访问内存时，首先会用虚拟地址经过MMU翻译成物理地址，然后用物理地址去查找Cache。如果Cache命中就直接返回数据，否则再去主存中查找。需要注意的是，页表项（PTE）本身也可以像普通数据一样被缓存在Cache中，以提升访问效率。\n\nTLBTLB就像是“地址翻译的加速器”，直接缓存了最近的页表项，大部分虚拟地址转换都能在MMU内部1步完成，从而让虚拟内存的性能几乎和物理内存一样高效。\n为什么需要TLB？\n问题：每次虚拟地址到物理地址的转换都要查页表（查PTE），\n最慢的时候要访问一次主存（几十到几百CPU周期）。\n如果PTE在L1 cache里，也要1-2个周期。\n\n\n优化目标：连这1-2个周期都想省掉，让地址翻译像查寄存器一样快。\n\nTLB是什么？\nTLB就是MMU内部的一个超快的小型Cache，专门缓存最近用过的一组“虚拟页号VPN ➔ 页表项PTE”的映射。\nTLB很小（几十到几百项）但速度极快，一般为全相联或组相联。\n\nTLB的工作机制TLB命中:\n\nCPU产生虚拟地址（VA）\nMMU用VA里的虚拟页号（VPN）查TLB\nTLB里通过索引和标记直接查到PTE（页表项）\n\n\nMMU用PTE快速翻译出物理地址（PA）\n用物理地址访问Cache&#x2F;主存，返回数据给CPU\n\nTLB未命中:\n\nTLB没找到对应的VPN\nMMU只能去查更慢的L1 Cache&#x2F;主存获取PTE\n查到PTE后，把它塞进TLB（可能覆盖掉最久未用的条目）\n继续用新查到的PTE翻译虚拟地址为物理地址，访问内存\n\n\n多级页表单级页表的缺点以32位系统为例，虚拟地址空间有4GB。\n每个页面大小4KB。\n那么虚拟地址空间里一共需要 4GB &#x2F; 4KB &#x3D; 1M（&#x3D;1048576）个页。\n每个PTE（页表项）用4字节，总共页表需要 4 * 1M &#x3D; 4MB。\n问题： 只要进程一启动，就需要给它分配4MB的页表，不管它实际只用了多少内存（哪怕你只用了1页，也要有4MB的页表）。\n多级页表\n思想： 把页表分成多级（比如两级），只为实际用到的部分分配内存。\n比如：两级页表，一级页表每一项指向一个二级页表（实际存放PTE），一级页表本身只有1024项（对应1024个“4MB片段”）。\n如果某个4MB范围内一个页面都没用过，那一级页表对应的那一项就设为NULL（不分配二级页表）。\n只有用到内存的那部分，才分配对应的二级页表。\n这样大部分没用到的虚拟地址空间都不用分配实际的页表存储空间，节省内存。\n\n\nK级页表\n虚拟地址被分为K段VPN（虚拟页号）和1段VPO（页内偏移）。\n每一级页表的VPN用于索引对应级别的页表，逐级查找下一级页表地址。\n最终第K级页表的PTE中，保存的是目标物理页号（PPN）或磁盘地址。\nMMU翻译一个虚拟地址时，要访问K个PTE（比如2级页表，要先查一级表，再查二级表）。\n对于最简单的只有一级页表结构，页内偏移（PPO）和VPO相同。\n\n\n","categories":["《深入理解计算机系统》"],"tags":["虚拟内存"]},{"title":"虚拟内存（五）","url":"/2025/07/06/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98(%E4%BA%94)/","content":"显示空闲链表隐式空闲链表查找效率低，适合块数很少的情况；显式空闲链表把空闲块用前后指针连成链表，比如用双向链表，可以大大加快查找速度（只跟空闲块数量有关），如果用后进先出插入，释放块和合并可以是常数时间，但如果按地址排序虽然释放块需要找位置（慢一些），但内存利用率更高；缺点是显式链表需要额外的前后指针，块最小尺寸增大，增加了内部碎片。\n\n与内存有关的错误间接引用坏指针\n间接引用就是用指针去访问内存。\n坏指针就是：\n指向了不该访问的区域（比如没分配过的虚拟内存洞）\n或者指向了只读区域（但你却想写它）\n或者根本就是乱的值（未初始化、被覆盖、拼错……）\n\n\n\n当你去解引用（访问 *p）一个坏指针时，操作系统会触发保护机制（段错误或保护错误），直接把程序干掉，防止你破坏别的内存。\n例子：\nint val;scanf(&quot;%d&quot;, &amp;val);  // 正确：传的是变量 val 的地址scanf(&quot;%d&quot;, val);  // 错误：把 val 的值当成地址传了\n\n可能的后果\n\n最好 的情况：那个“地址”根本是无效区域，操作系统直接让程序崩溃（段错误）。\n最坏 的情况：那个“值”刚好是你程序里有效的、可写的某块内存地址，于是 scanf 会往这块内存写数据 —— 但是你本意并不想动这里！ 这就等于是把脏数据写到你完全没打算改的地方。 当时可能没事，程序继续跑，等之后用到那块内存时，就会莫名出现奇怪的错误，甚至难以定位\n\n读未初始化的内存当 malloc 一块堆内存，或者定义一个局部变量（比如 int x;），编译器&#x2F;操作系统不会保证把它自动清零。\n这意味着，这块内存里之前残留的比特值还在，内容是随机的（别的程序或者以前的数据遗留）。\n栈缓冲区溢出栈（stack）：程序运行时，用来存放局部变量、函数调用时的返回地址、保存的寄存器等等。\n缓冲区（buffer）：在 C 里通常就是 char arr[100] 这种数组，用来临时存放数据（比如字符串）。\n溢出（overflow）：如果往数组里写的内容超过了它原本分配的大小，就会覆盖后面的内存。\n指针大小混淆错误int **makeArray1(int n, int m) &#123;    int i;    int **A = (int **)Malloc(n * sizeof(int));  // 错误    for (i = 0; i &lt; n; i++)        A[i] = (int *)Malloc(m * sizeof(int));    return A;&#125;\n\n意图：\n\n创建一个指针数组 A，包含 n 个指针（int *），\n每个指针再指向一块 m 个 int 的内存。\n\n也就是想要构造一个 n x m 的二维数组。\nint **A = (int **)Malloc(n * sizeof(int));\n\n这里的意思是：我要申请 n 个指针的空间，但是 sizeof 写错了。\n\nA 是 int ** 类型，对应是一个 int * 指针数组。\n\n越界写内存可能正好没引发崩溃，因为你写到的可能是 别的分配块的头尾，比如堆块的「脚部」信息（用于记录块大小和空闲状态）。\n当你以后释放内存时，分配器会用这些脚部标记做块合并，结果发现结构被破坏，合并逻辑就会崩溃。\n引用不存在的变量(悬空指针)int *stackref()&#123;    \tint val;    \t    \treturn &amp;val;&#125;\n\nint val; 是在函数 stackref 的 栈帧 上分配的局部变量。\nreturn &amp;val; 把 val 的地址返回给外面。\n但一旦 stackref 返回，这个函数的栈帧就会被弹出，局部变量 val 不复存在。\n外面的指针 p 仍然保存着 val 原来的地址，这个地址现在指向的内存已经 不再属于 val，而是留给以后别的函数调用重用。\n所以 p 成了一个 悬空指针，它指向一块已经失效的栈内存。\n访问或使用已释放内存先用 malloc 分配了 x，它指向一块有效的堆内存。\n然后调用 free(x) 把这块内存释放了，这块内存在分配器眼里已经不再有效，已经放回了空闲块。\n接着又访问 x：\n\n从语法上看 x 还是一个合法的指针，但它指向的内存已经不属于你了。\n可能这块内存被分配器用来给别的 malloc 复用了，数据可能已经被覆盖。\n\n后果：\n会读到脏数据（可能是别的对象的数据）。\n写的话会破坏别的对象或分配器内部结构，导致堆损坏（Heap Corruption）。\n内存泄露","categories":["《深入理解计算机系统》"],"tags":["虚拟内存"]},{"title":"虚拟内存（四）","url":"/2025/07/03/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"malloc()malloc如何管理一个堆的，如下所示：\n\na) p1 &#x3D; malloc(4 * sizeof(int))\n\n申请4个int的空间，malloc从最前面切一块给p1，前4格变深色。\np1指向新分配的这块内存起始处。\n\n\nb) p2 &#x3D; malloc(5 * sizeof(int))\n\n申请5个int的空间，为了对齐，实际分配6格，p2指向新分配块的起始处。\np2后面多出1格（深蓝），是为“对齐”而加的（比如要8字节对齐，补了1字节）。\n\n\nc) p3 &#x3D; malloc(6 * sizeof(int))\n\n申请6个int的空间，从剩余空闲区的头部分出去6格，p3指向新块。\n\n\nd) free(p2)\n\n释放p2指向的那一块（包括对齐的多余空间），这一块变成“空闲”状态（浅色），但p2指针本身没变，只是p2指向的内存已经不能再用。\n\n\ne) p4 &#x3D; malloc(2 * sizeof(int))\n\n再次申请2个int的空间，malloc会优先复用刚刚释放的空闲块的前2格，p4指向这里。\n剩下4格还是空闲，以后还可以分配。\n\n碎片碎片：就是虽然堆上还有空闲内存，但分配请求还是失败了（因为空闲块无法直接满足请求），这就导致了内存浪费，降低了利用率。\n碎片分为两种：\n内部碎\n指的是“已分配的内存块”比实际需要的要大，导致部分空间浪费在“块内部”。\n产生原因：\n有些分配器规定最小分配单位（比如你只要 2 字节，分配器必须分 8 字节）。\n为了对齐，比如分配 5 个字节，实际给你 8 个字节，后面空着不用。\n\n\n图 b 就是典型：malloc(5*sizeof(int))，但分配了6个int空间，多出来的1个就是“内部碎片”。\n内部碎片的量&#x3D;每个分配块的“总大小” - “实际需要大小”，全堆累加。\n\n外部碎片\n指的是：堆上总的空闲空间足够，但没有一块连续空闲块能满足分配请求。\n举例：图 9e 里，假如此时需要分配 6 个 int 空间，但空闲区被分成了两个小块（每块&lt;6），就没法直接分配出一块6的，必须要向系统再要新内存。\n外部碎片的根本问题：内存空闲块“被切得很碎”，不能凑出一块大的。\n外部碎片的难点是，它依赖于将来的分配请求：\n如果后续请求都很小（比如≤4字节），这些小空闲块可以用完，也就没有外部碎片。\n如果有大请求（比如要8字节），就没法直接满足了，外部碎片就出现了。\n\n\n\n分配器的实现难点：真实世界的分配器（如glibc中的malloc），既要快（高吞吐），又要省（高利用率），所以必须要解决很多实际问题，主要包括：\n（1）空闲块如何组织？\n释放后的内存如何记录下来（比如用链表、树，或者分段等方法管理）？\n\n（2）放置策略\n每次分配新内存时，从所有空闲块里怎么选一块？\n是选第一个合适的？最大&#x2F;最小的？还是最靠前的？\n\n\n\n（3）分割策略\n如果空闲块比请求的大，如何把剩下的部分保留下来？（比如分割成两块）\n\n（4）合并策略\n当有块被释放时，如果相邻块也是空闲的，是否应该合并成更大的空闲块？（防止出现太多小碎片）\n\n隐式空闲链表\n\n头部（Header）\n\n\n每个内存块的开头有一个“头部”（通常4字节或8字节），保存块的大小和分配状态。\n图中用“块大小”字段的低3位来记录状态：\n00a：a表示分配标志（allocated），如果a=1表示已分配，a=0表示空闲。\n因为块总是按8字节对齐，最低3位平时都为0，可以拿出来用。\n\n\n\n\n有效载荷（Payload）\n\n\n这是用户调用malloc后实际可以用的那块空间（也是malloc返回的指针指向的起始地址）。\n只包括用户申请到的那部分。\n\n\n填充（Padding，可选）\n\n\n为了保证对齐，如果用户申请的空间不是对齐大小，会在末尾加一些无用的填充字节，把整个块补齐到8字节的倍数。\n这个填充区是“可选的”，只在需要时加。\n\n分配器通过顺序扫描每块头部信息，来判断块边界、大小和分配状态\n\n内存分配时，怎么找“空闲块”主要有三种策略\n首次适配（First Fit）\n\n做法：每次都从链表头开始遍历，找到第一个大小足够的空闲块就用。\n优点：实现简单，通常速度比较快。\n缺点：容易在链表前面留下很多“小碎片”，这些碎片很难再利用，后面分配大块时就需要一直往后找，效率下降。\n\n下一次适配（Next Fit）\n\n做法：和首次适配类似，但每次搜索不是从头开始，而是从上一次分配结束的地方开始搜索。\n优点：如果前面已经被分成很多碎片，跳过它们会快一些。\n缺点：长期运行后，内存利用率往往比首次适配更低（因为大块可能被“跳过”，碎片问题更严重）。\n\n最佳适配（Best Fit）\n\n做法：遍历所有空闲块，找到“刚好能放下所需空间”的最小空闲块。\n优点：理论上能最大化内存利用率，减少碎片。\n缺点：要遍历所有空闲块，速度慢，效率低。\n\n分割空闲块 “用多少”\n当分配器找到一个足够大的空闲块时，如果这个块比需要分配的还要大，分配器要决定要不要把它分成两块——一块刚好满足用户的请求，剩下的留作空闲块。\n\n\n如果不分割，直接把整个空闲块都分配了，虽然简单，但会浪费空间，产生“内部碎片”（即：用户用不到的空间也被分出去了）。\n\n如果分割，就能更高效利用内存，把多出来的部分留作下次分配用。\n\n\n\n找不到合适的空闲块怎么办？\n当程序调用 malloc 申请内存时，分配器会遍历所有空闲块，想找到一个足够大的来满足请求。\n如果没有任何一个空闲块能满足分配请求（比如所有空闲块都太小，或根本没有空闲块）：\n\n第一步：尝试合并相邻空闲块（合并算法）\n\n如果有些空闲块在物理内存上是相邻的，可以把它们合并成一个更大的空闲块（这样有可能凑出足够大的空间）。\n合并的方法叫“块合并&#x2F;块归并（coalescing）”。\n\n第二步：还是不够怎么办？\n\n如果合并之后还是没有足够大的空闲块，或者本来就没什么空闲块了，怎么办？\n\n第三步：向内核要新内存\n\n分配器会调用操作系统的系统调用（ sbrk 或 mmap），\n让操作系统分配一块新的堆内存（把进程的堆往上扩大一块）。\n分配器把新获得的这块大内存当作一个新的“大空闲块”，放入自己的空闲块链表中。\n然后在这块新内存里，分出一块满足请求的空间，剩下的还作为空闲块继续管理\n\n合并空闲块\n合并就是把相邻的空闲块合成一个更大的空闲块，这样可以减少碎片。\n什么时候合并，有两种策略：\n\na) 立即合并（Immediate Coalescing）\n\n每次释放内存时，立刻检查相邻块，如果它们是空闲的，就直接合并。\n优点：简单，效率高，始终保持碎片最少。\n缺点：有时会导致“合并-分割-合并-分割”来回抖动（比如一直分配&#x2F;释放相邻的小块时）。\n\nb) 推迟合并（Deferred Coalescing）\n\n释放内存时不立即合并，而是等到某些条件触发时（比如分配失败时），再把所有空闲块扫描并合并一次。\n优点：减少不必要的合并&#x2F;分割操作，提高某些场景下的性能。\n缺点：在推迟期间会出现假碎片，直到触发合并操作。\n\n带边界标记的合井\n当前块释放时，可以直接看下一个块的头部，判断它是不是空闲。\n\n如果是，就可以把下一个块的大小合并到当前块，这一步是常数时间 O(1) 的。\n\n如果要合并前一个空闲块，问题在于：你不知道前一个块的起始位置（因为你的指针是指向当前块的）。\n\n如果用“隐式空闲链表”（即每个块只有头部，没有其他指针），那就只能从堆头顺着头部一直找，直到找到前一个块——这就是线性时间 O(n)，很慢。\n\n\n边界标记法（boundary tag）\n\n每个块除了头部外，还在结尾加一个“脚部”（footer，也叫边界标记）。\n脚部就是头部的一个副本，记录块的大小和是否已分配。\n这样你拿到当前块后，可以直接看当前块前面“一个字”位置的脚部，得到前一个块的信息（它的大小和起始地址）。\n所以，释放当前块时，可以在常数时间找到前一个块，并判断要不要合并。\n\n","categories":["《深入理解计算机系统》"],"tags":["虚拟内存"]},{"title":"虚拟内存（三）","url":"/2025/07/01/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%EF%BC%88%E4%B8%89%EF%BC%89/","content":"地址翻译以Core i7为例：如下图所示：\n\n**什么叫N路组相联? **8路组相联Cache例子如下：\nCache总大小：16KB\n块大小：64B\n8路组相联\n地址空间：32位\n\n计算组数和分段\n\n总块数 &#x3D; 16KB &#x2F; 64B &#x3D; 16384B &#x2F; 64B &#x3D; 256块\n组数 &#x3D; 256块 &#x2F; 8 &#x3D; 32组\n\n地址分解\n\n块内偏移：64B，需要6位（2^6&#x3D;64）\n组索引：32组，需要5位（2^5&#x3D;32）\nTag：剩下高21位（32-6-5&#x3D;21）\n\n举一个32位实际地址\n假设地址为0xCAFEBABE 二进制：1100 1010 1111 1110 1011 1010 1011 1110\n分段如下：\n\nTag: 21位，高位\n组索引: 接下来的5位\n块内偏移: 低6位\n\nTag &#x3D; 110010101111111010110 &#x3D; 0x657F56\n组索引 &#x3D; 10101 &#x3D; 21（十进制）\n块内偏移 &#x3D; 011110 &#x3D; 30（十进制\n查找流程\n\n先用组索引\n组索引21，直接定位到第21组。\n\n\n组内查找（8路）\n组内有8个块（可能存着不同Tag的数据）。\n查看这8个块，有没有Tag是0x657F56并且有效。\n如果有，命中！直接返回数据（取块内第30字节）。\n如果没有，发生缺失，需要把主存相应块装进来，替换组内某个块。\n\n\n\n页表项的结构页表项（Entry）格式：比如一级、二级、三级页表，每一项64位，内容包括“物理基地址”和一系列属性位（控制位）。\n物理基地址：页表项的高40位，用来指向下一级页表的物理地址（4KB对齐）。\n低12位：各种控制标志（如P、RW、US等\n\nlinux虚拟内存系统每个进程都有独立的虚拟地址空间\n用户空间（进程虚拟内存）：每个进程看到的都是“独立”的虚拟空间，包括：\n代码段（.text）：可执行指令\n数据段（.data&#x2F;.bss）：全局变量\n堆（heap）：由brk&#x2F;mmap增长，动态分配\n共享库：动态链接库映射区域\n栈（stack）：通常从高地址向低地址生长\n\n\n每个进程的这些区域虚拟地址相同，但对应的物理页面不同，互不影响。\n\n内核虚拟内存空间\n位于用户空间之上（高地址区，通常x86-64为0xffff800000000000及以上）。\n所有进程都映射同一份内核代码和全局数据，即物理页是共享的。\n包含：\n内核代码和数据（共享）\n物理内存映射区：连续虚拟页映射到所有物理内存页，为内核直接访问物理内存提供便利（如DMA、页表管理等）\n每个进程独有的数据结构：如页表、task_struct、mm_struct、内核栈（每个进程单独分配）\n\n\n\n地址翻译关系\n用户空间虚拟地址 → 多级页表翻译 → 进程自己的物理页面\n内核空间虚拟地址 → 所有进程页表都映射到同一物理内存区域\n\n\nLinux 虚拟内存区域Linux 将虚拟内存组织成一些区域（也叫做段）的集合。 一个区域(area)就是已经存在着的（已分配的）虚拟内存的连续片(chunk), 这些页是以某种方式相关联的。例如，代码 段、数据段、堆、共享库段，以及用户栈都是不同的区域。每个存在的虚拟页面都保存在某个区域中，而不属千某个区域的虚拟页是不存在的，并且不能被进程引用。 区域的概念很重要，因为它允许虚拟地址空间有间隙。内核不用记录那些不存在的虚拟页，而这样的页也不占用内存、磁盘或者内核本身中的任何额外资源。\nlinux组织虚拟内存的方式如下所示：\n\n触发缺页的场景缺页处理器的三个主要步骤1. 检查虚拟地址是否合法\n查找vm_area_struct链表&#x2F;树，判断A是否落在某个VMA区域内（vm_start &lt;= A &lt; vm_end）。\n不合法：直接发送段错误（SIGSEGV），终止进程（比如访问未分配内存）。\n优化：实际Linux用红黑树加速VMA查找，而不仅仅是链表。\n\n\n\n2. 检查访问权限是否合法\n检查对A的访问操作（读&#x2F;写&#x2F;执行）是否在当前VMA的权限（vm_prot&#x2F;vm_flags）内。\n比如只读区域不能写，用户进程不能访问内核空间。\n不合法：保护异常（如写只读、越权），终止进程。\n\n\n\n3. 进行缺页调度\n到这步说明是合法访问，但物理页不在内存。\n换入页面：如在swap&#x2F;磁盘，读入内存。\n分配新页：如第一次写入匿名页、堆&#x2F;栈自动扩展等。\n页替换：如果物理内存已满，选择牺牲页（如LRU），若被修改过先写回磁盘。\n更新页表：将新的物理页和虚拟地址A建立映射，置有效位。\n恢复指令：异常处理返回后，CPU重新执行导致缺页的那条指令，这次就能翻译成功。\n\n\n\n","categories":["《深入理解计算机系统》"],"tags":["虚拟内存"]},{"title":"链接(一)","url":"/2025/06/23/%E9%93%BE%E6%8E%A5(%E4%B8%80)/","content":"链接链接（Linking）是一个将多个代码和数据片段（来自编译器生成的目标文件）收集并组合为一个单一文件的过程，这个文件可以被加载到内存中执行。\n 链接的目标文件可以是：\n\n可执行文件\n共享库（动态库）\n或中间的可重定位文件\n\n链接可以发生在不同的阶段：\n\n\n\n链接阶段\n说明\n\n\n\n链接时（Link-time）\n最常见的，指用 ld 或 gcc 把多个 .o、.a、.so 链接为一个可执行文件或库。\n\n\n加载时（Load-time）\n程序被操作系统加载到内存时，动态链接器（如 /lib64/ld-linux.so.2）把 .so 文件装载并绑定符号。\n\n\n运行时（Run-time）\n程序运行过程中自行调用 dlopen() &#x2F; dlsym() 等函数动态加载库并解析符号。\n\n\n静态链接静态链接是指链接器将所有需要的代码、数据和库函数一次性合并到一个最终的可执行文件中，使这个文件在运行时不再依赖外部库\n举例：\n// main.cint add(int a, int b);int main() &#123;    return add(2, 3);&#125;\n\n// add.cint add(int a, int b) &#123;    return a + b;&#125;\n\ngcc -c main.c -o main.ogcc -c add.c -o add.ogcc -static main.o add.o -o app\n\n静态链接的两个核心任务：符号解析（Symbol Resolution）\n每个 .o 文件里都可能定义或引用符号（比如变量名、函数名）。\n链接器要搞清楚：main.o 中调用的 add 函数，在 add.o 中定义。\n它的任务就是：把所有引用的符号，找到它真正定义的位置。\n\n重定位（Relocation）\n编译器和汇编器把代码和变量的位置都写成从地址 0 开始，也就是未定地址。\n链接器决定每个函数&#x2F;变量在最终程序里的实际位置，并：\n修改机器码中的跳转地址\n修改数据引用的偏移地址\n\n\n\n这一步就叫做 “重定位” —— 把所有相对&#x2F;未知地址“补丁”打上真实的地址。\n目标文件保存了指令、数据和符号等信息，通常使用如 ELF等平台特定格式进行组织。\n三种目标文件类型\n\n\n\n类型\n英文名称\n可否执行\n是否可重定位\n生成者\n说明\n\n\n\n可重定位目标文件\nRelocatable Object File\n否\n是\n编译器 &#x2F; 汇编器\n.o 文件，可参与链接\n\n\n可执行目标文件\nExecutable Object File\n是\n否\n链接器\n.out、a.out、无扩展名，已可运行\n\n\n共享目标文件\nShared Object File\n否（被调用）\n是（动态）\n编译器（加 -shared）\n.so 文件，运行时加载\n\n\n可以用如下命令来查看目标文件的类型和格式\n~/workspace$ file main.o main.o: ELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), not stripped\n\n可重定位目标文件可重定位目标文件是由多个**节（section）**组成的结构化文件，包含编译后的机器指令、数据、符号表和重定位信息，用于链接器进一步生成可执行文件或共享库。\n┌─────────────┐│ ELF Header  │ ← 描述整个文件的结构、类型、偏移等├─────────────┤│ .text       │ ← 机器码指令（函数体）│ .rodata     │ ← 只读数据（字符串常量、跳转表等）│ .data       │ ← 初始化全局/静态变量│ .bss        │ ← 未初始化全局/静态变量（只记录大小，不占磁盘空间）│ .symtab     │ ← 符号表（函数名、变量名等）│ .rel.text   │ ← 标记 `.text` 中需要重定位的位置│ .rel.data   │ ← 标记 `.data` 中需要重定位的位置│ .debug      │ ← 调试信息（如变量、类型、源码行号等）│ .line       │ ← 行号映射信息（源码行 ↔ 指令地址）│ .strtab     │ ← 字符串表（`.symtab` 和 `.debug` 的名称信息）├─────────────┤│ 节头部表     │ ← 描述上面每个 section 的偏移、大小、名字等└─────────────┘\n\n具体例子如下：\n~/workspace$ readelf -S main.oThere are 14 section headers, starting at offset 0x208:\n\nSection Headers:  [Nr] Name              Type             Address           Offset       Size              EntSize          Flags  Link  Info  Align  [ 0]                   NULL             0000000000000000  00000000       0000000000000000  0000000000000000           0     0     0  [ 1] .text             PROGBITS         0000000000000000  00000040       0000000000000000  0000000000000000  AX       0     0     1  [ 2] .data             PROGBITS         0000000000000000  00000040       0000000000000000  0000000000000000  WA       0     0     1  [ 3] .bss              NOBITS           0000000000000000  00000040       0000000000000000  0000000000000000  WA       0     0     1  [ 4] .text.startup     PROGBITS         0000000000000000  00000040       000000000000000f  0000000000000000  AX       0     0     16  [ 5] .rela.text.s[...] RELA             0000000000000000  00000158       0000000000000018  0000000000000018   I      11     4     8  [ 6] .comment          PROGBITS         0000000000000000  0000004f       0000000000000013  0000000000000001  MS       0     0     1  [ 7] .note.GNU-stack   PROGBITS         0000000000000000  00000062       0000000000000000  0000000000000000           0     0     1  [ 8] .note.gnu.pr[...] NOTE             0000000000000000  00000068       0000000000000030  0000000000000000   A       0     0     8  [ 9] .eh_frame         PROGBITS         0000000000000000  00000098       0000000000000030  0000000000000000   A       0     0     8  [10] .rela.eh_frame    RELA             0000000000000000  00000170       0000000000000018  0000000000000018   I      11     9     8  [11] .symtab           SYMTAB           0000000000000000  000000c8       0000000000000078  0000000000000018          12     3     8  [12] .strtab           STRTAB           0000000000000000  00000140       0000000000000011  0000000000000000           0     0     1  [13] .shstrtab         STRTAB           0000000000000000  00000188       000000000000007a  0000000000000000           0     0     1\n\n| 节编号 | 节名                   | 类型       | 描述                                                           || --- | -------------------- | -------- | ------------------------------------------------------------ || 1   | `.text`              | PROGBITS | 空（Size = 0），可能没用上                                            || 2   | `.data`              | PROGBITS | 空，表示没有初始化的全局变量                                               || 3   | `.bss`               | NOBITS   | 空，表示没有未初始化的全局变量                                              || 4   | `.text.startup`      | PROGBITS | 真正的代码所在，包含 `main()` 函数，大小 0x0f（15 字节）                      || 5   | `.rela.text.startup` | RELA     | 重定位表，告诉链接器 `.text.startup` 中有符号（如 `puts()`）未解析，需要打补丁       || 6   | `.comment`           | PROGBITS | 编译器信息，如 `GCC: (Ubuntu 13.2.0...)`                            || 7   | `.note.GNU-stack`    | PROGBITS | 标记栈是否可执行（通常为空），现代系统中用于安全                                     || 8   | `.note.gnu.*`        | NOTE     | GNU 工具链信息，比如构建器版本                                            || 9   | `.eh_frame`          | PROGBITS | 栈展开信息（异常处理支持，main 里 `call` 会用到）                              || 10  | `.rela.eh_frame`     | RELA     | `.eh_frame` 的重定位表（地址待定）                                      || 11  | `.symtab`            | SYMTAB   | 符号表，记录了 `main` 函数、字符串 `.LC0`、外部符号 `puts` 等                 || 12  | `.strtab`            | STRTAB   | 字符串表，供 `.symtab` 使用，存储符号名字（如 `main`, `puts`, `.LC0`）       || 13  | `.shstrtab`          | STRTAB   | 节名字字符串表，供 section header 本身使用，存 `.text`, `.data`, `.bss` 等名字 |\n\n符号和符号表在可重定位目标文件中，每个模块都有符号表，符号分为三类：\n\n\n\n符号类型\n描述\n\n\n\n全局符号\n模块中定义，可以被其他模块引用（如非 static 的全局变量&#x2F;函数）\n\n\n外部符号\n模块中引用，但定义在其他模块中（如调用外部函数）\n\n\n局部符号\n只在当前模块中定义和引用（如 static 修饰的变量&#x2F;函数）\n\n\n局部程序变量（非 static 的局部变量）不会出现在符号表中，因为它们由运行时栈管理，不需要链接器关心。\n带 static 的局部变量 会被分配到 .bss 或 .data 段，并出现在符号表中，拥有唯一的内部名称（如 x.1, x.2）用于区分\n通过 readelf -s可以 看到的符号表条目\n举例：\n// main.cint add(int a, int b);int main() &#123; return add(2, 3); &#125;int f() &#123;  static int x = 0;  return x;&#125;\n\n对应的符号表如下：\nSymbol table &#x27;.symtab&#x27; contains 8 entries:   Num:    Value          Size Type    Bind   Vis      Ndx Name     0: 0000000000000000     0 NOTYPE  LOCAL  DEFAULT  UND      1: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS main.c     2: 0000000000000000     0 SECTION LOCAL  DEFAULT    1 .text     3: 0000000000000000     0 SECTION LOCAL  DEFAULT    4 .bss     4: 0000000000000000     4 OBJECT  LOCAL  DEFAULT    4 x.0     5: 0000000000000000    21 FUNC    GLOBAL DEFAULT    1 main     6: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  UND add     7: 0000000000000015    12 FUNC    GLOBAL DEFAULT    1 f\n\n注意：2\t0x0\t0\tSECTION\tLOCAL\tDEFAULT\t1\t.text 是符号表中的一个符号，类型为 SECTION。不是函数、变量等程序员写的“用户符号”，而是由汇编器自动为每个 section 添加的“段标识符”。\n上述段符号的用途主要是：\n\n供链接器使用，在链接阶段帮助识别每个节的位置；\n在汇编器内部和调试工具中用于定位节的起始地址。\n\n你可以把它当作是“代表一个 section 的名字和索引的特殊符号”，用于支持定位和重定位。\n哪些不会成为符号？非 static 的局部变量（比如 int y = 0; 在函数内部定义的）不会出现在符号表中，因为它们是栈上的临时变量，链接器不需要知道。\n","categories":["《深入理解计算机系统》"],"tags":["链接"]},{"title":"链接(二)","url":"/2025/06/25/%E9%93%BE%E6%8E%A5(%E4%BA%8C)/","content":"符号解析符号解析就是链接器将“引用的符号”与“定义的符号”进行匹配的过程。\n链接器如何解析多重定义的全局符号链接器处理多个 可重定位目标文件（.o 文件）。\n每个模块中可能会定义同名的全局符号（如变量、函数）。\n符号分为两类：\n\n强符号（strong）：函数、已初始化的全局变量（如 int x = 1;）\n弱符号（weak）：未初始化的全局变量（如 int x;）\n\n\n\n\n规则编号\n描述\n\n\n\n规则 1\n多个强符号同名 → 报错，链接失败（冲突无法解决）。\n\n\n规则 2\n强符号 vs 弱符号同名 → 选择强符号，忽略弱的。\n\n\n规则 3\n多个弱符号同名 → 任意选择一个（通常选择第一个出现的）。\n\n\n静态库静态库（static library）：是由多个可重定位目标文件（.o）打包成的一个 .a 文件，供链接器使用。\n链接器在使用静态库时：只提取被程序实际引用的目标文件，不会整个复制，从而节省磁盘和内存空间。\n优点：\n\n将函数各自编译为 .o 文件后，用工具打包成 .a 文件。\n编译程序时只需写出库文件名即可（如 -lm -lc），简洁方便。\n链接器会自动挑选需要的模块，避免冗余。\n.a 文件是一种存档格式，带有成员信息索引（如文件大小、位置等）。\n\n举例：\ngcc main.o -L. -lmylib -o myapp\n\n\n-L. 指定当前目录搜索库；\n-lmylib 会让链接器查找 libmylib.a（静态库）；\n链接器会从中挑出 main.o 需要用到的 .o 文件，做静态链接，生成 myapp。\n\n静态库与静态链接\n\n\n\n项目\n描述\n\n\n\n静态库\n存放多个 .o 文件的归档文件（.a）\n\n\n静态链接\n使用链接器把 .o 文件（含来自 .a 的）合并为一个最终的可执行文件\n\n\n二者关系\n静态链接 可能使用 静态库作为输入源之一\n\n\n链接器如何使用静态库来解析引用链接器符号解析的核心流程\n初始状态：\nE：已选中的目标文件集合（最终会链接成可执行文件）\nU：未解析的符号集合（只引用但还没找到定义）\nD：已定义的符号集合（已经找到定义的符号）\n初始时这三个集合是空的。\n\n\n处理目标文件（.o）：\n把 .o 文件加入到 E。\n收集该文件中定义的符号加到 D。\n把引用的、未定义的符号加到 U。\n\n\n处理静态库（.a）：\n对 .a 中的每个成员目标文件：\n如果它定义了 U 中某个符号，就把它加入到 E 中。\n然后更新 U 和 D。\n\n\n重复这个过程直到 U、D 不再变化。\n没有用到的库成员就丢弃（不会加入最终可执行程序中）。\n\n\n最终检查：\n如果还有未解析的符号（U 非空），就报错并终止。\n\n\n\n链接器是按命令行从左到右顺序处理的，这会导致以下情况\n举例：\ngcc -static ./libvector.a main2.c\n\n\n先处理 libvector.a，这时 U 是空（没有要解析的符号），所以库的任何成员都不会被加载；\n后处理 main2.c，其中调用了 addvec()，这会把 addvec 加入 U；\n但已经错过了加载库的机会，链接器不会回头重新看 libvector.a；\n所以 addvec 没找到 → 报错 undefined reference to &#39;addvec&#39;。\n\n如果 libx.a 和 liby.a 之间存在循环引用（互相调用），就需要重复指定，否则会有未解析符号\ngcc foo.o libx.a liby.a libx.a\n\n重定位重定位就是链接器将多个 .o 文件合并时，为每个代码和数据分配实际内存地址，并修正代码中对符号的引用地址，使它们能在程序运行时正确访问。\n重定位的两步骤：\n重定位节和符号定义（合并和分配地址）\n链接器会将多个 .o 文件中相同类型的节（比如 .data, .text）合并成一个大的节。\n然后：\n为这些合并后的节分配内存地址。\n为每个符号（函数、全局变量等）确定最终运行时地址。\n\n\n\n重定位节中的符号引用（修正引用地址）\n接下来，链接器会在代码和数据中找到所有对符号的引用（比如调用 printf 的指令）。\n使用 .o 文件中记录的**重定位条目（relocation entry）**来：\n替换或修改那些引用地址，让它们指向真正的地址。\n\n\n\n重定位条目什么是重定位条目：\n举例：\n当编译器&#x2F;汇编器生成一个目标文件（.o 文件）时，它还不知道变量或函数在内存里的最终地址比如：\nextern int x;void foo() &#123;    x = 42;&#125;\n\nx 是在别的文件里定义的，当前 .o 文件根本不知道 x 的地址。 所以，汇编器会留下一个“记号”告诉链接器：\n每一个重定位条目描述了：\n\n\n\n字段\n含义\n\n\n\noffset\n要修改的地方（代码或数据中的偏移）\n\n\nsymbol\n当前访问的是哪个符号（变量或函数）\n\n\ntype\n用哪种方式修改（是绝对地址还是相对地址）\n\n\naddend\n可选的补偿偏移（比如增加一个偏移量）\n\n\n这些信息一般存在 .rela.text（代码里的引用）或 .rela.data（数据段引用）段中。\n可执行目标文件：可执行文件是编译器和链接器将.o 文件转换成的最终程序二进制文件，操作系统可以直接把它加载到内存并运行\n可执行 ELF 文件的结构：\n+----------------+| ELF Header     | &lt;== 文件的元数据，如魔数、架构类型、入口点等+----------------+| 程序头表       | &lt;== OS 用来加载程序的结构（段的偏移、权限等）+----------------+| .init          | &lt;== 程序启动初始化函数（调用 _init）| .text          | &lt;== 代码段（程序指令）| .rodata        | &lt;== 只读数据（如字符串常量）| .data          | &lt;== 已初始化的全局变量| .bss           | &lt;== 未初始化的全局变量（运行时置 0）| .symtab        | &lt;== 符号表（调试信息用）| .debug         | &lt;== 调试信息| .line          | &lt;== 源码行号表| .strtab        | &lt;== 字符串表+----------------+| 节头部表       | &lt;== 每个节的信息（调试器用）+----------------+\n\n可执行目标文件的结构与可重定位目标文件类似，整体格式由 ELF 头描述。其中包含了程序的入口地址（entry point），即程序运行时将执行的第一条指令的位置。\n其中的 .text、.rodata 和 .data 等节与可重定位文件中的同名节基本相同，只是它们已经被重定位到程序运行时的实际内存地址。\n此外，可执行文件中的 .init 节中定义了一个初始化函数 _init，程序启动时会自动调用它。由于可执行文件已经是完全链接好的，不再需要用于重定位的 .rel 节。\n程序头部表（Program Header Table） 是 ELF（Executable and Linkable Format）可执行文件中的一个重要结构，用于指导操作系统的程序加载器如何将可执行文件映射到内存中运行。它和节头部表（Section Header Table）不同，节头部表更多是给链接器用的，而程序头部表是给操作系统加载程序用的。\n动态库动态库是一种编译好但不嵌入可执行程序本体的代码库，它提供了函数和变量，供多个程序在运行时动态加载并使用\n优点：\n磁盘空间共享：一个 .so 文件可以被多个程序共用，不像静态库那样每个程序都复制一份。\n内存空间共享：多个进程可以共享 .so 的 .text（代码段），减少了内存占用。\n动态链接过程\n编译器使用 -fPIC（生成位置无关代码）和 -shared 生成共享库文件：\n\ngcc -shared -fPIC -o libvector.so addvec.c multvec.c\n\n\n编译主程序并链接共享库\n\ngcc -o prog21 main2.c ./libvector.so\n\n\n运行时加载流程\n\n\n加载器（loader）启动 prog21。\n它发现程序有一个 .interp 段，里面指定了动态链接器（如 /lib64/ld-linux-x86-64.so.2）。\n动态链接器负责：\n加载 libvector.so 和 libc.so\n重定位 prog21 中对 printf、addvec 等函数的调用地址\n\n\n动态链接完成后，跳转到主程序入口地址开始执行\n\n位置无关代码背景\n共享库（.so 文件）常常被多个进程同时加载使用，为了节省内存，系统希望这些进程可以共享这段库的代码段（text segment）。\n但是每个进程的虚拟地址空间不同，无法保证这个共享库总是被加载到相同的地址。\n解决方法：使用位置无关代码（PIC）\nPIC 为了解决“全局变量地址不确定”的问题，引入了 GOT 表。可以把 GOT 理解成一个中间跳板表：\n程序访问变量不是直接访问变量地址，而是：\n\n先从 GOT 表中找到变量的地址\n然后跳转或读写这个地址\n\n注意：“GOT 表属于动态库本身，位于动态库的 .data 段的开始部分”。\n每个进程加载 .so 时，都会为 .so 分配一段自己的私有地址空间副本（包括 GOT 表）。所以：\n\nGOT 表内容每个进程是独立的\n写进去的地址是针对这个进程自己的虚拟地址\n\n库拦截通过替换或重定义动态库中的函数，以拦截或修改函数行为。\n运行时拦截：\n在程序启动时，通过设置环境变量（如 LD_PRELOAD），优先加载你自己的动态库，从而覆盖掉原本的库函数。\n 原理：\n动态链接器（如 Linux 上的 ld-linux.so）在程序启动时会加载所有依赖的 .so 文件，它会按照顺序查找函数符号。 如果你用 LD_PRELOAD 指定了一个动态库，这个库里的符号会优先被解析，就能**“拦截”掉原始函数**了。\n举例：\n\n写一个库，重定义 malloc\n\n// mymalloc.c#define _GNU_SOURCE#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;dlfcn.h&gt;void* malloc(size_t size) &#123;    void* (*real_malloc)(size_t) = dlsym(RTLD_NEXT, &quot;malloc&quot;);  // 调用原始 malloc    printf(&quot;malloc(%zu) called\\n&quot;, size);    return real_malloc(size);&#125;\n\n\n编译成共享库\n\ngcc -shared -fPIC -o libmymalloc.so mymalloc.c -ldl\n\n\n运行目标程序时使用 LD_PRELOAD\n\nLD_PRELOAD=./libmymalloc.so ./your_program\n\n","categories":["《深入理解计算机系统》"],"tags":["链接"]},{"title":"TCP协议的初始化","url":"/2025/07/24/TCP%E5%8D%8F%E8%AE%AE%E5%88%9D%E5%A7%8B%E5%8C%96/","content":"TCP协议的初始化tcp_init() 是 Linux 内核中 TCP 协议的全局初始化函数，在inet_init中被调用，主要完成了以下工作\n初始化listen哈希表，两个bind哈希表和ehash表，创建bind用到的slab分配器，根据cpu的数量来确定ehash表的锁的数量，hash 桶的大小是更具当前系统内存大小计算得来的 比如8G就是64k个桶，桶的大小是不会改变的，可以通过参数指定范围。\n根据系统内存大小设置tcp的可以使用页数的内存水位线，包括全局的tcp和单个的tcp。\n**调用tcp_v4_init**为每个cpu创建一个rawsocket套接字用来发送rst报文，为每个namespace注册init和exit的ops\n具体逻辑如下：\nvoid __init tcp_init(void)&#123;\tint max_rshare, max_wshare, cnt;\tunsigned long limit;\tunsigned int i;\t//最小的mss不小于tcp选项的长度\tBUILD_BUG_ON(TCP_MIN_SND_MSS &lt;= MAX_TCP_OPTION_SPACE);\tBUILD_BUG_ON(sizeof(struct tcp_skb_cb) &gt;\t\t     sizeof_field(struct sk_buff, cb));\t//初始化一个tcp数量的计数器\tpercpu_counter_init(&amp;tcp_sockets_allocated, 0, GFP_KERNEL);\t//设置一个定时器，不启动，处理孤儿连接\ttimer_setup(&amp;tcp_orphan_timer, tcp_orphan_update, TIMER_DEFERRABLE);\tmod_timer(&amp;tcp_orphan_timer, jiffies + TCP_ORPHAN_TIMER_PERIOD);\t//初始化listen hash表\tinet_hashinfo2_init(&amp;tcp_hashinfo, &quot;tcp_listen_portaddr_hash&quot;,\t\t\t    thash_entries, 21,  /* one slot per 2 MB*/\t\t\t    0, 64 * 1024);\t//创建两个bind用到的slab\ttcp_hashinfo.bind_bucket_cachep =\t\tkmem_cache_create(&quot;tcp_bind_bucket&quot;,\t\t\t\t  sizeof(struct inet_bind_bucket), 0,\t\t\t\t  SLAB_HWCACHE_ALIGN | SLAB_PANIC |\t\t\t\t  SLAB_ACCOUNT,\t\t\t\t  NULL);\ttcp_hashinfo.bind2_bucket_cachep =\t\tkmem_cache_create(&quot;tcp_bind2_bucket&quot;,\t\t\t\t  sizeof(struct inet_bind2_bucket), 0,\t\t\t\t  SLAB_HWCACHE_ALIGN | SLAB_PANIC |\t\t\t\t  SLAB_ACCOUNT,\t\t\t\t  NULL);\t/* Size and allocate the main established and bind bucket\t * hash tables.\t *\t * The methodology is similar to that of the buffer cache.\t */\t //创建ehash表 并初始化桶和锁，bhash是一个bucket一个桶\t //第三个参数的本质上就是根据系统的内存去算有多少个桶 比如8G就是64k个桶\ttcp_hashinfo.ehash =\t\talloc_large_system_hash(&quot;TCP established&quot;,\t\t\t\t\tsizeof(struct inet_ehash_bucket),\t\t\t\t\tthash_entries,\t\t\t\t\t17, /* one slot per 128 KB of memory */ \t\t\t\t\t0,\t\t\t\t\tNULL,\t\t\t\t\t&amp;tcp_hashinfo.ehash_mask,\t\t\t\t\t0,\t\t\t\t\tthash_entries ? 0 : 512 * 1024);\tfor (i = 0; i &lt;= tcp_hashinfo.ehash_mask; i++)\t\tINIT_HLIST_NULLS_HEAD(&amp;tcp_hashinfo.ehash[i].chain, i);\t//根据cpu的数量计算 自旋锁的数量，\tif (inet_ehash_locks_alloc(&amp;tcp_hashinfo))\t\tpanic(&quot;TCP: failed to alloc ehash_locks&quot;);\t//创建bhash表  注意，根据第二个参数这里实际申请了bhash和bhash2 他们在内存上是连续的\ttcp_hashinfo.bhash =\t\talloc_large_system_hash(&quot;TCP bind&quot;,\t\t\t\t\t2 * sizeof(struct inet_bind_hashbucket),\t\t\t\t\ttcp_hashinfo.ehash_mask + 1, //这里其实让ehash和bhash的数目保持一致了\t\t\t\t\t17, /* one slot per 128 KB of memory */\t\t\t\t\t0,\t\t\t\t\t&amp;tcp_hashinfo.bhash_size,\t\t\t\t\tNULL,\t\t\t\t\t0,\t\t\t\t\t64 * 1024);\t//左移 bhash_size 位变成真正的桶数 比如 1 &lt;&lt;12 = 4096\ttcp_hashinfo.bhash_size = 1U &lt;&lt; tcp_hashinfo.bhash_size; \t//设置bhash2的索引\ttcp_hashinfo.bhash2 = tcp_hashinfo.bhash + tcp_hashinfo.bhash_size; \tfor (i = 0; i &lt; tcp_hashinfo.bhash_size; i++) &#123;\t\tspin_lock_init(&amp;tcp_hashinfo.bhash[i].lock);\t\tINIT_HLIST_HEAD(&amp;tcp_hashinfo.bhash[i].chain);\t\tspin_lock_init(&amp;tcp_hashinfo.bhash2[i].lock);\t\tINIT_HLIST_HEAD(&amp;tcp_hashinfo.bhash2[i].chain);\t&#125;\ttcp_hashinfo.pernet = false;\t//根据ehash的桶的数目，计算了一个容许的最大孤儿连接的数量？，在tcp_close 中会用到\tcnt = tcp_hashinfo.ehash_mask + 1;\tsysctl_tcp_max_orphans = cnt / 2;\t//根据系统当前空闲的页数来确定tcp可以使用页数的水位线有三个值，注意这个是全局的\t//net.ipv4.tcp_mem \ttcp_init_mem();\t/* Set per-socket limits to no more than 1/128 the pressure threshold */\t//设置单个tcp的发送和接收使用的字节数，tcp_init_mem的是页数！\tlimit = nr_free_buffer_pages() &lt;&lt; (PAGE_SHIFT - 7);\tmax_wshare = min(4UL*1024*1024, limit);\tmax_rshare = min(6UL*1024*1024, limit);\tinit_net.ipv4.sysctl_tcp_wmem[0] = PAGE_SIZE;\tinit_net.ipv4.sysctl_tcp_wmem[1] = 16*1024;\tinit_net.ipv4.sysctl_tcp_wmem[2] = max(64*1024, max_wshare);\tinit_net.ipv4.sysctl_tcp_rmem[0] = PAGE_SIZE;\tinit_net.ipv4.sysctl_tcp_rmem[1] = 131072;\tinit_net.ipv4.sysctl_tcp_rmem[2] = max(131072, max_rshare);\tpr_info(&quot;Hash tables configured (established %u bind %u)\\n&quot;,\t\ttcp_hashinfo.ehash_mask + 1, tcp_hashinfo.bhash_size);\t//创建一个rawsocket然后初始化系统参数 sysctl -a \ttcp_v4_init();\ttcp_metrics_init();\tBUG_ON(tcp_register_congestion_control(&amp;tcp_reno) != 0);\t//注册tsq的软中端处理函数\ttcp_tasklet_init();\tmptcp_init();&#125;\n\n上述tcp_init_mem();中设置了tcp 内存的水位线具体如下所示，这里的limit的单位是页。\nstatic void __init tcp_init_mem(void)&#123;\tunsigned long limit = nr_free_buffer_pages() / 16;\t//最少用128个页\tlimit = max(limit, 128UL);\t//低阈值\tsysctl_tcp_mem[0] = limit / 4 * 3;\t\t/* 4.68 % */\t//中阈值\tsysctl_tcp_mem[1] = limit;\t\t\t/* 6.25 % */\t//高阈值\tsysctl_tcp_mem[2] = sysctl_tcp_mem[0] * 2;\t/* 9.37 % */&#125;\n\n在tcp_v4_init中创建了一个rawsocket套接字，当tcp需要回复rst报文的时候就是用的这个套接字。然后调用register_pernet_subsys(&amp;tcp_sk_ops)来注册初始化的系统的参数的回调函数。具体代码如下：\nvoid __init tcp_v4_init(void)&#123;\tint cpu, res;\t//这里为每个cpu创建了一个内核的rawsocekt套接字，当tcp发送reset报文的时候貌似就会用这个这个套件字，目的是让用户无感知？\tfor_each_possible_cpu(cpu) &#123;\t\tstruct sock *sk;\t\tres = inet_ctl_sock_create(&amp;sk, PF_INET, SOCK_RAW,\t\t\t\t\t   IPPROTO_TCP, &amp;init_net);\t\tif (res)\t\t\tpanic(&quot;Failed to create the TCP control socket.\\n&quot;);\t\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\t\t//rst报文规范\t\t/* Please enforce IP_DF and IPID==0 for RST and\t\t * ACK sent in SYN-RECV and TIME-WAIT state.\t\t */\t\t\tinet_sk(sk)-&gt;pmtudisc = IP_PMTUDISC_DO;\t\tper_cpu(ipv4_tcp_sk, cpu) = sk;\t&#125;\t//注册tcp协议栈，每个网络空间一个\tif (register_pernet_subsys(&amp;tcp_sk_ops))\t\tpanic(&quot;Failed to create the TCP control socket.\\n&quot;);//bpf相关#if defined(CONFIG_BPF_SYSCALL) &amp;&amp; defined(CONFIG_PROC_FS)\tbpf_iter_register();#endif&#125;\n\n上述tcp_sk_ops的具体结构如下所示：\nstatic struct pernet_operations __net_initdata tcp_sk_ops = &#123;       .init\t   = tcp_sk_init,       .exit\t   = tcp_sk_exit,       .exit_batch = tcp_sk_exit_batch,&#125;;\n\n上述tcp_sk_init中完成了对sysctl -a | grep tcp中绝大多数参数的初始化，以及注册拥塞算法，默认是reno，具体代码如下：\nstatic int __net_init tcp_sk_init(struct net *net)&#123;\tnet-&gt;ipv4.sysctl_tcp_ecn = 2;  //启用ecn\tnet-&gt;ipv4.sysctl_tcp_ecn_fallback = 1;\tnet-&gt;ipv4.sysctl_tcp_base_mss = TCP_BASE_MSS;//1024\tnet-&gt;ipv4.sysctl_tcp_min_snd_mss = TCP_MIN_SND_MSS;  //发送端最小的mss\tnet-&gt;ipv4.sysctl_tcp_probe_threshold = TCP_PROBE_THRESHOLD; //零窗口探测次数 8\tnet-&gt;ipv4.sysctl_tcp_probe_interval = TCP_PROBE_INTERVAL;  //两次探测的最小间隔 600\tnet-&gt;ipv4.sysctl_tcp_mtu_probe_floor = TCP_MIN_SND_MSS; //mtu探测的最小mss\tnet-&gt;ipv4.sysctl_tcp_keepalive_time = TCP_KEEPALIVE_TIME; //在空闲多少秒后启动这个保活探测\tnet-&gt;ipv4.sysctl_tcp_keepalive_probes = TCP_KEEPALIVE_PROBES;//保活探测次数 9\tnet-&gt;ipv4.sysctl_tcp_keepalive_intvl = TCP_KEEPALIVE_INTVL;\t//零窗口探测时间间隔\tnet-&gt;ipv4.sysctl_tcp_syn_retries = TCP_SYN_RETRIES;\t\t\t//syn包重传次数 6\tnet-&gt;ipv4.sysctl_tcp_synack_retries = TCP_SYNACK_RETRIES;\t//syn ack 重传次数 5\tnet-&gt;ipv4.sysctl_tcp_syncookies = 1;\t\t\t\t\t\t//开启syncookie\tnet-&gt;ipv4.sysctl_tcp_reordering = TCP_FASTRETRANS_THRESH;\t//乱续的阈值\tnet-&gt;ipv4.sysctl_tcp_retries1 = TCP_RETR1;\t\t\t\t\t//超过这个数量之后，好险\tnet-&gt;ipv4.sysctl_tcp_retries2 = TCP_RETR2;\t\t\t\t\t//已经建立连接尝试重传的最大次数同上\tnet-&gt;ipv4.sysctl_tcp_orphan_retries = 0;\t\t\t\t\t//孤儿连接的重传次数\tnet-&gt;ipv4.sysctl_tcp_fin_timeout = TCP_FIN_TIMEOUT;\t\t\t//timeout的时间\tnet-&gt;ipv4.sysctl_tcp_notsent_lowat = UINT_MAX;\t\t\t\t//当未发送数据量低于此值时，内核会通知应用程序可以继续写入数据\tnet-&gt;ipv4.sysctl_tcp_tw_reuse = 2;\t\t\t\t\t\t\t//timeout中是否可可以reuseport\tnet-&gt;ipv4.sysctl_tcp_no_ssthresh_metrics_save = 1;\t\t\t//设置为1 表示每次新建立连接的时候重新计算慢启动阈值\trefcount_set(&amp;net-&gt;ipv4.tcp_death_row.tw_refcount, 1);\ttcp_set_hashinfo(net);\tnet-&gt;ipv4.sysctl_tcp_sack = 1;\t\t\t\t\t\t\t\t//sack\tnet-&gt;ipv4.sysctl_tcp_window_scaling = 1;\t\t\t\t\t//窗口缩放\tnet-&gt;ipv4.sysctl_tcp_timestamps = 1;\t\t\t\t\t\t//tcp时间戳选项\tnet-&gt;ipv4.sysctl_tcp_early_retrans = 3;\t\t\t\t\t\t//TLP会用到\tnet-&gt;ipv4.sysctl_tcp_recovery = TCP_RACK_LOSS_DETECTION;\t//RACK\t用到\tnet-&gt;ipv4.sysctl_tcp_slow_start_after_idle = 1; /* By default, RFC2861 behavior.  */  //从idel开始发包后是是否经历慢启动\tnet-&gt;ipv4.sysctl_tcp_retrans_collapse = 1;\t\t\t\t\t//合并重传数据包\tnet-&gt;ipv4.sysctl_tcp_max_reordering = 300;\t\t\t\t\t//容许的乱续数量，怎么计算的？ 拥塞控制中会用到\tnet-&gt;ipv4.sysctl_tcp_dsack = 1;\t\t\t\t\t\t\t\t//启用dsack\t\tnet-&gt;ipv4.sysctl_tcp_app_win = 31;\t\t\t\t\t\t\t//限制 TCP 接收窗口不要占满整个buffer\tnet-&gt;ipv4.sysctl_tcp_adv_win_scale = 1;\t\t\t\t\t\t//好像没有地方用到\tnet-&gt;ipv4.sysctl_tcp_frto = 2;\t\t\t\t\t\t\t\t//FRTO\tnet-&gt;ipv4.sysctl_tcp_moderate_rcvbuf = 1;\t\t\t\t\t//可以动态调整接收缓冲区和接收窗口大小\t/* This limits the percentage of the congestion window which we\t * will allow a single TSO frame to consume.  Building TSO frames\t * which are too large can cause TCP streams to be bursty.\t */\tnet-&gt;ipv4.sysctl_tcp_tso_win_divisor = 3;\t\t\t\t\t//限制tso报文的最大大小\t/* Default TSQ limit of 16 TSO segments */\tnet-&gt;ipv4.sysctl_tcp_limit_output_bytes = 16 * 65536;\t\t//tsq用到！防止当个socket占用整个qdisc队列\t/* rfc5961 challenge ack rate limiting, per net-ns, disabled by default. */\tnet-&gt;ipv4.sysctl_tcp_challenge_ack_limit = INT_MAX;\t\t\t\tnet-&gt;ipv4.sysctl_tcp_min_tso_segs = 2;\t\t\t\t\t\t//最少几个tcp段\tnet-&gt;ipv4.sysctl_tcp_tso_rtt_log = 9;  /* 2^9 = 512 usec */ //计算tso的size会用到\tnet-&gt;ipv4.sysctl_tcp_min_rtt_wlen = 300;\t\t\t\t\t//计算多少轮中rtt的最小值 这里就是300\tnet-&gt;ipv4.sysctl_tcp_autocorking = 1;\t\t\t\t\t\t//是否开启corking\tnet-&gt;ipv4.sysctl_tcp_invalid_ratelimit = HZ/2;\t\t\t\t//参与处理不合法ack的速率的计算\tnet-&gt;ipv4.sysctl_tcp_pacing_ss_ratio = 200;\t\t\t\t\t//ss表示慢启动\tnet-&gt;ipv4.sysctl_tcp_pacing_ca_ratio = 120;\t\t\t\t\t//参与tcppacing的计算，上面的也是ca表示拥塞避免\tif (net != &amp;init_net) &#123;\t\tmemcpy(net-&gt;ipv4.sysctl_tcp_rmem,\t\t       init_net.ipv4.sysctl_tcp_rmem,\t\t       sizeof(init_net.ipv4.sysctl_tcp_rmem));\t\tmemcpy(net-&gt;ipv4.sysctl_tcp_wmem,\t\t       init_net.ipv4.sysctl_tcp_wmem,\t\t       sizeof(init_net.ipv4.sysctl_tcp_wmem));\t&#125;\tnet-&gt;ipv4.sysctl_tcp_comp_sack_delay_ns = NSEC_PER_MSEC;\t\t//压缩sack的时间1ms\tnet-&gt;ipv4.sysctl_tcp_comp_sack_slack_ns = 100 * NSEC_PER_USEC;\t//和上面的配合，提前的时间\tnet-&gt;ipv4.sysctl_tcp_comp_sack_nr = 44;\t\t\t\t\t\t\t//和上面的类似， 收到乱序数据包的数量 超过这个值也发送ack\tnet-&gt;ipv4.sysctl_tcp_fastopen = TFO_CLIENT_ENABLE;\t\t\t\t//是否开启TFO\tnet-&gt;ipv4.sysctl_tcp_fastopen_blackhole_timeout = 0;\t\t\t//TFO 失败后 放入黑名单持续的时间\tatomic_set(&amp;net-&gt;ipv4.tfo_active_disable_times, 0);\t/* Set default values for PLB */\t\t\t\t\t\t\t\t\tnet-&gt;ipv4.sysctl_tcp_plb_enabled = 0; /* Disabled by default */\tnet-&gt;ipv4.sysctl_tcp_plb_idle_rehash_rounds = 3;\tnet-&gt;ipv4.sysctl_tcp_plb_rehash_rounds = 12;\tnet-&gt;ipv4.sysctl_tcp_plb_suspend_rto_sec = 60;\t/* Default congestion threshold for PLB to mark a round is 50% */\tnet-&gt;ipv4.sysctl_tcp_plb_cong_thresh = (1 &lt;&lt; TCP_PLB_SCALE) / 2;\t/* Reno is always built in */\tif (!net_eq(net, &amp;init_net) &amp;&amp;\t    bpf_try_module_get(init_net.ipv4.tcp_congestion_control,\t\t\t       init_net.ipv4.tcp_congestion_control-&gt;owner))\t\tnet-&gt;ipv4.tcp_congestion_control = init_net.ipv4.tcp_congestion_control;\telse\t\tnet-&gt;ipv4.tcp_congestion_control = &amp;tcp_reno;  \tnet-&gt;ipv4.sysctl_tcp_syn_linear_timeouts = 4;\t\t\t\t//线性退避加指数退避\tnet-&gt;ipv4.sysctl_tcp_shrink_window = 0;\t\t\t\t\t\t//是否允许收缩接窗口\treturn 0;&#125;\n\ntcp_init中还会创建  tcp_metrics hash表，缓存rtt和拥塞窗口等信息，用于下次连接的时候参考？\n最终还会调用tcp_tasklet_init来注册tsq的软中断处理函数\nvoid __init tcp_tasklet_init(void)&#123;\tint i;\tfor_each_possible_cpu(i) &#123;\t\tstruct tsq_tasklet *tsq = &amp;per_cpu(tsq_tasklet, i);\t\tINIT_LIST_HEAD(&amp;tsq-&gt;head);\t\t//给tsq关联一个软中断处理函数\t\ttasklet_setup(&amp;tsq-&gt;tasklet, tcp_tasklet_func);\t&#125;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["sock","TCP"]},{"title":"TCP套接字的初始化","url":"/2025/07/28/tcp%E5%A5%97%E6%8E%A5%E5%AD%97%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96/","content":"TCP套接字的初始化在用户调用socket() 后，对于AF_INET协议族，会调用inet_create 而inet_create 中会调用具体协议的init函数，对于TCP来说就是tcp_v4_init_sock\nstatic int inet_create(struct net *net, struct socket *sock, int protocol,\t\t       int kern)&#123;···\t//这里是特定协议的初始化逻辑\tif (sk-&gt;sk_prot-&gt;init) &#123;\t\terr = sk-&gt;sk_prot-&gt;init(sk);\t\tif (err) &#123;\t\t\tsk_common_release(sk);\t\t\tgoto out;\t\t&#125;\t&#125;···&#125;\n\ntcp_v4_init_sock  具体实现如下：\nstatic int tcp_v4_init_sock(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\ttcp_init_sock(sk);\ticsk-&gt;icsk_af_ops = &amp;ipv4_specific;//ip层和tcp层直接的桥梁的ops#ifdef CONFIG_TCP_MD5SIG\ttcp_sk(sk)-&gt;af_specific = &amp;tcp_sock_ipv4_specific;//tcp的一些ops#endif\treturn 0;&#125;\n\ntcp_v4_init_sock  中调用tcp_init_sock 进一步完成初始化，除此之外还设置了与ip层相关的ops集合，其中包括ip层的发包函数，校验和的计算，三次握手中用到的回调函数，以及路由和MTU的设置等。具体代码如下：\nconst struct inet_connection_sock_af_ops ipv4_specific = &#123;\t.queue_xmit\t   = ip_queue_xmit, \t\t\t\t\t//交付给ip层的接口\t.send_check\t   = tcp_v4_send_check,\t\t\t\t\t//计算校验和\t.rebuild_header\t   = inet_sk_rebuild_header,\t\t//建连 或者重传 调用，查看是否有有效的路由信息\t.sk_rx_dst_set\t   = inet_sk_rx_dst_set,\t\t\t//设置dst 建连完成后会调用，其他地方好像没有了\t.conn_request\t   = tcp_v4_conn_request,\t\t\t//listen状态下，处理syn的回调\t.syn_recv_sock\t   = tcp_v4_syn_recv_sock,\t\t\t//完成建链 创建socket\t.net_header_len\t   = sizeof(struct iphdr),\t.setsockopt\t   = ip_setsockopt,\t.getsockopt\t   = ip_getsockopt,\t.addr2sockaddr\t   = inet_csk_addr2sockaddr,\t\t//好像没有地方用\t.sockaddr_len\t   = sizeof(struct sockaddr_in),\t.mtu_reduced\t   = tcp_v4_mtu_reduced,\t\t\t//icmp中会调用&#125;;\n\ntcp_init_sock 中，初始化了乱序队列和重传队列，初始化多个定时器，设置了拥塞算法相关的字段，设置了有写空间的回回调以及色设置mss的回调。\nvoid tcp_init_sock(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\ttp-&gt;out_of_order_queue = RB_ROOT;  //初始化红黑树\tsk-&gt;tcp_rtx_queue = RB_ROOT;\t\t//初始化重传队列\ttcp_init_xmit_timers(sk);\t\t\t//初始化tcp的定时器\tINIT_LIST_HEAD(&amp;tp-&gt;tsq_node);\t//初始化tsq 这个node是放到软中断中的\tINIT_LIST_HEAD(&amp;tp-&gt;tsorted_sent_queue);\t\t//按时间发送\ticsk-&gt;icsk_rto = TCP_TIMEOUT_INIT;\t//初始的重传超时时间 1\ticsk-&gt;icsk_rto_min = TCP_RTO_MIN;\t//最小的重传超时时间 200ms\ticsk-&gt;icsk_delack_max = TCP_DELACK_MAX; //延迟ack\ttp-&gt;mdev_us = jiffies_to_usecs(TCP_TIMEOUT_INIT); //rtt相关\tminmax_reset(&amp;tp-&gt;rtt_min, tcp_jiffies32, ~0U); //初始化历史链接中rtt的min\t/* So many TCP implementations out there (incorrectly) count the\t * initial SYN frame in their delayed-ACK and congestion control\t * algorithms that we must have the following bandaid to talk\t * efficiently to them.  -DaveM\t */\ttcp_snd_cwnd_set(tp, TCP_INIT_CWND);\t\t\t\t//初始化cwnd 10个mss\t/* There&#x27;s a bubble in the pipe until at least the first ACK. */\ttp-&gt;app_limited = ~0U;\t\t\t\t\t\t\t\t\t//这个字段表示应用程序未持续填充数据\ttp-&gt;rate_app_limited = 1;\t\t\t\t\t\t\t\t// bbr算法会yong到\t/* See draft-stevens-tcpca-spec-01 for discussion of the\t * initialization of these values.\t */\ttp-&gt;snd_ssthresh = TCP_INFINITE_SSTHRESH;  \t\t\t\t//慢启动阈值\ttp-&gt;snd_cwnd_clamp = ~0;\t\t\t\t\t\t\t\t//拥塞窗口最大值\ttp-&gt;mss_cache = TCP_MSS_DEFAULT;\t\t\t\t\t\t//初始哈uMSS\ttp-&gt;reordering = READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_reordering);  //容许的最大乱续程度 初始化为3\ttcp_assign_congestion_control(sk);\t\t\t//设置拥塞算法的ops\ttp-&gt;tsoffset = 0;\t\t\t\t\t\t\t//tcp时间戳选项会用到\ttp-&gt;rack.reo_wnd_steps = 1;\t\t\t\t\t//乱续窗口动态增长的步长\tsk-&gt;sk_write_space = sk_stream_write_space; \t//有写空间的回调\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\ticsk-&gt;icsk_sync_mss = tcp_sync_mss;\t\t\t//更新mss的回调\tWRITE_ONCE(sk-&gt;sk_sndbuf, READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_wmem[1]));\tWRITE_ONCE(sk-&gt;sk_rcvbuf, READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_rmem[1]));\ttcp_scaling_ratio_init(sk);\t\t\t\t\t//初始化窗口的缩放因子\tset_bit(SOCK_SUPPORT_ZC, &amp;sk-&gt;sk_socket-&gt;flags);\tsk_sockets_allocated_inc(sk);&#125;\n\n上述代码中的tcp_init_xmit_timers 设置了5个定时器，其中包括重传定时器，延迟ack定时器，保活定时器，pacing定时器，和压缩ack定时器，其中重传定时器，虽然只注册了一个定时器，但内部会根据pending位的不同处理不同的任务 其中包括rack，tlp探测，超时重传，零窗口探测。具体代码如下：\n//这里初始化了五个定时器，但是定时器的回调函数中，通过switch case 和 标志位会有不同的处理。void tcp_init_xmit_timers(struct sock *sk)&#123;\t//这里初始化了三个定时器，tcp_write_timer 中有多个处理函数，tcp_delack_timer 为延迟ack定时器的回调，tcp_keepalive_timer 为保活探测的定时器的回调\tinet_csk_init_xmit_timers(sk, &amp;tcp_write_timer, &amp;tcp_delack_timer,\t\t\t\t  &amp;tcp_keepalive_timer);\t//初始化了pacing相关的定时器，其实就是tsq的定时器\thrtimer_init(&amp;tcp_sk(sk)-&gt;pacing_timer, CLOCK_MONOTONIC,\t\t     HRTIMER_MODE_ABS_PINNED_SOFT);\ttcp_sk(sk)-&gt;pacing_timer.function = tcp_pace_kick;\t//压缩ack相关的定时器\thrtimer_init(&amp;tcp_sk(sk)-&gt;compressed_ack_timer, CLOCK_MONOTONIC,\t\t     HRTIMER_MODE_REL_PINNED_SOFT);\ttcp_sk(sk)-&gt;compressed_ack_timer.function = tcp_compressed_ack_kick;&#125;\n\ninet_csk_init_xmit_timers初始化了三个定时器，具体代码如下：\n/* * Using different timers for retransmit, delayed acks and probes * We may wish use just one timer maintaining a list of expire jiffies * to optimize. */void inet_csk_init_xmit_timers(struct sock *sk,\t\t\t       void (*retransmit_handler)(struct timer_list *t),\t\t\t       void (*delack_handler)(struct timer_list *t),\t\t\t       void (*keepalive_handler)(struct timer_list *t))&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\ttimer_setup(&amp;icsk-&gt;icsk_retransmit_timer, retransmit_handler, 0);//初始化重传定时器\t\ttimer_setup(&amp;icsk-&gt;icsk_delack_timer, delack_handler, 0); //延迟ack定时器\ttimer_setup(&amp;sk-&gt;sk_timer, keepalive_handler, 0);//保活定时器\ticsk-&gt;icsk_pending = icsk-&gt;icsk_ack.pending = 0; //初始化pending位&#125;\n\n以icsk_retransmit_timer为例，对应的定时器回调函数为tcp_write_timer\nstatic void tcp_write_timer(struct timer_list *t)&#123;\tstruct inet_connection_sock *icsk =\t\t\tfrom_timer(icsk, t, icsk_retransmit_timer);\tstruct sock *sk = &amp;icsk-&gt;icsk_inet.sk;\tbh_lock_sock(sk);\t//判断用户是否持有这个sock\tif (!sock_owned_by_user(sk)) &#123;\t\t//如果没有持有这个sock 就直接调用处理函数\t\ttcp_write_timer_handler(sk);\t&#125; else &#123;\t\t/* delegate our work to tcp_release_cb() */\t\t//用户持有sock 设置一个标志位 tcp_release_cb 中会处理这个定时器任务\t\tif (!test_and_set_bit(TCP_WRITE_TIMER_DEFERRED, &amp;sk-&gt;sk_tsq_flags))\t\t\tsock_hold(sk); \t&#125;\tbh_unlock_sock(sk);\tsock_put(sk);//减引用计数，加引用计数在reset timer中&#125;\n\n上述tcp_write_timer_handler为根据不同pending处理不同的逻辑，其中包括rack ， tlp ，重传， 和零窗口探测，也就是说一个定时器处理四种任务。\nvoid tcp_write_timer_handler(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tint event;\t//如果是close或者listern 或者没有设置pending直接返回\tif (((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_LISTEN)) ||\t    !icsk-&gt;icsk_pending)\t\treturn;\t//定时器没到期的情况，直接返回\tif (time_after(icsk-&gt;icsk_timeout, jiffies)) &#123;\t\tsk_reset_timer(sk, &amp;icsk-&gt;icsk_retransmit_timer, icsk-&gt;icsk_timeout);\t\treturn;\t&#125;\t//应该叫更新发包时间戳\ttcp_mstamp_refresh(tcp_sk(sk));\t//获取类型\tevent = icsk-&gt;icsk_pending;\tswitch (event) &#123;\tcase ICSK_TIME_REO_TIMEOUT:\t\ttcp_rack_reo_timeout(sk); //recent ack\t\tbreak;\tcase ICSK_TIME_LOSS_PROBE:\t\ttcp_send_loss_probe(sk); //tlp 探测\t\tbreak;\tcase ICSK_TIME_RETRANS:\t\ticsk-&gt;icsk_pending = 0;\t\ttcp_retransmit_timer(sk);//重传\t\tbreak;\tcase ICSK_TIME_PROBE0:\t\ticsk-&gt;icsk_pending = 0;\t\ttcp_probe_timer(sk); //零窗口探测\t\tbreak;\t&#125;&#125;\n\ntcp套接字的init函数中还设置了mss的回调函数，最总的mss大小取决于最大的历史窗口大小，和mtu探测得到大小，具体代码如下：\n //三次握手，icmp报文，或者pmtu探测 貌似都会调用它unsigned int tcp_sync_mss(struct sock *sk, u32 pmtu)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tint mss_now;\t//当前传入的pmtu小于历史 的最大值，缩小范围\tif (icsk-&gt;icsk_mtup.search_high &gt; pmtu)\t\ticsk-&gt;icsk_mtup.search_high = pmtu;\t\t//计算当前的mss\tmss_now = tcp_mtu_to_mss(sk, pmtu);\t//根据窗口大小调整再次调整mss,可能会变小\tmss_now = tcp_bound_to_half_wnd(tp, mss_now);\t/* And store cached results */\t//把传进来的pmtu保存起来\ticsk-&gt;icsk_pmtu_cookie = pmtu;\tif (icsk-&gt;icsk_mtup.enabled)\t\t//如果使能的mtu 探测， 那可能会再次变小这个mss ，这个mss的值不能超过search_low\t\tmss_now = min(mss_now, tcp_mtu_to_mss(sk, icsk-&gt;icsk_mtup.search_low));\t//\ttp-&gt;mss_cache = mss_now;\treturn mss_now;&#125;\n\n上述tcp_mtu_to_mss其实就是根据传入的mtu计算一个mss，这个mss不会超过三次握手中协商的mss值\n/* Calculate MSS not accounting any TCP options.  */static inline int __tcp_mtu_to_mss(struct sock *sk, int pmtu)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\tconst struct inet_connection_sock *icsk = inet_csk(sk);\tint mss_now;\t/* Calculate base mss without TCP options:\t   It is MMS_S - sizeof(tcphdr) of rfc1122\t */\t//跟据pmtu先算出一个基础mss 也就是 mtu减去包头的长度\tmss_now = pmtu - icsk-&gt;icsk_af_ops-&gt;net_header_len - sizeof(struct tcphdr);\t/* IPv6 adds a frag_hdr in case RTAX_FEATURE_ALLFRAG is set */\t//ipv6 的处理\tif (icsk-&gt;icsk_af_ops-&gt;net_frag_header_len) &#123;\t\tconst struct dst_entry *dst = __sk_dst_get(sk);\t\tif (dst &amp;&amp; dst_allfrag(dst))\t\t\tmss_now -= icsk-&gt;icsk_af_ops-&gt;net_frag_header_len;\t&#125;\t/* Clamp it (mss_clamp does not include tcp options) */\t//如果mss_now大于 双方协商的mss 设置为协商mss\tif (mss_now &gt; tp-&gt;rx_opt.mss_clamp)\t\tmss_now = tp-&gt;rx_opt.mss_clamp;\t/* Now subtract optional transport overhead */\t//减去选项的长度\tmss_now -= icsk-&gt;icsk_ext_hdr_len;\t/* Then reserve room for full set of TCP options and 8 bytes of data */\t//和系统的最小mss比较一下，确保最小的mss\tmss_now = max(mss_now,\t\t      READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_min_snd_mss));\treturn mss_now;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["sock","TCP"]},{"title":"TCP超时重传定时器","url":"/2025/08/03/TCP%E5%AE%9A%E6%97%B6%E5%99%A8-%E8%B6%85%E6%97%B6%E9%87%8D%E4%BC%A0%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"TCP的超时重传定时器TCP的超时重传定时器，是TCP最最基本，最重要的定时器。\nTCP超时重传定时器的初始化在用户调用socket() 后，对于AF_INET协议族，会调用inet_create 而inet_create 中会调用具体协议的init函数，对于TCP来说就是tcp_v4_init_sock 在tcp_v4_init_sock最终中会调用tcp_init_xmit_timers 会对多个定时器进行初始化，其中管理超时重传的的定时器为icsk-&gt;icsk_retransmit_timer如下所示：\nvoid inet_csk_init_xmit_timers(struct sock *sk,\t\t\t       void (*retransmit_handler)(struct timer_list *t),\t\t\t       void (*delack_handler)(struct timer_list *t),\t\t\t       void (*keepalive_handler)(struct timer_list *t))&#123;\t...\ttimer_setup(&amp;icsk-&gt;icsk_retransmit_timer, retransmit_handler, 0);//初始化重传定时器\t\t...&#125;\n\n注意:上述其实只是注册了一个定时器并没有真正的启动，定时器对应的超时处理函数为retransmit_handler , 该函数其实使用一个定时器实现了多个定时任务，超时重传定时器只是其中的一个定时任务。retransmit_handler中最终会调用tcp_write_timer_handler来处理不同的定时器任务，具体代码如下：\n/* Called with bottom-half processing disabled.   Called by tcp_write_timer() */void tcp_write_timer_handler(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tint event;\t//判断状态是否有效\tif (((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_LISTEN)) ||\t    !icsk-&gt;icsk_pending)\t\treturn;\tif (time_after(icsk-&gt;icsk_timeout, jiffies)) &#123;\t\tsk_reset_timer(sk, &amp;icsk-&gt;icsk_retransmit_timer, icsk-&gt;icsk_timeout);\t\treturn;\t&#125;\ttcp_mstamp_refresh(tcp_sk(sk));\tevent = icsk-&gt;icsk_pending;\tswitch (event) &#123;\tcase ICSK_TIME_REO_TIMEOUT:\t\ttcp_rack_reo_timeout(sk);\t\tbreak;\tcase ICSK_TIME_LOSS_PROBE:\t\ttcp_send_loss_probe(sk);\t\tbreak;\tcase ICSK_TIME_RETRANS:\t\t/*启动重传定时器的地方1.fastopen 2.正常发送syn包 3.建连后发送数据包 4.tcp_retransmit_timer中再次调度 5.sack撤销\t\t6.tcp_xmit_retransmit_queue中 什么时候会调用tcp_xmit_retransmit_queue？ 检测到丢包或者恢复传输的时候，基本都在tcp_ack中*/\t\t//重置重传定时器的地方 tcp_ack中\t\ticsk-&gt;icsk_pending = 0;\t\ttcp_retransmit_timer(sk);\t\tbreak;\tcase ICSK_TIME_PROBE0:\t\ticsk-&gt;icsk_pending = 0;\t\ttcp_probe_timer(sk);\t\tbreak;\t&#125;&#125;\n\n由上述代码可以看到，TCP的超时重传只是其中的一个‘case’，当设置ICSK_TIME_RETRANS pending位的时候才会真正的处理超时重传！\nTCP超时重传定时器的启动启动TCP定时器定时器的地方有如下几个位置：\n1.TFO 相关static void tcp_rcv_synrecv_state_fastopen(struct sock *sk)&#123;\t...\ttcp_rearm_rto(sk);//fastopen\t...&#125;\n\n\n\n2.TCP 第一次握手发送syn报int tcp_connect(struct sock *sk)&#123;\t...\t/* Timer for repeating the SYN until an answer. */\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\t\t\t\t  inet_csk(sk)-&gt;icsk_rto, TCP_RTO_MAX);\t...&#125;\n\n3. 建连后发送数据包建连后发送数据包若没有未确认的数据，就会启动超时重传定时器下面的tcp_event_new_data_sent 在tcp的发包函数tcp_write_xmit 中会被调用。\nstatic void tcp_event_new_data_sent(struct sock *sk, struct sk_buff *skb)&#123;\t...\tif (!prior_packets || icsk-&gt;icsk_pending == ICSK_TIME_LOSS_PROBE)\t\ttcp_rearm_rto(sk);//没有在途中的数据包\t...&#125;\n\n4.超时重传的回调函数中重新启动定时器void tcp_retransmit_timer(struct sock *sk)&#123;\t...\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\t\t\t\t  tcp_clamp_rto_to_user_timeout(sk), TCP_RTO_MAX);\t...&#125;\n\n5.sack撤销后启动重传定时器6.tcp_ack中重新启动重传定时器TCP  超时重传定时器的关闭当上述定时器被启动后，如果数据包正常到达（也就是数据都被ack掉了），则需要clear掉已经启动的定时器，具体位置在tcp_rearm_rto 中（有多个地方会调用tcp_rearm_rto ，比如tcp_ack ，发送数据包的时候）。\n */void tcp_rearm_rto(struct sock *sk)&#123;\t...\tif (!tp-&gt;packets_out) &#123;\t\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_RETRANS);\t...&#125;\n\nTCP超时重传定时器的回调函数tcp_retransmit_timer为定时器到期的处理函数，该函数主要做了如下工作：\n1.针对TFO或者零窗口情况做特殊处理\n2.从TCP管理的重传队列中取出待重传的数据包\n3.调用tcp_write_timeout判断是否到超过容许的超时时间(根据系统参数计算，超过retry1次 需要进行路由的黑洞检测。)，超过retry2 次则直接返回用户err（本质上是否需要返回err 是根据retry2计算得到的一个时间和当前数据包实际重传所耗费的时间进行比较！）\n4.进入拥塞控制的loss状态，因为已经超时了\n5.调用真正的重传函数tcp_retransmit_skb\n6.如果没有返回用户err, 计算定时器下次触发回调函数的时间(如果是thin flow 就是根据 rtt 计算超时时间，否则就是指数退避）来复位超时重传定时器\n具体代码如下所示：\nvoid tcp_retransmit_timer(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct net *net = sock_net(sk);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct request_sock *req;\tstruct sk_buff *skb;\t//处理TFO的逻辑\treq = rcu_dereference_protected(tp-&gt;fastopen_rsk,\t\t\t\t\tlockdep_sock_is_held(sk));\tif (req) &#123;\t\tWARN_ON_ONCE(sk-&gt;sk_state != TCP_SYN_RECV &amp;&amp;\t\t\t     sk-&gt;sk_state != TCP_FIN_WAIT1);\t\t//针对TFO的处理\t\ttcp_fastopen_synack_timer(sk, req);\t\t/* Before we receive ACK to our SYN-ACK don&#x27;t retransmit\t\t * anything else (e.g., data or FIN segments).\t\t */\t\treturn;\t&#125;\t//如果没有未确认的数据包，就直接返回\tif (!tp-&gt;packets_out)\t\treturn;\t//从重传队列中取出一个数据包\tskb = tcp_rtx_queue_head(sk);\tif (WARN_ON_ONCE(!skb))\t\treturn;\t//tlp探测管理的序号直接置位0 ， 因为都已经超时重传了，等于回退到最原始的重传方式了\ttp-&gt;tlp_high_seq = 0;\t//如果接收窗口为0， socket没有关闭，且不是三次握手阶段进入这个分支\t//注意！通常不会走到这个分支， 当接收端变成零窗口的时候，在超时时间段内通常就恢复了 大概率不会等到超时时间的时候还是零窗口\tif (!tp-&gt;snd_wnd &amp;&amp; !sock_flag(sk, SOCK_DEAD) &amp;&amp;\t    !((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_SYN_SENT | TCPF_SYN_RECV))) &#123;\t\t/* Receiver dastardly shrinks window. Our retransmits\t\t * become zero probes, but we should not timeout this\t\t * connection. If the socket is an orphan, time it out,\t\t * we cannot allow such beasts to hang infinitely.\t\t */\t\tstruct inet_sock *inet = inet_sk(sk);\t\tu32 rtx_delta;\t\t//这里是当前tcp的时间戳 减去数据包或者重传的时间戳，用于下面的打印 ，注意这里用到了ratelimit打印\t\trtx_delta = tcp_time_stamp(tp) - (tp-&gt;retrans_stamp ?: tcp_skb_timestamp(skb));\t\tif (sk-&gt;sk_family == AF_INET) &#123;\t\t\tnet_dbg_ratelimited(&quot;Probing zero-window on %pI4:%u/%u, seq=%u:%u, recv %ums ago, lasting %ums\\n&quot;,\t\t\t\t&amp;inet-&gt;inet_daddr, ntohs(inet-&gt;inet_dport),\t\t\t\tinet-&gt;inet_num, tp-&gt;snd_una, tp-&gt;snd_nxt,\t\t\t\tjiffies_to_msecs(jiffies - tp-&gt;rcv_tstamp),\t\t\t\trtx_delta);\t\t&#125;#if IS_ENABLED(CONFIG_IPV6)\t\telse if (sk-&gt;sk_family == AF_INET6) &#123;\t\t\tnet_dbg_ratelimited(&quot;Probing zero-window on %pI6:%u/%u, seq=%u:%u, recv %ums ago, lasting %ums\\n&quot;,\t\t\t\t&amp;sk-&gt;sk_v6_daddr, ntohs(inet-&gt;inet_dport),\t\t\t\tinet-&gt;inet_num, tp-&gt;snd_una, tp-&gt;snd_nxt,\t\t\t\tjiffies_to_msecs(jiffies - tp-&gt;rcv_tstamp),\t\t\t\trtx_delta);\t\t&#125;#endif\t\t//上面判断了是否是0窗口， 这里需要判断0探测是否超时\t\tif (tcp_rtx_probe0_timed_out(sk, skb)) &#123;\t\t\ttcp_write_err(sk);\t\t\tgoto out;\t\t&#125;\t\t//进入拥塞算法的loss状态\t\ttcp_enter_loss(sk);\t\t//重传数据包\t\ttcp_retransmit_skb(sk, skb, 1);\t\t//这个把路由信息给复位了！\t\t__sk_dst_reset(sk);\t\tgoto out_reset_timer;\t&#125;\t//增加timeout的统计计数\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPTIMEOUTS);\t//这里很关键！！！计算是否达已经超时了 ，如果超过了，里面会调用tcp_err\tif (tcp_write_timeout(sk))\t\tgoto out;\t//如果是一次丢包进入这个分支，增加统计计数，但是第一次感觉不会走任何一个分支，enterloss在下面啊，拥塞状态到这里已经被修改了吗？？可能是受到其他机制的影响\t//比如在tcpack中会改状态？因为icsk_retransmits的值只在超时重传的逻辑中被增加\tif (icsk-&gt;icsk_retransmits == 0) &#123;\t\tint mib_idx = 0;\t\tif (icsk-&gt;icsk_ca_state == TCP_CA_Recovery) &#123;\t\t\tif (tcp_is_sack(tp))\t\t\t\tmib_idx = LINUX_MIB_TCPSACKRECOVERYFAIL;\t\t\telse\t\t\t\tmib_idx = LINUX_MIB_TCPRENORECOVERYFAIL;\t\t&#125; else if (icsk-&gt;icsk_ca_state == TCP_CA_Loss) &#123;\t\t\tmib_idx = LINUX_MIB_TCPLOSSFAILURES;\t\t&#125; else if ((icsk-&gt;icsk_ca_state == TCP_CA_Disorder) ||\t\t\t   tp-&gt;sacked_out) &#123;\t\t\tif (tcp_is_sack(tp))\t\t\t\tmib_idx = LINUX_MIB_TCPSACKFAILURES;\t\t\telse\t\t\t\tmib_idx = LINUX_MIB_TCPRENOFAILURES;\t\t&#125;\t\tif (mib_idx)\t\t\t__NET_INC_STATS(sock_net(sk), mib_idx);\t&#125;\t//进入拥塞的loss状态\ttcp_enter_loss(sk);\ticsk-&gt;icsk_retransmits++;\t//真正的重传\tif (tcp_retransmit_skb(sk, tcp_rtx_queue_head(sk), 1) &gt; 0) &#123;\t\t/* Retransmission failed because of local congestion,\t\t * Let senders fight for local resources conservatively.\t\t */\t\t//走到这里是上面重传失败了，原因可能是本地资源不足，重新设置一下定时器\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\t\t\t\t\t  TCP_RESOURCE_PROBE_INTERVAL,\t\t\t\t\t  TCP_RTO_MAX);\t\t//直接退出\t\tgoto out;\t&#125;\t/* Increase the timeout each time we retransmit.  Note that\t * we do not increase the rtt estimate.  rto is initialized\t * from rtt, but increases here.  Jacobson (SIGCOMM 88) suggests\t * that doubling rto each time is the least we can get away with.\t * In KA9Q, Karn uses this for the first few times, and then\t * goes to quadratic.  netBSD doubles, but only goes up to *64,\t * and clamps at 1 to 64 sec afterwards.  Note that 120 sec is\t * defined in the protocol as the maximum possible RTT.  I guess\t * we&#x27;ll have to use something other than TCP to talk to the\t * University of Mars.\t *\t * PAWS allows us longer timeouts and large windows, so once\t * implemented ftp to mars will work nicely. We will have to fix\t * the 120 second clamps though!\t */\ticsk-&gt;icsk_backoff++;out_reset_timer:\t/* If stream is thin, use linear timeouts. Since &#x27;icsk_backoff&#x27; is\t * used to reset timer, set to 0. Recalculate &#x27;icsk_rto&#x27; as this\t * might be increased if the stream oscillates between thin and thick,\t * thus the old value might already be too high compared to the value\t * set by &#x27;tcp_set_rto&#x27; in tcp_input.c which resets the rto without\t * backoff. Limit to TCP_THIN_LINEAR_RETRIES before initiating\t * exponential backoff behaviour to avoid continue hammering\t * linear-timeout retransmissions into a black hole\t */\t //建连状态下，当前sock是瘦流（发出去的未确认的包很少就叫瘦流）\tif (sk-&gt;sk_state == TCP_ESTABLISHED &amp;&amp;\t    (tp-&gt;thin_lto || READ_ONCE(net-&gt;ipv4.sysctl_tcp_thin_linear_timeouts)) &amp;&amp;\t    tcp_stream_is_thin(tp) &amp;&amp;\t    icsk-&gt;icsk_retransmits &lt;= TCP_THIN_LINEAR_RETRIES) &#123;\t\ticsk-&gt;icsk_backoff = 0; //这里不使用指数退避\t\t//这个clamp可以理解为在rtt和minrto之间选择一个值\t\ticsk-&gt;icsk_rto = clamp(__tcp_set_rto(tp),\t\t//rtt\t\t\t\t       tcp_rto_min(sk),\t\t\t\t\t//tcp初始化的时候设置的200ms\t\t\t\t       TCP_RTO_MAX);\t\t\t\t\t//120s\t&#125; else if (sk-&gt;sk_state != TCP_SYN_SENT ||\t\t   icsk-&gt;icsk_backoff &gt;\t\t   READ_ONCE(net-&gt;ipv4.sysctl_tcp_syn_linear_timeouts)) &#123;\t\t/* Use normal (exponential) backoff unless linear timeouts are\t\t * activated.\t\t */\t\t//建连状态下不是瘦流的话就是指数退避\t\ticsk-&gt;icsk_rto = min(icsk-&gt;icsk_rto &lt;&lt; 1, TCP_RTO_MAX);\t&#125;\t//三个参数，超时重传的的定时器标志 ，定时器的时间， 定时器最大时间 \tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\t\t\t\t  tcp_clamp_rto_to_user_timeout(sk), TCP_RTO_MAX);\t//如果超过retry1 就复位路由 这里为啥+1呢\tif (retransmits_timed_out(sk, READ_ONCE(net-&gt;ipv4.sysctl_tcp_retries1) + 1, 0))\t\t__sk_dst_reset(sk);out:;&#125;\n\n上述代码中会用tcp_write_timeout 来判断是否发生真正的超时，这里的超时指的是需要直接返回给用户err了!\ntcp_write_timeout中其实就做了两件事，1.是否需要进行黑洞检测（根据retry1），2.调用retransmits_timed_out是否发生真正的超时（根据retry2计算）。计算是否真正超时就是根据系统参数计算一个容许最大的超时时间，如果当前重传的数据包（多次线性退避加指数退避之后）所消耗的时间大于根据系统参数计算出来的超时时间，就认为是超时了。\n\n如果当前的重传所用的时间超过了根据retry1这个系统参数计算的超时时间，则需要调用tcp_mtu_probing进行黑洞检测，也就是缩小数据包的mss，具体代码如下：\n\nstatic void tcp_mtu_probing(struct inet_connection_sock *icsk, struct sock *sk)&#123;\tconst struct net *net = sock_net(sk);\tint mss;\t/* Black hole detection */\t//如果没有使能mtu探测直接返回\tif (!READ_ONCE(net-&gt;ipv4.sysctl_tcp_mtu_probing))\t\treturn;\t//使能mtu探测\tif (!icsk-&gt;icsk_mtup.enabled) &#123;\t\ticsk-&gt;icsk_mtup.enabled = 1;\t\ticsk-&gt;icsk_mtup.probe_timestamp = tcp_jiffies32;\t&#125; else &#123;\t\t\t\t//这里直接把icsk-&gt;icsk_mtup.search_low 设置成一半了\t\tmss = tcp_mtu_to_mss(sk, icsk-&gt;icsk_mtup.search_low) &gt;&gt; 1;//tcp_mtup_init中设置了icsk-&gt;icsk_mtup.search_low为basemss（初始时1024） tcp_mtup_init 在connet中被调用的\t\t//找一个更小的\t\tmss = min(READ_ONCE(net-&gt;ipv4.sysctl_tcp_base_mss), mss);\t\t//但是别小于最小的，默认48\t\tmss = max(mss, READ_ONCE(net-&gt;ipv4.sysctl_tcp_mtu_probe_floor));\t\t//和发送端最小mss在选一个最大的\t\tmss = max(mss, READ_ONCE(net-&gt;ipv4.sysctl_tcp_min_snd_mss));\t\t//根据mss 和包头长度计算mtu\t\ticsk-&gt;icsk_mtup.search_low = tcp_mss_to_mtu(sk, mss);\t&#125;\t//调用这个的目的应该就是想设置sock的mss 保证最新的mss   上面设置low会影响这个函数计算mss\ttcp_sync_mss(sk, icsk-&gt;icsk_pmtu_cookie);&#125;\n\n\n调用retransmits_timed_out 计算当前耗费的时间是否超过了容许的最大传输时间（这个最大时间是根据retry2计算得到的）\n\n上述retransmits_timed_out 实现如下：\nstatic bool retransmits_timed_out(struct sock *sk,\t\t\t\t  unsigned int boundary,\t\t\t\t  unsigned int timeout)&#123;\tunsigned int start_ts;\t//第一次进来(也就是第一次重传)直接return\tif (!inet_csk(sk)-&gt;icsk_retransmits)\t\treturn false;\t//保存第一次重传的时间\tstart_ts = tcp_sk(sk)-&gt;retrans_stamp;\t//几乎走这里，用户如果setsockopt设置了就不走\tif (likely(timeout == 0)) &#123; \t\t\t\t\tunsigned int rto_base = TCP_RTO_MIN; //默认应该是200ms\t\tif ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_SYN_SENT | TCPF_SYN_RECV))\t\t //如果是建立连接阶段就是1s！！\t\t\trto_base = tcp_timeout_init(sk);\t\t //这里就是根据boundary和rto_base 计算线性+ 指数退避boundary次的timeout时间\t\ttimeout = tcp_model_timeout(sk, boundary, rto_base); \t&#125;\t//这里是根据当前时间 减去首次重传的时间 减去超时时间 如果 &gt;= 0 表示就是超时了！ 也就是超过计算的时间了。\treturn (s32)(tcp_time_stamp(tcp_sk(sk)) - start_ts - timeout) &gt;= 0;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP定时器"]},{"title":"TCP重传__tcp_retransmit_skb","url":"/2025/08/05/tcp%E6%95%B0%E6%8D%AE%E5%8C%85%E9%87%8D%E4%BC%A0/","content":"__tcp_retransmit_skb当TCP的超时重传定时器到期后，会调用tcp_retransmit_skb进行重传。\nvoid tcp_retransmit_timer(struct sock *sk)&#123;\t...\tif (tcp_retransmit_skb(sk, tcp_rtx_queue_head(sk), 1) &gt; 0) &#123;\t...&#125;\n\ntcp_retransmit_skb中会调用__tcp_retransmit_skb完成真正的重传，具体代码如下所示：\nint tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tint err = __tcp_retransmit_skb(sk, skb, segs);\tif (err == 0) &#123;#if FASTRETRANS_DEBUG &gt; 0\t\tif (TCP_SKB_CB(skb)-&gt;sacked &amp; TCPCB_SACKED_RETRANS) &#123;\t\t\tnet_dbg_ratelimited(&quot;retrans_out leaked\\n&quot;);\t\t&#125;#endif\t\tTCP_SKB_CB(skb)-&gt;sacked |= TCPCB_RETRANS;\t\t//更新重传出去数量，tcp_ack中会减\t\ttp-&gt;retrans_out += tcp_skb_pcount(skb);\t&#125;\t/* Save stamp of the first (attempted) retransmit. */\t//记录首次的重传时间\tif (!tp-&gt;retrans_stamp)\t\ttp-&gt;retrans_stamp = tcp_skb_timestamp(skb);\t//初始化的是是-1 用于拥塞欻港口\tif (tp-&gt;undo_retrans &lt; 0)\t\ttp-&gt;undo_retrans = 0;\t//记录重传的段数\ttp-&gt;undo_retrans += tcp_skb_pcount(skb);\treturn err;&#125;\n\n由上述可知tcp_retransmit_skb仅仅是调用了__tcp_retransmit_skb然后后更新了tcp_sock部分字段。真正的关键的操作在__tcp_retransmit_skb中\n__tcp_retransmit_skb的关键逻辑为，首先通过fclone标志来判断当前重传的数据包是否已经在本机的队列中，因为重传的时候会设置clone标志位，而重传的数据包可以直接使用fclone出来的结构体，所以说可以通过这个标志位来判断数据包是否已经重传。\n然后根据数据包的序列号对这个数据包进行处理，如果当前数据包的序列号已经被部分确认了，那就需要裁剪这个skb了。（裁剪这个skb的时候会调用tcp管理内存的相关接口）\n然后查看路由是否有效（即dst是否存在）在超过retry1，后可能会进行黑洞检测，这时候的dst可能就被rst了\n重新计算当前的mss，然后计算当前可用发送窗口的大小和当前可以发送的len（使用segs和mss的乘机计算）。这里分为两种情况，如果skb的len大于计算的得到len，在这里就需要执行分段了，否则的话（也就是小于的情况）会尝试合并数据包。\n最后设置tp_sock的一些统计字段，调用tcp_transmit_skb完成数据包的发送，注意这里设置了clone标志位。\n具体代码如下所示：\nint __tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tunsigned int cur_mss;\tint diff, len, err;\tint avail_wnd;\t/* Inconclusive MTU probe */\tif (icsk-&gt;icsk_mtup.probe_size)\t\ticsk-&gt;icsk_mtup.probe_size = 0;\t//通过fclone标志判断这个数据包是否在本机的队列中，\t//因为重传的数据包是从红黑树管理的数据包fclone出来的(调用tcp发包函数的时会设置clone标志位)，所以说如果有这个标志位\t//表示fastclone的数据包一定在本机队列中，所以就可以return -1 \tif (skb_still_in_host_queue(sk, skb))\t\treturn -EBUSY;\t//如果重传数据包的seq 的小于已经确认的序列号 则表示部分确认了，那肯定需要裁剪这个skb了\tif (before(TCP_SKB_CB(skb)-&gt;seq, tp-&gt;snd_una)) &#123;\t\t//如果这个数据包都被确认过了 那就不应该出现在重传队列中，需要直接返回\t\tif (unlikely(before(TCP_SKB_CB(skb)-&gt;end_seq, tp-&gt;snd_una))) &#123;\t\t\tWARN_ON_ONCE(1);\t\t\treturn -EINVAL;\t\t&#125;\t\t//裁剪掉已经确认的部分，因此里面也会减少skb的truesize 以及调整内存相关的使用参数\t\tif (tcp_trim_head(sk, skb, tp-&gt;snd_una - TCP_SKB_CB(skb)-&gt;seq))\t\t\treturn -ENOMEM;\t&#125;\t//看看路由是否有效，如果无效了，会调用查路由的接口，然后将结果关联到sk上\tif (inet_csk(sk)-&gt;icsk_af_ops-&gt;rebuild_header(sk))\t\treturn -EHOSTUNREACH; /* Routing failure or similar. */\t//根据tcp的选项长度，以及mtu是否变化来计算新的mss\tcur_mss = tcp_current_mss(sk);\t//这里计算了窗口右边界和数据包seq的差值，其实就是在计算可用的窗口大小有多少\tavail_wnd = tcp_wnd_end(tp) - TCP_SKB_CB(skb)-&gt;seq;\t/* If receiver has shrunk his window, and skb is out of\t * new window, do not retransmit it. The exception is the\t * case, when window is shrunk to zero. In this case\t * our retransmit of one segment serves as a zero window probe.\t */\t//如果没有可用的窗口大小\tif (avail_wnd &lt;= 0) &#123;\t\t//不是最早未确认的包，直接返回，什么场景呢？？？？\t\tif (TCP_SKB_CB(skb)-&gt;seq != tp-&gt;snd_una)\t\t\treturn -EAGAIN;\t\t//这里应该叫做强制设置一个大小？？ 因为走到这里表示没有可用的空间了\t\t//然而这是地一个没被确认的数据包，必须要强制发送？？？\t\tavail_wnd = cur_mss;\t&#125;\t//计算一个mss和一个段的乘积，也就是重传数据包的大小，貌似重传的数据包只能是一个段\tlen = cur_mss * segs;\t//当前空间够用的情况\tif (len &gt; avail_wnd) &#123;\t\t//向下取整\t\tlen = rounddown(avail_wnd, cur_mss);\t\tif (!len)\t\t\tlen = avail_wnd;\t&#125;\t//问题：下面大概率走哪个分支呢？？？？？\t//如果这个数据包len 大于上面计算len 那就需要分段了 大概率走这个分支吧\tif (skb-&gt;len &gt; len) &#123;\t\tif (tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb, len,\t\t\t\t cur_mss, GFP_ATOMIC))\t\t\treturn -ENOMEM; /* We&#x27;ll try again later. */\t&#125; else &#123;\t\t//当前空间够的情况！\t\t//检查是否被clone？？？？\t\tif (skb_unclone_keeptruesize(skb, GFP_ATOMIC))\t\t\treturn -ENOMEM;\t\t//获取分段的数量，这里应该大概率就是1吧\t\tdiff = tcp_skb_pcount(skb);\t\t//重新计算分段数量，大概率也是1吧\t\ttcp_set_skb_tso_segs(skb, cur_mss);\t\tdiff -= tcp_skb_pcount(skb); //这里的diff是变化值\t\tif (diff)//大概率不会进入这个分支\t\t\ttcp_adjust_pcount(sk, skb, diff);\t\t//取mss和窗口的最小值\t\tavail_wnd = min_t(int, avail_wnd, cur_mss);\t\t//如果数据包的大小，小于上面的最小值，则尝试合并小包\t\tif (skb-&gt;len &lt; avail_wnd)\t\t\ttcp_retrans_try_collapse(sk, skb, avail_wnd);\t&#125;\t/* RFC3168, section 6.1.1.1. ECN fallback */\t//如果是syn包 不容许有ecn标志？？？\tif ((TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_SYN_ECN) == TCPHDR_SYN_ECN)\t\ttcp_ecn_clear_syn(sk, skb);\t/* Update global and local TCP statistics. */\t//更新tcp的统计字段\tsegs = tcp_skb_pcount(skb);\tTCP_ADD_STATS(sock_net(sk), TCP_MIB_RETRANSSEGS, segs);\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_SYN)\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNRETRANS);\ttp-&gt;total_retrans += segs;\ttp-&gt;bytes_retrans += skb-&gt;len;\t/* make sure skb-&gt;data is aligned on arches that require it\t * and check if ack-trimming &amp; collapsing extended the headroom\t * beyond what csum_start can cover.\t */\t//headroom 大于64k?? 不可能把\tif (unlikely((NET_IP_ALIGN &amp;&amp; ((unsigned long)skb-&gt;data &amp; 3)) ||\t\t     skb_headroom(skb) &gt;= 0xFFFF)) &#123;\t\tstruct sk_buff *nskb;\t\ttcp_skb_tsorted_save(skb) &#123;\t\t\tnskb = __pskb_copy(skb, MAX_TCP_HEADER, GFP_ATOMIC);\t\t\tif (nskb) &#123;\t\t\t\tnskb-&gt;dev = NULL;\t\t\t\terr = tcp_transmit_skb(sk, nskb, 0, GFP_ATOMIC);\t\t\t&#125; else &#123;\t\t\t\terr = -ENOBUFS;\t\t\t&#125;\t\t&#125; tcp_skb_tsorted_restore(skb);\t\tif (!err) &#123;\t\t\ttcp_update_skb_after_send(sk, skb, tp-&gt;tcp_wstamp_ns);\t\t\ttcp_rate_skb_sent(sk, skb);\t\t&#125;\t&#125; else &#123;\t\t//设置了clone标志位\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\t&#125;\t/* To avoid taking spuriously low RTT samples based on a timestamp\t * for a transmit that never happened, always mark EVER_RETRANS\t */\t//这里设置skb 了曾经重传过的标志\tTCP_SKB_CB(skb)-&gt;sacked |= TCPCB_EVER_RETRANS;\t//bpf钩子\tif (BPF_SOCK_OPS_TEST_FLAG(tp, BPF_SOCK_OPS_RETRANS_CB_FLAG))\t\ttcp_call_bpf_3arg(sk, BPF_SOCK_OPS_RETRANS_CB,\t\t\t\t  TCP_SKB_CB(skb)-&gt;seq, segs, err);\tif (likely(!err)) &#123;\t\t//tracepoint\t\ttrace_tcp_retransmit_skb(sk, skb);\t&#125; else if (err != -EBUSY) &#123;\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPRETRANSFAIL, segs);\t&#125;\treturn err;&#125;\n\n上述代码中如果需要处理序列号已经被部分确认的情况，则会调用tcp_trim_head完成数据包的裁剪，具体代码如下\n/* Remove acked data from a packet in the transmit queue. */int tcp_trim_head(struct sock *sk, struct sk_buff *skb, u32 len)&#123;\tu32 delta_truesize;\tif (skb_unclone_keeptruesize(skb, GFP_ATOMIC))\t\treturn -ENOMEM;\t// 和__pskb_pull_tail()几乎一样\tdelta_truesize = __pskb_trim_head(skb, len);\tTCP_SKB_CB(skb)-&gt;seq += len;\t//更新skb的turesize\tskb-&gt;truesize\t   -= delta_truesize;\t//更新sk-&gt;sk_wmem_queued（tcp层管理使用内存量的大小）\tsk_wmem_queued_add(sk, -delta_truesize);\tif (!skb_zcopy_pure(skb))\t\t//内存回收\t\tsk_mem_uncharge(sk, delta_truesize);\t/* Any change of skb-&gt;len requires recalculation of tso factor. */\tif (tcp_skb_pcount(skb) &gt; 1)\t\t//根据mss更新数据包的gso_size\t\ttcp_set_skb_tso_segs(skb, tcp_skb_mss(skb));\treturn 0;&#125;\n\n上述代码__pskb_trim_head 完成数据包裁剪后，调用sk_wmem_queued_add修改了sk-&gt;sk_wmem_queued（相当于tcp层管理内存的变量）\n然后会调用sk_mem_uncharge进行内存回收（也就是会影响tcp内存的三个阈值）。sk_mem_uncharge开始后的具体调用栈如下所示：\nstatic inline void sk_mem_uncharge(struct sock *sk, int size)&#123;\tif (!sk_has_account(sk))\t\treturn;\t//先更新（增加）sk-&gt;sk_forward_alloc的值这个值的单位是字节\tsk_forward_alloc_add(sk, size);\t//真正的页面回收，这里如果达到了了回收的标准，还会在减少sk_forward_alloc\tsk_mem_reclaim(sk);&#125;static inline void sk_forward_alloc_add(struct sock *sk, int val)&#123;\t/* Paired with lockless reads of sk-&gt;sk_forward_alloc */\tWRITE_ONCE(sk-&gt;sk_forward_alloc, sk-&gt;sk_forward_alloc + val);&#125;static inline void sk_mem_reclaim(struct sock *sk)&#123;\tint reclaimable;\tif (!sk_has_account(sk))\t\treturn;\t//如果用户没有配置保留的内存sk_unused_reserved_mem 就是0 ，reclaimable 则等于sk_forward_alloc\treclaimable = sk-&gt;sk_forward_alloc - sk_unused_reserved_mem(sk);\t//字节多少大于一个页大小的时候触发回收\tif (reclaimable &gt;= (int)PAGE_SIZE)\t//调用回收的逻辑\t\t__sk_mem_reclaim(sk, reclaimable);&#125;void __sk_mem_reclaim(struct sock *sk, int amount)&#123;\tamount &gt;&gt;= PAGE_SHIFT;\t//这里的amount的单位为页数！  这里面又减去了sk_forward_alloc 的值\tsk_forward_alloc_add(sk, -(amount &lt;&lt; PAGE_SHIFT));\t__sk_mem_reduce_allocated(sk, amount);&#125;static inline void sk_forward_alloc_add(struct sock *sk, int val)&#123;\t/* Paired with lockless reads of sk-&gt;sk_forward_alloc */\tWRITE_ONCE(sk-&gt;sk_forward_alloc, sk-&gt;sk_forward_alloc + val);&#125;void __sk_mem_reduce_allocated(struct sock *sk, int amount)&#123;\t//修改tcp_memory_allocated的值\tsk_memory_allocated_sub(sk, amount);\t//cgroup相关\tif (mem_cgroup_sockets_enabled &amp;&amp; sk-&gt;sk_memcg)\t\tmem_cgroup_uncharge_skmem(sk-&gt;sk_memcg, amount);\t//更新内存压力\tif (sk_under_global_memory_pressure(sk) &amp;&amp;\t    (sk_memory_allocated(sk) &lt; sk_prot_mem_limits(sk, 0)))\t\tsk_leave_memory_pressure(sk);&#125;\n\n上述就是裁剪数据包的全部逻辑，之后的逻辑为调用inet_csk(sk)-&gt;icsk_af_ops-&gt;rebuild_header(sk)判断路由是否有效，具体的函数如下所示(是在初始化tcp套接字的时候注册的)。\n//超时重传的时候会调用到int inet_sk_rebuild_header(struct sock *sk)&#123;\tstruct inet_sock *inet = inet_sk(sk);\tstruct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);\t__be32 daddr;\tstruct ip_options_rcu *inet_opt;\tstruct flowi4 *fl4;\tint err;\t/* Route is OK, nothing to do. */\t//如果路由条目存在，那直接返回就ok，\t//什么时候会继续走？ 重传超过retry1，黑洞检测的时候不是就不能直接返回了？\tif (rt)\t\treturn 0;\t/* Reroute. */\trcu_read_lock();\t//拿到ip选项\tinet_opt = rcu_dereference(inet-&gt;inet_opt);\t//拿到目的地址\tdaddr = inet-&gt;inet_daddr;\t//如果有ip选项，且有ip源路由选项，那就用选项的daddr，通常不会走这\tif (inet_opt &amp;&amp; inet_opt-&gt;opt.srr)\t\tdaddr = inet_opt-&gt;opt.faddr;\trcu_read_unlock();\t//拿到一个流的信息，\tfl4 = &amp;inet-&gt;cork.fl.u.ip4;\t//调用查路由的接口\trt = ip_route_output_ports(sock_net(sk), fl4, sk, daddr, inet-&gt;inet_saddr,\t\t\t\t   inet-&gt;inet_dport, inet-&gt;inet_sport,\t\t\t\t   sk-&gt;sk_protocol, RT_CONN_FLAGS(sk),\t\t\t\t   sk-&gt;sk_bound_dev_if);\tif (!IS_ERR(rt)) &#123;\t\terr = 0;\t\t//这里关联了sk和dst\t\tsk_setup_caps(sk, &amp;rt-&gt;dst);\t&#125; else &#123;\t\terr = PTR_ERR(rt);\t\t/* Routing failed... */\t\tsk-&gt;sk_route_caps = 0;\t\t/*\t\t * Other protocols have to map its equivalent state to TCP_SYN_SENT.\t\t * DCCP maps its DCCP_REQUESTING state to TCP_SYN_SENT. -acme\t\t */\t\t//走到这里肯定是上面查找路由失败了，如果准备换一个源ip地址继续查路由，前提是开启这个选项，且不是建立连接阶段\t\t//就换个ip地址继续查，如果失败了，就返回err\t\tif (!READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_ip_dynaddr) ||\t\t    sk-&gt;sk_state != TCP_SYN_SENT ||\t\t    (sk-&gt;sk_userlocks &amp; SOCK_BINDADDR_LOCK) ||\t\t    (err = inet_sk_reselect_saddr(sk)) != 0)\t\t\tWRITE_ONCE(sk-&gt;sk_err_soft, -err);\t&#125;\treturn err;&#125;\n\n上述代码其实就是做了一件事情，就是判断sk的dst是否存在，如果不存在，则调用路由查找接口，然后为sk关联新的dst\n之后__tcp_retransmit_skb会调用tcp_current_mss来计算当前的mss具体代码如下：\nunsigned int tcp_current_mss(struct sock *sk)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\tconst struct dst_entry *dst = __sk_dst_get(sk);\tu32 mss_now;\tunsigned int header_len;\tstruct tcp_out_options opts;\tstruct tcp_md5sig_key *md5;\t//这个可以理解为最新的mss，本质上是上次的在某些地方更新的mss\tmss_now = tp-&gt;mss_cache;\tif (dst) &#123;\t\t//这里返回了路由表或者设备的mtu\t\tu32 mtu = dst_mtu(dst);\t\tif (mtu != inet_csk(sk)-&gt;icsk_pmtu_cookie)//这个值是调用tcp_sync_mss 用来计算mss的参数 如果没变化，不会进入这个分支\t\t\t//重新根据这个值在计算一次mtu ， 这个传入的mtu会被保存到icsk_pmtu_cookie 中 其实也就是因为mtu变化了，所以要重新计算mss 合理\t\t\tmss_now = tcp_sync_mss(sk, mtu); \t&#125;\t//计算tcp选项的长度\theader_len = tcp_established_options(sk, NULL, &amp;opts, &amp;md5) +\t\t     sizeof(struct tcphdr);\t/* The mss_cache is sized based on tp-&gt;tcp_header_len, which assumes\t * some common options. If this is an odd packet (because we have SACK\t * blocks etc) then our calculated header_len will be different, and\t * we have to adjust mss_now correspondingly */\t//如果长度变化了，重新调整mss\tif (header_len != tp-&gt;tcp_header_len) &#123;\t\tint delta = (int) header_len - tp-&gt;tcp_header_len;\t\tmss_now -= delta;\t&#125;\treturn mss_now;&#125;\n\ntcp_current_mss中首先拿到上一次的mss，然后在获取当前的mtu，因为mtu可能会变化，所以根据新的mtu算出一个新的mss\n继续往下，如果当前数据包的len小于一个mss的大小，则会尝试调用tcp_retrans_try_collapse合并重传队列中的数据包，否则的话会调用分段的处理函数。，合并重传队列中数据包的代码如下：\nstatic void tcp_retrans_try_collapse(struct sock *sk, struct sk_buff *to,\t\t\t\t     int space)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb = to, *tmp;\tbool first = true;\t//是否开启合并，默认是开启的\tif (!READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_retrans_collapse))\t\treturn;\t//syn包不能合并，但是会有重传队列中除了syn包还有其他包的情况吗？？？\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_SYN)\t\treturn;\t//遍历重传队列\tskb_rbtree_walk_from_safe(skb, tmp) &#123;\t\t//判断下一个包能否合并\t\tif (!tcp_can_collapse(sk, skb))\t\t\tbreak;\t\t//再次进行判断，如果是zc就无法合并\t\tif (!tcp_skb_can_collapse(to, skb))\t\t\tbreak;\t\t//可用的空间大小，减去被合并的大小\t\tspace -= skb-&gt;len;\t\t//第一个包，也就是to吧 所以直接continue了\t\tif (first) &#123;\t\t\tfirst = false;\t\t\tcontinue;\t\t&#125;\t\tif (space &lt; 0)\t\t\tbreak;\t\t//数据包的seq超出了窗口i的右边界，直接返回\t\tif (after(TCP_SKB_CB(skb)-&gt;end_seq, tcp_wnd_end(tp)))\t\t\tbreak;\t\tif (!tcp_collapse_retrans(sk, to))\t\t\tbreak;\t&#125;&#125;\n\ntcp_retrans_try_collapse遍历重传队列，调用tcp_can_collapse判断重传队列中的数据包能否合并（fclone过，或者segs不为1则无法合并），然后调用tcp_collapse_retrans完成真正的合并。具体代码如下：\n/* Check if coalescing SKBs is legal. */static bool tcp_can_collapse(const struct sock *sk, const struct sk_buff *skb)&#123;\t//如果数据包有多个段，不能合并，为什么？\tif (tcp_skb_pcount(skb) &gt; 1)\t\treturn false;\t//如果被clone过这不能合并\tif (skb_cloned(skb))\t\treturn false;\t/* Some heuristics for collapsing over SACK&#x27;d could be invented */\t//是sack过的数据包 不能合并\tif (TCP_SKB_CB(skb)-&gt;sacked &amp; TCPCB_SACKED_ACKED)\t\treturn false;\treturn true;&#125;\n\n/* Collapses two adjacent SKB&#x27;s during retransmission. */static bool tcp_collapse_retrans(struct sock *sk, struct sk_buff *skb)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *next_skb = skb_rb_next(skb);\tint next_skb_size;\t//重传队列合并下一个数据包的size\tnext_skb_size = next_skb-&gt;len;\t//必须只有一个段\tBUG_ON(tcp_skb_pcount(skb) != 1 || tcp_skb_pcount(next_skb) != 1);\t//这里调用tcp_skb_shift 完成了连个数据包的合并，但是只有数据仅有非线性部分的数据才能合并成功， 会有仅有非线性部分的数据吗？\tif (next_skb_size &amp;&amp; !tcp_skb_shift(skb, next_skb, 1, next_skb_size))\t\treturn false;\t//处理highest_sack字段指向第一个skb\ttcp_highest_sack_replace(sk, next_skb, skb);\t/* Update sequence range on original skb. */\t//更新第一个skb的序列号\tTCP_SKB_CB(skb)-&gt;end_seq = TCP_SKB_CB(next_skb)-&gt;end_seq;\t/* Merge over control information. This moves PSH/FIN etc. over */\t//设置tcp标志位\tTCP_SKB_CB(skb)-&gt;tcp_flags |= TCP_SKB_CB(next_skb)-&gt;tcp_flags;\t/* All done, get rid of second SKB and account for it so\t * packet counting does not break.\t */\t//设置重传过的标志(如果有的话) 这个标志是在外面设置的\tTCP_SKB_CB(skb)-&gt;sacked |= TCP_SKB_CB(next_skb)-&gt;sacked &amp; TCPCB_EVER_RETRANS;\tTCP_SKB_CB(skb)-&gt;eor = TCP_SKB_CB(next_skb)-&gt;eor;\t/* changed transmit queue under us so clear hints */\t//将tp-&gt;lost_skb_hint 置为空，因为tcp ack中会用到这个值，这里已经处理重传队列了，所以要重新赋值\ttcp_clear_retrans_hints_partial(tp);\t//设置下一个要重传的skb的指针\tif (next_skb == tp-&gt;retransmit_skb_hint)\t\ttp-&gt;retransmit_skb_hint = skb;\t//因为合并了数据包要修改一些 *out字段，因为表示已经发送出去还没确认的数量，因为合并了 所以需要修改\ttcp_adjust_pcount(sk, next_skb, tcp_skb_pcount(next_skb));\t//处理时间戳\ttcp_skb_collapse_tstamp(skb, next_skb);\t//unlink数据包，调用调整内存的相关接口\ttcp_rtx_queue_unlink_and_free(next_skb, sk);\treturn true;&#125;\n\ntcp_collapse_retrans 中调用tcp_skb_shift完成真实的合并操作，之后设置一些其他地方会用到的字段，比如数据包的序列号，tcp的标志位，时间戳等，然后将被合并的数据包从重传队列中移除，同时调用内存释放相关接口函数\nstatic inline void tcp_rtx_queue_unlink_and_free(struct sk_buff *skb, struct sock *sk)&#123;\tlist_del(&amp;skb-&gt;tcp_tsorted_anchor);\t//从重传队列中unlink\ttcp_rtx_queue_unlink(skb, sk);\t//freeskb 修改wmem_queue\ttcp_wmem_free_skb(sk, skb);&#125;\n\nstatic inline void tcp_wmem_free_skb(struct sock *sk, struct sk_buff *skb)&#123;\tsk_wmem_queued_add(sk, -skb-&gt;truesize);\tif (!skb_zcopy_pure(skb))\t\tsk_mem_uncharge(sk, skb-&gt;truesize);\telse\t\tsk_mem_uncharge(sk, SKB_TRUESIZE(skb_end_offset(skb)));\t__kfree_skb(skb);&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP定时器"]},{"title":"TCP 延迟确认定时器","url":"/2025/08/07/%E5%BB%B6%E8%BF%9Fack%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"TCP延迟确认定时器（Delayed ACK Timer）延迟确认定时器是TCP协议栈用来推迟发送ACK报文的一种机制。 它的目标是：减少纯ACK包数量，提升网络效率，尤其适用于交互式和小包频繁的应用场景。\n延迟定时器的初始化tcp的延迟定时器在初始化sock的时候完成，代码如下：\nvoid tcp_init_xmit_timers(struct sock *sk)&#123;    ...\tinet_csk_init_xmit_timers(sk, &amp;tcp_write_timer, &amp;tcp_delack_timer,\t\t\t\t  &amp;tcp_keepalive_timer);\t...&#125;\n\n第二个参数为延迟确认定时器的回调函数，tcp_delack_timer如果用户没有持有sock，则会调用tcp_delack_timer_handler处理重传逻辑，具体的逻辑为，如果存在压缩ack则跳转到压缩ack的逻辑中。否则进入正常的处理流程，首先会判断是否位pingPong模式，如果不是pingpong模式，则继续增加下次的超时时间，因为属于单向发送数据所以可以延迟的更久，如果不是pingpong模式，则会退出pingpong模式，因为pingpong模式是不会走到这定时器逻辑中的，因为pingpong模式都是捎带ack，所以说这里要退出pingpong模式吧。\n无论是上述那种情况，最终都会刷新时间戳，调用tcp_send_ack发送数据包。\n/* Called with BH disabled */void tcp_delack_timer_handler(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\t//不需要处理的状态，直接返回\tif ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_LISTEN))\t\treturn;\t/* Handling the sack compression case */\t//处理压缩ack 如果收到了乱序数据包就会增加，\tif (tp-&gt;compressed_ack) &#123;\t\t//跟新tp的时间戳字段\t\ttcp_mstamp_refresh(tp);\t\t//处理压缩ack\t\ttcp_sack_compress_send_ack(sk);\t\treturn;\t&#125;\t//没有这个标志，直接返回， 这个标志是在tcp_send_delayed_ack（接收路径上）设置的，表示已经启动的定时器\tif (!(icsk-&gt;icsk_ack.pending &amp; ICSK_ACK_TIMER))\t\treturn;\t//如果没到时间就重新设置一下\tif (time_after(icsk-&gt;icsk_ack.timeout, jiffies)) &#123;\t\tsk_reset_timer(sk, &amp;icsk-&gt;icsk_delack_timer, icsk-&gt;icsk_ack.timeout);\t\treturn;\t&#125;\t//清楚标志位\ticsk-&gt;icsk_ack.pending &amp;= ~ICSK_ACK_TIMER;\t//判断是否有ack要发送，这个是在收包的时候inet_csk_schedule_ack设置的,判断pending和上面的区别是已经有ack了\tif (inet_csk_ack_scheduled(sk)) &#123;\t\t//是否是pingpong模式。pingpong模式就是交互模式，通过时间判断是否是进入pingpong，在发包的时候判断的\t\tif (!inet_csk_in_pingpong_mode(sk)) &#123;\t\t\t/* Delayed ACK missed: inflate ATO. */\t\t\t//不是pingbong模式，就是批量数据流，这里左移让下次更久！合理， 但是不能超过rto\t\t\ticsk-&gt;icsk_ack.ato = min(icsk-&gt;icsk_ack.ato &lt;&lt; 1, icsk-&gt;icsk_rto);\t\t&#125; else &#123;\t\t\t/* Delayed ACK missed: leave pingpong mode and\t\t\t * deflate ATO.\t\t\t */\t\t\t//退出pingpong模式，因为是交互式场景，应该随着数据发ack？\t\t\tinet_csk_exit_pingpong_mode(sk);\t\t\t//40ms\t\t\ticsk-&gt;icsk_ack.ato      = TCP_ATO_MIN;\t\t&#125;\t\t//刷新时间戳\t\ttcp_mstamp_refresh(tp);\t\t//真正发送ack\t\ttcp_send_ack(sk);\t\t//增加统计计数\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKS);\t&#125;&#125;\n\n上述处理压缩ack的代码逻辑如下：\nvoid tcp_sack_compress_send_ack(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//直接返回\tif (!tp-&gt;compressed_ack)\t\treturn;\t//取消压缩ack的定时器\tif (hrtimer_try_to_cancel(&amp;tp-&gt;compressed_ack_timer) == 1)\t\t__sock_put(sk);\t/* Since we have to send one ack finally,\t * substract one from tp-&gt;compressed_ack to keep\t * LINUX_MIB_TCPACKCOMPRESSED accurate.\t */\t//更新统计计数\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPACKCOMPRESSED,\t\t      tp-&gt;compressed_ack - 1);\t//清零\ttp-&gt;compressed_ack = 0;\t//立即发送ack\ttcp_send_ack(sk);&#125;\n\n只有设置了icsk_ack.pending的ICSK_ACK_TIMER和ICSK_ACK_SCHED标志才能真正的发送一个数据包\n在inet_csk_schedule_ack会设置ICSK_ACK_SCHED，在收包的多个地方会调用这个函数。\n在tcp_send_delayed_ack中（也就是激活定时器的函数）会同时设置这两个标志位，接收端不存在乱序数据包的会调用这个函数。\n//不存在乱序的时候void tcp_send_delayed_ack(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tint ato = icsk-&gt;icsk_ack.ato;\tunsigned long timeout;\t//当前的ato 大于最小值\tif (ato &gt; TCP_DELACK_MIN) &#123;\t\tconst struct tcp_sock *tp = tcp_sk(sk);\t\tint max_ato = HZ / 2;\t\t//交互模式，或者 在push模式下就要增大ato？ 因为有要push的包所以延迟发送ack\t\tif (inet_csk_in_pingpong_mode(sk) ||\t\t    (icsk-&gt;icsk_ack.pending &amp; ICSK_ACK_PUSHED))\t\t\tmax_ato = TCP_DELACK_MAX;\t\t/* Slow path, intersegment interval is &quot;high&quot;. */\t\t/* If some rtt estimate is known, use it to bound delayed ack.\t\t * Do not use inet_csk(sk)-&gt;icsk_rto here, use results of rtt measurements\t\t * directly.\t\t */\t\t//拿到rtt\t\tif (tp-&gt;srtt_us) &#123;\t\t\tint rtt = max_t(int, usecs_to_jiffies(tp-&gt;srtt_us &gt;&gt; 3),\t\t\t\t\tTCP_DELACK_MIN);\t\t\t//最大ato 肯定不能大于rtt\t\t\tif (rtt &lt; max_ato)\t\t\t\tmax_ato = rtt;\t\t&#125;\t\t//更新ato\t\tato = min(ato, max_ato);\t&#125;\t//更新ato\tato = min_t(u32, ato, inet_csk(sk)-&gt;icsk_delack_max);\t/* Stay within the limit we were given */\t//定时器触发的时间\ttimeout = jiffies + ato;\t/* Use new timeout only if there wasn&#x27;t a older one earlier. */\t//如果已经启动了\tif (icsk-&gt;icsk_ack.pending &amp; ICSK_ACK_TIMER) &#123;\t\t/* If delack timer is about to expire, send ACK now. */\t\t//马上到定时器的触发时间了 直接发送ack\t\tif (time_before_eq(icsk-&gt;icsk_ack.timeout, jiffies + (ato &gt;&gt; 2))) &#123;\t\t\ttcp_send_ack(sk);\t\t\treturn;\t\t&#125;\t\t//如果还有一段时间到期，但是当前在之前的后面，设置为原来的\t\tif (!time_before(timeout, icsk-&gt;icsk_ack.timeout))\t\t\ttimeout = icsk-&gt;icsk_ack.timeout;\t&#125;\t//设置标志位，启动定时器。\ticsk-&gt;icsk_ack.pending |= ICSK_ACK_SCHED | ICSK_ACK_TIMER;\ticsk-&gt;icsk_ack.timeout = timeout;\tsk_reset_timer(sk, &amp;icsk-&gt;icsk_delack_timer, timeout);&#125;\n\n发送延迟ack的接口为tcp_send_ack ，主要逻辑为申请一个没有数据的skb之后直接调用__tcp_transmit_skb发送数据包。具体逻辑如下：\n/* This routine sends an ack and also updates the window. */void __tcp_send_ack(struct sock *sk, u32 rcv_nxt)&#123;\tstruct sk_buff *buff;\t/* If we have been reset, we may not send again. */\tif (sk-&gt;sk_state == TCP_CLOSE)\t\treturn;\t/* We are not putting this on the write queue, so\t * tcp_transmit_skb() will set the ownership to this\t * sock.\t */\t//这里size的大小是320 GFP_ATOMIC追求快速\tbuff = alloc_skb(MAX_TCP_HEADER,\t\t\t sk_gfp_mask(sk, GFP_ATOMIC | __GFP_NOWARN));\t//小概率分配失败\tif (unlikely(!buff)) &#123;\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\t\tunsigned long delay;\t\t//下次触发时间，相当于指数退避\t\tdelay = TCP_DELACK_MAX &lt;&lt; icsk-&gt;icsk_ack.retry;\t\tif (delay &lt; TCP_RTO_MAX)//相当于没超过容许的最大值\t\t\ticsk-&gt;icsk_ack.retry++;\t\tinet_csk_schedule_ack(sk); //设置了pending位再次调度\t\ticsk-&gt;icsk_ack.ato = TCP_ATO_MIN;//重置了ato 防止下次时间翻倍\t\t//复位\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK, delay, TCP_RTO_MAX);\t\treturn; //直接返回\t&#125;\t/* Reserve space for headers and prepare control bits. */\t//修改data和tailz指针\tskb_reserve(buff, MAX_TCP_HEADER);\ttcp_init_nondata_skb(buff, tcp_acceptable_seq(sk), TCPHDR_ACK);\t/* We do not want pure acks influencing TCP Small Queues or fq/pacing\t * too much.\t * SKB_TRUESIZE(max(1 .. 66, MAX_TCP_HEADER)) is unfortunately ~784\t */\t//设置ture_size 为2 表示为纯ack ？？？？\tskb_set_tcp_pure_ack(buff);\t/* Send it off, this clears delayed acks for us. */\t//调用发包函数\t__tcp_transmit_skb(sk, buff, 0, (__force gfp_t)0, rcv_nxt);&#125;\n\n延迟确认定时器的启动在tcp_send_delayed_ack中会启动延迟确认定时器，该函数在不存在乱序的时候会被调用\n在tcp_ack 中分配数据包失败的时候会启动延迟确认ack\n在发送syn包后可能会会启动延迟ack\nstatic int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,\t\t\t\t\t const struct tcphdr *th)&#123;    ...\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,TCP_DELACK_MAX, TCP_RTO_MAX);\t...&#125;\n\n\n\n延迟确认定时器的关闭在tcp_event_ack_sent会关闭延迟ack，该函数在__tcp_transmit_skb中被调用\nstatic int __tcp_transmit_skb(struct sock *sk, struct sk_buff *skb,\t\t\t      int clone_it, gfp_t gfp_mask, u32 rcv_nxt)&#123;\t...\tif (likely(tcb-&gt;tcp_flags &amp; TCPHDR_ACK))\t\ttcp_event_ack_sent(sk, rcv_nxt);\t...&#125;\n\n/* Account for an ACK we sent. */static inline void tcp_event_ack_sent(struct sock *sk, u32 rcv_nxt)&#123;\t...\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_DACK);\t...&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP定时器"]},{"title":"TCP 零窗口探测定时器","url":"/2025/08/11/%E9%9B%B6%E7%AA%97%E5%8F%A3%E6%8E%A2%E6%B5%8B%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"零窗口探测定时器TCP 零窗口探测定时器是在对端通告窗口为 0 时启动的，用来周期性发送探测包，检查对端窗口是否重新打开。 定时器到期时会发一个零长度或过时序号的 ACK 段，逼对端回 ACK，从而获知最新窗口大小。\n零窗口探测定时器和超时重传使用一个定时器，根据不同的pending来区分具体走哪个逻辑，具体如下所示：\nvoid tcp_write_timer_handler(struct sock *sk)&#123;\t...\ttcp_mstamp_refresh(tcp_sk(sk));\tevent = icsk-&gt;icsk_pending;\tswitch (event) &#123;\tcase ICSK_TIME_RETRANS:\t\ticsk-&gt;icsk_pending = 0;\t\ttcp_retransmit_timer(sk);\t\tbreak;\tcase ICSK_TIME_PROBE0:\t\ticsk-&gt;icsk_pending = 0;\t\t//零窗口探测\t\ttcp_probe_timer(sk);\t\tbreak;\t&#125;&#125;\n\ntcp_probe_timer 为零窗口探测到期的处理函数，主要逻辑为首先怕判断发送队列是否为空，或者没有未确认的数据包，这时候根本也不需要零窗口探测，因此直接返回，然后获取最大的探测次数，这里用的是retry2 默认 15次，如果超过次数了，直接err 否则的话调用\ntcp_send_probe0发送零窗口探测报文。\nstatic void tcp_probe_timer(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct sk_buff *skb = tcp_send_head(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tint max_probes;\t//如果发送队列为空，或者没有在途中的数据包，直接返回\tif (tp-&gt;packets_out || !skb) &#123;\t\ticsk-&gt;icsk_probes_out = 0;\t\ticsk-&gt;icsk_probes_tstamp = 0;\t\treturn;\t&#125;\t/* RFC 1122 4.2.2.17 requires the sender to stay open indefinitely as\t * long as the receiver continues to respond probes. We support this by\t * default and reset icsk_probes_out with incoming ACKs. But if the\t * socket is orphaned or the user specifies TCP_USER_TIMEOUT, we\t * kill the socket when the retry count and the time exceeds the\t * corresponding system limit. We also implement similar policy when\t * we use RTO to probe window in tcp_retransmit_timer().\t */\tif (!icsk-&gt;icsk_probes_tstamp) &#123;\t\ticsk-&gt;icsk_probes_tstamp = tcp_jiffies32; //第一次探测的时间戳\t&#125; else &#123;\t\tu32 user_timeout = READ_ONCE(icsk-&gt;icsk_user_timeout); //set_sock_opt 用户设置的超时时间\t\t//用户设置的超时时间大于当前时间减去第一个0窗口探测的时间，直接返回err 因为超时了\t\tif (user_timeout &amp;&amp;\t\t    (s32)(tcp_jiffies32 - icsk-&gt;icsk_probes_tstamp) &gt;=\t\t     msecs_to_jiffies(user_timeout))\t\tgoto abort;\t&#125;\tmax_probes = READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_retries2);\t//在零窗口探测中走这个分支的概率很小吧！先忽略\tif (sock_flag(sk, SOCK_DEAD)) &#123;\t\t//判断是否存活，为啥要用inet_csk_rto_backoff 这个呢？\t\tconst bool alive = inet_csk_rto_backoff(icsk, TCP_RTO_MAX) &lt; TCP_RTO_MAX;\t\t//更具是否存活返回一个最大探测次数 如果已经死了， 则大概率返0\t\tmax_probes = tcp_orphan_retries(sk, alive);\t\tif (!alive &amp;&amp; icsk-&gt;icsk_backoff &gt;= max_probes)\t\t\tgoto abort; //直接返回err\t\t//检查是否系统资源不足\t\tif (tcp_out_of_resources(sk, true))\t\t\treturn;\t&#125;\t//探测的次数超过retry2了 直接返回err\tif (icsk-&gt;icsk_probes_out &gt;= max_probes) &#123;abort:\t\ttcp_write_err(sk);\t&#125; else &#123;\t\t/* Only send another probe if we didn&#x27;t close things up. */\t\t//发送零窗口探测报文\t\ttcp_send_probe0(sk);\t&#125;&#125;\n\n上述逻辑最终调用tcp_send_probe0进一步处理零窗口探测，在tcp_send_probe0中，会调用tcp_write_wakeup发送实际的数据包，发送之后，会对重传次数，下一次的超时时间进行重新计算，并设置下一次零窗口探测的时间，具体代码逻辑如下：\n/* A window probe timeout has occurred.  If window is not closed send * a partial packet else a zero probe. */void tcp_send_probe0(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct net *net = sock_net(sk);\tunsigned long timeout;\tint err;\t//发送0窗口探测报文\terr = tcp_write_wakeup(sk, LINUX_MIB_TCPWINPROBE);\t//发送出去的数据包的未确认数量为0,且发送队列为空，认为不需要探测了\tif (tp-&gt;packets_out || tcp_write_queue_empty(sk)) &#123;\t\t/* Cancel probe timer, if it is not required. */\t\ticsk-&gt;icsk_probes_out = 0;\t\ticsk-&gt;icsk_backoff = 0;\t\ticsk-&gt;icsk_probes_tstamp = 0;//复位第一次的时间戳\t\treturn;//终止探测了！\t&#125;\t//增加0窗口探测次数\ticsk-&gt;icsk_probes_out++;\t//如果发送失败了\tif (err &lt;= 0) &#123;\t\tif (icsk-&gt;icsk_backoff &lt; READ_ONCE(net-&gt;ipv4.sysctl_tcp_retries2))//小于retry2\t\t\ticsk-&gt;icsk_backoff++; //增加退避次数\t\t//根据上面的退避次数计算一个超时时间\t\ttimeout = tcp_probe0_when(sk, TCP_RTO_MAX);\t&#125; else &#123;\t\t/* If packet was not sent due to local congestion,\t\t * Let senders fight for local resources conservatively.\t\t */\t\t//发送成功了，设置正常的间隔\t\ttimeout = TCP_RESOURCE_PROBE_INTERVAL;\t&#125;\ttimeout = tcp_clamp_probe0_to_user_timeout(sk, timeout);\t//设置下一次零窗口探测时间\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0, timeout, TCP_RTO_MAX);&#125;\n\ntcp_write_wakeup是实际调用发送数据包的函数，这里分为两种情况，第一种情况：发送队列数据包不为空，且序列号在右边界内，则首先尝试从发送队列中peek一个skb，然后根据mss等大小对数据包进行处理后直接发送。\n第二种情况，大概率走这个分支（因为都已经零窗口了那数据包的序列号几乎不会在右边界范围内吧），则直接发送一个对端已经确认的序列号-1的纯ack报文\n/* Initiate keepalive or window probe from timer. *///注意`tcp_write_wakeup`在保活或者零窗口探测中都会被调用int tcp_write_wakeup(struct sock *sk, int mib)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb;\tif (sk-&gt;sk_state == TCP_CLOSE)\t\treturn -1;\t//取出一个skb\tskb = tcp_send_head(sk);\t//数据包非空，且序列号落在右边界内\tif (skb &amp;&amp; before(TCP_SKB_CB(skb)-&gt;seq, tcp_wnd_end(tp))) &#123;\t\tint err;\t\t//拿到mss\t\t\t\tunsigned int mss = tcp_current_mss(sk);\t\t//计算可以用多少字节\t\tunsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)-&gt;seq;\t\t//下一个要push的序列号如果在endseq的前面\t\tif (before(tp-&gt;pushed_seq, TCP_SKB_CB(skb)-&gt;end_seq))\t\t\ttp-&gt;pushed_seq = TCP_SKB_CB(skb)-&gt;end_seq; //更新push seq，表示这学数据要尽快交付\t\t/* We are probing the opening of a window\t\t * but the window size is != 0\t\t * must have been a result SWS avoidance ( sender )\t\t */\t\t//如果可用的空间不够了，或者skb的len大于mss,就需要分段了\t\tif (seg_size &lt; TCP_SKB_CB(skb)-&gt;end_seq - TCP_SKB_CB(skb)-&gt;seq ||\t\t    skb-&gt;len &gt; mss) &#123;\t\t\tseg_size = min(seg_size, mss);\t\t\tTCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_PSH;\t\t\tif (tcp_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\t\t\t\t\t skb, seg_size, mss, GFP_ATOMIC))\t\t\t\treturn -1;\t\t&#125; else if (!tcp_skb_pcount(skb))//如果没有设置这个skb的segs数量\t\t\t//设置segs的数量\t\t\t\t\t\ttcp_set_skb_tso_segs(skb, mss);\t\t//设置push的标志位\t\tTCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_PSH;\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\t\tif (!err)\t\t\ttcp_event_new_data_sent(sk, skb);\t\treturn err;\t&#125; else &#123;\t\t//大概率应该走这个分支？\t\tif (between(tp-&gt;snd_up, tp-&gt;snd_una + 1, tp-&gt;snd_una + 0xFFFF))\t\t\ttcp_xmit_probe_skb(sk, 1, mib);   //有紧急数据的情况，发送数据包的序列号为snd_una\t\treturn tcp_xmit_probe_skb(sk, 0, mib);//发送数据包的序列号为 una - 1 真正的探测包，数据是无法交付给应用层的\t&#125;&#125;\n\n注意tcp_xmit_probe_skb(sk, 0, mib)中第二参数为0，则tcp_xmit_probe_skb在计算序列号的时候会将数据包的序列号设置为 una-1，防止对方应用层收到的同时还可以逼迫对方回复ack。\nstatic int tcp_xmit_probe_skb(struct sock *sk, int urgent, int mib)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb;\t/* We don&#x27;t queue it, tcp_transmit_skb() sets ownership. */\tskb = alloc_skb(MAX_TCP_HEADER,\t\t\tsk_gfp_mask(sk, GFP_ATOMIC | __GFP_NOWARN));\tif (!skb)\t\treturn -1;\t/* Reserve space for headers and set control bits. */\t\t//调整data和tail位置\tskb_reserve(skb, MAX_TCP_HEADER);\t/* Use a previous sequence.  This should cause the other\t * end to send an ack.  Don&#x27;t queue or clone SKB, just\t * send it.\t */\t//这里会根据传进来的参数设置序号\ttcp_init_nondata_skb(skb, tp-&gt;snd_una - !urgent, TCPHDR_ACK);\tNET_INC_STATS(sock_net(sk), mib);\treturn tcp_transmit_skb(sk, skb, 0, (__force gfp_t)0);&#125;\n\n零窗口探测定时器的激活__tcp_push_pending_frames中，如果发包之后如果未确认的包数为0，同时发送队列不为空会调用tcp_check_probe_timer启动零窗口探测定时器\nvoid __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,\t\t\t       int nonagle)&#123;\t/* If we are closed, the bytes will have to remain here.\t * In time closedown will finish, we empty the write queue and\t * all will be happy.\t */\tif (unlikely(sk-&gt;sk_state == TCP_CLOSE))\t\t\t\treturn;\t//返回值：如果未确认的包数为0，同时发送队列不为空（这叫做右数据要发但是没有数据在途中，所以符合场景，合理）\tif (tcp_write_xmit(sk, cur_mss, nonagle, 0,\t\t\t   sk_gfp_mask(sk, GFP_ATOMIC)))\t\ttcp_check_probe_timer(sk);&#125;static inline void tcp_check_probe_timer(struct sock *sk)&#123;\tif (!tcp_sk(sk)-&gt;packets_out &amp;&amp; !inet_csk(sk)-&gt;icsk_pending)\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0,\t\t\t\t     tcp_probe0_base(sk), TCP_RTO_MAX);&#125;\n\ntcp_ack_probe中也会启动零窗口探测定时器，tcp_ack_probe在tcp_ack中被调用\nstatic void tcp_ack_probe(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct sk_buff *head = tcp_send_head(sk);\tconst struct tcp_sock *tp = tcp_sk(sk);\t/* Was it a usable window open? */\tif (!head)\t\treturn;\tif (!after(TCP_SKB_CB(head)-&gt;end_seq, tcp_wnd_end(tp))) &#123;\t\ticsk-&gt;icsk_backoff = 0;\t\ticsk-&gt;icsk_probes_tstamp = 0;\t\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_PROBE0);\t\t/* Socket must be waked up by subsequent tcp_data_snd_check().\t\t * This function is not for random using!\t\t */\t&#125; else &#123;\t\t//窗口不够用了\t\tunsigned long when = tcp_probe0_when(sk, TCP_RTO_MAX);\t\twhen = tcp_clamp_probe0_to_user_timeout(sk, when);\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0, when, TCP_RTO_MAX);\t&#125;&#125;/* This routine deals with incoming acks, but not outgoing ones. */static int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)&#123;\t...no_queue:\ttcp_ack_probe(sk);\t...&#125;","categories":["网络协议栈源码学习"],"tags":["TCP","TCP定时器"]},{"title":"TCP保活定时器","url":"/2025/08/14/TCP%E4%BF%9D%E6%B4%BB%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"TCP保活定时器TCP 保活定时器（TCP Keepalive Timer）是 TCP 协议栈里用来检测长时间空闲连接是否还存活的一个定时机制。它不是强制标准功能，而是一个可选功能。通过setsockopt设置\n相关内核参数（Linux）这些值可以通过 /proc/sys/net/ipv4/ 目录下的文件查看和调整：\n\ntcp_keepalive_time（默认 7200 秒 &#x3D; 2 小时） 空闲多久后开始发送第一个保活探测包。\ntcp_keepalive_intvl（默认 75 秒） 如果没有收到响应，下一次保活探测的间隔。\ntcp_keepalive_probes（默认 9 次） 如果连续 N 次探测都无响应，内核认为连接已断开，关闭连接。\n\nTCP保活定时器到期的回调tcp_keepalive_timer为注册保活定时器到期的回调函数，该函数中还有针对fin_wait2定时器的逻辑，这里只关心保活定时器的逻辑，tcp_keepalive_timer中的核心逻辑就是计算当前已经空闲的时间，如果达到了超时时间，首先判断是不是已经超过最大的保活探测次数或者超过用户配置的超时时间了，如果是，则会发送rst报文来关闭连接，否则调用tcp_write_wakeup发送一个探测报文，注意这里和零窗口探测调用的是一个函数。具体代码如下：\nstatic void tcp_keepalive_timer (struct timer_list *t)&#123;\tstruct sock *sk = from_timer(sk, t, sk_timer);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 elapsed;\t/* Only process if socket is not in use. */\tbh_lock_sock(sk);\t//用户正在持有这个套接字\tif (sock_owned_by_user(sk)) &#123;\t\t/* Try again later. */\t\t//当前基础上加50ms后调度\t\tinet_csk_reset_keepalive_timer (sk, HZ/20);\t\tgoto out;\t&#125;\tif (sk-&gt;sk_state == TCP_LISTEN) &#123;\t\tpr_err(&quot;Hmm... keepalive on a LISTEN ???\\n&quot;);\t\tgoto out;\t&#125;\t//更新时间戳\ttcp_mstamp_refresh(tp);\t//  这个应该是fin_wait2 定时器，处于fin_wait2状态，且sock 为dead(这里的dead是tcp close中阻塞一个linger时间后（也可能不阻塞）设置的)\t//\t只有两个地方激活定时器后才会走到这个分支，一个是tcp_close 另一个是tcp_rcv_state_process()\tif (sk-&gt;sk_state == TCP_FIN_WAIT2 &amp;&amp; sock_flag(sk, SOCK_DEAD)) &#123;\t\t//用户是否设置linger2\t\tif (READ_ONCE(tp-&gt;linger2) &gt;= 0) &#123;\t\t\t//这里减去了timewait时间为什么？ 没太理解 因为linger2 是fin_wait2到结束的时间？？？ \t\t\tconst int tmo = tcp_fin_time(sk) - TCP_TIMEWAIT_LEN;\t\t\tif (tmo &gt; 0) &#123;\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\t\t\t\tgoto out;\t\t\t&#125;\t\t&#125;\t\t//如果没有设置linger2，因为定时器到期了，这里直接发rst\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\t\tgoto death;\t&#125;\t//是否开启了保活(通过setsockopt设置) 如果没开启保活或者是close或者syn sent状态，直接退出了 \t//！！！这个判断条件在下面的原因应该是不影响到fin_wait2定时器\tif (!sock_flag(sk, SOCK_KEEPOPEN) ||\t    ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_SYN_SENT)))\t\tgoto out;\telapsed = keepalive_time_when(tp);\t/* It is alive without keepalive 8) */\t//如果有发送出去未确认的数据包，或者发送队列不为空\tif (tp-&gt;packets_out || !tcp_write_queue_empty(sk))\t\tgoto resched;\t//拿到空闲的时间\telapsed = keepalive_time_elapsed(tp);\t//是否到达保活时间的阈值了\tif (elapsed &gt;= keepalive_time_when(tp)) &#123;\t\t//用户配置的时间\t\tu32 user_timeout = READ_ONCE(icsk-&gt;icsk_user_timeout);\t\t/* If the TCP_USER_TIMEOUT option is enabled, use that\t\t * to determine when to timeout instead.\t\t */\t\t//这里分为两种情况进入到这个分支\t\t//1. 用户配置了超时时间超过了保活探测时间，同时不是第一次探测\t\t//2. 用户没有配置超时时间，但是超过了保活探测的次数（9次）\t\tif ((user_timeout != 0 &amp;&amp;\t\t    elapsed &gt;= msecs_to_jiffies(user_timeout) &amp;&amp;\t\t    icsk-&gt;icsk_probes_out &gt; 0) ||\t\t    (user_timeout == 0 &amp;&amp;\t\t    icsk-&gt;icsk_probes_out &gt;= keepalive_probes(tp))) &#123;\t\t\t//发送rst报文\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\t\t\t//直接返回err\t\t\ttcp_write_err(sk);\t\t\tgoto out;\t\t&#125;\t\tif (tcp_write_wakeup(sk, LINUX_MIB_TCPKEEPALIVE) &lt;= 0) &#123;\t\t\t//增加探测次数\t\t\ticsk-&gt;icsk_probes_out++;\t\t\t//设置下次保活探测的时间\t\t\telapsed = keepalive_intvl_when(tp);\t\t&#125; else &#123;\t\t\t/* If keepalive was lost due to local congestion,\t\t\t * try harder.\t\t\t */\t\t\telapsed = TCP_RESOURCE_PROBE_INTERVAL; //75s\t\t&#125;\t&#125; else &#123;\t\t/* It is tp-&gt;rcv_tstamp + keepalive_time_when(tp) */\t\t//如果还没有达到发送保活数据包的时间，就重新计算时间\t\telapsed = keepalive_time_when(tp) - elapsed;\t&#125;resched:\t//更新定时器的到期时间\tinet_csk_reset_keepalive_timer (sk, elapsed);\tgoto out;death: //fin_wait2定时器使用\ttcp_done(sk);out:\tbh_unlock_sock(sk);\tsock_put(sk);&#125;\n\nkeepalive_time_when为空闲多久后发送探测报文，如果用户没有设置则为2个小时，keepalive_time_elapsed用来计算当前距离上次收到数据包的时间，具体代码如下：\nstatic inline int keepalive_time_when(const struct tcp_sock *tp)&#123;\tstruct net *net = sock_net((struct sock *)tp);\tint val;\t/* Paired with WRITE_ONCE() in tcp_sock_set_keepidle_locked() */\t//读取用户的配置\tval = READ_ONCE(tp-&gt;keepalive_time);\t//使用系统的配置\treturn val ? : READ_ONCE(net-&gt;ipv4.sysctl_tcp_keepalive_time);&#125;static inline u32 keepalive_time_elapsed(const struct tcp_sock *tp)&#123;\tconst struct inet_connection_sock *icsk = &amp;tp-&gt;inet_conn;\t//返回的是一个当前时间减去最后一个ack或者收到数据包的时间\treturn min_t(u32, tcp_jiffies32 - icsk-&gt;icsk_ack.lrcvtime,\t\t\t  tcp_jiffies32 - tp-&gt;rcv_tstamp);&#125;\n\n如果没有超过次数限制或者用户配置的阈值则会调用tcp_write_wakeup 发送一个保活探测报文，这里其实是和零窗口探测使用的是一个函数\n/* Initiate keepalive or window probe from timer. */int tcp_write_wakeup(struct sock *sk, int mib)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb;\tif (sk-&gt;sk_state == TCP_CLOSE)\t\treturn -1;\t//取出一个skb\tskb = tcp_send_head(sk);\t//数据包非空，且序列号落在右边界内\tif (skb &amp;&amp; before(TCP_SKB_CB(skb)-&gt;seq, tcp_wnd_end(tp))) &#123;\t\tint err;\t\t//拿到mss\t\t\t\tunsigned int mss = tcp_current_mss(sk);\t\t//计算可以用多少字节\t\tunsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)-&gt;seq;\t\t//下一个要push的序列号如果在endseq的前面\t\tif (before(tp-&gt;pushed_seq, TCP_SKB_CB(skb)-&gt;end_seq))\t\t\ttp-&gt;pushed_seq = TCP_SKB_CB(skb)-&gt;end_seq; //更新push seq，表示这学数据要尽快交付\t\t/* We are probing the opening of a window\t\t * but the window size is != 0\t\t * must have been a result SWS avoidance ( sender )\t\t */\t\t//如果可用的空间不够了，或者skb的len大于mss,就需要分段了\t\tif (seg_size &lt; TCP_SKB_CB(skb)-&gt;end_seq - TCP_SKB_CB(skb)-&gt;seq ||\t\t    skb-&gt;len &gt; mss) &#123;\t\t\tseg_size = min(seg_size, mss);\t\t\tTCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_PSH;\t\t\tif (tcp_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\t\t\t\t\t skb, seg_size, mss, GFP_ATOMIC))\t\t\t\treturn -1;\t\t&#125; else if (!tcp_skb_pcount(skb))//如果没有设置这个skb的segs数量\t\t\t//设置segs的数量\t\t\t\t\t\ttcp_set_skb_tso_segs(skb, mss);\t\t//设置push的标志位\t\tTCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_PSH;\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\t\tif (!err)\t\t\ttcp_event_new_data_sent(sk, skb);\t\treturn err;\t&#125; else &#123;\t\t//大概率应该走这个分支？\t\tif (between(tp-&gt;snd_up, tp-&gt;snd_una + 1, tp-&gt;snd_una + 0xFFFF))\t\t\ttcp_xmit_probe_skb(sk, 1, mib);   //有紧急数据的情况，发送数据包的序列号为snd_una\t\treturn tcp_xmit_probe_skb(sk, 0, mib);//发送数据包的序列号为 una - 1 真正的探测包，数据是无法交付给应用层的\t&#125;&#125;\n\nTCP保活定时器到期的激活用户需要setsockopt显示启用tcp保活\nint sk_setsockopt(struct sock *sk, int level, int optname,\t\t  sockptr_t optval, unsigned int optlen)&#123;...\tcase SO_KEEPALIVE:\t\tif (sk-&gt;sk_prot-&gt;keepalive)\t\t\tsk-&gt;sk_prot-&gt;keepalive(sk, valbool);\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\t\tbreak;...&#125;\n\n对于tcp4 则sk-&gt;sk_prot-&gt;keepalive(sk, valbool); 最终调用到tcp_set_keepalive\nvoid tcp_set_keepalive(struct sock *sk, int val)&#123;\tif ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_LISTEN))\t\treturn;\t\t//保活定时器\tif (val &amp;&amp; !sock_flag(sk, SOCK_KEEPOPEN))\t\tinet_csk_reset_keepalive_timer(sk, keepalive_time_when(tcp_sk(sk)));\telse if (!val)\t\tinet_csk_delete_keepalive_timer(sk);&#125;\n\n三次握手阶段，会启动保活定时器\nvoid tcp_finish_connect(struct sock *sk, struct sk_buff *skb)&#123;...\t//保活定时器\tif (sock_flag(sk, SOCK_KEEPOPEN))\t\tinet_csk_reset_keepalive_timer(sk, keepalive_time_when(tp));...&#125;//客户端\n\nstruct sock *tcp_create_openreq_child(const struct sock *sk,\t\t\t\t      struct request_sock *req,\t\t\t\t      struct sk_buff *skb)&#123;...\t//保活定时器\tif (sock_flag(newsk, SOCK_KEEPOPEN))\t\tinet_csk_reset_keepalive_timer(newsk,\t\t\t\t\t       keepalive_time_when(newtp));...&#125;//服务端\n\n\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP定时器"]},{"title":"TCP尾部丢包探测定时器","url":"/2025/08/16/TLP%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"尾部丢包探测(TLP)定时器因为 TCP 的 快速重传需要重复 ACK 才能触发，而尾部丢包时往往没有新数据推动 ACK；如果依赖 RTO 定时器又太慢，动辄几百毫秒甚至秒级，造成应用延迟。于是引入 TLP 定时器，在 RTO 之前提前触发一次探测报文，用来快速确认是否真的丢包，从而避免长时间等待 RTO，提高尾部丢包情况下的恢复速度。\nTLP使用的定时器与超时重传用的是同一个定时器，具体定定时器到期的主要逻辑为，首先判断发送队列是否为空，如果不为空或者空间不够发送，则尝试从发送队列拿出一个个数据包直接发送，若发送队列为空，则从重传队列中拿到一个数据包发送\n//主动发生一个数据包void tcp_send_loss_probe(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb;\tint pcount;\tint mss = tcp_current_mss(sk);\t/* At most one outstanding TLP */\tif (tp-&gt;tlp_high_seq) //tcpack中会把这个设置为0\t\tgoto rearm_timer;\ttp-&gt;tlp_retrans = 0;  //标记是否触发了TLP\tskb = tcp_send_head(sk); //从发送队列拿到一个数据包。其实就是看一下数据包是否为空\t//如果数据包不为空，且当前空间有一个mss的大小，\tif (skb &amp;&amp; tcp_snd_wnd_test(tp, skb, mss)) &#123;\t\tpcount = tp-&gt;packets_out;\t\ttcp_write_xmit(sk, mss, TCP_NAGLE_OFF, 2, GFP_ATOMIC);\t\t//相当于在判断是否发送成功\t\tif (tp-&gt;packets_out &gt; pcount) \t\t\tgoto probe_sent;\t\tgoto rearm_timer;\t&#125;\t//如果发送队列为空或者可用空间不足，就从重传队列中拿到一个数据包\tskb = skb_rb_last(&amp;sk-&gt;tcp_rtx_queue);\tif (unlikely(!skb)) &#123;\t\tWARN_ONCE(tp-&gt;packets_out,\t\t\t  &quot;invalid inflight: %u state %u cwnd %u mss %d\\n&quot;,\t\t\t  tp-&gt;packets_out, sk-&gt;sk_state, tcp_snd_cwnd(tp), mss);\t\tinet_csk(sk)-&gt;icsk_pending = 0;\t\treturn;\t&#125;\t//通过fclone判断是否已经在主机中了\tif (skb_still_in_host_queue(sk, skb))\t\tgoto rearm_timer;\t//计算当前数据包有几个段\tpcount = tcp_skb_pcount(skb);\tif (WARN_ON(!pcount))\t\tgoto rearm_timer;\t//超过一个段的情况需要分段\tif ((pcount &gt; 1) &amp;&amp; (skb-&gt;len &gt; (pcount - 1) * mss)) &#123;\t\tif (unlikely(tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb,\t\t\t\t\t  (pcount - 1) * mss, mss,\t\t\t\t\t  GFP_ATOMIC)))\t\t\tgoto rearm_timer;\t\tskb = skb_rb_next(skb);\t&#125;\tif (WARN_ON(!skb || !tcp_skb_pcount(skb)))\t\tgoto rearm_timer;\t//重传一个数据包\tif (__tcp_retransmit_skb(sk, skb, 1))\t\tgoto rearm_timer;\ttp-&gt;tlp_retrans = 1;probe_sent:\t/* Record snd_nxt for loss detection. */\t//这里其实相当与标记了tlp重传\ttp-&gt;tlp_high_seq = tp-&gt;snd_nxt;\t//更新统计信息\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPLOSSPROBES);\t/* Reset s.t. tcp_rearm_rto will restart timer from now */\tinet_csk(sk)-&gt;icsk_pending = 0;rearm_timer:\ttcp_rearm_rto(sk);&#125;\n\n如果从重传队列中取出一个数据包发送，在经过一系列处理后会调用__tcp_retransmit_skb 注意重传定时器也使用这个函数，这里面主要对数据包进行了裁剪，查看路由是否有效，计算mss，计算数据包的的断数，尝试合并重传数据包，然后调用tcp_transmit_skb这里不在粘贴这部分代码了。\n尾部丢包探测(TLP)定时器的激活TLP定时器的激活由tcp_schedule_loss_probe完成，具体代码如下：\nbool tcp_schedule_loss_probe(struct sock *sk, bool advancing_rto)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 timeout, timeout_us, rto_delta_us;\tint early_retrans;\t/* Don&#x27;t do any loss probe on a Fast Open connection before 3WHS\t * finishes.\t */\t//fastopen 不启动tlp\tif (rcu_access_pointer(tp-&gt;fastopen_rsk))\t\treturn false;\t//默认是3\tearly_retrans = READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_early_retrans);\t/* Schedule a loss probe in 2*RTT for SACK capable connections\t * not in loss recovery, that are either limited by cwnd or application.\t */\tif ((early_retrans != 3 &amp;&amp; early_retrans != 4) ||\t    !tp-&gt;packets_out || !tcp_is_sack(tp) ||  //是否支持sack，如果不支持sack也不能启动tlp，为什么呢？\t    (icsk-&gt;icsk_ca_state != TCP_CA_Open &amp;&amp;\t     icsk-&gt;icsk_ca_state != TCP_CA_CWR))\t\treturn false;\t/* Probe timeout is 2*rtt. Add minimum RTO to account\t * for delayed ack when there&#x27;s one outstanding packet. If no RTT\t * sample is available then probe after TCP_TIMEOUT_INIT.\t */\t//先拿到rtt\tif (tp-&gt;srtt_us) &#123;\t\ttimeout_us = tp-&gt;srtt_us &gt;&gt; 2;//先算一个pto 这里除了4,\t\tif (tp-&gt;packets_out == 1)\t//只有一个未确认数据包的情况\t\t\ttimeout_us += tcp_rto_min_us(sk);  //加一个最小rto？ 200ms\t\telse //有多个未确认数据包的情况\t\t\ttimeout_us += TCP_TIMEOUT_MIN_US; //2ms\t\ttimeout = usecs_to_jiffies(timeout_us);\t&#125; else &#123;\t\ttimeout = TCP_TIMEOUT_INIT; //如果没有rtt就是1s\t&#125;\t/* If the RTO formula yields an earlier time, then use that time. */\t//直接用当前rto的原始值，还是rto的剩余时间，由传入的参数决定\trto_delta_us = advancing_rto ?\t\t\tjiffies_to_usecs(inet_csk(sk)-&gt;icsk_rto) :\t\t\ttcp_rto_delta_us(sk);  /* How far in future is RTO? */\tif (rto_delta_us &gt; 0)\t//取最小值\t\ttimeout = min_t(u32, timeout, usecs_to_jiffies(rto_delta_us));\ttcp_reset_xmit_timer(sk, ICSK_TIME_LOSS_PROBE, timeout, TCP_RTO_MAX);\treturn true;&#125;\n\ntcp_schedule_loss_probe 首先判断是否可以调度定时器，如果拥塞状态机的状态不是open或拥塞状态， 或者系统参数没有开启TLP或者没有未确认的数据包，就直接返回了，不需要调度丢包探测定时器。否则的话就先获取rtt，然后根据rtt计算一个超时时间，启动定时器。\n接下来看上述调度tcp_schedule_loss_probe的地方，有两个地方，一个是发送数据包的路径上，另一个是tcp_ack的处理中，具体代码如下所示：\nstatic bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,\t\t\t   int push_one, gfp_t gfp)&#123;...\t\t//激活tlp定时器，如果等于2表示已经激活过了\t\tif (push_one != 2)\t\t\ttcp_schedule_loss_probe(sk, false);...&#125;\n\n/* This routine deals with incoming acks, but not outgoing ones. */static int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)&#123;...\t/* If needed, reset TLP/RTO timer when RACK doesn&#x27;t set. */\t//在清理重传队列的时候可能会设置上这个标志位，比如有新的数据包被确认的时候，或者检测到乱续或者丢包 肯定需要重新设置这个定时器了\tif (flag &amp; FLAG_SET_XMIT_TIMER)\t\ttcp_set_xmit_timer(sk);...&#125;static void tcp_set_xmit_timer(struct sock *sk)&#123;\tif (!tcp_schedule_loss_probe(sk, true))\t\ttcp_rearm_rto(sk);&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP定时器"]},{"title":"TCP FIN_WATI2定时器","url":"/2025/08/18/FIN_WAIT2%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"FIN_WATI2定时器TCP 的 FIN_WAIT2 定时器是在主动关闭一方发出 FIN 并收到对端 ACK 后启动的一个保护机制，此时连接进入 FIN_WAIT2 状态，主动方已经不能再发送数据，但仍然可以接收数据，如果对端应用层迟迟不关闭，连接就可能长期卡在这里，因此内核需要定时器来避免资源泄漏。在 Linux 中，当 socket 已经是 SOCK_DEAD 状态（应用进程不再持有该 fd）时，内核会启动 FIN_WAIT2 定时器，默认超时时间由 tcp_fin_timeout 控制，通常是 60 秒，同时还可以通过 linger2 参数覆盖（最大 120 秒，负值则表示不等待，直接 RST 关闭）。当定时器到期，如果还没收到对端的 FIN，内核就会强制将连接销毁或转入 TIME_WAIT 状态，从而保证不会无限期占用内核资源。\n内核中FIN_WAIT2 与保活定时器共享一个定时器，具体代码如下所示：\nstatic void tcp_keepalive_timer (struct timer_list *t)&#123;...\t//  这个应该是fin_wait2 定时器，处于fin_wait2状态，且sock 为dead(这里的dead是tcp close中设置的)\t//\t只有两个地方激活定时器后才会走到这个分支，一个是tcp_close 另一个是tcp_rcv_state_process()\tif (sk-&gt;sk_state == TCP_FIN_WAIT2 &amp;&amp; sock_flag(sk, SOCK_DEAD)) &#123;\t\t//用户是否设置linger2\t\tif (READ_ONCE(tp-&gt;linger2) &gt;= 0) &#123;\t\t\t//这里减去了timewait时间为什么？ 没太理解 因为linger2 是fin_wait2到结束的时间？？？ \t\t\tconst int tmo = tcp_fin_time(sk) - TCP_TIMEWAIT_LEN;\t\t\tif (tmo &gt; 0) &#123;\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\t\t\t\tgoto out;\t\t\t&#125;\t\t&#125;\t\t//如果没有设置linger2，因为定时器到期了，这里直接发rst\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\t\tgoto death;\t&#125;...&#125;static inline int tcp_fin_time(const struct sock *sk)&#123;\t//如果设置了linger2就用用户配置的，否则用系统的\tint fin_timeout = tcp_sk(sk)-&gt;linger2 ? :\t\tREAD_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_fin_timeout);\t//在拿到rto\tconst int rto = inet_csk(sk)-&gt;icsk_rto;\t//如果基础超时时间 fin_timeout小于3.5 * RTO 则设置为3.5个rto \t// 大概率不会进来吧 \tif (fin_timeout &lt; (rto &lt;&lt; 2) - (rto &gt;&gt; 1))\t\tfin_timeout = (rto &lt;&lt; 2) - (rto &gt;&gt; 1);\treturn fin_timeout;&#125;\n\n上述代码为定时器到期的回调函数，其实就分为两种情况，第一种情况，也就是绝大多数情况，用户没有设置linger2(注意这里要区分linger)，则直接发送rst报文，第二种情况，如果设置了linger2则大概率会使用配置的时间减去TCP_TIMEWAIT_LEN（60s）计算得到一个超时时间，然后调用tcp_time_wait而不是发送rst报文，如果在这个超时时间内收到了对端的fin则正常走timewait流程否则这个超时时间到了之后就是直接释放资源了。\n__tcp_close中sock_orphan会设置SOCK_DEAD，具体代码如下所示：\nvoid __tcp_close(struct sock *sk, long timeout)&#123;\t...\tsock_orphan(sk);\t...&#125;static inline void sock_orphan(struct sock *sk)&#123;\twrite_lock_bh(&amp;sk-&gt;sk_callback_lock);\tsock_set_flag(sk, SOCK_DEAD);\tsk_set_socket(sk, NULL);\tsk-&gt;sk_wq  = NULL;\twrite_unlock_bh(&amp;sk-&gt;sk_callback_lock);&#125;\n\nFIN_WATI2定时器的激活收包处理函数tcp_rcv_state_process在从TCP_FIN_WAIT1进入TCP_FIN_WAIT2状态时，会激活 fin_wait2定时器，具体代码如下所示：\nint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)&#123;\t...\tcase TCP_FIN_WAIT1: &#123;\t\tint tmo;\t\tif (req)\t\t\ttcp_rcv_synrecv_state_fastopen(sk);\t\tif (tp-&gt;snd_una != tp-&gt;write_seq)\t\t\tbreak;\t\ttcp_set_state(sk, TCP_FIN_WAIT2);\t\tWRITE_ONCE(sk-&gt;sk_shutdown, sk-&gt;sk_shutdown | SEND_SHUTDOWN);\t\tsk_dst_confirm(sk);\t\t//如果在close中还没有设置dead 那这里就直接推出了！ 什么情况下还么没有设置dead？ 当启用linger的时候（注意区分linger2）\t\tif (!sock_flag(sk, SOCK_DEAD)) &#123;\t\t\t/* Wake up lingering close() */\t\t\tsk-&gt;sk_state_change(sk);\t\t\tbreak;\t\t&#125;\t\tif (READ_ONCE(tp-&gt;linger2) &lt; 0) &#123;\t\t\ttcp_done(sk);\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\t\t\treturn 1;\t\t&#125;\t\tif (TCP_SKB_CB(skb)-&gt;end_seq != TCP_SKB_CB(skb)-&gt;seq &amp;&amp;\t\t    after(TCP_SKB_CB(skb)-&gt;end_seq - th-&gt;fin, tp-&gt;rcv_nxt)) &#123;\t\t\t/* Receive out of order FIN after close() */\t\t\tif (tp-&gt;syn_fastopen &amp;&amp; th-&gt;fin)\t\t\t\ttcp_fastopen_active_disable(sk);\t\t\ttcp_done(sk);\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\t\t\treturn 1;\t\t&#125;\t\t/* 这里的代码逻辑如下：首先是围绕这个tmo来处理，这个tmo的值简单可以概括成用户是否配置，如果配置了那就要看这个值大还是小，如果没有配置，那就是\t\t走正常1分钟超时逻辑。如果用户配置了linger2 且 大于1分钟的话，那就先启动fin_wait2d定时器，注意这个定时器到期的时间时配置的时间减去一分钟，然后在这样就会先在 FIN_WAIT2 等 tmo - TIMEWAIT_LEN，\t\t到点再进 TIME_WAIT*/\t\t//用户配置的时间，或者是默认的60s\t\ttmo = tcp_fin_time(sk);\t\t//tmo超过60s的情况\t\tif (tmo &gt; TCP_TIMEWAIT_LEN) &#123;\t\t\t//fin_wait2定时器\t\t\tinet_csk_reset_keepalive_timer(sk, tmo - TCP_TIMEWAIT_LEN);\t\t&#125; else if (th-&gt;fin || sock_owned_by_user(sk)) &#123;\t\t\t/* Bad case. We could lose such FIN otherwise.\t\t\t * It is not a big problem, but it looks confusing\t\t\t * and not so rare event. We still can lose it now,\t\t\t * if it spins in bh_lock_sock(), but it is really\t\t\t * marginal case.\t\t\t */\t\t\t////对端的数据包带fin或者用户持有sock\t\t\t//fin_wait2定时器\t\t\tinet_csk_reset_keepalive_timer(sk, tmo);\t\t&#125; else &#123;\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\t\t\tgoto consume;\t\t&#125;\t\tbreak;\t&#125;\t...&#125;\n\n注意到上述激活重传定时器的前提的是sock的标志为dead ，与上面一样，这个标志位也是__tcp_close中设置的，主要的逻辑也可以分为两种情况\n第一种情况，用户没有设置linger2的时间，或者设置linger2的时间小于一分钟，则定时器默认时间为1分钟，会走else if 或者else分支（取决于用户是否持有sock或者也是一个fin包）。\n第二种情况，如果用户设置了linger2的时间，且大于TCP_TIMEWAIT_LEN（60s），则定时器的时间设置为配置的时间减去一分钟的时间。\n除了上述tcp_rcv_state_process会激活FIN_WAIT2定时器，__tcp_close中也会激活定时器\nvoid __tcp_close(struct sock *sk, long timeout)&#123;  \t...\tsk_stream_wait_close(sk, timeout);    \t...\tif (sk-&gt;sk_state == TCP_FIN_WAIT2) &#123;\t\tstruct tcp_sock *tp = tcp_sk(sk);\t\tif (READ_ONCE(tp-&gt;linger2) &lt; 0) &#123;\t\t\ttcp_set_state(sk, TCP_CLOSE);\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\t\t\t__NET_INC_STATS(sock_net(sk),\t\t\t\t\tLINUX_MIB_TCPABORTONLINGER);\t\t&#125; else &#123;\t\t\tconst int tmo = tcp_fin_time(sk);\t\t\tif (tmo &gt; TCP_TIMEWAIT_LEN) &#123;\t\t\t\tinet_csk_reset_keepalive_timer(sk,\t\t\t\t\t\ttmo - TCP_TIMEWAIT_LEN);\t\t\t&#125; else &#123;\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\t\t\t\tgoto out;\t\t\t&#125;\t\t&#125;\t&#125;&#125;\n\n上述代码与tcp_rcv_state_process类似，都有激活定时器的处理，但是到底是哪里激活这个定时器呢？应该取决于close是否阻塞在\nsk_stream_wait_close？？？如果阻塞了则大概率是__tcp_close中激活定时器，因为这个时候sock还没有被orphan。如果没有阻塞那可能就不一定了吧。这里不太清楚。感觉大概率不会是这里启动的了\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP定时器"]},{"title":"RACK定时器","url":"/2025/08/22/RACK%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"RACK定时器背景传统 TCP 的重传主要靠 RTO（超时重传）和 DupACK（快速重传）：\n\nRTO：基于 RTT 的超时，触发比较慢；\nDupACK：需要多个重复 ACK 才能触发，对乱序&#x2F;丢包检测不灵敏。\n\nRACK 的核心思想\n基本原则：只要某个数据段比“最近被确认的段”更早发送出去，但至今还没有被确认，就可以认为它可能丢失。\n使用 发送时间戳 来比较，而不是依赖 DupACK 计数。\n\n例如：\n\nS1, S2, S3, S4 四个包按顺序发送；\n假如收到 ACK 确认了 S4，但 S2 还没确认；\n那么 RACK 就能推断：S2 可能丢了（因为比 S4 更早发出，按理说应该早就确认了）。\n\nRACK与TLP等定时器类似，与重传共用一个定时器，定时器到期的回调函数如下所示：\nvoid tcp_rack_reo_timeout(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 timeout, prior_inflight;\tu32 lost = tp-&gt;lost;\t//计算在网络中飞行的包的数量\tprior_inflight = tcp_packets_in_flight(tp);\t//遍历按时间排序的数据包队列，根据时间戳，会将数据包标记为loss\ttcp_rack_detect_loss(sk, &amp;timeout);\t//如果现在这两个值不相等了，则证明认为有包丢失了，如果此时不再快速恢复状态则恩据到快速恢复状态，并调用注册的拥塞算法的钩子\tif (prior_inflight != tcp_packets_in_flight(tp)) &#123;\t\tif (inet_csk(sk)-&gt;icsk_ca_state != TCP_CA_Recovery) &#123;\t\t\ttcp_enter_recovery(sk, false);\t\t\tif (!inet_csk(sk)-&gt;icsk_ca_ops-&gt;cong_control)\t\t\t\ttcp_cwnd_reduction(sk, 1, tp-&gt;lost - lost, 0);\t\t&#125;\t\t//重传数据包\t\ttcp_xmit_retransmit_queue(sk);\t&#125;\t//设置了重传定时器，用来兜底？\tif (inet_csk(sk)-&gt;icsk_pending != ICSK_TIME_RETRANS)\t\ttcp_rearm_rto(sk);&#125;\n\n上述代码首先先计算在网络中’‘飞行的数据包的数量’‘，然后遍历按时间排序的数据包队列（发包的时候会放入这个队列中），然后根据时间戳标记可能丢失的数据包，然后重新计算网络中飞行的数据包数量，如果不相等了（因为有数据包被认为丢失了），就进入重传逻辑。\n标记可能丢失的逻辑tcp_rack_detect_loss如下所示：\nstatic void  tcp_rack_detect_loss(struct sock *sk, u32 *reo_timeout)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb, *n;\tu32 reo_wnd;\t*reo_timeout = 0;\t//reo_wnd的单位为时间，返回0或者根据rtt算一个值，取决于统计乱续数据包的字段\treo_wnd = tcp_rack_reo_wnd(sk);\t//遍历已经发送按按时间排序的数据包队列\tlist_for_each_entry_safe(skb, n, &amp;tp-&gt;tsorted_sent_queue,\t\t\t\t tcp_tsorted_anchor) &#123;\t\tstruct tcp_skb_cb *scb = TCP_SKB_CB(skb);\t\ts32 remaining;\t\t/* Skip ones marked lost but not yet retransmitted */\t\t//跳过已经标记丢失的数据包\t\tif ((scb-&gt;sacked &amp; TCPCB_LOST) &amp;&amp;\t\t    !(scb-&gt;sacked &amp; TCPCB_SACKED_RETRANS))\t\t\tcontinue;\t\t//返回false的条件是，当前数据包的时间戳在当前rack时间戳的前面。。。，表示没问题。所以直接beak 合理\t\tif (!tcp_skb_sent_after(tp-&gt;rack.mstamp,\t\t\t\t\ttcp_skb_timestamp_us(skb),\t\t\t\t\ttp-&gt;rack.end_seq, scb-&gt;end_seq))\t\t\tbreak;\t\t/* A packet is lost if it has not been s/acked beyond\t\t * the recent RTT plus the reordering window.\t\t */\t\t//内部根据rtt和上面算出来到窗口时间 减去这个数据包已经发出取得时间得到一个超时时间\t\tremaining = tcp_rack_skb_timeout(tp, skb, reo_wnd);\t\tif (remaining &lt;= 0) &#123;\t\t\t//没有剩余时间了，直接标记为丢失数据包\t\t\ttcp_mark_skb_lost(sk, skb);\t\t\t//这里把数据包从按时间排序的队列中移除了？？？，那需要重传的包在哪拿到呢？貌似是重传队列\t\t\tlist_del_init(&amp;skb-&gt;tcp_tsorted_anchor);\t\t&#125; else &#123;\t\t\t/* Record maximum wait time */\t\t\t//传入传出这个timeout\t\t\t*reo_timeout = max_t(u32, *reo_timeout, remaining);\t\t&#125;\t&#125;&#125;\n\n上述代码为检测是否有丢失的逻辑，首先调用tcp_rack_reo_wnd获取一个时间窗口，这个时间窗口就是用来计算下面超时时间，会因为是否存在数据包乱序影响计算结果，如果不存在乱序，就直接返回0了（返回0表示数据包更有可能被标记为丢失），否则返回根据rtt计算得到的时间。\ntcp_rack_reo_wnd 具体逻辑如下所示：\nstatic u32 tcp_rack_reo_wnd(const struct sock *sk)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\t//是否存在乱续，由tcp_check_sack_reordering设置\tif (!tp-&gt;reord_seen) &#123;\t\t/* If reordering has not been observed, be aggressive during\t\t * the recovery or starting the recovery by DUPACK threshold.\t\t */\t\t//快恢复或者是loss，直接返回0 表示乱续容忍时间窗口是0\t\tif (inet_csk(sk)-&gt;icsk_ca_state &gt;= TCP_CA_Recovery)\t\t\treturn 0;\t\t//sack确认的数量比乱需容忍的值要大  同时没有禁用重复ack 直接返回0 \t\tif (tp-&gt;sacked_out &gt;= tp-&gt;reordering &amp;&amp;\t\t    !(READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_recovery) &amp;\t\t      TCP_RACK_NO_DUPTHRESH))\t\t\treturn 0;\t&#125;\t/* To be more reordering resilient, allow min_rtt/4 settling delay.\t * Use min_rtt instead of the smoothed RTT because reordering is\t * often a path property and less related to queuing or delayed ACKs.\t * Upon receiving DSACKs, linearly increase the window up to the\t * smoothed RTT.\t */\treturn min((tcp_min_rtt(tp) &gt;&gt; 2) * tp-&gt;rack.reo_wnd_steps,\t\t   tp-&gt;srtt_us &gt;&gt; 3);&#125;\n\n进入上述代码逻辑的条件是reord_seen为0 而更新这个字段的函数为tcp_check_sack_reordering在拥塞控制，sack处理，ack处理中会调用这个函数。具体代码如下所示，主要逻辑就是用来检测是否数据包乱序，如果存在乱序数据包，会更新reord_seen这个字段，同时更新统计信息：\nstatic void tcp_check_sack_reordering(struct sock *sk, const u32 low_seq,\t\t\t\t      const int ts)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tconst u32 mss = tp-&gt;mss_cache;\tu32 fack, metric;\t//fack 为当前选择确认中最大的序列号\tfack = tcp_highest_sack_seq(tp);\t//如果待检查的序列号已经在选择确认最高的后面，则直接返回\tif (!before(low_seq, fack))\t\treturn;\t//可能丢包的范围？\tmetric = fack - low_seq;\tif ((metric &gt; tp-&gt;reordering * mss) &amp;&amp; mss) &#123;#if FASTRETRANS_DEBUG &gt; 1\t\tpr_debug(&quot;Disorder%d %d %u f%u s%u rr%d\\n&quot;,\t\t\t tp-&gt;rx_opt.sack_ok, inet_csk(sk)-&gt;icsk_ca\t\t\t _state,\t\t\t tp-&gt;reordering,\t\t\t 0,\t\t\t tp-&gt;sacked_out,\t\t\t tp-&gt;undo_marker ? tp-&gt;undo_retrans : 0);#endif\t\t//算一下有几个mss和默认值300取一个最小值，reordering的单位为最大乱续包的数量，会决定收到几个ack快速重传吗？\t\ttp-&gt;reordering = min_t(u32, (metric + mss - 1) / mss,\t\t\t\t       READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_max_reordering));\t&#125;\t/* This exciting event is worth to be remembered. 8) */\t//乱续事件计数\ttp-&gt;reord_seen++;\tNET_INC_STATS(sock_net(sk),\t\t      ts ? LINUX_MIB_TCPTSREORDER : LINUX_MIB_TCPSACKREORDER);&#125;\n\n回到上面计算时间窗口的逻辑，计算完成后，会遍历按时排序的数据包队列，如果当前遍历到的数据包，如果当前的rtt加上计算出来的时间窗口（也叫做容忍时间吧）减去，这个数据包发给的时间，如果小于等于0直接调用tcp_mark_skb_lost标记为丢失，具体代码如下所示：\n//标记skb为丢失，并且更新相关字段 enterloss中会调用，tcp超时重传中会调用，rack中会调用void tcp_mark_skb_lost(struct sock *sk, struct sk_buff *skb)&#123;\t//拿到sack的标志位\t__u8 sacked = TCP_SKB_CB(skb)-&gt;sacked;\tstruct tcp_sock *tp = tcp_sk(sk);\t//如果数据包已经被确认了，直接返回不用标记丢失\tif (sacked &amp; TCPCB_SACKED_ACKED)\t\treturn;\ttcp_verify_retransmit_hint(tp, skb);\t//这个包已经丢过一次了\tif (sacked &amp; TCPCB_LOST) &#123;\t\t//是否重传过一次了\t\tif (sacked &amp; TCPCB_SACKED_RETRANS) &#123;\t\t\t/* Account for retransmits that are lost again */\t\t\t//需要重新重传了，清除掉重传标志\t\t\tTCP_SKB_CB(skb)-&gt;sacked &amp;= ~TCPCB_SACKED_RETRANS;\t\t\t//减少重重的统计激素\t\t\ttp-&gt;retrans_out -= tcp_skb_pcount(skb);\t\t\t//增加统计计数 这个表示重传后又丢包的数量\t\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPLOSTRETRANSMIT,\t\t\t\t      tcp_skb_pcount(skb));\t\t\t//更新历史丢包总数\t\t\ttcp_notify_skb_loss_event(tp, skb);\t\t&#125;\t&#125; else &#123;\t\t//增加 当前待重传的丢包数\t\ttp-&gt;lost_out += tcp_skb_pcount(skb);\t\t//设置丢包标志\t\tTCP_SKB_CB(skb)-&gt;sacked |= TCPCB_LOST;\t\t//更新历史丢包总数\t\ttcp_notify_skb_loss_event(tp, skb);\t&#125;&#125;\n\n回到上面最初的定时器到期的回调函数中，也就是遍历按时间排序的数据包队列，标记可能loss的数据包后，如果重新计算飞行的书包的数量，如果不相等了就遍历重传队列发送标记重传的数据包，注意：这里不像超时重传一样，而是只重传被标记为loss的数据包，具体代码如下所示：\nvoid tcp_rack_reo_timeout(struct sock *sk)&#123;...\tif (prior_inflight != tcp_packets_in_flight(tp)) &#123;...\t\t//重传数据包\t\ttcp_xmit_retransmit_queue(sk);\t&#125;...&#125;//注意： 这个函数貌似只重传丢失的数据包。void tcp_xmit_retransmit_queue(struct sock *sk)&#123;\tconst struct inet_connection_sock *icsk = inet_csk(sk);\tstruct sk_buff *skb, *rtx_head, *hole = NULL;\tstruct tcp_sock *tp = tcp_sk(sk);\tbool rearm_timer = false;\tu32 max_segs;\tint mib_idx;\t//没有未确认的数据包，直接返回\tif (!tp-&gt;packets_out)\t\treturn;\t//拿到重传队列的头\trtx_head = tcp_rtx_queue_head(sk);\t//是否有能够快速定位的能重传的数据包，比如说在rack的重传逻辑中就会设置\tskb = tp-&gt;retransmit_skb_hint ?: rtx_head;\t//计算一个最大的段数\tmax_segs = tcp_tso_segs(sk, tcp_current_mss(sk));\t//从上面拿到的skb开始遍历重传队列\tskb_rbtree_walk_from(skb) &#123;\t\t__u8 sacked;\t\tint segs;\t\t//pacing相关， 是否需要直接break，tcp_write_xmit也会调用\t\tif (tcp_pacing_check(sk))\t\t\tbreak;\t\t/* we could do better than to assign each time */\t\t//用来避免每次都从开始遍历这个重传队列\t\tif (!hole)\t\t\ttp-&gt;retransmit_skb_hint = skb;\t\t//地一个单位是段，第二个是包吧，这里相减得到的还是段？\t\tsegs = tcp_snd_cwnd(tp) - tcp_packets_in_flight(tp);\t\tif (segs &lt;= 0)\t\t\tbreak;\t\tsacked = TCP_SKB_CB(skb)-&gt;sacked;\t\t/* In case tcp_shift_skb_data() have aggregated large skbs,\t\t * we need to make sure not sending too bigs TSO packets\t\t */\t\t//取一个最小值，不要超过tso的最大段数\t\tsegs = min_t(int, segs, max_segs);\t\t//已经重传的包数都超过了待重传的包数，那就直接break了\t\tif (tp-&gt;retrans_out &gt;= tp-&gt;lost_out) &#123;\t\t\tbreak;\t\t//如果未标记为丢失，这里直接continue\t\t&#125; else if (!(sacked &amp; TCPCB_LOST)) &#123;\t\t\tif (!hole &amp;&amp; !(sacked &amp; (TCPCB_SACKED_RETRANS|TCPCB_SACKED_ACKED)))\t\t\t\thole = skb;\t\t\tcontinue;\t\t//设置需要更新哪个统计计数\t\t&#125; else &#123;\t\t\tif (icsk-&gt;icsk_ca_state != TCP_CA_Loss)\t\t\t\tmib_idx = LINUX_MIB_TCPFASTRETRANS;\t\t\telse\t\t\t\tmib_idx = LINUX_MIB_TCPSLOWSTARTRETRANS;\t\t&#125;\t\t//如果已经被sack了，或者已经重传过了 直接continue\t\tif (sacked &amp; (TCPCB_SACKED_ACKED|TCPCB_SACKED_RETRANS))\t\t\tcontinue;\t\t//tsq相关，tcp_write_xmit中也会调用\t\tif (tcp_small_queue_check(sk, skb, 1))\t\t\tbreak;\t\t//真正调用发包函数，重传多少个段\t\tif (tcp_retransmit_skb(sk, skb, segs))\t\t\tbreak;\t\tNET_ADD_STATS(sock_net(sk), mib_idx, tcp_skb_pcount(skb));\t\t//快恢复状态返回true，这里的快恢复状态应该是外面设置\t\tif (tcp_in_cwnd_reduction(sk))\t\t\ttp-&gt;prr_out += tcp_skb_pcount(skb);\t\t//如果当前数据包是重传队列的地一个数据包，表示丢包概率比较大？\t\t//同时如果没有设置rack定时器？，就要设置超时重传？\t\tif (skb == rtx_head &amp;&amp;\t\t    icsk-&gt;icsk_pending != ICSK_TIME_REO_TIMEOUT)\t\t\trearm_timer = true;\t&#125;\tif (rearm_timer)\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\t\t\t\t     inet_csk(sk)-&gt;icsk_rto,\t\t\t\t     TCP_RTO_MAX);&#125;\n\ntcp_xmit_retransmit_queue中首先遍历重传队列，会首先检查pacing，tsq机制，或重传出去的数据包是否已经超过待重传的数据包，如果满足条件按，则直接就break了，如果当前数据包没有被标记为需要重传，则直接continue，否则直接调用tcp_retransmit_skb重传数据包。如果重传的数据包是第一个数据包，则还需要启动重传定时器，因为第一个包就要重传？表示很有可能大概率丢包？\nRACK定时器的启动在tcpack的拥塞处理中，会启动rack定时器\n//tcpack中最终会调用bool tcp_rack_mark_lost(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 timeout;\t//tcp_rack_advance 中设置\tif (!tp-&gt;rack.advanced)\t\treturn false;\t/* Reset the advanced flag to avoid unnecessary queue scanning */\ttp-&gt;rack.advanced = 0;\t//标记丢失的数据包，拿到timeout\ttcp_rack_detect_loss(sk, &amp;timeout);\tif (timeout) &#123;\t\ttimeout = usecs_to_jiffies(timeout + TCP_TIMEOUT_MIN_US);\t\t//启动RACK定时器\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_REO_TIMEOUT,\t\t\t\t\t  timeout, inet_csk(sk)-&gt;icsk_rto);\t&#125;\treturn !!timeout;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP定时器"]},{"title":"Pacing定时器","url":"/2025/08/25/TCP%20pacing%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"pacing定时器TCP 的 pacing 定时器用于在缺少支持 pacing 的队列规则时，由 TCP 内部高精度定时器控制发包间隔。它根据拥塞控制算法计算出的速率生成未来发送时间戳，如果时间未到则挂起 hrtimer，到期后回调继续发包，从而保证出包平滑、避免突发，提升链路利用率和时延表现。\npacing定时器的初始化pacing定时器的初始化在tcp_init_xmit_timers（与重传定时器，延迟ack定时器，保活探测定时器位置相同）中，初始化与定时器到期的回调函数（tcp_pace_kick）如下所示：\nvoid tcp_init_xmit_timers(struct sock *sk)&#123;\thrtimer_init(&amp;tcp_sk(sk)-&gt;pacing_timer, CLOCK_MONOTONIC,\t\t     HRTIMER_MODE_ABS_PINNED_SOFT);\ttcp_sk(sk)-&gt;pacing_timer.function = tcp_pace_kick;&#125;enum hrtimer_restart tcp_pace_kick(struct hrtimer *timer)&#123;\tstruct tcp_sock *tp = container_of(timer, struct tcp_sock, pacing_timer);\tstruct sock *sk = (struct sock *)tp;\ttcp_tsq_handler(sk);\tsock_put(sk);\treturn HRTIMER_NORESTART;&#125;\n\n上述定时器到期后会调用tcp_tsq_handler，具体代码如下所示：\nstatic void tcp_tsq_handler(struct sock *sk)&#123;\tbh_lock_sock(sk);\tif (!sock_owned_by_user(sk))\t\ttcp_tsq_write(sk);\telse if (!test_and_set_bit(TCP_TSQ_DEFERRED, &amp;sk-&gt;sk_tsq_flags))\t\tsock_hold(sk);\tbh_unlock_sock(sk);&#125;\n\n如果此时用户没有持有sock则继续调用tcp_tsq_write进一步处理\nstatic void tcp_tsq_write(struct sock *sk)&#123;\t//是否处于可以发包的状态\tif ((1 &lt;&lt; sk-&gt;sk_state) &amp;\t    (TCPF_ESTABLISHED | TCPF_FIN_WAIT1 | TCPF_CLOSING |\t     TCPF_CLOSE_WAIT  | TCPF_LAST_ACK)) &#123;\t\tstruct tcp_sock *tp = tcp_sk(sk);\t\t//如果标记丢失的包数多于已经重传出去的包数，且拥塞窗口的大小还够用就先发重传包\t\tif (tp-&gt;lost_out &gt; tp-&gt;retrans_out &amp;&amp; //计算的结果为存在丢失，但还没重传的数量\t\t    tcp_snd_cwnd(tp) &gt; tcp_packets_in_flight(tp)) &#123;\t\t\ttcp_mstamp_refresh(tp);\t\t\ttcp_xmit_retransmit_queue(sk);\t\t&#125;\t\t//正常发包函数\t\ttcp_write_xmit(sk, tcp_current_mss(sk), tp-&gt;nonagle,\t\t\t       0, GFP_ATOMIC);\t&#125;&#125;\n\n上述代码逻辑很简单，首先判断是否处于可以发送数据包的状态，然后如果有需要重传的数据包，同时窗口大小足够大的话，则优先发送重传队列中的数据包，然后发送调用tcp_write_xmit发送正常的数据包。\npacing定时器的启动pacing定时器的启动由tcp_pacing_check完成，在tcp_write_xmit会调用，具体代码如下所示：\n//tcp_write_xnmit 和重传数据包中会调用，返回false表示立即发送static bool tcp_pacing_check(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//sch_fq 开启这个就会返回false，表示不需要tcp内部自己pacing ，反之，如果为brr这里就不进入这个分支\tif (!tcp_needs_internal_pacing(sk))\t\treturn false;\t//下一个数据包要发送的时间小于等于当前的时钟，则可以直接发送，不需要启动定时器\tif (tp-&gt;tcp_wstamp_ns &lt;= tp-&gt;tcp_clock_cache)\t\treturn false;\t//如果定时器还没在排队，就启动一个 hrtimer\tif (!hrtimer_is_queued(&amp;tp-&gt;pacing_timer)) &#123;\t\thrtimer_start(&amp;tp-&gt;pacing_timer,\t\t\t      ns_to_ktime(tp-&gt;tcp_wstamp_ns),\t\t\t      HRTIMER_MODE_ABS_PINNED_SOFT);\t\tsock_hold(sk);//这里引用计数++，防止socket被提前释放\t&#125;\treturn true;&#125;\n\ntcp_pacing_check逻辑很简单，首先判断是否开启pacing，比如tc模块启用sch_fq则代码不会进入下面的逻辑，如果使用的是bbr拥塞算法，则会进入下面的逻辑。下面的逻辑为，如果当前待发送数据包的时间戳（tp-&gt;tcp_wstamp_ns）小于等于当前的时间戳，则直接返回false，表示不需要启动重传定时器。否则如果没有启动pacing定时器则启动pacing定时器。\n上述设置时间戳tp-&gt;tcp_wstamp_ns在tcp_update_skb_after_send中，每次调用__tcp_transmit_skb后会调用，具体代码如下所示：\n//pacing相关，以及把数据包报道按时间排序的队列中static void tcp_update_skb_after_send(struct sock *sk, struct sk_buff *skb,\t\t\t\t      u64 prior_wstamp)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//是否启用pacing，注意，在TCP层面，默认是不启用的（如果使用了bbr拥塞算法，就启用）或者setsocktopt可以开启。\tif (sk-&gt;sk_pacing_status != SK_PACING_NONE) &#123;\t\t//这个是用户配置的单位是多少字节每秒，或者是bbr配置的\t\tunsigned long rate = sk-&gt;sk_pacing_rate;\t\t/* Original sch_fq does not pace first 10 MSS\t\t * Note that tp-&gt;data_segs_out overflows after 2^32 packets,\t\t * this is a minor annoyance.\t\t */\t\t//如果rate值有效同时发出去10个tcp段以上了则更具用户配置更新tcp_wstamp_ns也就是下一次待发送的时间戳\t\tif (rate != ~0UL &amp;&amp; rate &amp;&amp; tp-&gt;data_segs_out &gt;= 10) &#123;\t\t\tu64 len_ns = div64_ul((u64)skb-&gt;len * NSEC_PER_SEC, rate);\t\t\tu64 credit = tp-&gt;tcp_wstamp_ns - prior_wstamp;\t\t\t/* take into account OS jitter */\t\t\tlen_ns -= min_t(u64, len_ns / 2, credit);\t\t\t//pacingcheck会用到\t\t\ttp-&gt;tcp_wstamp_ns += len_ns;\t\t&#125;\t&#125;\t//将数据包加入到按时间排序的队列中，rack会用到\tlist_move_tail(&amp;skb-&gt;tcp_tsorted_anchor, &amp;tp-&gt;tsorted_sent_queue);&#125;\n\ntcp_update_skb_after_send中首先判断是否启用pacing，之后拿到pacing速率，如果是前十个数据包则不限速，目的是快速建立连接，之后依据速率把数据长度换算成应当等待的时间，然后更新下一次可发时间。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP定时器"]},{"title":"TCP建立连接bind(一)","url":"/2025/08/27/TCP%20bind/","content":"TCP bindTCP 的bind用于将一个套接字与本地的 IP 地址和端口号进行显式绑定，从而在内核中建立地址与套接字的对应关系。服务器端通常通过 bind 指定固定端口，以便客户端能够通过该端口发起连接；客户端若未显式调用 bind，内核会在 connect 时自动分配一个临时端口。需要注意的是，bind 仅涉及内核数据结构的操作，不会产生任何网络报文。\n用户态程序通过glibc调用bind系统调用后，最终会调用到内核的__sys_bind由它完成套接字与地址的绑定逻辑，具体代码如下所示：\nint __sys_bind(int fd, struct sockaddr __user *umyaddr, int addrlen)&#123;\tstruct socket *sock;\tstruct sockaddr_storage address;\tint err, fput_needed;\t//通过fd 查找到file，通过file返回私有指针，这个私有指针就是socket\tsock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed);\tif (sock) &#123;\t\t//copy_from_user\t\terr = move_addr_to_kernel(umyaddr, addrlen, &amp;address);\t\tif (!err) &#123;\t\t\terr = security_socket_bind(sock,\t\t\t\t\t\t   (struct sockaddr *)&amp;address,\t\t\t\t\t\t   addrlen);\t\t\tif (!err)\t\t\t\t//调用具体sock类型的ops\t\t\t\terr = READ_ONCE(sock-&gt;ops)-&gt;bind(sock,\t\t\t\t\t\t      (struct sockaddr *)\t\t\t\t\t\t      &amp;address, addrlen);\t\t&#125;\t\tfput_light(sock-&gt;file, fput_needed);\t&#125;\treturn err;&#125;\n\n上述代码通过用户fd找到对应的socket结构，然后将用户传入的端口和地址拷贝到内核态，然后调用具体协议族的bind回调，对于AF_INET就是调用的inet_bind，inet_bind实现如下所示：\nint inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)&#123;\treturn inet_bind_sk(sock-&gt;sk, uaddr, addr_len);&#125;int inet_bind_sk(struct sock *sk, struct sockaddr *uaddr, int addr_len)&#123;\tu32 flags = BIND_WITH_LOCK;\tint err;\t/* If the socket has its own bind function then use it. (RAW) */\t//判断有没有bind， tcp是没有的\tif (sk-&gt;sk_prot-&gt;bind) &#123;\t\treturn sk-&gt;sk_prot-&gt;bind(sk, uaddr, addr_len);\t&#125;\tif (addr_len &lt; sizeof(struct sockaddr_in))\t\treturn -EINVAL;\t/* BPF prog is run before any checks are done so that if the prog\t * changes context in a wrong way it will be caught.\t */\t//BPF相关的钩子\terr = BPF_CGROUP_RUN_PROG_INET_BIND_LOCK(sk, uaddr,\t\t\t\t\t\t CGROUP_INET4_BIND, &amp;flags);\tif (err)\t\treturn err;\treturn __inet_bind(sk, uaddr, addr_len, flags);&#125;\n\ninet_bind中调用了inet_bind_sk，首先判断当前的sock是否有bind回调的实现，这里要注意，tcp是没有实现bind回调的，因此继续向下走，调用__inet_bind具体代码如下所示：\nint __inet_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len,\t\tu32 flags)&#123;\tstruct sockaddr_in *addr = (struct sockaddr_in *)uaddr;\tstruct inet_sock *inet = inet_sk(sk);\tstruct net *net = sock_net(sk);\tunsigned short snum;\tint chk_addr_ret;\tu32 tb_id = RT_TABLE_LOCAL;\tint err;\t//判断协议族是否正确\tif (addr-&gt;sin_family != AF_INET) &#123;\t\t/* Compatibility games : accept AF_UNSPEC (mapped to AF_INET)\t\t * only if s_addr is INADDR_ANY.\t\t */\t\terr = -EAFNOSUPPORT;\t\tif (addr-&gt;sin_family != AF_UNSPEC ||\t\t    addr-&gt;sin_addr.s_addr != htonl(INADDR_ANY))\t\t\tgoto out;\t&#125;\t//这里返回了上面的local表\ttb_id = l3mdev_fib_table_by_index(net, sk-&gt;sk_bound_dev_if) ? : tb_id;\t//调用查路由的接口，返回地址类型\tchk_addr_ret = inet_addr_type_table(net, addr-&gt;sin_addr.s_addr, tb_id);\t/* Not specified by any standard per-se, however it breaks too\t * many applications when removed.  It is unfortunate since\t * allowing applications to make a non-local bind solves\t * several problems with systems using dynamic addressing.\t * (ie. your servers still start up even if your ISDN link\t *  is temporarily down)\t */\terr = -EADDRNOTAVAIL;\t//注意：用户的地址是否可以绑定，注意这里如果开启了系统选项（或者setsockopt），是可以绑定非本机地址的\tif (!inet_addr_valid_or_nonlocal(net, inet, addr-&gt;sin_addr.s_addr,\t                                 chk_addr_ret))\t\tgoto out;\t//获取用户bind 的port\tsnum = ntohs(addr-&gt;sin_port);\terr = -EACCES;\t//这里的flags为2 inet_port_requires_bind_service//为判断是否可以绑定1024一下的端口\t//最后判断是否有权限绑定这个端口\tif (!(flags &amp; BIND_NO_CAP_NET_BIND_SERVICE) &amp;&amp;\t    snum &amp;&amp; inet_port_requires_bind_service(net, snum) &amp;&amp;\t    !ns_capable(net-&gt;user_ns, CAP_NET_BIND_SERVICE))\t\tgoto out;\t/*      We keep a pair of addresses. rcv_saddr is the one\t *      used by hash lookups, and saddr is used for transmit.\t *\t *      In the BSD API these are the same except where it\t *      would be illegal to use them (multicast/broadcast) in\t *      which case the sending device address is used.\t */\tif (flags &amp; BIND_WITH_LOCK)\t\tlock_sock(sk);\t/* Check these errors (active socket, double bind). */\terr = -EINVAL;\tif (sk-&gt;sk_state != TCP_CLOSE || inet-&gt;inet_num)\t\tgoto out_release_sock;\t//注意:这里设置了源地址\tinet-&gt;inet_rcv_saddr = inet-&gt;inet_saddr = addr-&gt;sin_addr.s_addr;\t//如果用户配下来的地址类型为多播或者广播就使用设备的地址？\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\t\tinet-&gt;inet_saddr = 0;  /* Use device */\t/* Make sure we are allowed to bind here. */\tif (snum || !(inet_test_bit(BIND_ADDRESS_NO_PORT, sk) ||\t\t      (flags &amp; BIND_FORCE_ADDRESS_NO_PORT))) &#123;\t\t//调用具体协议的getport！\t\terr = sk-&gt;sk_prot-&gt;get_port(sk, snum);\t\tif (err) &#123;\t\t\tinet-&gt;inet_saddr = inet-&gt;inet_rcv_saddr = 0;\t\t\tgoto out_release_sock;\t\t&#125;\t\t//bpf相关\t\tif (!(flags &amp; BIND_FROM_BPF)) &#123;\t\t\terr = BPF_CGROUP_RUN_PROG_INET4_POST_BIND(sk);\t\t\tif (err) &#123;\t\t\t\tinet-&gt;inet_saddr = inet-&gt;inet_rcv_saddr = 0;\t\t\t\tif (sk-&gt;sk_prot-&gt;put_port)\t\t\t\t\tsk-&gt;sk_prot-&gt;put_port(sk);\t\t\t\tgoto out_release_sock;\t\t\t&#125;\t\t&#125;\t&#125;\tif (inet-&gt;inet_rcv_saddr)\t\tsk-&gt;sk_userlocks |= SOCK_BINDADDR_LOCK;\tif (snum)\t\tsk-&gt;sk_userlocks |= SOCK_BINDPORT_LOCK;\t//设置了源port 这个是不是从上面getport获取的？\tinet-&gt;inet_sport = htons(inet-&gt;inet_num);\tinet-&gt;inet_daddr = 0;\tinet-&gt;inet_dport = 0;\t//先复位路由相关的信息\tsk_dst_reset(sk);\terr = 0;out_release_sock:\tif (flags &amp; BIND_WITH_LOCK)\t\trelease_sock(sk);out:\treturn err;&#125;\n\n__inet_bind中首先进行一系列检查，其中会调用inet_addr_type_table来获取要bind的ip地址类型（注意：通过查local路由表），然后会根据返回的类型进一步判断是否容许绑定，具体代码如下所示：\nunsigned int inet_addr_type_table(struct net *net, __be32 addr, u32 tb_id)&#123;\treturn __inet_dev_addr_type(net, NULL, addr, tb_id);&#125;static inline unsigned int __inet_dev_addr_type(struct net *net,\t\t\t\t\t\tconst struct net_device *dev,\t\t\t\t\t\t__be32 addr, u32 tb_id)&#123;\tstruct flowi4\t\tfl4 = &#123; .daddr = addr &#125;;\tstruct fib_result\tres;\tunsigned int ret = RTN_BROADCAST;\tstruct fib_table *table;\t//快速判断是否是全0或者全255（有限广播地址）\tif (ipv4_is_zeronet(addr) || ipv4_is_lbcast(addr))\t\treturn RTN_BROADCAST;\t//判断是否是多播\tif (ipv4_is_multicast(addr))\t\treturn RTN_MULTICAST;\trcu_read_lock();\t//根据id拿到路由表\ttable = fib_get_table(net, tb_id);\tif (table) &#123;\t\t//默认先设置为单播地址\t\tret = RTN_UNICAST;\t\t//这里直接调用查路路由的接口，fl4只有一个目的ip\t\tif (!fib_table_lookup(table, &amp;fl4, &amp;res, FIB_LOOKUP_NOREF)) &#123;\t\t\tstruct fib_nh_common *nhc = fib_info_nhc(res.fi, 0);\t\t\tif (!dev || dev == nhc-&gt;nhc_dev)\t\t\t\tret = res.type;//重新设置结果\t\t&#125;\t&#125;\trcu_read_unlock();\treturn ret;&#125;static inline bool inet_addr_valid_or_nonlocal(struct net *net,\t\t\t\t\t       struct inet_sock *inet,\t\t\t\t\t       __be32 addr,\t\t\t\t\t       int addr_type)&#123;\treturn inet_can_nonlocal_bind(net, inet) ||\t\taddr == htonl(INADDR_ANY) ||\t\taddr_type == RTN_LOCAL ||\t\taddr_type == RTN_MULTICAST ||\t\taddr_type == RTN_BROADCAST;&#125;\n\n之后会调用具体协议的get_port回调，对于TCP来说就是inet_csk_get_port，inet_csk_get_port主要做的工作就是分配端口，检测是否冲突。分配端口可以具体分为两种情况，一个用户没有指定具体的端口，另一个是用户指定了具体端口，不论那种情况，都会进行冲突检测（reuseport，repair模式，reusaddr等除外）。冲突检测的逻辑中，如果用户bind 的ip地址不是anyaddr(也就是具体的ip地址)则冲突检测首先判断是否与有anyaddr +port 的entry冲突，如果冲突了直接bind失败，如果没有冲突，则进一步从bhash2中检测是否存在冲突，如果不存在冲突则bind直接返回成功。注意：这里相较于低版本仅有的bhash的情况要加速了很多。\n上述具体代码如下所示：\nint inet_csk_get_port(struct sock *sk, unsigned short snum)&#123;\tstruct inet_hashinfo *hinfo = tcp_or_dccp_get_hashinfo(sk);\t//用户是否设置了reuseaddr\tbool reuse = sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN;\tbool found_port = false, check_bind_conflict = true;\tbool bhash_created = false, bhash2_created = false;\tint ret = -EADDRINUSE, port = snum, l3mdev;\tstruct inet_bind_hashbucket *head, *head2;\tstruct inet_bind2_bucket *tb2 = NULL;\tstruct inet_bind_bucket *tb = NULL;\tbool head2_lock_acquired = false;\tstruct net *net = sock_net(sk);\tl3mdev = inet_sk_bound_l3mdev(sk);\t//如果用户没有显示指定端口，走这个分支\tif (!port) &#123;\t\thead = inet_csk_find_open_port(sk, &amp;tb, &amp;tb2, &amp;head2, &amp;port);\t\tif (!head)\t\t\treturn ret;\t\thead2_lock_acquired = true;\t\tif (tb &amp;&amp; tb2)\t\t\tgoto success;\t\tfound_port = true;\t&#125; else &#123;\t\t//这里是用户指定了bind端口的情况，先拿到bhash的桶\t\thead = &amp;hinfo-&gt;bhash[inet_bhashfn(net, port,\t\t\t\t\t\t  hinfo-&gt;bhash_size)];\t\tspin_lock_bh(&amp;head-&gt;lock);\t\tinet_bind_bucket_for_each(tb, &amp;head-&gt;chain)\t\t\t//匹配port网络命名空间是否相等，这个是从bindhash中找\t\t\tif (inet_bind_bucket_match(tb, net, port, l3mdev))\t\t\t\tbreak;\t&#125;\t//条目是否为空，如果为空则创建一个entry\tif (!tb) &#123;\t\ttb = inet_bind_bucket_create(hinfo-&gt;bind_bucket_cachep, net,\t\t\t\t\t     head, port, l3mdev);\t\tif (!tb)\t\t\tgoto fail_unlock;\t\tbhash_created = true;\t&#125;\t//用户bind指定了具体的port就会走这个分支\tif (!found_port) &#123;\t\t/* ！！！下面五行代码判断是否可以直接判断跳过冲突检测 */\t\t//如果有其他socket被这个entry管理，就进入这个分支\t\tif (!hlist_empty(&amp;tb-&gt;owners)) &#123;\t\t\tif (sk-&gt;sk_reuse == SK_FORCE_REUSE || //setsockopt设置repair模式才会设置\t\t\t    (tb-&gt;fastreuse &gt; 0 &amp;&amp; reuse) ||  //reuse表示当前sk是否是reuseaddr，这个值在下面update中设置\t\t\t    sk_reuseport_match(tb, sk))\t\t//是否启用了reuseport\t\t\t\tcheck_bind_conflict = false;\t\t&#125;\t\t/* 首先判断是否跟anyaddr + port冲突 */\t\t//如果需要检查冲突，且bind的地址是具体的ip地址，则要看是否跟bhash2中的anyaddr + port冲突\t\tif (check_bind_conflict &amp;&amp; inet_use_bhash2_on_bind(sk)) &#123;\t\t\tif (inet_bhash2_addr_any_conflict(sk, port, l3mdev, true, true))\t\t\t\tgoto fail_unlock;\t\t&#125;\t\t//根据port + addr 找到 桶\t\thead2 = inet_bhashfn_portaddr(hinfo, sk, net, port);\t\tspin_lock(&amp;head2-&gt;lock);\t\thead2_lock_acquired = true;\t\t//遍历桶下挂的entry比较addr是否相等\t\ttb2 = inet_bind2_bucket_find(head2, net, port, l3mdev, sk);\t&#125;\tif (!tb2) &#123;\t\t//不存在这个条目，创建一个\t\ttb2 = inet_bind2_bucket_create(hinfo-&gt;bind2_bucket_cachep,\t\t\t\t\t       net, head2, port, l3mdev, sk);\t\tif (!tb2)\t\t\tgoto fail_unlock;\t\tbhash2_created = true;\t&#125;\t//进行真正的冲突检测的地方，如果bind的不是any addr 则直接去跟bhash2中的entry比较\t//如果bind的是any addr 则在bhash中进行比较\tif (!found_port &amp;&amp; check_bind_conflict) &#123;\t\tif (inet_csk_bind_conflict(sk, tb, tb2, true, true))\t\t\tgoto fail_unlock;\t&#125;success:\t//更新tb的fastreuse字段和fastreuseport字段\tinet_csk_update_fastreuse(tb, sk);\t//将当前的sk挂到两个entry的owns中\tif (!inet_csk(sk)-&gt;icsk_bind_hash)\t\tinet_bind_hash(sk, tb, tb2, port);\tWARN_ON(inet_csk(sk)-&gt;icsk_bind_hash != tb);\tWARN_ON(inet_csk(sk)-&gt;icsk_bind2_hash != tb2);\tret = 0;fail_unlock:\tif (ret) &#123;\t\tif (bhash_created)\t\t\tinet_bind_bucket_destroy(hinfo-&gt;bind_bucket_cachep, tb);\t\tif (bhash2_created)\t\t\tinet_bind2_bucket_destroy(hinfo-&gt;bind2_bucket_cachep,\t\t\t\t\t\t  tb2);\t&#125;\tif (head2_lock_acquired)\t\tspin_unlock(&amp;head2-&gt;lock);\tspin_unlock_bh(&amp;head-&gt;lock);\treturn ret;&#125;\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建立连接"]},{"title":"TCP建立连接bind(二)","url":"/2025/08/30/TCP%20BIND2/","content":"没有bind具体的端口首先看用户没有指定bind的情况，也就是port为空，则会调用inet_csk_find_open_port来找到一个port，注意：如果传入传出的参数tb和tb2不为空，则直接返回sucess，如果tb或tb2为空，则会创建对应的entry。注意：不论那种情况，调用inet_csk_find_open_port后都不会进行冲突检测。因为在内部已经做完了，外面的冲突检测逻辑都是给bind具体端口用的。。。\ninet_csk_find_open_port逻辑如下所示：\n/* * Find an open port number for the socket.  Returns with the * inet_bind_hashbucket locks held if successful. */static struct inet_bind_hashbucket *inet_csk_find_open_port(const struct sock *sk, struct inet_bind_bucket **tb_ret,\t\t\tstruct inet_bind2_bucket **tb2_ret,\t\t\tstruct inet_bind_hashbucket **head2_ret, int *port_ret)&#123;\tstruct inet_hashinfo *hinfo = tcp_or_dccp_get_hashinfo(sk);\tint i, low, high, attempt_half, port, l3mdev;\tstruct inet_bind_hashbucket *head, *head2;\tstruct net *net = sock_net(sk);\tstruct inet_bind2_bucket *tb2;\tstruct inet_bind_bucket *tb;\tu32 remaining, offset;\tbool relax = false;\tl3mdev = inet_sk_bound_l3mdev(sk);ports_exhausted:\t//是否可以reuseaddr\tattempt_half = (sk-&gt;sk_reuse == SK_CAN_REUSE) ? 1 : 0;other_half_scan:\t//根据系统参数确定端口范围，默认是32768 - 60999\tinet_sk_get_local_port_range(sk, &amp;low, &amp;high);\t//+1确保可以取到60999\thigh++; /* [32768, 60999] -&gt; [32768, 61000[ */\t//小于四个可用端口，就扫描整个范围\tif (high - low &lt; 4)\t\tattempt_half = 0;\t//如果可以reuseaddr 这个attempt_half为1或者2,如果是1就扫描上半部分，否则扫描下办部分（goto 控制的）\tif (attempt_half) &#123;\t\tint half = low + (((high - low) &gt;&gt; 2) &lt;&lt; 1);\t\tif (attempt_half == 1)\t\t\thigh = half;\t\telse\t\t\tlow = half;\t&#125;\t//扫描端口的总范围\tremaining = high - low;\tif (likely(remaining &gt; 1))\t\tremaining &amp;= ~1U;//变为偶数\t//返回一个均匀分布在 [0, remaining-1] 范围内的随机整数\toffset = get_random_u32_below(remaining);\t/* __inet_hash_connect() favors ports having @low parity\t * We do the opposite to not pollute connect() users.\t */\t//强制设置为奇数\toffset |= 1U;other_parity_scan:\t//计算一个起始端口\tport = low + offset;\tfor (i = 0; i &lt; remaining; i += 2, port += 2) &#123;\t\tif (unlikely(port &gt;= high))//处理回绕\t\t\tport -= remaining;\t\t//跳过保留端口\t\tif (inet_is_local_reserved_port(net, port))\t\t\tcontinue;\t\t//根据port找到一个桶\t\thead = &amp;hinfo-&gt;bhash[inet_bhashfn(net, port,\t\t\t\t\t\t  hinfo-&gt;bhash_size)];\t\tspin_lock_bh(&amp;head-&gt;lock);\t\t//如果bind的ip地址不是0 就进入这个分支，\t\t//这个判断的是目的是，如果用户bind的是具体的ip地址，先查一下bind2的 anyaddr+port桶如果冲突了，那就是直接找下一个了\t\tif (inet_use_bhash2_on_bind(sk)) &#123;\t\t\t//进入这里的条件就是ip不为any，然后去bind2hash中找entry如果冲突了直接返回true继续找下一个port\t\t\t//返回true 就是和anyaddr有冲突直接找下一个port\t\t\tif (inet_bhash2_addr_any_conflict(sk, port, l3mdev, relax, false))\t\t\t\tgoto next_port;\t\t&#125;\t\t//根据port和addr找到一个桶\t\thead2 = inet_bhashfn_portaddr(hinfo, sk, net, port);\t\tspin_lock(&amp;head2-&gt;lock);\t\t//从bind2中匹配网络命名空间、端口、地址等找到具体的一个entry，可能是空\t\ttb2 = inet_bind2_bucket_find(head2, net, port, l3mdev, sk);\t\t//遍历传统hash表1\t\tinet_bind_bucket_for_each(tb, &amp;head-&gt;chain)\t\t\t//匹配port 网络命名空间的话就检测是否冲突\t\t\tif (inet_bind_bucket_match(tb, net, port, l3mdev)) &#123;\t\t\t\tif (!inet_csk_bind_conflict(sk, tb, tb2,\t\t\t\t\t\t\t    relax, false))\t\t\t\t\t//不冲突返回success\t\t\t\t\tgoto success;\t\t\t\tspin_unlock(&amp;head2-&gt;lock);\t\t\t\tgoto next_port;\t\t\t&#125;\t\ttb = NULL;\t\t//根本没有这个entry返回\t\tgoto success;next_port:\t\tspin_unlock_bh(&amp;head-&gt;lock);\t\tcond_resched();\t&#125;\t//如果上面没有找到合适的port，这里会重试\toffset--;\t//如果修改后的offset是偶数，则还是扫描刚才的范围，只不过这次是偶数\tif (!(offset &amp; 1))\t\tgoto other_parity_scan;\t//如果进入到这个分支，则扫描另一半\tif (attempt_half == 1) &#123;\t\t/* OK we now try the upper half of the range */\t\tattempt_half = 2;\t\tgoto other_half_scan;\t&#125;\t//走到这里设置为检查冲突的时候为宽松\tif (READ_ONCE(net-&gt;ipv4.sysctl_ip_autobind_reuse) &amp;&amp; !relax) &#123;\t\t/* We still have a chance to connect to different destinations */\t\trelax = true;\t\tgoto ports_exhausted;\t&#125;\treturn NULL;success:\t*port_ret = port;\t*tb_ret = tb; //可能为空\t*tb2_ret = tb2;//可能为空\t*head2_ret = head2;\treturn head;&#125;\n\ninet_csk_find_open_port中首先确定可以扫描的端口范围，系统参数默认是[32768, 61000[，然后会根据是否reuseaddr来确定扫描上半部分还是下半部分，之后随机生成一个扫描的起始端口，这里注意：系统默认使用的bind的端口都是单数。然后进入正题，首先根据当前的port找到bhash的一个桶，之后判断用户的bind的ip地址是否是具体的ip地址，如果是则会调用inet_bhash2_addr_any_conflict判断当前addr+port 是否于anyaddr +port 冲突，具体代码如下所示:\nstatic bool inet_bhash2_addr_any_conflict(const struct sock *sk, int port, int l3mdev,\t\t\t\t\t  bool relax, bool reuseport_ok)&#123;\tkuid_t uid = sock_i_uid((struct sock *)sk);\tconst struct net *net = sock_net(sk);\tstruct sock_reuseport *reuseport_cb;\tstruct inet_bind_hashbucket *head2;\tstruct inet_bind2_bucket *tb2;\tbool reuseport_cb_ok;\trcu_read_lock();\t//若没有启用resuerport reuseport_cb为NULL\treuseport_cb = rcu_dereference(sk-&gt;sk_reuseport_cb);\t/* paired with WRITE_ONCE() in __reuseport_(add|detach)_closed_sock */\treuseport_cb_ok = !reuseport_cb || READ_ONCE(reuseport_cb-&gt;num_closed_socks);\trcu_read_unlock();\t//找到bhash2桶 注意：索引这个桶的key是anyaddr + 用户的port\thead2 = inet_bhash2_addr_any_hashbucket(sk, net, port);\tspin_lock(&amp;head2-&gt;lock);\tinet_bind_bucket_for_each(tb2, &amp;head2-&gt;chain)\t\t//返回ture表示端口网络命名空间等都配置上了，且当前tb的rcv_addr是0 （因为找的就是any addr），否则返回false\t\tif (inet_bind2_bucket_match_addr_any(tb2, net, port, l3mdev, sk))\t\t\tbreak;\t//reuseport_ok 为false 表示不允许端口重用，\t//如果上面循环，没有找到可能冲突的条目的话，则不会进入下面这个分支，直接返回false\t//如果tb2不为空，表示从bhash2(port+0地址)桶中找到了，那就要看bhash2中的entry和当前sk是否冲突了\tif (tb2 &amp;&amp; inet_bhash2_conflict(sk, tb2, uid, relax, reuseport_cb_ok,\t\t\t\t\treuseport_ok)) &#123;\t\tspin_unlock(&amp;head2-&gt;lock);\t\treturn true;\t&#125;\tspin_unlock(&amp;head2-&gt;lock);\treturn false;&#125;\n\ninet_bhash2_addr_any_conflict中，首先根据port和addr找到bhash2的桶，之后调用inet_bind2_bucket_match_addr_any遍历桶挂的entry判断是否有port相同，同时地址是全0的，如果有则表示找到了可能冲突的entry，接下来会进行冲突检查。那如果没有找到，则直接返回false表示不存在冲突。上述遍历entry和检测冲突的代码如下所示：\nbool inet_bind2_bucket_match_addr_any(const struct inet_bind2_bucket *tb, const struct net *net,\t\t\t\t      unsigned short port, int l3mdev, const struct sock *sk)&#123;\tif (!net_eq(ib2_net(tb), net) || tb-&gt;port != port ||\t    tb-&gt;l3mdev != l3mdev)\t\treturn false;#if IS_ENABLED(CONFIG_IPV6)\tif (sk-&gt;sk_family != tb-&gt;family) &#123;\t\tif (sk-&gt;sk_family == AF_INET)\t\t\treturn ipv6_addr_any(&amp;tb-&gt;v6_rcv_saddr) ||\t\t\t\tipv6_addr_v4mapped_any(&amp;tb-&gt;v6_rcv_saddr);\t\treturn false;\t&#125;\tif (sk-&gt;sk_family == AF_INET6)\t\treturn ipv6_addr_any(&amp;tb-&gt;v6_rcv_saddr);#endif\treturn tb-&gt;rcv_saddr == 0;&#125;static bool inet_bhash2_conflict(const struct sock *sk,\t\t\t\t const struct inet_bind2_bucket *tb2,\t\t\t\t kuid_t sk_uid,\t\t\t\t bool relax, bool reuseport_cb_ok,\t\t\t\t bool reuseport_ok)&#123;\tstruct inet_timewait_sock *tw2;\tstruct sock *sk2;\t//对不处于timewait状态的套接字检查冲突，注意这里是遍历了没一个entry的sock\tsk_for_each_bound_bhash2(sk2, &amp;tb2-&gt;owners) &#123;\t\tif (__inet_bhash2_conflict(sk, sk2, sk_uid, relax,\t\t\t\t\t   reuseport_cb_ok, reuseport_ok))\t\t\treturn true;\t&#125;\t//对处于timewait状态的套接字检查冲突\ttwsk_for_each_bound_bhash2(tw2, &amp;tb2-&gt;deathrow) &#123;\t\tsk2 = (struct sock *)tw2;\t\tif (__inet_bhash2_conflict(sk, sk2, sk_uid, relax,\t\t\t\t\t   reuseport_cb_ok, reuseport_ok))\t\t\treturn true;\t&#125;\treturn false;&#125;\n\n上述inet_bhash2_conflict的逻辑为检测一个sk与bhash2上的一个entry是否存在冲突，具体做的工作为，遍历当前tb上面挂的所有socket逐个检测是否存在冲突。具体代码如下所示：\nstatic bool inet_bind_conflict(const struct sock *sk, struct sock *sk2,\t\t\t       kuid_t sk_uid, bool relax,\t\t\t       bool reuseport_cb_ok, bool reuseport_ok)&#123;\tint bound_dev_if2;\tif (sk == sk2)\t\treturn false;\tbound_dev_if2 = READ_ONCE(sk2-&gt;sk_bound_dev_if);\t//如果两个socket绑定到相同的网络设备，或者都没有绑定设备，需要检查冲突,貌似是因为当socket绑定到特定网络设备时，它只接收从该设备来的数据包\tif (!sk-&gt;sk_bound_dev_if || !bound_dev_if2 ||\t    sk-&gt;sk_bound_dev_if == bound_dev_if2) &#123;\t\t//必须都启用reuseaddr，且不处于listen\t\tif (sk-&gt;sk_reuse &amp;&amp; sk2-&gt;sk_reuse &amp;&amp;\t\t    sk2-&gt;sk_state != TCP_LISTEN) &#123;\t\t\t\t//！relax表示是否为严格模式，随机port是严格模式，就直接返回trure了？reuseport_ok为外层传入的为false。\t\t\t\t//两个都要设置reuseport，且（旧的socket是timewait或 两个是同一个用户）则认为冲突（注意这里是具体的ip+port 和 any ip + port）\t\t\tif (!relax || (!reuseport_ok &amp;&amp; sk-&gt;sk_reuseport &amp;&amp;\t\t\t\t       sk2-&gt;sk_reuseport &amp;&amp; reuseport_cb_ok &amp;&amp;\t\t\t\t       (sk2-&gt;sk_state == TCP_TIME_WAIT ||\t\t\t\t\tuid_eq(sk_uid, sock_i_uid(sk2)))))\t\t\t\treturn true;\t\t\t\t//reuseport_ok 传进来的时候就是false， 或者两个socket有一个没有启用reuseport或者(不是timewait去额不是一个用户)\t\t&#125; else if (!reuseport_ok || !sk-&gt;sk_reuseport ||\t\t\t   !sk2-&gt;sk_reuseport || !reuseport_cb_ok ||\t\t\t   (sk2-&gt;sk_state != TCP_TIME_WAIT &amp;&amp;\t\t\t    !uid_eq(sk_uid, sock_i_uid(sk2)))) &#123;\t\t\treturn true;\t\t&#125;\t&#125;\treturn false;&#125;\n\n上述的大概逻辑就是，如果没有启用reuseport或者reuseaddr那必然冲突。如果这里返回冲突(也就是bind的当前的随机端口冲突)，则直接会继续找下一个port，直到找到不冲突的为止，如果没冲突（也就是当前port+addr 没有和anyaddr + port 冲突），则inet_csk_find_open_port中继续向下处理，会调用inet_bind2_bucket_find去寻找bhash2中的具体entry（这里的enry可能是空）。之后则会遍历bhash整个桶，找到port相同的entry然后调用inet_csk_bind_conflict完成最终的检测，上述冲突检测可以理解为一个加速处理的逻辑吧，这里才是完整的冲突检测。具体代码如下所示：\n/* This should be called only when the tb and tb2 hashbuckets&#x27; locks are held */static int inet_csk_bind_conflict(const struct sock *sk,\t\t\t\t  const struct inet_bind_bucket *tb,\t\t\t\t  const struct inet_bind2_bucket *tb2, /* may be null */\t\t\t\t  bool relax, bool reuseport_ok)&#123;\tbool reuseport_cb_ok;\tstruct sock_reuseport *reuseport_cb;\tkuid_t uid = sock_i_uid((struct sock *)sk);\trcu_read_lock();\treuseport_cb = rcu_dereference(sk-&gt;sk_reuseport_cb);\t/* paired with WRITE_ONCE() in __reuseport_(add|detach)_closed_sock */\treuseport_cb_ok = !reuseport_cb || READ_ONCE(reuseport_cb-&gt;num_closed_socks);\trcu_read_unlock();\t/*\t * Unlike other sk lookup places we do not check\t * for sk_net here, since _all_ the socks listed\t * in tb-&gt;owners and tb2-&gt;owners list belong\t * to the same net - the one this bucket belongs to.\t */\t//如果是any addr 走这里的传统路径\tif (!inet_use_bhash2_on_bind(sk)) &#123;\t\tstruct sock *sk2;\t\t//遍历传入的socket和entry中的socket是否存在冲突（地址必须相同）\t\tsk_for_each_bound(sk2, &amp;tb-&gt;owners)\t\t\tif (inet_bind_conflict(sk, sk2, uid, relax,\t\t\t\t\t       reuseport_cb_ok, reuseport_ok) &amp;&amp;\t\t\t    inet_rcv_saddr_equal(sk, sk2, true))\t\t\t\treturn true;\t\treturn false;\t&#125;\t/* Conflicts with an existing IPV6_ADDR_ANY (if ipv6) or INADDR_ANY (if\t * ipv4) should have been checked already. We need to do these two\t * checks separately because their spinlocks have to be acquired/released\t * independently of each other, to prevent possible deadlocks\t */\t//如果bind的是具体的ip，走这里，如果bhash2中没有这个条目，这里的tb2是空！，如果tb2不是空的话进一步\t//检测是否冲突，这个和外面的检测anyaddr + port的逻辑是一样的\treturn tb2 &amp;&amp; inet_bhash2_conflict(sk, tb2, uid, relax, reuseport_cb_ok,\t\t\t\t\t   reuseport_ok);&#125;\n\n如果传入的sk所bind的地址是anyaddr则不走 bhash2 路径（典型场景是绑定 ANY 地址）否则走bhash2路径，但是如果tb2传入是空则必然不会冲突。直接就返回false了。如果冲突了，外层还会在当前的扫描范围内继续选择一个端口，继续重复上述逻辑。如果都扫描完了都没有找到合适端口，则会扫描另一部分(同时可能会根据系统参数来放宽冲突范围。)如果还是没找到，这返回NULL外层会判断此次bind失败。\nbind具体的端口如果用户bind的具体的端口则会走下述else分支：\nint inet_csk_get_port(struct sock *sk, unsigned short snum)&#123;...\t//如果用户没有显示指定端口，走这个分支\tif (!port) &#123;...\t&#125; else &#123;\t\t//这里是用户指定了bind端口的情况，先拿到bhash的桶\t\thead = &amp;hinfo-&gt;bhash[inet_bhashfn(net, port,\t\t\t\t\t\t  hinfo-&gt;bhash_size)];\t\tspin_lock_bh(&amp;head-&gt;lock);\t\tinet_bind_bucket_for_each(tb, &amp;head-&gt;chain)\t\t\t//匹配port网络命名空间是否相等，这个是从bindhash中找\t\t\tif (inet_bind_bucket_match(tb, net, port, l3mdev))\t\t\t\tbreak;\t&#125;\t//条目是否为空，如果为空则创建一个entry\tif (!tb) &#123;\t\ttb = inet_bind_bucket_create(hinfo-&gt;bind_bucket_cachep, net,\t\t\t\t\t     head, port, l3mdev);\t\tif (!tb)\t\t\tgoto fail_unlock;\t\tbhash_created = true;\t&#125;\t//用户bind指定了具体的port就会走这个分支\tif (!found_port) &#123;\t\t/* ！！！下面五行代码判断是否可以直接判断跳过冲突检测 */\t\t//如果有其他socket被这个entry管理，就进入这个分支\t\tif (!hlist_empty(&amp;tb-&gt;owners)) &#123;\t\t\tif (sk-&gt;sk_reuse == SK_FORCE_REUSE || //setsockopt设置repair模式才会设置\t\t\t    (tb-&gt;fastreuse &gt; 0 &amp;&amp; reuse) ||  //reuse表示当前sk是否是reuseaddr，这个值在下面update中设置\t\t\t    sk_reuseport_match(tb, sk))\t\t//是否启用了reuseport\t\t\t\tcheck_bind_conflict = false;\t\t&#125;\t\t/* 首先判断是否跟anyaddr + port冲突 */\t\t//如果需要检查冲突，且bind的地址是具体的ip地址，则要看是否跟bhash2中的anyaddr + port冲突\t\tif (check_bind_conflict &amp;&amp; inet_use_bhash2_on_bind(sk)) &#123;\t\t\tif (inet_bhash2_addr_any_conflict(sk, port, l3mdev, true, true))\t\t\t\tgoto fail_unlock;\t\t&#125;\t\t//根据port + addr 找到 桶\t\thead2 = inet_bhashfn_portaddr(hinfo, sk, net, port);\t\tspin_lock(&amp;head2-&gt;lock);\t\thead2_lock_acquired = true;\t\t//遍历桶下挂的entry比较addr是否相等\t\ttb2 = inet_bind2_bucket_find(head2, net, port, l3mdev, sk);\t&#125;\tif (!tb2) &#123;\t\t//不存在这个条目，创建一个\t\ttb2 = inet_bind2_bucket_create(hinfo-&gt;bind2_bucket_cachep,\t\t\t\t\t       net, head2, port, l3mdev, sk);\t\tif (!tb2)\t\t\tgoto fail_unlock;\t\tbhash2_created = true;\t&#125;\t//进行真正的冲突检测的地方，如果bind的不是any addr 则直接去跟bhash2中的entry比较\t//如果bind的是any addr 则在bhash中进行比较\tif (!found_port &amp;&amp; check_bind_conflict) &#123;\t\tif (inet_csk_bind_conflict(sk, tb, tb2, true, true))\t\t\tgoto fail_unlock;\t&#125;success:\t//更新tb的fastreuse字段和fastreuseport字段\tinet_csk_update_fastreuse(tb, sk);\t//将当前的sk挂到两个entry的owns中\tif (!inet_csk(sk)-&gt;icsk_bind_hash)\t\tinet_bind_hash(sk, tb, tb2, port);\tWARN_ON(inet_csk(sk)-&gt;icsk_bind_hash != tb);\tWARN_ON(inet_csk(sk)-&gt;icsk_bind2_hash != tb2);\tret = 0;fail_unlock:\tif (ret) &#123;\t\tif (bhash_created)\t\t\tinet_bind_bucket_destroy(hinfo-&gt;bind_bucket_cachep, tb);\t\tif (bhash2_created)\t\t\tinet_bind2_bucket_destroy(hinfo-&gt;bind2_bucket_cachep,\t\t\t\t\t\t  tb2);\t&#125;\tif (head2_lock_acquired)\t\tspin_unlock(&amp;head2-&gt;lock);\tspin_unlock_bh(&amp;head-&gt;lock);\treturn ret;&#125;\n\n首先根据port确定bhash中的桶，然后从bhash的桶中找到具体的entry，这里可能是空，如果为空，会在下面创建一个新的entry\n然后会进入判断是否可以跳过冲突的逻辑，这里需要满足三个条件之一，分别是设置了reuseaddr（sk_reuseport_match），reuseport，或者是设置了repair。如果没有跳过冲突检测，则和上述用户没有指定bind的情况类似，首先判断是否跟anyaddr+port存在冲突，如果冲突直接返回bind失败，否则找到bhash2的桶，之后调用inet_csk_bind_conflict完成冲突检测，如果没有冲突则会调用inet_csk_update_fastreuse更新tb的fastreuse字段和fastreuseport字段，这两个字段在跳过冲突检测中会用到（上述三个条件）。之后会调用inet_bind_hash将当前的sk挂到bhash2和bhash的对应的tb和tb2上。上述更新fastreuse字段和sk_reuseport_match代码如下所示：\nvoid inet_csk_update_fastreuse(struct inet_bind_bucket *tb,\t\t\t       struct sock *sk)&#123;\tkuid_t uid = sock_i_uid(sk);\t//是否reuse addr 且不处于listen状态\tbool reuse = sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN;\t//当前entry中没有socket\tif (hlist_empty(&amp;tb-&gt;owners)) &#123;\t\ttb-&gt;fastreuse = reuse;\t\t//处理reuseport\t\t//用户设置了reuseport\t\tif (sk-&gt;sk_reuseport) &#123;\t\t\ttb-&gt;fastreuseport = FASTREUSEPORT_ANY;\t\t\ttb-&gt;fastuid = uid;\t\t\ttb-&gt;fast_rcv_saddr = sk-&gt;sk_rcv_saddr;\t\t\ttb-&gt;fast_ipv6_only = ipv6_only_sock(sk);\t\t\ttb-&gt;fast_sk_family = sk-&gt;sk_family;#if IS_ENABLED(CONFIG_IPV6)\t\t\ttb-&gt;fast_v6_rcv_saddr = sk-&gt;sk_v6_rcv_saddr;#endif\t\t&#125; else &#123;\t\t\ttb-&gt;fastreuseport = 0;\t\t&#125;\t&#125; else &#123;\t\t//如果挂了其他socket，根据情况设置fastreuse 和fastreuseport\t\tif (!reuse)\t\t\ttb-&gt;fastreuse = 0;\t\tif (sk-&gt;sk_reuseport) &#123;\t\t\t/* We didn&#x27;t match or we don&#x27;t have fastreuseport set on\t\t\t * the tb, but we have sk_reuseport set on this socket\t\t\t * and we know that there are no bind conflicts with\t\t\t * this socket in this tb, so reset our tb&#x27;s reuseport\t\t\t * settings so that any subsequent sockets that match\t\t\t * our current socket will be put on the fast path.\t\t\t *\t\t\t * If we reset we need to set FASTREUSEPORT_STRICT so we\t\t\t * do extra checking for all subsequent sk_reuseport\t\t\t * socks.\t\t\t */\t\t\tif (!sk_reuseport_match(tb, sk)) &#123;\t\t\t\ttb-&gt;fastreuseport = FASTREUSEPORT_STRICT;\t\t\t\ttb-&gt;fastuid = uid;\t\t\t\ttb-&gt;fast_rcv_saddr = sk-&gt;sk_rcv_saddr;\t\t\t\ttb-&gt;fast_ipv6_only = ipv6_only_sock(sk);\t\t\t\ttb-&gt;fast_sk_family = sk-&gt;sk_family;#if IS_ENABLED(CONFIG_IPV6)\t\t\t\ttb-&gt;fast_v6_rcv_saddr = sk-&gt;sk_v6_rcv_saddr;#endif\t\t\t&#125;\t\t&#125; else &#123;\t\t\ttb-&gt;fastreuseport = 0\t\t&#125;\t&#125;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建立连接"]}]