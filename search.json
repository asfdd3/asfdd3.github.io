[{"title":"TCP FIN_WATI2定时器","url":"/2025/08/18/FIN_WAIT2%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"FIN_WATI2定时器TCP 的 FIN_WAIT2 定时器是在主动关闭一方发出 FIN 并收到对端 ACK 后启动的一个保护机制，此时连接进入 FIN_WAIT2 状态，主动方已经不能再发送数据，但仍然可以接收数据，如果对端应用层迟迟不关闭，连接就可能长期卡在这里，因此内核需要定时器来避免资源泄漏。在 Linux 中，当 socket 已经是 SOCK_DEAD 状态（应用进程不再持有该 fd）时，内核会启动 FIN_WAIT2 定时器，默认超时时间由 tcp_fin_timeout 控制，通常是 60 秒，同时还可以通过 linger2 参数覆盖（最大 120 秒，负值则表示不等待，直接 RST 关闭）。当定时器到期，如果还没收到对端的 FIN，内核就会强制将连接销毁或转入 TIME_WAIT 状态，从而保证不会无限期占用内核资源。\n内核中FIN_WAIT2 与保活定时器共享一个定时器，具体代码如下所示：\nstatic void tcp_keepalive_timer (struct timer_list *t)&#123;...\t//  这个应该是fin_wait2 定时器，处于fin_wait2状态，且sock 为dead(这里的dead是tcp close中设置的)\t//\t只有两个地方激活定时器后才会走到这个分支，一个是tcp_close 另一个是tcp_rcv_state_process()\tif (sk-&gt;sk_state == TCP_FIN_WAIT2 &amp;&amp; sock_flag(sk, SOCK_DEAD)) &#123;\t\t//用户是否设置linger2\t\tif (READ_ONCE(tp-&gt;linger2) &gt;= 0) &#123;\t\t\t//这里减去了timewait时间为什么？ 没太理解 因为linger2 是fin_wait2到结束的时间？？？ \t\t\tconst int tmo = tcp_fin_time(sk) - TCP_TIMEWAIT_LEN;\t\t\tif (tmo &gt; 0) &#123;\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\t\t\t\tgoto out;\t\t\t&#125;\t\t&#125;\t\t//如果没有设置linger2，因为定时器到期了，这里直接发rst\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\t\tgoto death;\t&#125;...&#125;static inline int tcp_fin_time(const struct sock *sk)&#123;\t//如果设置了linger2就用用户配置的，否则用系统的\tint fin_timeout = tcp_sk(sk)-&gt;linger2 ? :\t\tREAD_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_fin_timeout);\t//在拿到rto\tconst int rto = inet_csk(sk)-&gt;icsk_rto;\t//如果基础超时时间 fin_timeout小于3.5 * RTO 则设置为3.5个rto \t// 大概率不会进来吧 \tif (fin_timeout &lt; (rto &lt;&lt; 2) - (rto &gt;&gt; 1))\t\tfin_timeout = (rto &lt;&lt; 2) - (rto &gt;&gt; 1);\treturn fin_timeout;&#125;\n\n上述代码为定时器到期的回调函数，其实就分为两种情况，第一种情况，也就是绝大多数情况，用户没有设置linger2(注意这里要区分linger)，则直接发送rst报文，第二种情况，如果设置了linger2则大概率会使用配置的时间减去TCP_TIMEWAIT_LEN（60s）计算得到一个超时时间，然后调用tcp_time_wait而不是发送rst报文，如果在这个超时时间内收到了对端的fin则正常走timewait流程否则这个超时时间到了之后就是直接释放资源了。\n__tcp_close中sock_orphan会设置SOCK_DEAD，具体代码如下所示：\nvoid __tcp_close(struct sock *sk, long timeout)&#123;\t...\tsock_orphan(sk);\t...&#125;static inline void sock_orphan(struct sock *sk)&#123;\twrite_lock_bh(&amp;sk-&gt;sk_callback_lock);\tsock_set_flag(sk, SOCK_DEAD);\tsk_set_socket(sk, NULL);\tsk-&gt;sk_wq  = NULL;\twrite_unlock_bh(&amp;sk-&gt;sk_callback_lock);&#125;\n\nFIN_WATI2定时器的激活收包处理函数tcp_rcv_state_process在从TCP_FIN_WAIT1进入TCP_FIN_WAIT2状态时，会激活 fin_wait2定时器，具体代码如下所示：\nint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)&#123;\t...\tcase TCP_FIN_WAIT1: &#123;\t\tint tmo;\t\tif (req)\t\t\ttcp_rcv_synrecv_state_fastopen(sk);\t\tif (tp-&gt;snd_una != tp-&gt;write_seq)\t\t\tbreak;\t\ttcp_set_state(sk, TCP_FIN_WAIT2);\t\tWRITE_ONCE(sk-&gt;sk_shutdown, sk-&gt;sk_shutdown | SEND_SHUTDOWN);\t\tsk_dst_confirm(sk);\t\t//如果在close中还没有设置dead 那这里就直接推出了！ 什么情况下还么没有设置dead？ 当启用linger的时候（注意区分linger2）\t\tif (!sock_flag(sk, SOCK_DEAD)) &#123;\t\t\t/* Wake up lingering close() */\t\t\tsk-&gt;sk_state_change(sk);\t\t\tbreak;\t\t&#125;\t\tif (READ_ONCE(tp-&gt;linger2) &lt; 0) &#123;\t\t\ttcp_done(sk);\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\t\t\treturn 1;\t\t&#125;\t\tif (TCP_SKB_CB(skb)-&gt;end_seq != TCP_SKB_CB(skb)-&gt;seq &amp;&amp;\t\t    after(TCP_SKB_CB(skb)-&gt;end_seq - th-&gt;fin, tp-&gt;rcv_nxt)) &#123;\t\t\t/* Receive out of order FIN after close() */\t\t\tif (tp-&gt;syn_fastopen &amp;&amp; th-&gt;fin)\t\t\t\ttcp_fastopen_active_disable(sk);\t\t\ttcp_done(sk);\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\t\t\treturn 1;\t\t&#125;\t\t/* 这里的代码逻辑如下：首先是围绕这个tmo来处理，这个tmo的值简单可以概括成用户是否配置，如果配置了那就要看这个值大还是小，如果没有配置，那就是\t\t走正常1分钟超时逻辑。如果用户配置了linger2 且 大于1分钟的话，那就先启动fin_wait2d定时器，注意这个定时器到期的时间时配置的时间减去一分钟，然后在这样就会先在 FIN_WAIT2 等 tmo - TIMEWAIT_LEN，\t\t到点再进 TIME_WAIT*/\t\t//用户配置的时间，或者是默认的60s\t\ttmo = tcp_fin_time(sk);\t\t//tmo超过60s的情况\t\tif (tmo &gt; TCP_TIMEWAIT_LEN) &#123;\t\t\t//fin_wait2定时器\t\t\tinet_csk_reset_keepalive_timer(sk, tmo - TCP_TIMEWAIT_LEN);\t\t&#125; else if (th-&gt;fin || sock_owned_by_user(sk)) &#123;\t\t\t/* Bad case. We could lose such FIN otherwise.\t\t\t * It is not a big problem, but it looks confusing\t\t\t * and not so rare event. We still can lose it now,\t\t\t * if it spins in bh_lock_sock(), but it is really\t\t\t * marginal case.\t\t\t */\t\t\t////对端的数据包带fin或者用户持有sock\t\t\t//fin_wait2定时器\t\t\tinet_csk_reset_keepalive_timer(sk, tmo);\t\t&#125; else &#123;\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\t\t\tgoto consume;\t\t&#125;\t\tbreak;\t&#125;\t...&#125;\n\n注意到上述激活重传定时器的前提的是sock的标志为dead ，与上面一样，这个标志位也是__tcp_close中设置的，主要的逻辑也可以分为两种情况\n第一种情况，用户没有设置linger2的时间，或者设置linger2的时间小于一分钟，则定时器默认时间为1分钟，会走else if 或者else分支（取决于用户是否持有sock或者也是一个fin包）。\n第二种情况，如果用户设置了linger2的时间，且大于TCP_TIMEWAIT_LEN（60s），则定时器的时间设置为配置的时间减去一分钟的时间。\n除了上述tcp_rcv_state_process会激活FIN_WAIT2定时器，__tcp_close中也会激活定时器\nvoid __tcp_close(struct sock *sk, long timeout)&#123;  \t...\tsk_stream_wait_close(sk, timeout);    \t...\tif (sk-&gt;sk_state == TCP_FIN_WAIT2) &#123;\t\tstruct tcp_sock *tp = tcp_sk(sk);\t\tif (READ_ONCE(tp-&gt;linger2) &lt; 0) &#123;\t\t\ttcp_set_state(sk, TCP_CLOSE);\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\t\t\t__NET_INC_STATS(sock_net(sk),\t\t\t\t\tLINUX_MIB_TCPABORTONLINGER);\t\t&#125; else &#123;\t\t\tconst int tmo = tcp_fin_time(sk);\t\t\tif (tmo &gt; TCP_TIMEWAIT_LEN) &#123;\t\t\t\tinet_csk_reset_keepalive_timer(sk,\t\t\t\t\t\ttmo - TCP_TIMEWAIT_LEN);\t\t\t&#125; else &#123;\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\t\t\t\tgoto out;\t\t\t&#125;\t\t&#125;\t&#125;&#125;\n\n上述代码与tcp_rcv_state_process类似，都有激活定时器的处理，但是到底是哪里激活这个定时器呢？应该取决于close是否阻塞在\nsk_stream_wait_close？？？如果阻塞了则大概率是__tcp_close中激活定时器，因为这个时候sock还没有被orphan。如果没有阻塞那可能就不一定了吧。这里不太清楚。感觉大概率不会是这里启动的了\n","categories":["网络协议栈源码学习"],"tags":["TCP定时器","TCP"]},{"title":"TCP建立连接-accept","url":"/2025/09/03/TCP%20ACCEPT/","content":"在 TCP 中，accept 是服务端监听套接字完成三次握手后，用来“接收”新连接的关键系统调用。它的作用是从内核的全连接队列（已经握手成功的连接请求队列）中取出一个连接，并为其创建一个新的 socket，交给应用层使用。这样，监听 socket 继续保持监听状态，而新返回的 socket 专门用于与客户端的数据交互。简单来说，accept 是服务端真正进入与客户端“对话”的起点，它标志着从“等待连接”到“建立会话”的转换。\n用户态程序通过glibc调用accept系统调用后，最终会调用到内核的__sys_accept4，具体代码如下所示：\nint __sys_accept4(int fd, struct sockaddr __user *upeer_sockaddr,\t\t  int __user *upeer_addrlen, int flags)&#123;\tint ret = -EBADF;\tstruct fd f;\t//根据传入的fd拿到fd结构体\tf = fdget(fd);\tif (f.file) &#123;\t\tret = __sys_accept4_file(f.file, upeer_sockaddr,\t\t\t\t\t upeer_addrlen, flags);\t\tfdput(f);\t&#125;\treturn ret;&#125;\n\n上述代码根据监听套接字的文件描述转换成struct file *调用 __sys_accept4_file() 来执行真正的 accept 核心逻辑，具体代码如下所示：\nstatic int __sys_accept4_file(struct file *file, struct sockaddr __user *upeer_sockaddr,\t\t\t      int __user *upeer_addrlen, int flags)&#123;\tstruct file *newfile;\tint newfd;\t//检查标志位是否合法\tif (flags &amp; ~(SOCK_CLOEXEC | SOCK_NONBLOCK))\t\treturn -EINVAL;\tif (SOCK_NONBLOCK != O_NONBLOCK &amp;&amp; (flags &amp; SOCK_NONBLOCK))\t\tflags = (flags &amp; ~SOCK_NONBLOCK) | O_NONBLOCK;\t//这里先获取一个没有使用的fd，下面会跟一个文件相关联\tnewfd = get_unused_fd_flags(flags);\tif (unlikely(newfd &lt; 0))\t\treturn newfd;\t//真正的accpet，这里返回了一个file，传入的file为用户fd关联的file\tnewfile = do_accept(file, 0, upeer_sockaddr, upeer_addrlen,\t\t\t    flags);\tif (IS_ERR(newfile)) &#123;\t\tput_unused_fd(newfd);\t\treturn PTR_ERR(newfile);\t&#125;\t//将上面获取的fd与返回的newfile关联并返回给用户\tfd_install(newfd, newfile);\treturn newfd;&#125;\n\n__sys_accept4_file做了三件事情，首先获取一个fd也就是要返回给用户的fd，然后调用do_accept返回一个file（也相当于socket），最后将获取到的file和fd关联起来。do_accept实现代码如下：\nstruct file *do_accept(struct file *file, unsigned file_flags,\t\t       struct sockaddr __user *upeer_sockaddr,\t\t       int __user *upeer_addrlen, int flags)&#123;\tstruct socket *sock, *newsock;\tstruct file *newfile;\tint err, len;\tstruct sockaddr_storage address;\tconst struct proto_ops *ops;\t//通过accpet 传入的listensock的私有结构中拿到socket\tsock = sock_from_file(file);\tif (!sock)\t\treturn ERR_PTR(-ENOTSOCK);\t//申请一个inode和socket结构体\tnewsock = sock_alloc();\tif (!newsock)\t\treturn ERR_PTR(-ENFILE);\tops = READ_ONCE(sock-&gt;ops);\t//设置新socket的ops\tnewsock-&gt;type = sock-&gt;type;\tnewsock-&gt;ops = ops;\t/*\t * We don&#x27;t need try_module_get here, as the listening socket (sock)\t * has the protocol module (sock-&gt;ops-&gt;owner) held.\t */\t__module_get(ops-&gt;owner);\t//创建一个file结构体(也就是把上面申请的socket的结构提挂到file的私有结构上)\tnewfile = sock_alloc_file(newsock, flags, sock-&gt;sk-&gt;sk_prot_creator-&gt;name);\tif (IS_ERR(newfile))\t\treturn newfile;\t//安全相关的钩子\terr = security_socket_accept(sock, newsock);\tif (err)\t\tgoto out_fd;\t//调用套接子的accept回调，对于tcp就是inet_accept\terr = ops-&gt;accept(sock, newsock, sock-&gt;file-&gt;f_flags | file_flags,\t\t\t\t\tfalse);\tif (err &lt; 0)\t\tgoto out_fd;\t//是否需要获取对端ip地址\tif (upeer_sockaddr) &#123;\t\tlen = ops-&gt;getname(newsock, (struct sockaddr *)&amp;address, 2);\t\tif (len &lt; 0) &#123;\t\t\terr = -ECONNABORTED;\t\t\tgoto out_fd;\t\t&#125;\t\terr = move_addr_to_user(&amp;address,\t\t\t\t\tlen, upeer_sockaddr, upeer_addrlen);\t\tif (err &lt; 0)\t\t\tgoto out_fd;\t&#125;\t/* File flags are not inherited via accept() unlike another OSes. */\t//这里返回的是file 外面会关联fd和这个file\treturn newfile;out_fd:\tfput(newfile);\treturn ERR_PTR(err);&#125;\n\ndo_accept中首先根据外面传入的file拿到监听的socket，之后申请一个新的socket并创建一个新的file结构(给建立连接的套接字)，然后调用套接字层的accept函数并传入上述新的sock，具体代码如下所示：\nint inet_accept(struct socket *sock, struct socket *newsock, int flags,\t\tbool kern)&#123;\tstruct sock *sk1 = sock-&gt;sk, *sk2;\tint err = -EINVAL;\t/* IPV6_ADDRFORM can change sk-&gt;sk_prot under us. */\t//调用监听套接字的accept对于tcp来说就是inet_csk_accept\tsk2 = READ_ONCE(sk1-&gt;sk_prot)-&gt;accept(sk1, flags, &amp;err, kern);\tif (!sk2)\t\treturn err;\tlock_sock(sk2);\t//这里关联了socket和sock结构\t__inet_accept(sock, newsock, sk2);\trelease_sock(sk2);\treturn 0;&#125;\n\n套接字层的accept回调的逻辑很简单，首先调用具体sock的accept（对于tcp来说就是inet_csk_accept），获取到的返回值为一个已经建立连接的sock，然后调用inet_csk_accept，将这个sock和参数传进来的socket相关联（注意：这个函数外层会将file，fd，socket三者关联，也就是说除了sock结构，其他的三个都是在调用accept时候创建的）。\n具体代码如下所示：\nstruct sock *inet_csk_accept(struct sock *sk, int flags, int *err, bool kern)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct request_sock_queue *queue = &amp;icsk-&gt;icsk_accept_queue;\tstruct request_sock *req;\tstruct sock *newsk;\tint error;\tlock_sock(sk);\t/* We need to make sure that this socket is listening,\t * and that it has something pending.\t */\terror = -EINVAL;\t//一定处于监听状态\tif (sk-&gt;sk_state != TCP_LISTEN)\t\tgoto out_err;\t/* Find already established connection */\t//首先判断接收队列是否为空，如果不为空，直接取出来\tif (reqsk_queue_empty(queue)) &#123;\t\t//非阻塞模式，timeo是0\t\tlong timeo = sock_rcvtimeo(sk, flags &amp; O_NONBLOCK);\t\t/* If this is a non blocking socket don&#x27;t sleep */\t\terror = -EAGAIN;\t\tif (!timeo)\t\t\tgoto out_err;//直接返回eagain\t\t//阻塞等待\t\terror = inet_csk_wait_for_connect(sk, timeo);\t\tif (error)\t\t\tgoto out_err;\t&#125;\t//从全连接队列中取出一个已经建立的连接\treq = reqsk_queue_remove(queue, sk);\t//拿到sock\tnewsk = req-&gt;sk;\t//TFO相关\tif (sk-&gt;sk_protocol == IPPROTO_TCP &amp;&amp;\t    tcp_rsk(req)-&gt;tfo_listener) &#123;\t\tspin_lock_bh(&amp;queue-&gt;fastopenq.lock);\t\tif (tcp_rsk(req)-&gt;tfo_listener) &#123;\t\t\t/* We are still waiting for the final ACK from 3WHS\t\t\t * so can&#x27;t free req now. Instead, we set req-&gt;sk to\t\t\t * NULL to signify that the child socket is taken\t\t\t * so reqsk_fastopen_remove() will free the req\t\t\t * when 3WHS finishes (or is aborted).\t\t\t */\t\t\treq-&gt;sk = NULL;\t\t\treq = NULL;\t\t&#125;\t\tspin_unlock_bh(&amp;queue-&gt;fastopenq.lock);\t&#125;out:\trelease_sock(sk);\t//cgroup相关\tif (newsk &amp;&amp; mem_cgroup_sockets_enabled) &#123;\t\tint amt = 0;\t\t/* atomically get the memory usage, set and charge the\t\t * newsk-&gt;sk_memcg.\t\t */\t\tlock_sock(newsk);\t\tmem_cgroup_sk_alloc(newsk);\t\tif (newsk-&gt;sk_memcg) &#123;\t\t\t/* The socket has not been accepted yet, no need\t\t\t * to look at newsk-&gt;sk_wmem_queued.\t\t\t */\t\t\tamt = sk_mem_pages(newsk-&gt;sk_forward_alloc +\t\t\t\t\t   atomic_read(&amp;newsk-&gt;sk_rmem_alloc));\t\t&#125;\t\tif (amt)\t\t\tmem_cgroup_charge_skmem(newsk-&gt;sk_memcg, amt,\t\t\t\t\t\tGFP_KERNEL | __GFP_NOFAIL);\t\trelease_sock(newsk);\t&#125;\tif (req)\t\treqsk_put(req);\treturn newsk;out_err:\tnewsk = NULL;\treq = NULL;\t*err = error;\tgoto out;&#125;static inline bool reqsk_queue_empty(const struct request_sock_queue *queue)&#123;\treturn READ_ONCE(queue-&gt;rskq_accept_head) == NULL;&#125;\n\ninet_csk_accept中的逻辑为，在确保监听状态下，调用reqsk_queue_empty判断全连接队列是否为空，如果队列不为空，则直接取出sock结构，如果为空则会根据是否阻塞或者非阻塞模式进一步处理，如果非阻塞则直接返回EAGAIN，否则调用inet_csk_wait_for_connect让出cpu。inet_csk_wait_for_connect代码如下所示：\nstatic int inet_csk_wait_for_connect(struct sock *sk, long timeo)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tDEFINE_WAIT(wait); //初始化等待队列\tint err;\t/*\t * True wake-one mechanism for incoming connections: only\t * one process gets woken up, not the &#x27;whole herd&#x27;.\t * Since we do not &#x27;race &amp; poll&#x27; for established sockets\t * anymore, the common case will execute the loop only once.\t *\t * Subtle issue: &quot;add_wait_queue_exclusive()&quot; will be added\t * after any current non-exclusive waiters, and we know that\t * it will always _stay_ after any new non-exclusive waiters\t * because all non-exclusive waiters are added at the\t * beginning of the wait-queue. As such, it&#x27;s ok to &quot;drop&quot;\t * our exclusiveness temporarily when we get woken up without\t * having to remove and re-insert us on the wait queue.\t */\t//死循环\tfor (;;) &#123;\t\t//将当前进程以独占模式加入到 socket 的等待队列（有事件发生的时候，只唤醒一个？）\t\tprepare_to_wait_exclusive(sk_sleep(sk), &amp;wait,\t\t\t\t\t  TASK_INTERRUPTIBLE);\t\trelease_sock(sk);\t\tif (reqsk_queue_empty(&amp;icsk-&gt;icsk_accept_queue))\t\t\t//让出cpu进入睡眠状态，这里真正让出cpu\t\t\ttimeo = schedule_timeout(timeo);\t\tsched_annotate_sleep();\t\tlock_sock(sk);\t\terr = 0;\t\t//如果队列不为空了，就直接返回了\t\tif (!reqsk_queue_empty(&amp;icsk-&gt;icsk_accept_queue))\t\t\tbreak;\t\terr = -EINVAL;\t\t//不是listen状态了？\t\tif (sk-&gt;sk_state != TCP_LISTEN)\t\t\tbreak;        //比如收到一个信号？ ctrl + c?\t\terr = sock_intr_errno(timeo);\t\tif (signal_pending(current))\t\t\tbreak;\t\terr = -EAGAIN;\t\tif (!timeo)\t\t\tbreak;\t&#125;\tfinish_wait(sk_sleep(sk), &amp;wait);\treturn err;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建接"]},{"title":"RACK定时器","url":"/2025/08/22/RACK%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"RACK定时器背景传统 TCP 的重传主要靠 RTO（超时重传）和 DupACK（快速重传）：\n\nRTO：基于 RTT 的超时，触发比较慢；\nDupACK：需要多个重复 ACK 才能触发，对乱序&#x2F;丢包检测不灵敏。\n\nRACK 的核心思想\n基本原则：只要某个数据段比“最近被确认的段”更早发送出去，但至今还没有被确认，就可以认为它可能丢失。\n使用 发送时间戳 来比较，而不是依赖 DupACK 计数。\n\n例如：\n\nS1, S2, S3, S4 四个包按顺序发送；\n假如收到 ACK 确认了 S4，但 S2 还没确认；\n那么 RACK 就能推断：S2 可能丢了（因为比 S4 更早发出，按理说应该早就确认了）。\n\nRACK与TLP等定时器类似，与重传共用一个定时器，定时器到期的回调函数如下所示：\nvoid tcp_rack_reo_timeout(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 timeout, prior_inflight;\tu32 lost = tp-&gt;lost;\t//计算在网络中飞行的包的数量\tprior_inflight = tcp_packets_in_flight(tp);\t//遍历按时间排序的数据包队列，根据时间戳，会将数据包标记为loss\ttcp_rack_detect_loss(sk, &amp;timeout);\t//如果现在这两个值不相等了，则证明认为有包丢失了，如果此时不再快速恢复状态则恩据到快速恢复状态，并调用注册的拥塞算法的钩子\tif (prior_inflight != tcp_packets_in_flight(tp)) &#123;\t\tif (inet_csk(sk)-&gt;icsk_ca_state != TCP_CA_Recovery) &#123;\t\t\ttcp_enter_recovery(sk, false);\t\t\tif (!inet_csk(sk)-&gt;icsk_ca_ops-&gt;cong_control)\t\t\t\ttcp_cwnd_reduction(sk, 1, tp-&gt;lost - lost, 0);\t\t&#125;\t\t//重传数据包\t\ttcp_xmit_retransmit_queue(sk);\t&#125;\t//设置了重传定时器，用来兜底？\tif (inet_csk(sk)-&gt;icsk_pending != ICSK_TIME_RETRANS)\t\ttcp_rearm_rto(sk);&#125;\n\n上述代码首先先计算在网络中’‘飞行的数据包的数量’‘，然后遍历按时间排序的数据包队列（发包的时候会放入这个队列中），然后根据时间戳标记可能丢失的数据包，然后重新计算网络中飞行的数据包数量，如果不相等了（因为有数据包被认为丢失了），就进入重传逻辑。\n标记可能丢失的逻辑tcp_rack_detect_loss如下所示：\nstatic void  tcp_rack_detect_loss(struct sock *sk, u32 *reo_timeout)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb, *n;\tu32 reo_wnd;\t*reo_timeout = 0;\t//reo_wnd的单位为时间，返回0或者根据rtt算一个值，取决于统计乱续数据包的字段\treo_wnd = tcp_rack_reo_wnd(sk);\t//遍历已经发送按按时间排序的数据包队列\tlist_for_each_entry_safe(skb, n, &amp;tp-&gt;tsorted_sent_queue,\t\t\t\t tcp_tsorted_anchor) &#123;\t\tstruct tcp_skb_cb *scb = TCP_SKB_CB(skb);\t\ts32 remaining;\t\t/* Skip ones marked lost but not yet retransmitted */\t\t//跳过已经标记丢失的数据包\t\tif ((scb-&gt;sacked &amp; TCPCB_LOST) &amp;&amp;\t\t    !(scb-&gt;sacked &amp; TCPCB_SACKED_RETRANS))\t\t\tcontinue;\t\t//返回false的条件是，当前数据包的时间戳在当前rack时间戳的前面。。。，表示没问题。所以直接beak 合理\t\tif (!tcp_skb_sent_after(tp-&gt;rack.mstamp,\t\t\t\t\ttcp_skb_timestamp_us(skb),\t\t\t\t\ttp-&gt;rack.end_seq, scb-&gt;end_seq))\t\t\tbreak;\t\t/* A packet is lost if it has not been s/acked beyond\t\t * the recent RTT plus the reordering window.\t\t */\t\t//内部根据rtt和上面算出来到窗口时间 减去这个数据包已经发出取得时间得到一个超时时间\t\tremaining = tcp_rack_skb_timeout(tp, skb, reo_wnd);\t\tif (remaining &lt;= 0) &#123;\t\t\t//没有剩余时间了，直接标记为丢失数据包\t\t\ttcp_mark_skb_lost(sk, skb);\t\t\t//这里把数据包从按时间排序的队列中移除了？？？，那需要重传的包在哪拿到呢？貌似是重传队列\t\t\tlist_del_init(&amp;skb-&gt;tcp_tsorted_anchor);\t\t&#125; else &#123;\t\t\t/* Record maximum wait time */\t\t\t//传入传出这个timeout\t\t\t*reo_timeout = max_t(u32, *reo_timeout, remaining);\t\t&#125;\t&#125;&#125;\n\n上述代码为检测是否有丢失的逻辑，首先调用tcp_rack_reo_wnd获取一个时间窗口，这个时间窗口就是用来计算下面超时时间，会因为是否存在数据包乱序影响计算结果，如果不存在乱序，就直接返回0了（返回0表示数据包更有可能被标记为丢失），否则返回根据rtt计算得到的时间。\ntcp_rack_reo_wnd 具体逻辑如下所示：\nstatic u32 tcp_rack_reo_wnd(const struct sock *sk)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\t//是否存在乱续，由tcp_check_sack_reordering设置\tif (!tp-&gt;reord_seen) &#123;\t\t/* If reordering has not been observed, be aggressive during\t\t * the recovery or starting the recovery by DUPACK threshold.\t\t */\t\t//快恢复或者是loss，直接返回0 表示乱续容忍时间窗口是0\t\tif (inet_csk(sk)-&gt;icsk_ca_state &gt;= TCP_CA_Recovery)\t\t\treturn 0;\t\t//sack确认的数量比乱需容忍的值要大  同时没有禁用重复ack 直接返回0 \t\tif (tp-&gt;sacked_out &gt;= tp-&gt;reordering &amp;&amp;\t\t    !(READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_recovery) &amp;\t\t      TCP_RACK_NO_DUPTHRESH))\t\t\treturn 0;\t&#125;\t/* To be more reordering resilient, allow min_rtt/4 settling delay.\t * Use min_rtt instead of the smoothed RTT because reordering is\t * often a path property and less related to queuing or delayed ACKs.\t * Upon receiving DSACKs, linearly increase the window up to the\t * smoothed RTT.\t */\treturn min((tcp_min_rtt(tp) &gt;&gt; 2) * tp-&gt;rack.reo_wnd_steps,\t\t   tp-&gt;srtt_us &gt;&gt; 3);&#125;\n\n进入上述代码逻辑的条件是reord_seen为0 而更新这个字段的函数为tcp_check_sack_reordering在拥塞控制，sack处理，ack处理中会调用这个函数。具体代码如下所示，主要逻辑就是用来检测是否数据包乱序，如果存在乱序数据包，会更新reord_seen这个字段，同时更新统计信息：\nstatic void tcp_check_sack_reordering(struct sock *sk, const u32 low_seq,\t\t\t\t      const int ts)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tconst u32 mss = tp-&gt;mss_cache;\tu32 fack, metric;\t//fack 为当前选择确认中最大的序列号\tfack = tcp_highest_sack_seq(tp);\t//如果待检查的序列号已经在选择确认最高的后面，则直接返回\tif (!before(low_seq, fack))\t\treturn;\t//可能丢包的范围？\tmetric = fack - low_seq;\tif ((metric &gt; tp-&gt;reordering * mss) &amp;&amp; mss) &#123;#if FASTRETRANS_DEBUG &gt; 1\t\tpr_debug(&quot;Disorder%d %d %u f%u s%u rr%d\\n&quot;,\t\t\t tp-&gt;rx_opt.sack_ok, inet_csk(sk)-&gt;icsk_ca\t\t\t _state,\t\t\t tp-&gt;reordering,\t\t\t 0,\t\t\t tp-&gt;sacked_out,\t\t\t tp-&gt;undo_marker ? tp-&gt;undo_retrans : 0);#endif\t\t//算一下有几个mss和默认值300取一个最小值，reordering的单位为最大乱续包的数量，会决定收到几个ack快速重传吗？\t\ttp-&gt;reordering = min_t(u32, (metric + mss - 1) / mss,\t\t\t\t       READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_max_reordering));\t&#125;\t/* This exciting event is worth to be remembered. 8) */\t//乱续事件计数\ttp-&gt;reord_seen++;\tNET_INC_STATS(sock_net(sk),\t\t      ts ? LINUX_MIB_TCPTSREORDER : LINUX_MIB_TCPSACKREORDER);&#125;\n\n回到上面计算时间窗口的逻辑，计算完成后，会遍历按时排序的数据包队列，如果当前遍历到的数据包，如果当前的rtt加上计算出来的时间窗口（也叫做容忍时间吧）减去，这个数据包发给的时间，如果小于等于0直接调用tcp_mark_skb_lost标记为丢失，具体代码如下所示：\n//标记skb为丢失，并且更新相关字段 enterloss中会调用，tcp超时重传中会调用，rack中会调用void tcp_mark_skb_lost(struct sock *sk, struct sk_buff *skb)&#123;\t//拿到sack的标志位\t__u8 sacked = TCP_SKB_CB(skb)-&gt;sacked;\tstruct tcp_sock *tp = tcp_sk(sk);\t//如果数据包已经被确认了，直接返回不用标记丢失\tif (sacked &amp; TCPCB_SACKED_ACKED)\t\treturn;\ttcp_verify_retransmit_hint(tp, skb);\t//这个包已经丢过一次了\tif (sacked &amp; TCPCB_LOST) &#123;\t\t//是否重传过一次了\t\tif (sacked &amp; TCPCB_SACKED_RETRANS) &#123;\t\t\t/* Account for retransmits that are lost again */\t\t\t//需要重新重传了，清除掉重传标志\t\t\tTCP_SKB_CB(skb)-&gt;sacked &amp;= ~TCPCB_SACKED_RETRANS;\t\t\t//减少重重的统计激素\t\t\ttp-&gt;retrans_out -= tcp_skb_pcount(skb);\t\t\t//增加统计计数 这个表示重传后又丢包的数量\t\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPLOSTRETRANSMIT,\t\t\t\t      tcp_skb_pcount(skb));\t\t\t//更新历史丢包总数\t\t\ttcp_notify_skb_loss_event(tp, skb);\t\t&#125;\t&#125; else &#123;\t\t//增加 当前待重传的丢包数\t\ttp-&gt;lost_out += tcp_skb_pcount(skb);\t\t//设置丢包标志\t\tTCP_SKB_CB(skb)-&gt;sacked |= TCPCB_LOST;\t\t//更新历史丢包总数\t\ttcp_notify_skb_loss_event(tp, skb);\t&#125;&#125;\n\n回到上面最初的定时器到期的回调函数中，也就是遍历按时间排序的数据包队列，标记可能loss的数据包后，如果重新计算飞行的书包的数量，如果不相等了就遍历重传队列发送标记重传的数据包，注意：这里不像超时重传一样，而是只重传被标记为loss的数据包，具体代码如下所示：\nvoid tcp_rack_reo_timeout(struct sock *sk)&#123;...\tif (prior_inflight != tcp_packets_in_flight(tp)) &#123;...\t\t//重传数据包\t\ttcp_xmit_retransmit_queue(sk);\t&#125;...&#125;//注意： 这个函数貌似只重传丢失的数据包。void tcp_xmit_retransmit_queue(struct sock *sk)&#123;\tconst struct inet_connection_sock *icsk = inet_csk(sk);\tstruct sk_buff *skb, *rtx_head, *hole = NULL;\tstruct tcp_sock *tp = tcp_sk(sk);\tbool rearm_timer = false;\tu32 max_segs;\tint mib_idx;\t//没有未确认的数据包，直接返回\tif (!tp-&gt;packets_out)\t\treturn;\t//拿到重传队列的头\trtx_head = tcp_rtx_queue_head(sk);\t//是否有能够快速定位的能重传的数据包，比如说在rack的重传逻辑中就会设置\tskb = tp-&gt;retransmit_skb_hint ?: rtx_head;\t//计算一个最大的段数\tmax_segs = tcp_tso_segs(sk, tcp_current_mss(sk));\t//从上面拿到的skb开始遍历重传队列\tskb_rbtree_walk_from(skb) &#123;\t\t__u8 sacked;\t\tint segs;\t\t//pacing相关， 是否需要直接break，tcp_write_xmit也会调用\t\tif (tcp_pacing_check(sk))\t\t\tbreak;\t\t/* we could do better than to assign each time */\t\t//用来避免每次都从开始遍历这个重传队列\t\tif (!hole)\t\t\ttp-&gt;retransmit_skb_hint = skb;\t\t//地一个单位是段，第二个是包吧，这里相减得到的还是段？\t\tsegs = tcp_snd_cwnd(tp) - tcp_packets_in_flight(tp);\t\tif (segs &lt;= 0)\t\t\tbreak;\t\tsacked = TCP_SKB_CB(skb)-&gt;sacked;\t\t/* In case tcp_shift_skb_data() have aggregated large skbs,\t\t * we need to make sure not sending too bigs TSO packets\t\t */\t\t//取一个最小值，不要超过tso的最大段数\t\tsegs = min_t(int, segs, max_segs);\t\t//已经重传的包数都超过了待重传的包数，那就直接break了\t\tif (tp-&gt;retrans_out &gt;= tp-&gt;lost_out) &#123;\t\t\tbreak;\t\t//如果未标记为丢失，这里直接continue\t\t&#125; else if (!(sacked &amp; TCPCB_LOST)) &#123;\t\t\tif (!hole &amp;&amp; !(sacked &amp; (TCPCB_SACKED_RETRANS|TCPCB_SACKED_ACKED)))\t\t\t\thole = skb;\t\t\tcontinue;\t\t//设置需要更新哪个统计计数\t\t&#125; else &#123;\t\t\tif (icsk-&gt;icsk_ca_state != TCP_CA_Loss)\t\t\t\tmib_idx = LINUX_MIB_TCPFASTRETRANS;\t\t\telse\t\t\t\tmib_idx = LINUX_MIB_TCPSLOWSTARTRETRANS;\t\t&#125;\t\t//如果已经被sack了，或者已经重传过了 直接continue\t\tif (sacked &amp; (TCPCB_SACKED_ACKED|TCPCB_SACKED_RETRANS))\t\t\tcontinue;\t\t//tsq相关，tcp_write_xmit中也会调用\t\tif (tcp_small_queue_check(sk, skb, 1))\t\t\tbreak;\t\t//真正调用发包函数，重传多少个段\t\tif (tcp_retransmit_skb(sk, skb, segs))\t\t\tbreak;\t\tNET_ADD_STATS(sock_net(sk), mib_idx, tcp_skb_pcount(skb));\t\t//快恢复状态返回true，这里的快恢复状态应该是外面设置\t\tif (tcp_in_cwnd_reduction(sk))\t\t\ttp-&gt;prr_out += tcp_skb_pcount(skb);\t\t//如果当前数据包是重传队列的地一个数据包，表示丢包概率比较大？\t\t//同时如果没有设置rack定时器？，就要设置超时重传？\t\tif (skb == rtx_head &amp;&amp;\t\t    icsk-&gt;icsk_pending != ICSK_TIME_REO_TIMEOUT)\t\t\trearm_timer = true;\t&#125;\tif (rearm_timer)\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\t\t\t\t     inet_csk(sk)-&gt;icsk_rto,\t\t\t\t     TCP_RTO_MAX);&#125;\n\ntcp_xmit_retransmit_queue中首先遍历重传队列，会首先检查pacing，tsq机制，或重传出去的数据包是否已经超过待重传的数据包，如果满足条件按，则直接就break了，如果当前数据包没有被标记为需要重传，则直接continue，否则直接调用tcp_retransmit_skb重传数据包。如果重传的数据包是第一个数据包，则还需要启动重传定时器，因为第一个包就要重传？表示很有可能大概率丢包？\nRACK定时器的启动在tcpack的拥塞处理中，会启动rack定时器\n//tcpack中最终会调用bool tcp_rack_mark_lost(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 timeout;\t//tcp_rack_advance 中设置\tif (!tp-&gt;rack.advanced)\t\treturn false;\t/* Reset the advanced flag to avoid unnecessary queue scanning */\ttp-&gt;rack.advanced = 0;\t//标记丢失的数据包，拿到timeout\ttcp_rack_detect_loss(sk, &amp;timeout);\tif (timeout) &#123;\t\ttimeout = usecs_to_jiffies(timeout + TCP_TIMEOUT_MIN_US);\t\t//启动RACK定时器\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_REO_TIMEOUT,\t\t\t\t\t  timeout, inet_csk(sk)-&gt;icsk_rto);\t&#125;\treturn !!timeout;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP定时器","TCP"]},{"title":"TCP建立连接-bind(二)","url":"/2025/08/30/TCP%20BIND2/","content":"没有bind具体的端口首先看用户没有指定bind的情况，也就是port为空，则会调用inet_csk_find_open_port来找到一个port，注意：如果传入传出的参数tb和tb2不为空，则直接返回sucess，如果tb或tb2为空，则会创建对应的entry。注意：不论那种情况，调用inet_csk_find_open_port后都不会进行冲突检测。因为在内部已经做完了，外面的冲突检测逻辑都是给bind具体端口用的。。。\ninet_csk_find_open_port逻辑如下所示：\n/* * Find an open port number for the socket.  Returns with the * inet_bind_hashbucket locks held if successful. */static struct inet_bind_hashbucket *inet_csk_find_open_port(const struct sock *sk, struct inet_bind_bucket **tb_ret,\t\t\tstruct inet_bind2_bucket **tb2_ret,\t\t\tstruct inet_bind_hashbucket **head2_ret, int *port_ret)&#123;\tstruct inet_hashinfo *hinfo = tcp_or_dccp_get_hashinfo(sk);\tint i, low, high, attempt_half, port, l3mdev;\tstruct inet_bind_hashbucket *head, *head2;\tstruct net *net = sock_net(sk);\tstruct inet_bind2_bucket *tb2;\tstruct inet_bind_bucket *tb;\tu32 remaining, offset;\tbool relax = false;\tl3mdev = inet_sk_bound_l3mdev(sk);ports_exhausted:\t//是否可以reuseaddr\tattempt_half = (sk-&gt;sk_reuse == SK_CAN_REUSE) ? 1 : 0;other_half_scan:\t//根据系统参数确定端口范围，默认是32768 - 60999\tinet_sk_get_local_port_range(sk, &amp;low, &amp;high);\t//+1确保可以取到60999\thigh++; /* [32768, 60999] -&gt; [32768, 61000[ */\t//小于四个可用端口，就扫描整个范围\tif (high - low &lt; 4)\t\tattempt_half = 0;\t//如果可以reuseaddr 这个attempt_half为1或者2,如果是1就扫描上半部分，否则扫描下办部分（goto 控制的）\tif (attempt_half) &#123;\t\tint half = low + (((high - low) &gt;&gt; 2) &lt;&lt; 1);\t\tif (attempt_half == 1)\t\t\thigh = half;\t\telse\t\t\tlow = half;\t&#125;\t//扫描端口的总范围\tremaining = high - low;\tif (likely(remaining &gt; 1))\t\tremaining &amp;= ~1U;//变为偶数\t//返回一个均匀分布在 [0, remaining-1] 范围内的随机整数\toffset = get_random_u32_below(remaining);\t/* __inet_hash_connect() favors ports having @low parity\t * We do the opposite to not pollute connect() users.\t */\t//强制设置为奇数\toffset |= 1U;other_parity_scan:\t//计算一个起始端口\tport = low + offset;\tfor (i = 0; i &lt; remaining; i += 2, port += 2) &#123;\t\tif (unlikely(port &gt;= high))//处理回绕\t\t\tport -= remaining;\t\t//跳过保留端口\t\tif (inet_is_local_reserved_port(net, port))\t\t\tcontinue;\t\t//根据port找到一个桶\t\thead = &amp;hinfo-&gt;bhash[inet_bhashfn(net, port,\t\t\t\t\t\t  hinfo-&gt;bhash_size)];\t\tspin_lock_bh(&amp;head-&gt;lock);\t\t//如果bind的ip地址不是0 就进入这个分支，\t\t//这个判断的是目的是，如果用户bind的是具体的ip地址，先查一下bind2的 anyaddr+port桶如果冲突了，那就是直接找下一个了\t\tif (inet_use_bhash2_on_bind(sk)) &#123;\t\t\t//进入这里的条件就是ip不为any，然后去bind2hash中找entry如果冲突了直接返回true继续找下一个port\t\t\t//返回true 就是和anyaddr有冲突直接找下一个port\t\t\tif (inet_bhash2_addr_any_conflict(sk, port, l3mdev, relax, false))\t\t\t\tgoto next_port;\t\t&#125;\t\t//根据port和addr找到一个桶\t\thead2 = inet_bhashfn_portaddr(hinfo, sk, net, port);\t\tspin_lock(&amp;head2-&gt;lock);\t\t//从bind2中匹配网络命名空间、端口、地址等找到具体的一个entry，可能是空\t\ttb2 = inet_bind2_bucket_find(head2, net, port, l3mdev, sk);\t\t//遍历传统hash表1\t\tinet_bind_bucket_for_each(tb, &amp;head-&gt;chain)\t\t\t//匹配port 网络命名空间的话就检测是否冲突\t\t\tif (inet_bind_bucket_match(tb, net, port, l3mdev)) &#123;\t\t\t\tif (!inet_csk_bind_conflict(sk, tb, tb2,\t\t\t\t\t\t\t    relax, false))\t\t\t\t\t//不冲突返回success\t\t\t\t\tgoto success;\t\t\t\tspin_unlock(&amp;head2-&gt;lock);\t\t\t\tgoto next_port;\t\t\t&#125;\t\ttb = NULL;\t\t//根本没有这个entry返回\t\tgoto success;next_port:\t\tspin_unlock_bh(&amp;head-&gt;lock);\t\tcond_resched();\t&#125;\t//如果上面没有找到合适的port，这里会重试\toffset--;\t//如果修改后的offset是偶数，则还是扫描刚才的范围，只不过这次是偶数\tif (!(offset &amp; 1))\t\tgoto other_parity_scan;\t//如果进入到这个分支，则扫描另一半\tif (attempt_half == 1) &#123;\t\t/* OK we now try the upper half of the range */\t\tattempt_half = 2;\t\tgoto other_half_scan;\t&#125;\t//走到这里设置为检查冲突的时候为宽松\tif (READ_ONCE(net-&gt;ipv4.sysctl_ip_autobind_reuse) &amp;&amp; !relax) &#123;\t\t/* We still have a chance to connect to different destinations */\t\trelax = true;\t\tgoto ports_exhausted;\t&#125;\treturn NULL;success:\t*port_ret = port;\t*tb_ret = tb; //可能为空\t*tb2_ret = tb2;//可能为空\t*head2_ret = head2;\treturn head;&#125;\n\ninet_csk_find_open_port中首先确定可以扫描的端口范围，系统参数默认是[32768, 61000[，然后会根据是否reuseaddr来确定扫描上半部分还是下半部分，之后随机生成一个扫描的起始端口，这里注意：系统默认使用的bind的端口都是单数。然后进入正题，首先根据当前的port找到bhash的一个桶，之后判断用户的bind的ip地址是否是具体的ip地址，如果是则会调用inet_bhash2_addr_any_conflict判断当前addr+port 是否于anyaddr +port 冲突，具体代码如下所示:\nstatic bool inet_bhash2_addr_any_conflict(const struct sock *sk, int port, int l3mdev,\t\t\t\t\t  bool relax, bool reuseport_ok)&#123;\tkuid_t uid = sock_i_uid((struct sock *)sk);\tconst struct net *net = sock_net(sk);\tstruct sock_reuseport *reuseport_cb;\tstruct inet_bind_hashbucket *head2;\tstruct inet_bind2_bucket *tb2;\tbool reuseport_cb_ok;\trcu_read_lock();\t//若没有启用resuerport reuseport_cb为NULL\treuseport_cb = rcu_dereference(sk-&gt;sk_reuseport_cb);\t/* paired with WRITE_ONCE() in __reuseport_(add|detach)_closed_sock */\treuseport_cb_ok = !reuseport_cb || READ_ONCE(reuseport_cb-&gt;num_closed_socks);\trcu_read_unlock();\t//找到bhash2桶 注意：索引这个桶的key是anyaddr + 用户的port\thead2 = inet_bhash2_addr_any_hashbucket(sk, net, port);\tspin_lock(&amp;head2-&gt;lock);\tinet_bind_bucket_for_each(tb2, &amp;head2-&gt;chain)\t\t//返回ture表示端口网络命名空间等都配置上了，且当前tb的rcv_addr是0 （因为找的就是any addr），否则返回false\t\tif (inet_bind2_bucket_match_addr_any(tb2, net, port, l3mdev, sk))\t\t\tbreak;\t//reuseport_ok 为false 表示不允许端口重用，\t//如果上面循环，没有找到可能冲突的条目的话，则不会进入下面这个分支，直接返回false\t//如果tb2不为空，表示从bhash2(port+0地址)桶中找到了，那就要看bhash2中的entry和当前sk是否冲突了\tif (tb2 &amp;&amp; inet_bhash2_conflict(sk, tb2, uid, relax, reuseport_cb_ok,\t\t\t\t\treuseport_ok)) &#123;\t\tspin_unlock(&amp;head2-&gt;lock);\t\treturn true;\t&#125;\tspin_unlock(&amp;head2-&gt;lock);\treturn false;&#125;\n\ninet_bhash2_addr_any_conflict中，首先根据port和addr找到bhash2的桶，之后调用inet_bind2_bucket_match_addr_any遍历桶挂的entry判断是否有port相同，同时地址是全0的，如果有则表示找到了可能冲突的entry，接下来会进行冲突检查。那如果没有找到，则直接返回false表示不存在冲突。上述遍历entry和检测冲突的代码如下所示：\nbool inet_bind2_bucket_match_addr_any(const struct inet_bind2_bucket *tb, const struct net *net,\t\t\t\t      unsigned short port, int l3mdev, const struct sock *sk)&#123;\tif (!net_eq(ib2_net(tb), net) || tb-&gt;port != port ||\t    tb-&gt;l3mdev != l3mdev)\t\treturn false;#if IS_ENABLED(CONFIG_IPV6)\tif (sk-&gt;sk_family != tb-&gt;family) &#123;\t\tif (sk-&gt;sk_family == AF_INET)\t\t\treturn ipv6_addr_any(&amp;tb-&gt;v6_rcv_saddr) ||\t\t\t\tipv6_addr_v4mapped_any(&amp;tb-&gt;v6_rcv_saddr);\t\treturn false;\t&#125;\tif (sk-&gt;sk_family == AF_INET6)\t\treturn ipv6_addr_any(&amp;tb-&gt;v6_rcv_saddr);#endif\treturn tb-&gt;rcv_saddr == 0;&#125;static bool inet_bhash2_conflict(const struct sock *sk,\t\t\t\t const struct inet_bind2_bucket *tb2,\t\t\t\t kuid_t sk_uid,\t\t\t\t bool relax, bool reuseport_cb_ok,\t\t\t\t bool reuseport_ok)&#123;\tstruct inet_timewait_sock *tw2;\tstruct sock *sk2;\t//对不处于timewait状态的套接字检查冲突，注意这里是遍历了没一个entry的sock\tsk_for_each_bound_bhash2(sk2, &amp;tb2-&gt;owners) &#123;\t\tif (__inet_bhash2_conflict(sk, sk2, sk_uid, relax,\t\t\t\t\t   reuseport_cb_ok, reuseport_ok))\t\t\treturn true;\t&#125;\t//对处于timewait状态的套接字检查冲突\ttwsk_for_each_bound_bhash2(tw2, &amp;tb2-&gt;deathrow) &#123;\t\tsk2 = (struct sock *)tw2;\t\tif (__inet_bhash2_conflict(sk, sk2, sk_uid, relax,\t\t\t\t\t   reuseport_cb_ok, reuseport_ok))\t\t\treturn true;\t&#125;\treturn false;&#125;\n\n上述inet_bhash2_conflict的逻辑为检测一个sk与bhash2上的一个entry是否存在冲突，具体做的工作为，遍历当前tb上面挂的所有socket逐个检测是否存在冲突。具体代码如下所示：\nstatic bool inet_bind_conflict(const struct sock *sk, struct sock *sk2,\t\t\t       kuid_t sk_uid, bool relax,\t\t\t       bool reuseport_cb_ok, bool reuseport_ok)&#123;\tint bound_dev_if2;\tif (sk == sk2)\t\treturn false;\tbound_dev_if2 = READ_ONCE(sk2-&gt;sk_bound_dev_if);\t//如果两个socket绑定到相同的网络设备，或者都没有绑定设备，需要检查冲突,貌似是因为当socket绑定到特定网络设备时，它只接收从该设备来的数据包\tif (!sk-&gt;sk_bound_dev_if || !bound_dev_if2 ||\t    sk-&gt;sk_bound_dev_if == bound_dev_if2) &#123;\t\t//必须都启用reuseaddr，且不处于listen\t\tif (sk-&gt;sk_reuse &amp;&amp; sk2-&gt;sk_reuse &amp;&amp;\t\t    sk2-&gt;sk_state != TCP_LISTEN) &#123;\t\t\t\t//！relax表示是否为严格模式，随机port是严格模式，就直接返回trure了？reuseport_ok为外层传入的为false。\t\t\t\t//两个都要设置reuseport，且（旧的socket是timewait或 两个是同一个用户）则认为冲突（注意这里是具体的ip+port 和 any ip + port）\t\t\tif (!relax || (!reuseport_ok &amp;&amp; sk-&gt;sk_reuseport &amp;&amp;\t\t\t\t       sk2-&gt;sk_reuseport &amp;&amp; reuseport_cb_ok &amp;&amp;\t\t\t\t       (sk2-&gt;sk_state == TCP_TIME_WAIT ||\t\t\t\t\tuid_eq(sk_uid, sock_i_uid(sk2)))))\t\t\t\treturn true;\t\t\t\t//reuseport_ok 传进来的时候就是false， 或者两个socket有一个没有启用reuseport或者(不是timewait去额不是一个用户)\t\t&#125; else if (!reuseport_ok || !sk-&gt;sk_reuseport ||\t\t\t   !sk2-&gt;sk_reuseport || !reuseport_cb_ok ||\t\t\t   (sk2-&gt;sk_state != TCP_TIME_WAIT &amp;&amp;\t\t\t    !uid_eq(sk_uid, sock_i_uid(sk2)))) &#123;\t\t\treturn true;\t\t&#125;\t&#125;\treturn false;&#125;\n\n上述的大概逻辑就是，如果没有启用reuseport或者reuseaddr那必然冲突。如果这里返回冲突(也就是bind的当前的随机端口冲突)，则直接会继续找下一个port，直到找到不冲突的为止，如果没冲突（也就是当前port+addr 没有和anyaddr + port 冲突），则inet_csk_find_open_port中继续向下处理，会调用inet_bind2_bucket_find去寻找bhash2中的具体entry（这里的enry可能是空）。之后则会遍历bhash整个桶，找到port相同的entry然后调用inet_csk_bind_conflict完成最终的检测，上述冲突检测可以理解为一个加速处理的逻辑吧，这里才是完整的冲突检测。具体代码如下所示：\n/* This should be called only when the tb and tb2 hashbuckets&#x27; locks are held */static int inet_csk_bind_conflict(const struct sock *sk,\t\t\t\t  const struct inet_bind_bucket *tb,\t\t\t\t  const struct inet_bind2_bucket *tb2, /* may be null */\t\t\t\t  bool relax, bool reuseport_ok)&#123;\tbool reuseport_cb_ok;\tstruct sock_reuseport *reuseport_cb;\tkuid_t uid = sock_i_uid((struct sock *)sk);\trcu_read_lock();\treuseport_cb = rcu_dereference(sk-&gt;sk_reuseport_cb);\t/* paired with WRITE_ONCE() in __reuseport_(add|detach)_closed_sock */\treuseport_cb_ok = !reuseport_cb || READ_ONCE(reuseport_cb-&gt;num_closed_socks);\trcu_read_unlock();\t/*\t * Unlike other sk lookup places we do not check\t * for sk_net here, since _all_ the socks listed\t * in tb-&gt;owners and tb2-&gt;owners list belong\t * to the same net - the one this bucket belongs to.\t */\t//如果是any addr 走这里的传统路径\tif (!inet_use_bhash2_on_bind(sk)) &#123;\t\tstruct sock *sk2;\t\t//遍历传入的socket和entry中的socket是否存在冲突（地址必须相同）\t\tsk_for_each_bound(sk2, &amp;tb-&gt;owners)\t\t\tif (inet_bind_conflict(sk, sk2, uid, relax,\t\t\t\t\t       reuseport_cb_ok, reuseport_ok) &amp;&amp;\t\t\t    inet_rcv_saddr_equal(sk, sk2, true))\t\t\t\treturn true;\t\treturn false;\t&#125;\t/* Conflicts with an existing IPV6_ADDR_ANY (if ipv6) or INADDR_ANY (if\t * ipv4) should have been checked already. We need to do these two\t * checks separately because their spinlocks have to be acquired/released\t * independently of each other, to prevent possible deadlocks\t */\t//如果bind的是具体的ip，走这里，如果bhash2中没有这个条目，这里的tb2是空！，如果tb2不是空的话进一步\t//检测是否冲突，这个和外面的检测anyaddr + port的逻辑是一样的\treturn tb2 &amp;&amp; inet_bhash2_conflict(sk, tb2, uid, relax, reuseport_cb_ok,\t\t\t\t\t   reuseport_ok);&#125;\n\n如果传入的sk所bind的地址是anyaddr则不走 bhash2 路径（典型场景是绑定 ANY 地址）否则走bhash2路径，但是如果tb2传入是空则必然不会冲突。直接就返回false了。如果冲突了，外层还会在当前的扫描范围内继续选择一个端口，继续重复上述逻辑。如果都扫描完了都没有找到合适端口，则会扫描另一部分(同时可能会根据系统参数来放宽冲突范围。)如果还是没找到，这返回NULL外层会判断此次bind失败。\nbind具体的端口如果用户bind的具体的端口则会走下述else分支：\nint inet_csk_get_port(struct sock *sk, unsigned short snum)&#123;...\t//如果用户没有显示指定端口，走这个分支\tif (!port) &#123;...\t&#125; else &#123;\t\t//这里是用户指定了bind端口的情况，先拿到bhash的桶\t\thead = &amp;hinfo-&gt;bhash[inet_bhashfn(net, port,\t\t\t\t\t\t  hinfo-&gt;bhash_size)];\t\tspin_lock_bh(&amp;head-&gt;lock);\t\tinet_bind_bucket_for_each(tb, &amp;head-&gt;chain)\t\t\t//匹配port网络命名空间是否相等，这个是从bindhash中找\t\t\tif (inet_bind_bucket_match(tb, net, port, l3mdev))\t\t\t\tbreak;\t&#125;\t//条目是否为空，如果为空则创建一个entry\tif (!tb) &#123;\t\ttb = inet_bind_bucket_create(hinfo-&gt;bind_bucket_cachep, net,\t\t\t\t\t     head, port, l3mdev);\t\tif (!tb)\t\t\tgoto fail_unlock;\t\tbhash_created = true;\t&#125;\t//用户bind指定了具体的port就会走这个分支\tif (!found_port) &#123;\t\t/* ！！！下面五行代码判断是否可以直接判断跳过冲突检测 */\t\t//如果有其他socket被这个entry管理，就进入这个分支\t\tif (!hlist_empty(&amp;tb-&gt;owners)) &#123;\t\t\tif (sk-&gt;sk_reuse == SK_FORCE_REUSE || //setsockopt设置repair模式才会设置\t\t\t    (tb-&gt;fastreuse &gt; 0 &amp;&amp; reuse) ||  //reuse表示当前sk是否是reuseaddr，这个值在下面update中设置\t\t\t    sk_reuseport_match(tb, sk))\t\t//是否启用了reuseport\t\t\t\tcheck_bind_conflict = false;\t\t&#125;\t\t/* 首先判断是否跟anyaddr + port冲突 */\t\t//如果需要检查冲突，且bind的地址是具体的ip地址，则要看是否跟bhash2中的anyaddr + port冲突\t\tif (check_bind_conflict &amp;&amp; inet_use_bhash2_on_bind(sk)) &#123;\t\t\tif (inet_bhash2_addr_any_conflict(sk, port, l3mdev, true, true))\t\t\t\tgoto fail_unlock;\t\t&#125;\t\t//根据port + addr 找到 桶\t\thead2 = inet_bhashfn_portaddr(hinfo, sk, net, port);\t\tspin_lock(&amp;head2-&gt;lock);\t\thead2_lock_acquired = true;\t\t//遍历桶下挂的entry比较addr是否相等\t\ttb2 = inet_bind2_bucket_find(head2, net, port, l3mdev, sk);\t&#125;\tif (!tb2) &#123;\t\t//不存在这个条目，创建一个\t\ttb2 = inet_bind2_bucket_create(hinfo-&gt;bind2_bucket_cachep,\t\t\t\t\t       net, head2, port, l3mdev, sk);\t\tif (!tb2)\t\t\tgoto fail_unlock;\t\tbhash2_created = true;\t&#125;\t//进行真正的冲突检测的地方，如果bind的不是any addr 则直接去跟bhash2中的entry比较\t//如果bind的是any addr 则在bhash中进行比较\tif (!found_port &amp;&amp; check_bind_conflict) &#123;\t\tif (inet_csk_bind_conflict(sk, tb, tb2, true, true))\t\t\tgoto fail_unlock;\t&#125;success:\t//更新tb的fastreuse字段和fastreuseport字段\tinet_csk_update_fastreuse(tb, sk);\t//将当前的sk挂到两个entry的owns中\tif (!inet_csk(sk)-&gt;icsk_bind_hash)\t\tinet_bind_hash(sk, tb, tb2, port);\tWARN_ON(inet_csk(sk)-&gt;icsk_bind_hash != tb);\tWARN_ON(inet_csk(sk)-&gt;icsk_bind2_hash != tb2);\tret = 0;fail_unlock:\tif (ret) &#123;\t\tif (bhash_created)\t\t\tinet_bind_bucket_destroy(hinfo-&gt;bind_bucket_cachep, tb);\t\tif (bhash2_created)\t\t\tinet_bind2_bucket_destroy(hinfo-&gt;bind2_bucket_cachep,\t\t\t\t\t\t  tb2);\t&#125;\tif (head2_lock_acquired)\t\tspin_unlock(&amp;head2-&gt;lock);\tspin_unlock_bh(&amp;head-&gt;lock);\treturn ret;&#125;\n\n首先根据port确定bhash中的桶，然后从bhash的桶中找到具体的entry，这里可能是空，如果为空，会在下面创建一个新的entry\n然后会进入判断是否可以跳过冲突的逻辑，这里需要满足三个条件之一，分别是设置了reuseaddr（sk_reuseport_match），reuseport，或者是设置了repair。如果没有跳过冲突检测，则和上述用户没有指定bind的情况类似，首先判断是否跟anyaddr+port存在冲突，如果冲突直接返回bind失败，否则找到bhash2的桶，之后调用inet_csk_bind_conflict完成冲突检测，如果没有冲突则会调用inet_csk_update_fastreuse更新tb的fastreuse字段和fastreuseport字段，这两个字段在跳过冲突检测中会用到（上述三个条件）。之后会调用inet_bind_hash将当前的sk挂到bhash2和bhash的对应的tb和tb2上。上述更新fastreuse字段和sk_reuseport_match代码如下所示：\nvoid inet_csk_update_fastreuse(struct inet_bind_bucket *tb,\t\t\t       struct sock *sk)&#123;\tkuid_t uid = sock_i_uid(sk);\t//是否reuse addr 且不处于listen状态\tbool reuse = sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN;\t//当前entry中没有socket\tif (hlist_empty(&amp;tb-&gt;owners)) &#123;\t\ttb-&gt;fastreuse = reuse;\t\t//处理reuseport\t\t//用户设置了reuseport\t\tif (sk-&gt;sk_reuseport) &#123;\t\t\ttb-&gt;fastreuseport = FASTREUSEPORT_ANY;\t\t\ttb-&gt;fastuid = uid;\t\t\ttb-&gt;fast_rcv_saddr = sk-&gt;sk_rcv_saddr;\t\t\ttb-&gt;fast_ipv6_only = ipv6_only_sock(sk);\t\t\ttb-&gt;fast_sk_family = sk-&gt;sk_family;#if IS_ENABLED(CONFIG_IPV6)\t\t\ttb-&gt;fast_v6_rcv_saddr = sk-&gt;sk_v6_rcv_saddr;#endif\t\t&#125; else &#123;\t\t\ttb-&gt;fastreuseport = 0;\t\t&#125;\t&#125; else &#123;\t\t//如果挂了其他socket，根据情况设置fastreuse 和fastreuseport\t\tif (!reuse)\t\t\ttb-&gt;fastreuse = 0;\t\tif (sk-&gt;sk_reuseport) &#123;\t\t\t/* We didn&#x27;t match or we don&#x27;t have fastreuseport set on\t\t\t * the tb, but we have sk_reuseport set on this socket\t\t\t * and we know that there are no bind conflicts with\t\t\t * this socket in this tb, so reset our tb&#x27;s reuseport\t\t\t * settings so that any subsequent sockets that match\t\t\t * our current socket will be put on the fast path.\t\t\t *\t\t\t * If we reset we need to set FASTREUSEPORT_STRICT so we\t\t\t * do extra checking for all subsequent sk_reuseport\t\t\t * socks.\t\t\t */\t\t\tif (!sk_reuseport_match(tb, sk)) &#123;\t\t\t\ttb-&gt;fastreuseport = FASTREUSEPORT_STRICT;\t\t\t\ttb-&gt;fastuid = uid;\t\t\t\ttb-&gt;fast_rcv_saddr = sk-&gt;sk_rcv_saddr;\t\t\t\ttb-&gt;fast_ipv6_only = ipv6_only_sock(sk);\t\t\t\ttb-&gt;fast_sk_family = sk-&gt;sk_family;#if IS_ENABLED(CONFIG_IPV6)\t\t\t\ttb-&gt;fast_v6_rcv_saddr = sk-&gt;sk_v6_rcv_saddr;#endif\t\t\t&#125;\t\t&#125; else &#123;\t\t\ttb-&gt;fastreuseport = 0\t\t&#125;\t&#125;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP建立连接-bind(一)","url":"/2025/08/27/TCP%20bind/","content":"TCP bindTCP 的bind用于将一个套接字与本地的 IP 地址和端口号进行显式绑定，从而在内核中建立地址与套接字的对应关系。服务器端通常通过 bind 指定固定端口，以便客户端能够通过该端口发起连接；客户端若未显式调用 bind，内核会在 connect 时自动分配一个临时端口。需要注意的是，bind 仅涉及内核数据结构的操作，不会产生任何网络报文。\n用户态程序通过glibc调用bind系统调用后，最终会调用到内核的__sys_bind由它完成套接字与地址的绑定逻辑，具体代码如下所示：\nint __sys_bind(int fd, struct sockaddr __user *umyaddr, int addrlen)&#123;\tstruct socket *sock;\tstruct sockaddr_storage address;\tint err, fput_needed;\t//通过fd 查找到file，通过file返回私有指针，这个私有指针就是socket\tsock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed);\tif (sock) &#123;\t\t//copy_from_user\t\terr = move_addr_to_kernel(umyaddr, addrlen, &amp;address);\t\tif (!err) &#123;\t\t\terr = security_socket_bind(sock,\t\t\t\t\t\t   (struct sockaddr *)&amp;address,\t\t\t\t\t\t   addrlen);\t\t\tif (!err)\t\t\t\t//调用具体sock类型的ops\t\t\t\terr = READ_ONCE(sock-&gt;ops)-&gt;bind(sock,\t\t\t\t\t\t      (struct sockaddr *)\t\t\t\t\t\t      &amp;address, addrlen);\t\t&#125;\t\tfput_light(sock-&gt;file, fput_needed);\t&#125;\treturn err;&#125;\n\n上述代码通过用户fd找到对应的socket结构，然后将用户传入的端口和地址拷贝到内核态，然后调用具体协议族的bind回调，对于AF_INET就是调用的inet_bind，inet_bind实现如下所示：\nint inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)&#123;\treturn inet_bind_sk(sock-&gt;sk, uaddr, addr_len);&#125;int inet_bind_sk(struct sock *sk, struct sockaddr *uaddr, int addr_len)&#123;\tu32 flags = BIND_WITH_LOCK;\tint err;\t/* If the socket has its own bind function then use it. (RAW) */\t//判断有没有bind， tcp是没有的\tif (sk-&gt;sk_prot-&gt;bind) &#123;\t\treturn sk-&gt;sk_prot-&gt;bind(sk, uaddr, addr_len);\t&#125;\tif (addr_len &lt; sizeof(struct sockaddr_in))\t\treturn -EINVAL;\t/* BPF prog is run before any checks are done so that if the prog\t * changes context in a wrong way it will be caught.\t */\t//BPF相关的钩子\terr = BPF_CGROUP_RUN_PROG_INET_BIND_LOCK(sk, uaddr,\t\t\t\t\t\t CGROUP_INET4_BIND, &amp;flags);\tif (err)\t\treturn err;\treturn __inet_bind(sk, uaddr, addr_len, flags);&#125;\n\ninet_bind中调用了inet_bind_sk，首先判断当前的sock是否有bind回调的实现，这里要注意，tcp是没有实现bind回调的，因此继续向下走，调用__inet_bind具体代码如下所示：\nint __inet_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len,\t\tu32 flags)&#123;\tstruct sockaddr_in *addr = (struct sockaddr_in *)uaddr;\tstruct inet_sock *inet = inet_sk(sk);\tstruct net *net = sock_net(sk);\tunsigned short snum;\tint chk_addr_ret;\tu32 tb_id = RT_TABLE_LOCAL;\tint err;\t//判断协议族是否正确\tif (addr-&gt;sin_family != AF_INET) &#123;\t\t/* Compatibility games : accept AF_UNSPEC (mapped to AF_INET)\t\t * only if s_addr is INADDR_ANY.\t\t */\t\terr = -EAFNOSUPPORT;\t\tif (addr-&gt;sin_family != AF_UNSPEC ||\t\t    addr-&gt;sin_addr.s_addr != htonl(INADDR_ANY))\t\t\tgoto out;\t&#125;\t//这里返回了上面的local表\ttb_id = l3mdev_fib_table_by_index(net, sk-&gt;sk_bound_dev_if) ? : tb_id;\t//调用查路由的接口，返回地址类型\tchk_addr_ret = inet_addr_type_table(net, addr-&gt;sin_addr.s_addr, tb_id);\t/* Not specified by any standard per-se, however it breaks too\t * many applications when removed.  It is unfortunate since\t * allowing applications to make a non-local bind solves\t * several problems with systems using dynamic addressing.\t * (ie. your servers still start up even if your ISDN link\t *  is temporarily down)\t */\terr = -EADDRNOTAVAIL;\t//注意：用户的地址是否可以绑定，注意这里如果开启了系统选项（或者setsockopt），是可以绑定非本机地址的\tif (!inet_addr_valid_or_nonlocal(net, inet, addr-&gt;sin_addr.s_addr,\t                                 chk_addr_ret))\t\tgoto out;\t//获取用户bind 的port\tsnum = ntohs(addr-&gt;sin_port);\terr = -EACCES;\t//这里的flags为2 inet_port_requires_bind_service//为判断是否可以绑定1024一下的端口\t//最后判断是否有权限绑定这个端口\tif (!(flags &amp; BIND_NO_CAP_NET_BIND_SERVICE) &amp;&amp;\t    snum &amp;&amp; inet_port_requires_bind_service(net, snum) &amp;&amp;\t    !ns_capable(net-&gt;user_ns, CAP_NET_BIND_SERVICE))\t\tgoto out;\t/*      We keep a pair of addresses. rcv_saddr is the one\t *      used by hash lookups, and saddr is used for transmit.\t *\t *      In the BSD API these are the same except where it\t *      would be illegal to use them (multicast/broadcast) in\t *      which case the sending device address is used.\t */\tif (flags &amp; BIND_WITH_LOCK)\t\tlock_sock(sk);\t/* Check these errors (active socket, double bind). */\terr = -EINVAL;\tif (sk-&gt;sk_state != TCP_CLOSE || inet-&gt;inet_num)\t\tgoto out_release_sock;\t//注意:这里设置了源地址\tinet-&gt;inet_rcv_saddr = inet-&gt;inet_saddr = addr-&gt;sin_addr.s_addr;\t//如果用户配下来的地址类型为多播或者广播就使用设备的地址？\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\t\tinet-&gt;inet_saddr = 0;  /* Use device */\t/* Make sure we are allowed to bind here. */\tif (snum || !(inet_test_bit(BIND_ADDRESS_NO_PORT, sk) ||\t\t      (flags &amp; BIND_FORCE_ADDRESS_NO_PORT))) &#123;\t\t//调用具体协议的getport！\t\terr = sk-&gt;sk_prot-&gt;get_port(sk, snum);\t\tif (err) &#123;\t\t\tinet-&gt;inet_saddr = inet-&gt;inet_rcv_saddr = 0;\t\t\tgoto out_release_sock;\t\t&#125;\t\t//bpf相关\t\tif (!(flags &amp; BIND_FROM_BPF)) &#123;\t\t\terr = BPF_CGROUP_RUN_PROG_INET4_POST_BIND(sk);\t\t\tif (err) &#123;\t\t\t\tinet-&gt;inet_saddr = inet-&gt;inet_rcv_saddr = 0;\t\t\t\tif (sk-&gt;sk_prot-&gt;put_port)\t\t\t\t\tsk-&gt;sk_prot-&gt;put_port(sk);\t\t\t\tgoto out_release_sock;\t\t\t&#125;\t\t&#125;\t&#125;\tif (inet-&gt;inet_rcv_saddr)\t\tsk-&gt;sk_userlocks |= SOCK_BINDADDR_LOCK;\tif (snum)\t\tsk-&gt;sk_userlocks |= SOCK_BINDPORT_LOCK;\t//设置了源port 这个是不是从上面getport获取的？\tinet-&gt;inet_sport = htons(inet-&gt;inet_num);\tinet-&gt;inet_daddr = 0;\tinet-&gt;inet_dport = 0;\t//先复位路由相关的信息\tsk_dst_reset(sk);\terr = 0;out_release_sock:\tif (flags &amp; BIND_WITH_LOCK)\t\trelease_sock(sk);out:\treturn err;&#125;\n\n__inet_bind中首先进行一系列检查，其中会调用inet_addr_type_table来获取要bind的ip地址类型（注意：通过查local路由表），然后会根据返回的类型进一步判断是否容许绑定，具体代码如下所示：\nunsigned int inet_addr_type_table(struct net *net, __be32 addr, u32 tb_id)&#123;\treturn __inet_dev_addr_type(net, NULL, addr, tb_id);&#125;static inline unsigned int __inet_dev_addr_type(struct net *net,\t\t\t\t\t\tconst struct net_device *dev,\t\t\t\t\t\t__be32 addr, u32 tb_id)&#123;\tstruct flowi4\t\tfl4 = &#123; .daddr = addr &#125;;\tstruct fib_result\tres;\tunsigned int ret = RTN_BROADCAST;\tstruct fib_table *table;\t//快速判断是否是全0或者全255（有限广播地址）\tif (ipv4_is_zeronet(addr) || ipv4_is_lbcast(addr))\t\treturn RTN_BROADCAST;\t//判断是否是多播\tif (ipv4_is_multicast(addr))\t\treturn RTN_MULTICAST;\trcu_read_lock();\t//根据id拿到路由表\ttable = fib_get_table(net, tb_id);\tif (table) &#123;\t\t//默认先设置为单播地址\t\tret = RTN_UNICAST;\t\t//这里直接调用查路路由的接口，fl4只有一个目的ip\t\tif (!fib_table_lookup(table, &amp;fl4, &amp;res, FIB_LOOKUP_NOREF)) &#123;\t\t\tstruct fib_nh_common *nhc = fib_info_nhc(res.fi, 0);\t\t\tif (!dev || dev == nhc-&gt;nhc_dev)\t\t\t\tret = res.type;//重新设置结果\t\t&#125;\t&#125;\trcu_read_unlock();\treturn ret;&#125;static inline bool inet_addr_valid_or_nonlocal(struct net *net,\t\t\t\t\t       struct inet_sock *inet,\t\t\t\t\t       __be32 addr,\t\t\t\t\t       int addr_type)&#123;\treturn inet_can_nonlocal_bind(net, inet) ||\t\taddr == htonl(INADDR_ANY) ||\t\taddr_type == RTN_LOCAL ||\t\taddr_type == RTN_MULTICAST ||\t\taddr_type == RTN_BROADCAST;&#125;\n\n之后会调用具体协议的get_port回调，对于TCP来说就是inet_csk_get_port，inet_csk_get_port主要做的工作就是分配端口，检测是否冲突。分配端口可以具体分为两种情况，一个用户没有指定具体的端口，另一个是用户指定了具体端口，不论那种情况，都会进行冲突检测（reuseport，repair模式，reusaddr等除外）。冲突检测的逻辑中，如果用户bind 的ip地址不是anyaddr(也就是具体的ip地址)则冲突检测首先判断是否与有anyaddr +port 的entry冲突，如果冲突了直接bind失败，如果没有冲突，则进一步从bhash2中检测是否存在冲突，如果不存在冲突则bind直接返回成功。注意：这里相较于低版本仅有的bhash的情况要加速了很多。\n上述具体代码如下所示：\nint inet_csk_get_port(struct sock *sk, unsigned short snum)&#123;\tstruct inet_hashinfo *hinfo = tcp_or_dccp_get_hashinfo(sk);\t//用户是否设置了reuseaddr\tbool reuse = sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN;\tbool found_port = false, check_bind_conflict = true;\tbool bhash_created = false, bhash2_created = false;\tint ret = -EADDRINUSE, port = snum, l3mdev;\tstruct inet_bind_hashbucket *head, *head2;\tstruct inet_bind2_bucket *tb2 = NULL;\tstruct inet_bind_bucket *tb = NULL;\tbool head2_lock_acquired = false;\tstruct net *net = sock_net(sk);\tl3mdev = inet_sk_bound_l3mdev(sk);\t//如果用户没有显示指定端口，走这个分支\tif (!port) &#123;\t\thead = inet_csk_find_open_port(sk, &amp;tb, &amp;tb2, &amp;head2, &amp;port);\t\tif (!head)\t\t\treturn ret;\t\thead2_lock_acquired = true;\t\tif (tb &amp;&amp; tb2)\t\t\tgoto success;\t\tfound_port = true;\t&#125; else &#123;\t\t//这里是用户指定了bind端口的情况，先拿到bhash的桶\t\thead = &amp;hinfo-&gt;bhash[inet_bhashfn(net, port,\t\t\t\t\t\t  hinfo-&gt;bhash_size)];\t\tspin_lock_bh(&amp;head-&gt;lock);\t\tinet_bind_bucket_for_each(tb, &amp;head-&gt;chain)\t\t\t//匹配port网络命名空间是否相等，这个是从bindhash中找\t\t\tif (inet_bind_bucket_match(tb, net, port, l3mdev))\t\t\t\tbreak;\t&#125;\t//条目是否为空，如果为空则创建一个entry\tif (!tb) &#123;\t\ttb = inet_bind_bucket_create(hinfo-&gt;bind_bucket_cachep, net,\t\t\t\t\t     head, port, l3mdev);\t\tif (!tb)\t\t\tgoto fail_unlock;\t\tbhash_created = true;\t&#125;\t//用户bind指定了具体的port就会走这个分支\tif (!found_port) &#123;\t\t/* ！！！下面五行代码判断是否可以直接判断跳过冲突检测 */\t\t//如果有其他socket被这个entry管理，就进入这个分支\t\tif (!hlist_empty(&amp;tb-&gt;owners)) &#123;\t\t\tif (sk-&gt;sk_reuse == SK_FORCE_REUSE || //setsockopt设置repair模式才会设置\t\t\t    (tb-&gt;fastreuse &gt; 0 &amp;&amp; reuse) ||  //reuse表示当前sk是否是reuseaddr，这个值在下面update中设置\t\t\t    sk_reuseport_match(tb, sk))\t\t//是否启用了reuseport\t\t\t\tcheck_bind_conflict = false;\t\t&#125;\t\t/* 首先判断是否跟anyaddr + port冲突 */\t\t//如果需要检查冲突，且bind的地址是具体的ip地址，则要看是否跟bhash2中的anyaddr + port冲突\t\tif (check_bind_conflict &amp;&amp; inet_use_bhash2_on_bind(sk)) &#123;\t\t\tif (inet_bhash2_addr_any_conflict(sk, port, l3mdev, true, true))\t\t\t\tgoto fail_unlock;\t\t&#125;\t\t//根据port + addr 找到 桶\t\thead2 = inet_bhashfn_portaddr(hinfo, sk, net, port);\t\tspin_lock(&amp;head2-&gt;lock);\t\thead2_lock_acquired = true;\t\t//遍历桶下挂的entry比较addr是否相等\t\ttb2 = inet_bind2_bucket_find(head2, net, port, l3mdev, sk);\t&#125;\tif (!tb2) &#123;\t\t//不存在这个条目，创建一个\t\ttb2 = inet_bind2_bucket_create(hinfo-&gt;bind2_bucket_cachep,\t\t\t\t\t       net, head2, port, l3mdev, sk);\t\tif (!tb2)\t\t\tgoto fail_unlock;\t\tbhash2_created = true;\t&#125;\t//进行真正的冲突检测的地方，如果bind的不是any addr 则直接去跟bhash2中的entry比较\t//如果bind的是any addr 则在bhash中进行比较\tif (!found_port &amp;&amp; check_bind_conflict) &#123;\t\tif (inet_csk_bind_conflict(sk, tb, tb2, true, true))\t\t\tgoto fail_unlock;\t&#125;success:\t//更新tb的fastreuse字段和fastreuseport字段\tinet_csk_update_fastreuse(tb, sk);\t//将当前的sk挂到两个entry的owns中\tif (!inet_csk(sk)-&gt;icsk_bind_hash)\t\tinet_bind_hash(sk, tb, tb2, port);\tWARN_ON(inet_csk(sk)-&gt;icsk_bind_hash != tb);\tWARN_ON(inet_csk(sk)-&gt;icsk_bind2_hash != tb2);\tret = 0;fail_unlock:\tif (ret) &#123;\t\tif (bhash_created)\t\t\tinet_bind_bucket_destroy(hinfo-&gt;bind_bucket_cachep, tb);\t\tif (bhash2_created)\t\t\tinet_bind2_bucket_destroy(hinfo-&gt;bind2_bucket_cachep,\t\t\t\t\t\t  tb2);\t&#125;\tif (head2_lock_acquired)\t\tspin_unlock(&amp;head2-&gt;lock);\tspin_unlock_bh(&amp;head-&gt;lock);\treturn ret;&#125;\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"Pacing定时器","url":"/2025/08/25/TCP%20pacing%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"pacing定时器TCP 的 pacing 定时器用于在缺少支持 pacing 的队列规则时，由 TCP 内部高精度定时器控制发包间隔。它根据拥塞控制算法计算出的速率生成未来发送时间戳，如果时间未到则挂起 hrtimer，到期后回调继续发包，从而保证出包平滑、避免突发，提升链路利用率和时延表现。\npacing定时器的初始化pacing定时器的初始化在tcp_init_xmit_timers（与重传定时器，延迟ack定时器，保活探测定时器位置相同）中，初始化与定时器到期的回调函数（tcp_pace_kick）如下所示：\nvoid tcp_init_xmit_timers(struct sock *sk)&#123;\thrtimer_init(&amp;tcp_sk(sk)-&gt;pacing_timer, CLOCK_MONOTONIC,\t\t     HRTIMER_MODE_ABS_PINNED_SOFT);\ttcp_sk(sk)-&gt;pacing_timer.function = tcp_pace_kick;&#125;enum hrtimer_restart tcp_pace_kick(struct hrtimer *timer)&#123;\tstruct tcp_sock *tp = container_of(timer, struct tcp_sock, pacing_timer);\tstruct sock *sk = (struct sock *)tp;\ttcp_tsq_handler(sk);\tsock_put(sk);\treturn HRTIMER_NORESTART;&#125;\n\n上述定时器到期后会调用tcp_tsq_handler，具体代码如下所示：\nstatic void tcp_tsq_handler(struct sock *sk)&#123;\tbh_lock_sock(sk);\tif (!sock_owned_by_user(sk))\t\ttcp_tsq_write(sk);\telse if (!test_and_set_bit(TCP_TSQ_DEFERRED, &amp;sk-&gt;sk_tsq_flags))\t\tsock_hold(sk);\tbh_unlock_sock(sk);&#125;\n\n如果此时用户没有持有sock则继续调用tcp_tsq_write进一步处理\nstatic void tcp_tsq_write(struct sock *sk)&#123;\t//是否处于可以发包的状态\tif ((1 &lt;&lt; sk-&gt;sk_state) &amp;\t    (TCPF_ESTABLISHED | TCPF_FIN_WAIT1 | TCPF_CLOSING |\t     TCPF_CLOSE_WAIT  | TCPF_LAST_ACK)) &#123;\t\tstruct tcp_sock *tp = tcp_sk(sk);\t\t//如果标记丢失的包数多于已经重传出去的包数，且拥塞窗口的大小还够用就先发重传包\t\tif (tp-&gt;lost_out &gt; tp-&gt;retrans_out &amp;&amp; //计算的结果为存在丢失，但还没重传的数量\t\t    tcp_snd_cwnd(tp) &gt; tcp_packets_in_flight(tp)) &#123;\t\t\ttcp_mstamp_refresh(tp);\t\t\ttcp_xmit_retransmit_queue(sk);\t\t&#125;\t\t//正常发包函数\t\ttcp_write_xmit(sk, tcp_current_mss(sk), tp-&gt;nonagle,\t\t\t       0, GFP_ATOMIC);\t&#125;&#125;\n\n上述代码逻辑很简单，首先判断是否处于可以发送数据包的状态，然后如果有需要重传的数据包，同时窗口大小足够大的话，则优先发送重传队列中的数据包，然后发送调用tcp_write_xmit发送正常的数据包。\npacing定时器的启动pacing定时器的启动由tcp_pacing_check完成，在tcp_write_xmit会调用，具体代码如下所示：\n//tcp_write_xnmit 和重传数据包中会调用，返回false表示立即发送static bool tcp_pacing_check(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//sch_fq 开启这个就会返回false，表示不需要tcp内部自己pacing ，反之，如果为brr这里就不进入这个分支\tif (!tcp_needs_internal_pacing(sk))\t\treturn false;\t//下一个数据包要发送的时间小于等于当前的时钟，则可以直接发送，不需要启动定时器\tif (tp-&gt;tcp_wstamp_ns &lt;= tp-&gt;tcp_clock_cache)\t\treturn false;\t//如果定时器还没在排队，就启动一个 hrtimer\tif (!hrtimer_is_queued(&amp;tp-&gt;pacing_timer)) &#123;\t\thrtimer_start(&amp;tp-&gt;pacing_timer,\t\t\t      ns_to_ktime(tp-&gt;tcp_wstamp_ns),\t\t\t      HRTIMER_MODE_ABS_PINNED_SOFT);\t\tsock_hold(sk);//这里引用计数++，防止socket被提前释放\t&#125;\treturn true;&#125;\n\ntcp_pacing_check逻辑很简单，首先判断是否开启pacing，比如tc模块启用sch_fq则代码不会进入下面的逻辑，如果使用的是bbr拥塞算法，则会进入下面的逻辑。下面的逻辑为，如果当前待发送数据包的时间戳（tp-&gt;tcp_wstamp_ns）小于等于当前的时间戳，则直接返回false，表示不需要启动重传定时器。否则如果没有启动pacing定时器则启动pacing定时器。\n上述设置时间戳tp-&gt;tcp_wstamp_ns在tcp_update_skb_after_send中，每次调用__tcp_transmit_skb后会调用，具体代码如下所示：\n//pacing相关，以及把数据包报道按时间排序的队列中static void tcp_update_skb_after_send(struct sock *sk, struct sk_buff *skb,\t\t\t\t      u64 prior_wstamp)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//是否启用pacing，注意，在TCP层面，默认是不启用的（如果使用了bbr拥塞算法，就启用）或者setsocktopt可以开启。\tif (sk-&gt;sk_pacing_status != SK_PACING_NONE) &#123;\t\t//这个是用户配置的单位是多少字节每秒，或者是bbr配置的\t\tunsigned long rate = sk-&gt;sk_pacing_rate;\t\t/* Original sch_fq does not pace first 10 MSS\t\t * Note that tp-&gt;data_segs_out overflows after 2^32 packets,\t\t * this is a minor annoyance.\t\t */\t\t//如果rate值有效同时发出去10个tcp段以上了则更具用户配置更新tcp_wstamp_ns也就是下一次待发送的时间戳\t\tif (rate != ~0UL &amp;&amp; rate &amp;&amp; tp-&gt;data_segs_out &gt;= 10) &#123;\t\t\tu64 len_ns = div64_ul((u64)skb-&gt;len * NSEC_PER_SEC, rate);\t\t\tu64 credit = tp-&gt;tcp_wstamp_ns - prior_wstamp;\t\t\t/* take into account OS jitter */\t\t\tlen_ns -= min_t(u64, len_ns / 2, credit);\t\t\t//pacingcheck会用到\t\t\ttp-&gt;tcp_wstamp_ns += len_ns;\t\t&#125;\t&#125;\t//将数据包加入到按时间排序的队列中，rack会用到\tlist_move_tail(&amp;skb-&gt;tcp_tsorted_anchor, &amp;tp-&gt;tsorted_sent_queue);&#125;\n\ntcp_update_skb_after_send中首先判断是否启用pacing，之后拿到pacing速率，如果是前十个数据包则不限速，目的是快速建立连接，之后依据速率把数据长度换算成应当等待的时间，然后更新下一次可发时间。\n","categories":["网络协议栈源码学习"],"tags":["TCP定时器","TCP"]},{"title":"TCP确认 tcp_ack（二）","url":"/2025/12/14/TCP%20ack%E6%8A%A5%E6%96%87%E5%A4%84%E7%90%86%20tcp_ack(2)/","content":"慢速路径中收上来的数据包在tcp_ack 中会调用tcp_ack_update_window尝试更新窗口，之后通过tcp_sacktag_write_queue处理sack，这里对sack的处理逻辑非常复杂，后续分析。处理完成后会更新sack确认的段数，并判断窗口是否被推进并调用拥塞算法的钩子（如果存在）。\n接下来调用tcp_clean_rtx_queue移除重传队列中已经确认的数据包，也稍微有点复杂，后续分析，主要工作就是传出flag（标识是否为可疑的ack），用于指导下面拥塞控制处理。\n接下来调用tcp_rack_update_reo_wnd 根据上面sack的处理结果（是否看到了重复ack)来调整乱序因子，进而影响后续rack丢包机制的判断，具体代码如下所示：\nvoid tcp_rack_update_reo_wnd(struct sock *sk, struct rate_sample *rs)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//默认不进入会在个分支\tif ((READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_recovery) &amp;\t     TCP_RACK_STATIC_REO_WND) ||\t    !rs-&gt;prior_delivered)  //本次速率采样区间开始时确认的包数\t\treturn;\t/* Disregard DSACK if a rtt has not passed since we adjusted reo_wnd */\t//还没有经过一个完整的RTT\tif (before(rs-&gt;prior_delivered, tp-&gt;rack.last_delivered))\t\ttp-&gt;rack.dsack_seen = 0;\t/* Adjust the reo_wnd if update is pending */\t//这个是处理sack中设置的，如果看到了daack就增大reo_wnd_steps，这个值会影响rack是否标记为丢包，合理\tif (tp-&gt;rack.dsack_seen) &#123;\t\ttp-&gt;rack.reo_wnd_steps = min_t(u32, 0xFF,\t\t\t\t\t       tp-&gt;rack.reo_wnd_steps + 1);\t\ttp-&gt;rack.dsack_seen = 0;\t\ttp-&gt;rack.last_delivered = tp-&gt;delivered;\t\ttp-&gt;rack.reo_wnd_persist = TCP_RACK_RECOVERY_THRESH;\t&#125; else if (!tp-&gt;rack.reo_wnd_persist) &#123;//这个值是误判丢包的数量\t\ttp-&gt;rack.reo_wnd_steps = 1;\t&#125;&#125;\n\ntcp_rack_update_reo_wnd首先判断当前采样区间是否经过了一个rtt，如果经过了一个rtt且sack的处理中发现了重复ack则更新 reo_wnd_steps 。后续会在tcp_rack_reo_wnd中根据reo_wnd_steps计算得到的时间窗口决定 将哪些数据包标记为丢失。\n回到tcp_ack中，上述工作完成后会调用tcp_process_tlp_ack处理TLP重传数据包(如果当前存在tlp数据包的话)，核心思想就是判断是否发生了尾部丢包，如果发生则会进入拥塞状态，具体代码如下所示：\nstatic void tcp_process_tlp_ack(struct sock *sk, u32 ack, int flag)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//没有确认到tlp的序列号，直接返回\tif (before(ack, tp-&gt;tlp_high_seq))\t\treturn;\tif (!tp-&gt;tlp_retrans) &#123;\t\t/* TLP of new data has been acknowledged */\t\t//TLP探测的是新数据\t\ttp-&gt;tlp_high_seq = 0;\t&#125; else if (flag &amp; FLAG_DSACK_TLP) &#123;//收到了tlp包的重复ack\t\t/* This DSACK means original and TLP probe arrived; no loss */\t\ttp-&gt;tlp_high_seq = 0;\t//表示确实发生了丢包，这里有点疑惑，不是有可能也是延迟的情况吗？是不是因为上面没有回复sack？？\t&#125; else if (after(ack, tp-&gt;tlp_high_seq)) &#123;\t\t/* ACK advances: there was a loss, so reduce cwnd. Reset\t\t * tlp_high_seq in tcp_init_cwnd_reduction()\t\t */\t\ttcp_init_cwnd_reduction(sk);\t\ttcp_set_ca_state(sk, TCP_CA_CWR); //将状态设置为cwr\t\ttcp_end_cwnd_reduction(sk);\t\ttcp_try_keep_open(sk);\t\tNET_INC_STATS(sock_net(sk),\t\t\t\tLINUX_MIB_TCPLOSSPROBERECOVERY);\t//单纯的重复ack，没有推进窗口，表示就单纯的收到一个重复ack\t&#125; else if (!(flag &amp; (FLAG_SND_UNA_ADVANCED |\t\t\t     FLAG_NOT_DUP | FLAG_DATA_SACKED))) &#123;\t\t/* Pure dupack: original and TLP probe arrived; no loss */\t\ttp-&gt;tlp_high_seq = 0;\t&#125;&#125;\n\n上述代码中tlp_high_seq为 TLP 探测中的最高序列号边界，这里主要分成以下几种情况对序列号进行处理：\n\n如果当前ack还没有到tlp的范围直接返回。\n如果 ACK 已经到达 tlp_high_seq，且TLP数据包为新数据包（从发送队列中拿到的），则认为没有丢失数据包，清空tlp_high_seq结束本次tlp探测。\n如果收到了tlp包的重复ack则表示也没有发生丢包，直接清掉tlp_high_seq\n如果ack确认了超过tlp_high_seq且tlp数据包是从重传队列中发送的，则认为数据包丢了（这里有个问题，不会是因为延迟没遇到到达的情况吗？）执行一系列拥塞算法的处理流程（重新计算慢启动阈值，设置拥塞状态等）\n如果收到是单纯的一个重复ack同时没有推进窗口则也认为没有丢包，清掉tlp_high_seq（这种情况几乎不会有吧）。\n\n回到tcp_ack中，接下来会根据ack的处理结果决定是否进入拥塞状态转换的处理逻辑中，在文章TCP拥塞控制-拥塞状态转换分析过，这里不再重复：-）。\n接下来根据清重传队列后的处理结果决定是否需要重新设置TLP定时器或者超时重传定时器，并调用tcp_rate_gen更新bbr算法用到的字段。\n接下来调用tcp_cong_control处理拥塞避免逻辑或者更新拥塞窗口（这里注意上面tcp_fastretrans_alert可能是没有对拥塞窗口进行处理的！！）具体代码如下所示：\nstatic void tcp_cong_control(struct sock *sk, u32 ack, u32 acked_sacked,\t\t\t     int flag, const struct rate_sample *rs)&#123;\tconst struct inet_connection_sock *icsk = inet_csk(sk);\t//调用拥塞算法的钩子，bbr会用到\tif (icsk-&gt;icsk_ca_ops-&gt;cong_control) &#123;\t\ticsk-&gt;icsk_ca_ops-&gt;cong_control(sk, rs);\t\treturn;\t&#125;\t//拥塞状态\tif (tcp_in_cwnd_reduction(sk)) &#123;\t\t/* Reduce cwnd if state mandates */\t\ttcp_cwnd_reduction(sk, acked_sacked, rs-&gt;losses, flag);\t&#125; else if (tcp_may_raise_cwnd(sk, flag)) &#123;//乱需严重可能提升拥塞窗口\t\t/* Advance cwnd if state allows */\t\t//拥塞避免的钩子\t\ttcp_cong_avoid(sk, ack, acked_sacked);\t&#125;\t//更新pacing速率\ttcp_update_pacing_rate(sk);&#125;\n\n如果拥塞算法实现了cong_control则直接调用回调后返回**，否则根据当前的拥塞状态，决定是增加拥塞窗口或者减小拥塞窗口**。 如果为TCPF_CA_CWR | TCPF_CA_Recovery状态则调用tcp_cwnd_reduction尝试减小拥塞窗口具体代码如下所示：\nvoid tcp_cwnd_reduction(struct sock *sk, int newly_acked_sacked, int newly_lost, int flag)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tint sndcnt = 0;\tint delta = tp-&gt;snd_ssthresh - tcp_packets_in_flight(tp);\t//没有新确认的数据包\tif (newly_acked_sacked &lt;= 0 || WARN_ON_ONCE(!tp-&gt;prior_cwnd))\t\treturn;\t//快速恢复期间累计确认的数据包数\ttp-&gt;prr_delivered += newly_acked_sacked;\t//计算允许发送的新数据包数\tif (delta &lt; 0) &#123;\t//如果确认了N个包，最多允许发送N/2个新包。\t\tu64 dividend = (u64)tp-&gt;snd_ssthresh * tp-&gt;prr_delivered +\t\t\t       tp-&gt;prior_cwnd - 1;\t\tsndcnt = div_u64(dividend, tp-&gt;prior_cwnd) - tp-&gt;prr_out;\t&#125; else &#123;\t//每确认一个数据包，至少可以发送一个新数据包\t\tsndcnt = max_t(int, tp-&gt;prr_delivered - tp-&gt;prr_out,\t\t\t       newly_acked_sacked);\t\tif (flag &amp; FLAG_SND_UNA_ADVANCED &amp;&amp; !newly_lost)\t\t\tsndcnt++;\t\tsndcnt = min(delta, sndcnt);\t&#125;\t/* Force a fast retransmit upon entering fast recovery */\tsndcnt = max(sndcnt, (tp-&gt;prr_out ? 0 : 1));\t//更新拥塞窗口\ttcp_snd_cwnd_set(tp, tcp_packets_in_flight(tp) + sndcnt);&#125;\n\ntcp_cwnd_reduction设置拥塞窗口的核心思想可以总结为：每收到一个ack就计算还能发送多少个数据包，并把 cwnd 设置成 当前在途数据+允许再发。\n回到tcp_cong_control中，如果数据包确认了新数据或乱序严重，则调用tcp_cong_avoid执行拥塞避免（直接调用拥塞算法注册的钩子）。\n最后调用tcp_update_pacing_rate根据当前的cwnd和rtt计算一个sk_pacing_rate给拥塞算法使用。具体代码如下所示：\n//三次握手收到ack完成后会调用，tcpack中也会调用static void tcp_update_pacing_rate(struct sock *sk)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\tu64 rate;\t/* set sk_pacing_rate to 200 % of current rate (mss * cwnd / srtt) */\trate = (u64)tp-&gt;mss_cache * ((USEC_PER_SEC / 100) &lt;&lt; 3);\t/* current rate is (cwnd * mss) / srtt\t * In Slow Start [1], set sk_pacing_rate to 200 % the current rate.\t * In Congestion Avoidance phase, set it to 120 % the current rate.\t *\t * [1] : Normal Slow Start condition is (tp-&gt;snd_cwnd &lt; tp-&gt;snd_ssthresh)\t *\t If snd_cwnd &gt;= (tp-&gt;snd_ssthresh / 2), we are approaching\t *\t end of slow start and should slow down.\t */\t//慢启动和拥塞避免 \tif (tcp_snd_cwnd(tp) &lt; tp-&gt;snd_ssthresh / 2)\t\trate *= READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_pacing_ss_ratio);\telse\t\trate *= READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_pacing_ca_ratio);\trate *= max(tcp_snd_cwnd(tp), tp-&gt;packets_out);\t//核心公式\tif (likely(tp-&gt;srtt_us))\t\tdo_div(rate, tp-&gt;srtt_us);\t/* WRITE_ONCE() is needed because sch_fq fetches sk_pacing_rate\t * without any lock. We want to make sure compiler wont store\t * intermediate values in this location.\t */\tWRITE_ONCE(sk-&gt;sk_pacing_rate, min_t(u64, rate,\t\t\t\t\t     sk-&gt;sk_max_pacing_rate));&#125;\n\ntcp_ack中完成对拥塞窗口处理后，调用tcp_xmit_recovery尝试发送重传队列中或者发送队列中的数据包（正常是REXMIT_LOST， 如果是FRTO则会设置为REXMIT_NEW）具体代码如下所示：\nstatic void tcp_xmit_recovery(struct sock *sk, int rexmit)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tif (rexmit == REXMIT_NONE || sk-&gt;sk_state == TCP_SYN_SENT)\t\treturn;\t//根据拥塞处理的结果 决定是否从发送队列中发送数据包\tif (unlikely(rexmit == REXMIT_NEW)) &#123;\t\t__tcp_push_pending_frames(sk, tcp_current_mss(sk),\t\t\t\t\t  TCP_NAGLE_OFF);\t\tif (after(tp-&gt;snd_nxt, tp-&gt;high_seq))\t\t\treturn;\t\ttp-&gt;frto = 0;\t&#125;\t//从重传队列中发送数据包\ttcp_xmit_retransmit_queue(sk);&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP确认"]},{"title":"TCP输入 tcp_v4_rcv 及RFS/RPS处理","url":"/2025/12/31/TCP%20%E8%BE%93%E5%85%A5tcp_v4_rcv/","content":"当IP层收到报文处理完成后，会根据数据包中的PROTO找到对应的传输层处理函数，对于TCP来说就是tcp_v4_rcv（通过inet_add_protocol注册），tcp_v4_rcv作为接收数据的总入口，首先进行合法性校验，然后根据数据包查找对应的套接字结构，之后根据查找结果进行不同状态的处理，这里重点关注数据包建立连接状态下的处理逻辑（三次握手状态处理在前文有过介绍），tcp_v4_rcv具体代码如下所示：\nint tcp_v4_rcv(struct sk_buff *skb)&#123;\tstruct net *net = dev_net(skb-&gt;dev);\tenum skb_drop_reason drop_reason;\tint sdif = inet_sdif(skb);\tint dif = inet_iif(skb);\tconst struct iphdr *iph;\tconst struct tcphdr *th;\tbool refcounted;\tstruct sock *sk;\tint ret;\tdrop_reason = SKB_DROP_REASON_NOT_SPECIFIED;\t//不是发给本机的包，直接丢\tif (skb-&gt;pkt_type != PACKET_HOST)\t\tgoto discard_it;\t/* Count it even if it&#x27;s bad */\t__TCP_INC_STATS(net, TCP_MIB_INSEGS);\t//拉一下tcp头，确保线性部分至少有TCP头\tif (!pskb_may_pull(skb, sizeof(struct tcphdr)))\t\tgoto discard_it;\tth = (const struct tcphdr *)skb-&gt;data;\t//坏包\tif (unlikely(th-&gt;doff &lt; sizeof(struct tcphdr) / 4)) &#123;\t\tdrop_reason = SKB_DROP_REASON_PKT_TOO_SMALL;\t\tgoto bad_packet;\t&#125;\t//把选项部分也拉到线性部分\tif (!pskb_may_pull(skb, th-&gt;doff * 4))\t\tgoto discard_it;\t/* An explanation is required here, I think.\t * Packet length and doff are validated by header prediction,\t * provided case of th-&gt;doff==0 is eliminated.\t * So, we defer the checks. */\tif (skb_checksum_init(skb, IPPROTO_TCP, inet_compute_pseudo))\t\tgoto csum_error;\tth = (const struct tcphdr *)skb-&gt;data;\tiph = ip_hdr(skb);lookup:\t//查找数据包所属的sock，(正常三次握手流程中，对于服务端来说，第一次握手找到的是listensock，收到\t//客户端的ack后查找到的应该是reqsock)\tsk = __inet_lookup_skb(net-&gt;ipv4.tcp_death_row.hashinfo,\t\t\t       skb, __tcp_hdrlen(th), th-&gt;source,\t\t\t       th-&gt;dest, sdif, &amp;refcounted);\tif (!sk)\t\tgoto no_tcp_socket;process:\tif (sk-&gt;sk_state == TCP_TIME_WAIT)\t\tgoto do_time_wait;\t//如果是正常的三次握手，服务端收到客户端的ack包就会进入这个分支\tif (sk-&gt;sk_state == TCP_NEW_SYN_RECV) &#123;\t\t//先获取管理半连接状态的结构\t\tstruct request_sock *req = inet_reqsk(sk);\t\tbool req_stolen = false;\t\tstruct sock *nsk;\t\t//这个sk是监听套接字\t\tsk = req-&gt;rsk_listener;\t\t//ipsec相关\t\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\t\t\tdrop_reason = SKB_DROP_REASON_XFRM_POLICY;\t\telse\t\t\tdrop_reason = tcp_inbound_md5_hash(sk, skb,\t\t\t\t\t\t   &amp;iph-&gt;saddr, &amp;iph-&gt;daddr,\t\t\t\t\t\t   AF_INET, dif, sdif);\t\tif (unlikely(drop_reason)) &#123;\t\t\tsk_drops_add(sk, skb);\t\t\treqsk_put(req);\t\t\tgoto discard_it;\t\t&#125;\t\t//计算校验和，如果这里硬件计算过了，软件就不算了\t\tif (tcp_checksum_complete(skb)) &#123;\t\t\treqsk_put(req);\t\t\tgoto csum_error;\t\t&#125;\t\t//这里sk是listen的sk，会不是listen状态吗？比如close了？\t\tif (unlikely(sk-&gt;sk_state != TCP_LISTEN)) &#123;\t\t\t//如果启用了端口重用看看能不找到一个可以替代的sk，然后继续，否则goto要重新查找一下\t\t\t//这里如果再找一便没找到是不是就直接rst了？？？\t\t\tnsk = reuseport_migrate_sock(sk, req_to_sk(req), skb);\t\t\tif (!nsk) &#123;\t\t\t\tinet_csk_reqsk_queue_drop_and_put(sk, req);\t\t\t\tgoto lookup;\t\t\t//这里是找到了可以迁移的sock\t\t\tsk = nsk;\t\t\t/* reuseport_migrate_sock() has already held one sk_refcnt\t\t\t * before returning.\t\t\t */\t\t&#125; else &#123;\t\t\t/* We own a reference on the listener, increase it again\t\t\t * as we might lose it too soon.\t\t\t */\t\t\tsock_hold(sk);\t\t&#125;\t\trefcounted = true;\t\tnsk = NULL;\t\t//bfp相关的过滤钩子，返回0表示被接收\t\tif (!tcp_filter(sk, skb)) &#123;\t\t\tth = (const struct tcphdr *)skb-&gt;data;\t\t\tiph = ip_hdr(skb);\t\t\t//根据数据包字段填充私有控制结构\t\t\ttcp_v4_fill_cb(skb, iph, th);\t\t\t//创建新的sock，这里可能返回监听sk，或者空，或者新的sock\t\t\tnsk = tcp_check_req(sk, skb, req, false, &amp;req_stolen);\t\t&#125; else &#123;\t\t\tdrop_reason = SKB_DROP_REASON_SOCKET_FILTER;\t\t&#125;\t\tif (!nsk) &#123;//如果返回空则表示收到的报文可能不合法，打概率默默丢弃了\t\t\treqsk_put(req);\t\t\tif (req_stolen) &#123;\t\t\t\t/* Another cpu got exclusive access to req\t\t\t\t * and created a full blown socket.\t\t\t\t * Try to feed this packet to this socket\t\t\t\t * instead of discarding it.\t\t\t\t */\t\t\t\ttcp_v4_restore_cb(skb);\t\t\t\tsock_put(sk);\t\t\t\tgoto lookup;\t\t\t&#125;\t\t\tgoto discard_and_relse;\t\t&#125;\t\t//连接跟踪相关复位\t\tnf_reset_ct(skb);\t\tif (nsk == sk) &#123;\t\t\treqsk_put(req);\t\t\ttcp_v4_restore_cb(skb);\t\t//正常情况下走这个流程并返回0 \t\t&#125; else if (tcp_child_process(sk, nsk, skb)) &#123;\t\t\ttcp_v4_send_reset(nsk, skb);\t\t\tgoto discard_and_relse;\t\t&#125; else &#123;\t\t\tsock_put(sk);\t\t\treturn 0;\t\t&#125;\t&#125;\tif (static_branch_unlikely(&amp;ip4_min_ttl)) &#123;\t\t/* min_ttl can be changed concurrently from do_ip_setsockopt() */\t\tif (unlikely(iph-&gt;ttl &lt; READ_ONCE(inet_sk(sk)-&gt;min_ttl))) &#123;\t\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMINTTLDROP);\t\t\tdrop_reason = SKB_DROP_REASON_TCP_MINTTL;\t\t\tgoto discard_and_relse;\t\t&#125;\t&#125;\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb)) &#123;\t\tdrop_reason = SKB_DROP_REASON_XFRM_POLICY;\t\tgoto discard_and_relse;\t&#125;\tdrop_reason = tcp_inbound_md5_hash(sk, skb, &amp;iph-&gt;saddr,\t\t\t\t\t   &amp;iph-&gt;daddr, AF_INET, dif, sdif);\tif (drop_reason)\t\tgoto discard_and_relse;\tnf_reset_ct(skb);\t//bpf相关 安全相关的钩子\tif (tcp_filter(sk, skb)) &#123;\t\tdrop_reason = SKB_DROP_REASON_SOCKET_FILTER;\t\tgoto discard_and_relse;\t&#125;\tth = (const struct tcphdr *)skb-&gt;data;\tiph = ip_hdr(skb);\ttcp_v4_fill_cb(skb, iph, th);\tskb-&gt;dev = NULL;\t//处理三次握手被动打开接收syn段\tif (sk-&gt;sk_state == TCP_LISTEN) &#123;\t\tret = tcp_v4_do_rcv(sk, skb);\t\tgoto put_and_return;\t&#125;\tsk_incoming_cpu_update(sk);\tbh_lock_sock_nested(sk);\ttcp_segs_in(tcp_sk(sk), skb);\tret = 0;\t//用户没有持有sock \tif (!sock_owned_by_user(sk)) &#123;\t\tret = tcp_v4_do_rcv(sk, skb);\t&#125; else &#123;\t//用户持有sock 将数据包放入后备队列\t\tif (tcp_add_backlog(sk, skb, &amp;drop_reason))\t\t\tgoto discard_and_relse;\t&#125;\tbh_unlock_sock(sk);put_and_return:\tif (refcounted)\t\tsock_put(sk);\treturn ret;//没有找到sk的情况no_tcp_socket:\tdrop_reason = SKB_DROP_REASON_NO_SOCKET;\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\t\tgoto discard_it;\ttcp_v4_fill_cb(skb, iph, th);\tif (tcp_checksum_complete(skb)) &#123;csum_error:\t\tdrop_reason = SKB_DROP_REASON_TCP_CSUM;\t\ttrace_tcp_bad_csum(skb);\t\t__TCP_INC_STATS(net, TCP_MIB_CSUMERRORS);bad_packet:\t\t__TCP_INC_STATS(net, TCP_MIB_INERRS);\t&#125; else &#123;\t\ttcp_v4_send_reset(NULL, skb);//这里会发rst\t&#125;discard_it:\tSKB_DR_OR(drop_reason, NOT_SPECIFIED);\t/* Discard frame. */\tkfree_skb_reason(skb, drop_reason);\treturn 0;discard_and_relse:\tsk_drops_add(sk, skb);\tif (refcounted)\t\tsock_put(sk);\tgoto discard_it;//tw状态的处理do_time_wait:\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) &#123;\t\tdrop_reason = SKB_DROP_REASON_XFRM_POLICY;\t\tinet_twsk_put(inet_twsk(sk));\t\tgoto discard_it;\t&#125;\ttcp_v4_fill_cb(skb, iph, th);\tif (tcp_checksum_complete(skb)) &#123;\t\tinet_twsk_put(inet_twsk(sk));\t\tgoto csum_error;\t&#125;\tswitch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) &#123;\tcase TCP_TW_SYN: &#123;\t\tstruct sock *sk2 = inet_lookup_listener(net,\t\t\t\t\t\t\tnet-&gt;ipv4.tcp_death_row.hashinfo,\t\t\t\t\t\t\tskb, __tcp_hdrlen(th),\t\t\t\t\t\t\tiph-&gt;saddr, th-&gt;source,\t\t\t\t\t\t\tiph-&gt;daddr, th-&gt;dest,\t\t\t\t\t\t\tinet_iif(skb),\t\t\t\t\t\t\tsdif);\t\tif (sk2) &#123;\t\t\tinet_twsk_deschedule_put(inet_twsk(sk));\t\t\tsk = sk2;\t\t\ttcp_v4_restore_cb(skb);\t\t\trefcounted = false;\t\t\tgoto process;\t\t&#125;\t&#125;\t\t/* to ACK */\t\tfallthrough;\tcase TCP_TW_ACK:\t\ttcp_v4_timewait_ack(sk, skb);\t\tbreak;\tcase TCP_TW_RST:\t\ttcp_v4_send_reset(sk, skb);\t\tinet_twsk_deschedule_put(inet_twsk(sk));\t\tgoto discard_it;\tcase TCP_TW_SUCCESS:;\t&#125;\tgoto discard_it;&#125;&#125;\n\ntcp_v4_rcv首先判断报文合法性，判断是否是发往本机的数据包，是否能能将头部拉取到线性部分，以及checksum是否合法，之后根据四元组查找对应的sk\n对于已经建立连接的套接字来说，通常会steal一个sk，如果当前sk处于tw状态则直接跳转到timewait状态进行处理，如果处于三次握手TCP_NEW_SYN_RCV则会进入找到req sk后进行进一步处理，三次握手中有过具体分析。\n接下来进一步判断ttl，如果ttl过小，可能还会丢弃数据包，之后进行ipsec和安全相关的钩子判断，这里不介绍，如果是处于监听状态，则调用tcp_v4_do_rcv进行处理，如果是处于建立连接状态或其他状态同时用户没有锁住sock的话会调用tcp_v4_do_rcv进行处理，如果用户持有sock则会调用tcp_add_backlog将数据包放入后被队列，当用户释放sock的时候会重新走一便收包流程。\ntcp_v4_do_rcv具体实现如下所示：\nint tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)&#123;\tenum skb_drop_reason reason;\tstruct sock *rsk;\t//建立连接状态下的处理\tif (sk-&gt;sk_state == TCP_ESTABLISHED) &#123; /* Fast path */\t\tstruct dst_entry *dst;\t\t\t\tdst = rcu_dereference_protected(sk-&gt;sk_rx_dst,\t\t\t\t\t\tlockdep_sock_is_held(sk));\t\t//保存hash 给rps用\t\tsock_rps_save_rxhash(sk, skb);\t\t//记录数据包时从哪个硬件队列收上来的，rfs相关？？\t\tsk_mark_napi_id(sk, skb);\t\t//严重路由有效性\t\tif (dst) &#123;\t\t\tif (sk-&gt;sk_rx_dst_ifindex != skb-&gt;skb_iif ||\t\t\t    !INDIRECT_CALL_1(dst-&gt;ops-&gt;check, ipv4_dst_check,\t\t\t\t\t     dst, 0)) &#123;\t\t\t\tRCU_INIT_POINTER(sk-&gt;sk_rx_dst, NULL);\t\t\t\tdst_release(dst);\t\t\t&#125;\t\t&#125;\t\t//快速路径和慢速路径的处理\t\ttcp_rcv_established(sk, skb);\t\treturn 0;\t&#125;\treason = SKB_DROP_REASON_NOT_SPECIFIED;\tif (tcp_checksum_complete(skb))\t\tgoto csum_err;\t//处理第一次握手收到syn包和第三次（使用cookies的情况下）是不是都走这个分支？？？\t//好像这个分支里面的逻辑是只有cookies的情况下走的吧\tif (sk-&gt;sk_state == TCP_LISTEN) &#123;\t\t//处理cookie\t\tstruct sock *nsk = tcp_v4_cookie_check(sk, skb);\t\tif (!nsk)\t\t\tgoto discard;\t\tif (nsk != sk) &#123;\t\t\tif (tcp_child_process(sk, nsk, skb)) &#123;\t\t\t\trsk = nsk;\t\t\t\tgoto reset;\t\t\t&#125;\t\t\treturn 0;\t\t&#125;\t&#125; else\t\tsock_rps_save_rxhash(sk, skb);\t//第一次握手肯定走这里\t//客户端收到服务端回复的syn ack也走这里\t//返回1就会发送rst\tif (tcp_rcv_state_process(sk, skb)) &#123;\t\trsk = sk;\t\tgoto reset;\t&#125;\treturn 0;reset:\ttcp_v4_send_reset(rsk, skb);discard:\tkfree_skb_reason(skb, reason);\t/* Be careful here. If this function gets more complicated and\t * gcc suffers from register pressure on the x86, sk (in %ebx)\t * might be destroyed here. This current version compiles correctly,\t * but you have been warned.\t */\treturn 0;csum_err:\treason = SKB_DROP_REASON_TCP_CSUM;\ttrace_tcp_bad_csum(skb);\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\tgoto discard;&#125;\n\ntcp_v4_do_rcv中主要处理以下三种情况：\n\n处理已经建立连接的数据包\n监听状态下的处理\n对于其他状态（三次握手，四次挥手的中间状态）则会调用tcp_rcv_state_process进行处理\n\n这里重点关注建立连接状态的下的处理逻辑，主要分析影响rps的逻辑：\n建立连接状态下，首先调用sock_rps_save_rxhash记录数据包的hash值到sk-&gt;sk_rxhash字段中，这个hash通常是硬件回写的，而sk_rxhash 则直接影响了RPS &#x2F; RFS， 也就是会根据这个hash值将数据包丢到具体的某一个cpu上进行处理（这里注意RPS可以理解为是RFS的一种fallback机制），当建立连接完成后或者每次recv时都会将hash和当前用户程序所在的cpu上进行绑定，这一过程通过sock_rps_record_flow实现的（注意每次accept和inet_recvmsg的时候都会调用）**\n上述sock_rps_save_rxhash具体代码如下所示：\nstatic inline void sock_rps_save_rxhash(struct sock *sk,\t\t\t\t\tconst struct sk_buff *skb)&#123;#ifdef CONFIG_RPS\t/* The following WRITE_ONCE() is paired with the READ_ONCE()\t * here, and another one in sock_rps_record_flow().\t */\t//注意这个hash值是硬件计算的\tif (unlikely(READ_ONCE(sk-&gt;sk_rxhash) != skb-&gt;hash))\t\tWRITE_ONCE(sk-&gt;sk_rxhash, skb-&gt;hash);#endif&#125;\n\n上述sock_rps_record_flow代码如下所示：\n//非常关键 开启rps下决定数据包被丢到哪个cpu  这个cpu也是应用程序所在的cpustatic inline void sock_rps_record_flow(const struct sock *sk)&#123;#ifdef CONFIG_RPS\tif (static_branch_unlikely(&amp;rfs_needed)) &#123;\t\t/* Reading sk-&gt;sk_rxhash might incur an expensive cache line\t\t * miss.\t\t *\t\t * TCP_ESTABLISHED does cover almost all states where RFS\t\t * might be useful, and is cheaper [1] than testing :\t\t *\tIPv4: inet_sk(sk)-&gt;inet_daddr\t\t * \tIPv6: ipv6_addr_any(&amp;sk-&gt;sk_v6_daddr)\t\t * OR\tan additional socket flag\t\t * [1] : sk_state and sk_prot are in the same cache line.\t\t */\t\t//必须是建立连接状态\t\tif (sk-&gt;sk_state == TCP_ESTABLISHED) &#123;\t\t\t/* This READ_ONCE() is paired with the WRITE_ONCE()\t\t\t * from sock_rps_save_rxhash() and sock_rps_reset_rxhash().\t\t\t */\t\t\tsock_rps_record_flow_hash(READ_ONCE(sk-&gt;sk_rxhash));\t\t&#125;\t&#125;#endif&#125;static inline void sock_rps_record_flow_hash(__u32 hash)&#123;#ifdef CONFIG_RPS\tstruct rps_sock_flow_table *sock_flow_table;\trcu_read_lock();\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\trps_record_sock_flow(sock_flow_table, hash);\trcu_read_unlock();#endif&#125;\n\n上述sock_rps_record_flow最终会调用rps_record_sock_flow，主要工作就干一件事，就是把hash值和cpu关联起来写入一张全局表，后续开启的rps的时候会调用get_rps_cpu对数据包进行重定向！！！rps_record_sock_flow具体代码如下所示：\nstatic inline void rps_record_sock_flow(struct rps_sock_flow_table *table,\t\t\t\t\tu32 hash)&#123;\tif (table &amp;&amp; hash) &#123;\t\tunsigned int index = hash &amp; table-&gt;mask;\t\tu32 val = hash &amp; ~rps_cpu_mask;\t\t/* We only give a hint, preemption can change CPU under us */        //当前cpu\t\tval |= raw_smp_processor_id();\t\t/* The following WRITE_ONCE() is paired with the READ_ONCE()\t\t * here, and another one in get_rps_cpu().\t\t */\t\tif (READ_ONCE(table-&gt;ents[index]) != val)\t\t\tWRITE_ONCE(table-&gt;ents[index], val);\t&#125;&#125;\n\n上面代码逻辑中关联了cpu和流的hash会写入一个全局表中，但是貌似好像会冲突，如果冲突应将就退回到rps了吧。\n这里总结一下，如果开启了rps则数据包会根据hash值重定向到应用程序所在的cpu上\n回到tcp_v4_do_rcv的逻辑中，处理完rps后会进一步验证路由的有效性，之后调用tcp_rcv_established处理tcp的快速路径和慢速路径，后续分析：-）。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输入","RFS/RPS"]},{"title":"TCP确认 tcp_ack（一）","url":"/2025/12/11/TCPack%E6%8A%A5%E6%96%87%E5%A4%84%E7%90%86tcp_ack()/","content":"tcp_ack(）用于处理接收到所有携带ack标志位的数据包，建立连接状态下的所有数据包几乎都携带ack标志位，慢速路径以及快速路径都会调用。\ntcp_ack主要目的是根据ack报文调整发送端的状态，整体工作可以分为以下几个部分：\n\nACK 合法性校验\n更新 snd_una &#x2F; 窗口 &#x2F; SACK &#x2F; ECN（确认信息 &amp; 窗口信息）\n清理重传队列，更新 RACK &#x2F; TLP &#x2F; 丢包信息\n判断 ACK 是否可疑（重复 ACK、DSACK 等），触发快速重传&#x2F;恢复逻辑\n拥塞状态转换，调用拥塞算法钩子\n\n具体代码如下所示：\n/* This routine deals with incoming acks, but not outgoing ones. */static int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct tcp_sacktag_state sack_state;\tstruct rate_sample rs = &#123; .prior_delivered = 0 &#125;;\tu32 prior_snd_una = tp-&gt;snd_una; \t\t//已经发送未被确认的序列号\tbool is_sack_reneg = tp-&gt;is_sack_reneg;  //是否是sack反悔\tu32 ack_seq = TCP_SKB_CB(skb)-&gt;seq;     //提取数据包的序号\tu32 ack = TCP_SKB_CB(skb)-&gt;ack_seq;     ///提取当前数据包确认的序列号\tint num_dupack = 0;\tint prior_packets = tp-&gt;packets_out;    //发送出去没有被ack的数量\tu32 delivered = tp-&gt;delivered;\t\t   //被确认的ack总数\tu32 lost = tp-&gt;lost;\t\t\t\t   //历史丢包数\tint rexmit = REXMIT_NONE; /* Flag to (re)transmit to recover losses *///是否需要重传的标记\tu32 prior_fack;\tsack_state.first_sackt = 0;\tsack_state.rate = &amp;rs;\tsack_state.sack_delivered = 0;\t/* We very likely will need to access rtx queue. */\t//预取\tprefetch(sk-&gt;tcp_rtx_queue.rb_node);\t/* If the ack is older than previous acks\t * then we can probably ignore it.\t */\t//当前确认的序列号在una之前\tif (before(ack, prior_snd_una)) &#123;\t\t/* RFC 5961 5.2 [Blind Data Injection Attack].[Mitigation] */\t\t//这个ack太老了(可能是注入攻击)，比往前算一个窗口还要小，那就回一个挑战ack\t\tif (before(ack, prior_snd_una - tp-&gt;max_window)) &#123;\t\t\tif (!(flag &amp; FLAG_NO_CHALLENGE_ACK))\t\t\t\ttcp_send_challenge_ack(sk);\t\t\treturn -SKB_DROP_REASON_TCP_TOO_OLD_ACK;\t\t&#125;\t\tgoto old_ack; //返回0，外面也会回复挑战ack\t&#125;\t/* If the ack includes data we haven&#x27;t sent yet, discard\t * this segment (RFC793 Section 3.9).\t */\t//太超前了\tif (after(ack, tp-&gt;snd_nxt))\t\treturn -SKB_DROP_REASON_TCP_ACK_UNSENT_DATA;\t//确认了新数据\tif (after(ack, prior_snd_una)) &#123;\t\tflag |= FLAG_SND_UNA_ADVANCED; //设置推进ack标志了，下面会用到\t\ticsk-&gt;icsk_retransmits = 0;    //重传清零#if IS_ENABLED(CONFIG_TLS_DEVICE)\t\tif (static_branch_unlikely(&amp;clean_acked_data_enabled.key))\t\t\tif (icsk-&gt;icsk_clean_acked)\t\t\t\ticsk-&gt;icsk_clean_acked(sk, ack);#endif\t&#125;\t//返回una或者sack最高的序列号\tprior_fack = tcp_is_sack(tp) ? tcp_highest_sack_seq(tp) : tp-&gt;snd_una;\t//设置在途数据包数\trs.prior_in_flight = tcp_packets_in_flight(tp);\t/* ts_recent update must be made after we are sure that the packet\t * is in window.\t */\t//slowpath会设置这个标志，这里会通过PAWS检查后更新时间戳\tif (flag &amp; FLAG_UPDATE_TS_RECENT)\t\ttcp_replace_ts_recent(tp, TCP_SKB_CB(skb)-&gt;seq);\t//这里是fastpath\tif ((flag &amp; (FLAG_SLOWPATH | FLAG_SND_UNA_ADVANCED)) ==\t    FLAG_SND_UNA_ADVANCED) &#123;\t\t/* Window is constant, pure forward advance.\t\t * No more checks are required.\t\t * Note, we use the fact that SND.UNA&gt;=SND.WL2.\t\t */\t\t//更新发送窗口更新时候的序列号，第二个参数是数据包的序号\t\ttcp_update_wl(tp, ack_seq);\t\t//更新una和确认的字节数\t\ttcp_snd_una_update(tp, ack);\t\t//设置更新窗口标志位\t\tflag |= FLAG_WIN_UPDATE;\t\t//调用拥塞算法钩子，如果有\t\ttcp_in_ack_event(sk, CA_ACK_WIN_UPDATE);\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPHPACKS);\t//这里是走slowpah\t&#125; else &#123;\t\tu32 ack_ev_flags = CA_ACK_SLOWPATH;\t\t//标记数据包是否携带数据\t\tif (ack_seq != TCP_SKB_CB(skb)-&gt;end_seq)\t\t\tflag |= FLAG_DATA;\t\telse\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPUREACKS);\t\t//这里的第三个参数是确认号， 第四个参数ack_seq是报文的序列号\t\tflag |= tcp_ack_update_window(sk, skb, ack, ack_seq);\t\t//处理sack\t\tif (TCP_SKB_CB(skb)-&gt;sacked)\t\t\tflag |= tcp_sacktag_write_queue(sk, skb, prior_snd_una,\t\t\t\t\t\t\t&amp;sack_state);\t\t//发生了拥塞，设置标志位\t\tif (tcp_ecn_rcv_ecn_echo(tp, tcp_hdr(skb))) &#123;\t\t\tflag |= FLAG_ECE;\t\t\tack_ev_flags |= CA_ACK_ECE;\t\t&#125;\t\t//有sack确认的段数，需要更新确认的总数，合理\t\tif (sack_state.sack_delivered)\t\t\ttcp_count_delivered(tp, sack_state.sack_delivered,\t\t\t\t\t    flag &amp; FLAG_ECE);\t\t//窗口是否推进了\t\tif (flag &amp; FLAG_WIN_UPDATE)\t\t\tack_ev_flags |= CA_ACK_WIN_UPDATE;\t\t//调用拥塞算法的钩子\t\ttcp_in_ack_event(sk, ack_ev_flags);\t&#125;\t/* This is a deviation from RFC3168 since it states that:\t * &quot;When the TCP data sender is ready to set the CWR bit after reducing\t * the congestion window, it SHOULD set the CWR bit only on the first\t * new data packet that it transmits.&quot;\t * We accept CWR on pure ACKs to be more robust\t * with widely-deployed TCP implementations that do this.\t */\t//如果是显示拥塞，这里立即设置回复ack的标志位，因为发送端窗口已经很小了，需要立即回复ack\ttcp_ecn_accept_cwr(sk, skb);\t/* We passed data and got it acked, remove any soft error\t * log. Something worked...\t */\tWRITE_ONCE(sk-&gt;sk_err_soft, 0);\ticsk-&gt;icsk_probes_out = 0;\ttp-&gt;rcv_tstamp = tcp_jiffies32;\tif (!prior_packets)\t\tgoto no_queue;\t/* See if we can take anything off of the retransmit queue. */\t//清重传队列\tflag |= tcp_clean_rtx_queue(sk, skb, prior_fack, prior_snd_una,\t\t\t\t    &amp;sack_state, flag &amp; FLAG_ECE);\t//根据sack的处理来调整乱续增长因子进而影响丢包判断\ttcp_rack_update_reo_wnd(sk, &amp;rs);\tif (tp-&gt;tlp_high_seq)\t\ttcp_process_tlp_ack(sk, ack, flag); \t//当前ack是否可疑（\t没有确认数据，窗口更新 纯ack sack 或者dsack ）\tif (tcp_ack_is_dubious(sk, flag)) &#123;\t\t//是否是一个纯粹的重复ack（没有确认新数据）\t\tif (!(flag &amp; (FLAG_SND_UNA_ADVANCED | \t\t\t      FLAG_NOT_DUP | FLAG_DSACKING_ACK))) &#123;\t\t\tnum_dupack = 1;\t\t\t/* Consider if pure acks were aggregated in tcp_add_backlog() */\t\t\t//统计纯ack的计数，注意这里协议站可能会聚合纯ack\t\t\tif (!(flag &amp; FLAG_DATA)) \t\t\t\tnum_dupack = max_t(u16, 1, skb_shinfo(skb)-&gt;gso_segs);\t\t&#125;\t\t//传入的是snd_una, 重复ack的数量，ack的标志位 ，传入传出rexmit会指导下面的重传\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &amp;flag,\t\t\t\t      &amp;rexmit);\t&#125;\t/* If needed, reset TLP/RTO timer when RACK doesn&#x27;t set. */\t//在清理重传队列的时候可能会设置上这个标志位，比如有新的数据包被确认的时候，或者检测到乱续或者丢包 肯定需要重新设置这个定时器了\tif (flag &amp; FLAG_SET_XMIT_TIMER)\t\ttcp_set_xmit_timer(sk);\tif ((flag &amp; FLAG_FORWARD_PROGRESS) || !(flag &amp; FLAG_NOT_DUP))\t\tsk_dst_confirm(sk);\t//更新统计字段累计收到了多少包\tdelivered = tcp_newly_delivered(sk, delivered, flag);\t//更新丢包总数\tlost = tp-&gt;lost - lost;\t\t\t/* freshly marked lost */\trs.is_ack_delayed = !!(flag &amp; FLAG_ACK_MAYBE_DELAYED);\t//更新bbr算法用到的字段\ttcp_rate_gen(sk, delivered, lost, is_sack_reneg, sack_state.rate);\ttcp_cong_control(sk, ack, delivered, flag, sack_state.rate);\ttcp_xmit_recovery(sk, rexmit);\treturn 1;no_queue:\t/* If data was DSACKed, see if we can undo a cwnd reduction. */\tif (flag &amp; FLAG_DSACKING_ACK) &#123;\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &amp;flag,\t\t\t\t      &amp;rexmit);\t\t////更新统计字段累计收到了多少包\t\ttcp_newly_delivered(sk, delivered, flag);\t&#125;\t/* If this ack opens up a zero window, clear backoff.  It was\t * being used to time the probes, and is probably far higher than\t * it needs to be for normal retransmission.\t */\ttcp_ack_probe(sk);\tif (tp-&gt;tlp_high_seq)\t\ttcp_process_tlp_ack(sk, ack, flag);\treturn 1;old_ack:\t/* If data was SACKed, tag it and see if we should send more data.\t * If data was DSACKed, see if we can undo a cwnd reduction.\t */\tif (TCP_SKB_CB(skb)-&gt;sacked) &#123;\t\tflag |= tcp_sacktag_write_queue(sk, skb, prior_snd_una,\t\t\t\t\t\t&amp;sack_state);\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &amp;flag,\t\t\t\t      &amp;rexmit);\t\ttcp_newly_delivered(sk, delivered, flag);\t\ttcp_xmit_recovery(sk, rexmit);\t&#125;\treturn 0;&#125;\n\ntcp_ack中首先判断数据包确认数据的序列号是否太老了，如果确认的序列号比往前移动一个最大接收窗口的的序号还老，会回一个challenge ack （也可能不回复，有速率限制）。如果这个确认号不是太老的话，则直接goto到最下面，判断是否协携带了sack信息，如果没有携带直接返回0，同时也有可能回challenge ack（取决于是否是建立连接的通路上）。\n如果数据包确认了还没发送的数据，则直接丢弃。\n如果数据包确认了新数据（最常见的情况），则设置推进窗口标志为下面判断slowpath和fastpath会用到。之后判断如果是慢速路径收上来的数据包，则调用tcp_replace_ts_recent通过PAWS机制决定是否需要更新时间戳，（这里注意，如果是快速路径上收到 包已经更新过了），具体代码如下所示：\nstatic void tcp_replace_ts_recent(struct tcp_sock *tp, u32 seq)&#123;\t//进入这个分支应该是处理异常的数据包，小心的更新时间戳？\tif (tp-&gt;rx_opt.saw_tstamp &amp;&amp; !after(seq, tp-&gt;rcv_wup)) &#123;\t\t/* PAWS bug workaround wrt. ACK frames, the PAWS discard\t\t * extra check below makes sure this can only happen\t\t * for pure ACK frames.  -DaveM\t\t *\t\t * Not only, also it occurs for expired timestamps.\t\t */\t\tif (tcp_paws_check(&amp;tp-&gt;rx_opt, 0))\t\t\ttcp_store_ts_recent(tp);\t&#125;&#125;\n\n接下来处理fastpath路径收包的情况，调用tcp_snd_una_update更新发送窗口（更新una）和确认的总字节数，并设置标志位。具体代码如下所示：\n/* If we update tp-&gt;snd_una, also update tp-&gt;bytes_acked */static void tcp_snd_una_update(struct tcp_sock *tp, u32 ack)&#123;\tu32 delta = ack - tp-&gt;snd_una;\tsock_owned_by_me((struct sock *)tp);\ttp-&gt;bytes_acked += delta;\ttp-&gt;snd_una = ack;&#125;\n\n如果走slowpath，则稍微复杂，首先调用tcp_ack_update_window尝试更新窗口，并尝试切换到快速路径，具体代码如下所示：\nstatic int tcp_ack_update_window(struct sock *sk, const struct sk_buff *skb, u32 ack,\t\t\t\t u32 ack_seq)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tint flag = 0;\tu32 nwin = ntohs(tcp_hdr(skb)-&gt;window); //ack报文中通告的窗口大小\t//不是syn包计算窗口缩放后的大小，因为syn包没有窗口缩放\tif (likely(!tcp_hdr(skb)-&gt;syn))\t\tnwin &lt;&lt;= tp-&gt;rx_opt.snd_wscale;\t//判断是否允许窗口更新\tif (tcp_may_update_window(tp, ack, ack_seq, nwin)) &#123;\t\tflag |= FLAG_WIN_UPDATE; \t\t //更新了发送窗口\t\ttcp_update_wl(tp, ack_seq); //记录更新发送窗口的序列号 \t\t//不同就更新窗口\t\tif (tp-&gt;snd_wnd != nwin) &#123;\t\t\ttp-&gt;snd_wnd = nwin;\t\t\t/* Note, it is the only place, where\t\t\t * fast path is recovered for sending TCP.\t\t\t */\t\t\t//注意发送恢复fast path的唯一位置\t\t\ttp-&gt;pred_flags = 0;\t\t\t//是否切换到fast的path\t\t\ttcp_fast_path_check(sk);\t\t\t//如果发送队列是空，则判断是否慢启动(由系统参数，拥塞算法等决定)\t\t\tif (!tcp_write_queue_empty(sk))\t\t\t\ttcp_slow_start_after_idle_check(sk);\t\t\t//更新最大值\t\t\tif (nwin &gt; tp-&gt;max_window) &#123;\t\t\t\ttp-&gt;max_window = nwin;\t\t\t\t//计算mss\t\t\t\ttcp_sync_mss(sk, inet_csk(sk)-&gt;icsk_pmtu_cookie);\t\t\t&#125;\t\t&#125;\t&#125;\t//更新snd_una 以及确认的byte数\ttcp_snd_una_update(tp, ack);\treturn flag;&#125;\n\ntcp_ack_update_window首先计算对端通告窗口大小，之后调用tcp_may_update_window判断是否可以进入发送更新窗口的逻辑，具体代码如下所示：\nstatic inline bool tcp_may_update_window(const struct tcp_sock *tp,\t\t\t\t\tconst u32 ack, const u32 ack_seq,\t\t\t\t\tconst u32 nwin)&#123;\treturn\tafter(ack, tp-&gt;snd_una) ||  //ACK 前进\t\tafter(ack_seq, tp-&gt;snd_wl1) ||\t//ACK 没前进 但 Seq 前进\t\t(ack_seq == tp-&gt;snd_wl1 &amp;&amp; (nwin &gt; tp-&gt;snd_wnd || !nwin)); //相桶，但窗口增大或变为0&#125;\n\ntcp_may_update_window中，如果 ACK 前进 || ACK 没前进但 SEQ 前进 || SEQ 和上次一样，但窗口变大或窗口变为 0则可以更新发送窗口，并调用tcp_fast_path_check检查下次收报是否可以切换到fastpath，这里注意**:除了三次握手之外，这里是唯一可以设置切换到fastpath的地方**，具体代码如下所示：\nstatic inline void tcp_fast_path_check(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tif (RB_EMPTY_ROOT(&amp;tp-&gt;out_of_order_queue) &amp;&amp;\t    tp-&gt;rcv_wnd &amp;&amp;\t    atomic_read(&amp;sk-&gt;sk_rmem_alloc) &lt; sk-&gt;sk_rcvbuf &amp;&amp;\t    !tp-&gt;urg_data)\t\ttcp_fast_path_on(tp);&#125;\n\ntcp_fast_path_check如果乱序队列为空，且接收窗口不为0，同时没有紧急数据则开启快速路径，开启快速路径代码如下所示：\nstatic inline void tcp_fast_path_on(struct tcp_sock *tp)&#123;\t__tcp_fast_path_on(tp, tp-&gt;snd_wnd &gt;&gt; tp-&gt;rx_opt.snd_wscale);&#125;static inline void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)&#123;\t/* mptcp hooks are only on the slow path */\tif (sk_is_mptcp((struct sock *)tp))\t\treturn;\t//设置判断是否走快速路径的标志\ttp-&gt;pred_flags = htonl((tp-&gt;tcp_header_len &lt;&lt; 26) |\t\t\t       ntohl(TCP_FLAG_ACK) |\t\t\t       snd_wnd);//通告窗口的大小&#125;\n\ntcp_ack_update_window中计算是否开启快速路径之后，如果发送队列不为空，则调用tcp_slow_start_after_idle_check判断是否由于过长时间（一个rto）没有发送数据包而进入慢启动状态。\n最后判断是否需要更新历史最大的窗口数，以及调用tcp_snd_una_update更新snd_una 以及确认的byte数。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP确认"]},{"title":"TCP确认 tcp_sacktag_write_queue（一）","url":"/2025/12/16/TCPsack%E5%A4%84%E7%90%86tcp_sacktag_write_queue/","content":"在TCP确认 tcp_ack（一）的处理流程中可看到，如果数据包携带了sack的信息则会调用tcp_sacktag_write_queue进行处理，核心逻辑可以总结成一句话：把ack报文中携带sack信息关联到重传队列的skb上，给skb打上已经选择确认的标记，同时更新各种统计信息（例如乱序检测状态），具体流程如下所示：\n\n解析sack选项并计算有多少个block\n判断第一个块是否为dsack\n处理太旧的ack\n遍历sack block，使用冒泡排序得到排序后的block\n如果能使用sack的cache则加速定位需要从队列哪里开始标记\n执行真正的标记流程\n更新cache为下次处理sack提供加速\n\n上述流程具体代码如下所示：\nstatic inttcp_sacktag_write_queue(struct sock *sk, const struct sk_buff *ack_skb,\t\t\tu32 prior_snd_una, struct tcp_sacktag_state *state)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//从TCP选项头中提取SACK信息\tconst unsigned char *ptr = (skb_transport_header(ack_skb) +\t\t\t\t    TCP_SKB_CB(ack_skb)-&gt;sacked);\t//跳过SACK选项类型和长度字段\tstruct tcp_sack_block_wire *sp_wire = (struct tcp_sack_block_wire *)(ptr+2);\tstruct tcp_sack_block sp[TCP_NUM_SACKS];\tstruct tcp_sack_block *cache;\tstruct sk_buff *skb;\t//计算SACK块数量\tint num_sacks = min(TCP_NUM_SACKS, (ptr[1] - TCPOLEN_SACK_BASE) &gt;&gt; 3);\tint used_sacks;\tbool found_dup_sack = false;\tint i, j;\tint first_sack_index;\tstate-&gt;flag = 0;\tstate-&gt;reord = tp-&gt;snd_nxt; //置重排序检测的基准为下一个发送序号\tif (!tp-&gt;sacked_out)\t\ttcp_highest_sack_reset(sk);\t//检查是否有重复的SACK\tfound_dup_sack = tcp_check_dsack(sk, ack_skb, sp_wire,\t\t\t\t\t num_sacks, prior_snd_una, state);\t/* Eliminate too old ACKs, but take into\t * account more or less fresh ones, they can\t * contain valid SACK info.\t */\t//丢弃太旧的ACK\tif (before(TCP_SKB_CB(ack_skb)-&gt;ack_seq, prior_snd_una - tp-&gt;max_window))\t\treturn 0;\t//数据都被确认了\tif (!tp-&gt;packets_out)\t\tgoto out;\tused_sacks = 0;\tfirst_sack_index = 0;\t//从 ACK 中提取所有 SACK block，对每一块做合法性验证、排除无效块、排除过旧块\t//最后得到一个干净、有意义的 sp[] SACK 列表，用于后续真正标记发送队列\tfor (i = 0; i &lt; num_sacks; i++) &#123;\t\tbool dup_sack = !i &amp;&amp; found_dup_sack;   //如果 i == 0 并且这次 ACK 确实检测出 DSACK，那么第 0 个块就是 DSACK 块\t\t//转换网络序到本地序\t\tsp[used_sacks].start_seq = get_unaligned_be32(&amp;sp_wire[i].start_seq);\t\tsp[used_sacks].end_seq = get_unaligned_be32(&amp;sp_wire[i].end_seq);\t\t//合法性检查，不合法进入这个分支，表示不值得处理\t\tif (!tcp_is_sackblock_valid(tp, dup_sack,\t\t\t\t\t    sp[used_sacks].start_seq,\t\t\t\t\t    sp[used_sacks].end_seq)) &#123;\t\t\tint mib_idx;\t\t\tif (dup_sack) &#123; //DSACK 块\t\t\t\tif (!tp-&gt;undo_marker)  //D-SACK 太乱太旧\t\t\t\t\tmib_idx = LINUX_MIB_TCPDSACKIGNOREDNOUNDO;\t\t\t\telse\t\t\t\t\tmib_idx = LINUX_MIB_TCPDSACKIGNOREDOLD; //不在关心的 undo 区间\t\t\t&#125; else &#123; //普通sack块\t\t\t\t/* Don&#x27;t count olds caused by ACK reordering */\t\t\t\tif ((TCP_SKB_CB(ack_skb)-&gt;ack_seq != tp-&gt;snd_una) &amp;&amp;\t\t\t\t    !after(sp[used_sacks].end_seq, tp-&gt;snd_una))\t\t\t\t\tcontinue;\t\t\t\tmib_idx = LINUX_MIB_TCPSACKDISCARD;\t\t\t&#125;\t\t\tNET_INC_STATS(sock_net(sk), mib_idx);\t\t\tif (i == 0)\t\t\t\tfirst_sack_index = -1;\t\t\tcontinue;\t\t&#125;\t\t/* Ignore very old stuff early */\t\tif (!after(sp[used_sacks].end_seq, prior_snd_una)) &#123;\t\t\tif (i == 0)\t\t\t\tfirst_sack_index = -1;\t\t\tcontinue;\t\t&#125;\t\tused_sacks++; //tcp_is_sackblock_valid 判定为合法\t&#125;\t/* order SACK blocks to allow in order walk of the retrans queue */\t//把合法的sack块 按序号从小排序\tfor (i = used_sacks - 1; i &gt; 0; i--) &#123;\t\tfor (j = 0; j &lt; i; j++) &#123;\t\t\tif (after(sp[j].start_seq, sp[j + 1].start_seq)) &#123;\t\t\t\tswap(sp[j], sp[j + 1]);\t\t\t\t/* Track where the first SACK block goes to */\t\t\t\tif (j == first_sack_index) //first_sack_index 排序后它可能不再是第 0 个了\t\t\t\t\tfirst_sack_index = j + 1; \t\t\t&#125;\t\t&#125;\t&#125;\tstate-&gt;mss_now = tcp_current_mss(sk);\tskb = NULL;\ti = 0;\t//当前发送队列sack 标记的个数\tif (!tp-&gt;sacked_out) &#123;\t\t/* It&#x27;s already past, so skip checking against it */\t\tcache = tp-&gt;recv_sack_cache + ARRAY_SIZE(tp-&gt;recv_sack_cache); //指向数组末尾之后的位置，表示禁用cache\t&#125; else &#123; //那证明上一次SACK处理中 打过一些标签 并且这些数据还没完全ack 那就找到这个cache\t\tcache = tp-&gt;recv_sack_cache;\t\t/* Skip empty blocks in at head of the cache */\t\twhile (tcp_sack_cache_ok(tp, cache) &amp;&amp; !cache-&gt;start_seq &amp;&amp;\t\t       !cache-&gt;end_seq)\t\t\tcache++;\t&#125;\twhile (i &lt; used_sacks) &#123;\t\tu32 start_seq = sp[i].start_seq;\t\tu32 end_seq = sp[i].end_seq;\t\tbool dup_sack = (found_dup_sack &amp;&amp; (i == first_sack_index));\t\tstruct tcp_sack_block *next_dup = NULL;\t\tif (found_dup_sack &amp;&amp; ((i + 1) == first_sack_index)) //DSACK 有两种编码方式，单块模式和双块模式 双块模式就是 SACK block#1: [1000,1200)  SACK block#2: [1000,1500)    \t\t\tnext_dup = &amp;sp[i + 1];\t\t/* Skip too early cached blocks */\t\t//如果本次收到的 SACK block 落在以前缓存的区间之后，那么把 cache 向前推进\t\twhile (tcp_sack_cache_ok(tp, cache) &amp;&amp;\t\t       !before(start_seq, cache-&gt;end_seq))\t\t\tcache++;\t\t/* Can skip some work by looking recv_sack_cache? */\t\t//有cache可以用！！！！，不是dsack块，右边界在cache start的右边(有重叠)\t\tif (tcp_sack_cache_ok(tp, cache) &amp;&amp; !dup_sack &amp;&amp;\t\t    after(end_seq, cache-&gt;start_seq)) &#123;\t\t\t/* Head todo? */\t\t\t//头部没有被缓存覆盖\t\t\tif (before(start_seq, cache-&gt;start_seq)) &#123;\t\t\t\t//二分找到发送队列中第一个 skb，用于作为后续 sacktag_walk 扫描的起点\t\t\t\tskb = tcp_sacktag_skip(skb, sk, start_seq); \t\t\t\t//对 [start_seq, cache-&gt;start_seq) 这段序列范围的数据进行 SACK 标记处理？\t\t\t\tskb = tcp_sacktag_walk(skb, sk, next_dup,\t\t\t\t\t\t       state,\t\t\t\t\t\t       start_seq,\t\t\t\t\t\t       cache-&gt;start_seq,\t\t\t\t\t\t       dup_sack);\t\t\t&#125;\t\t\t/* Rest of the block already fully processed? */\t\t\t//尾部在缓存内的情况，表示已经可以cover住了，这里直接goto出去\t\t\tif (!after(end_seq, cache-&gt;end_seq))\t\t\t\tgoto advance_sp;\t\t\tskb = tcp_maybe_skipping_dsack(skb, sk, next_dup,\t\t\t\t\t\t       state,\t\t\t\t\t\t       cache-&gt;end_seq);\t\t\t/* ...tail remains todo... */\t\t\t//尾部超出了缓存\t\t\tif (tcp_highest_sack_seq(tp) == cache-&gt;end_seq) &#123;\t\t\t\t/* ...but better entrypoint exists! */\t\t\t\tskb = tcp_highest_sack(sk);\t\t\t\tif (!skb)\t\t\t\t\tbreak;\t\t\t\tcache++;\t\t\t\tgoto walk;\t\t\t&#125;\t\t\t//二分查找下一个skb\t\t\tskb = tcp_sacktag_skip(skb, sk, cache-&gt;end_seq);\t\t\t/* Check overlap against next cached too (past this one already) */\t\t\tcache++;\t\t\tcontinue;\t\t&#125;\t\t//cache不可用的情况 找到一个skb 这个skb是下面二分查找的起点\t\tif (!before(start_seq, tcp_highest_sack_seq(tp))) &#123;\t\t\tskb = tcp_highest_sack(sk);\t\t\tif (!skb)\t\t\t\tbreak;\t\t&#125;\t\t//二分查找找到skb\t\tskb = tcp_sacktag_skip(skb, sk, start_seq);walk:\t\tskb = tcp_sacktag_walk(skb, sk, next_dup, state,\t\t\t\t       start_seq, end_seq, dup_sack);advance_sp:\t\ti++;\t&#125;\t/* Clear the head of the cache sack blocks so we can skip it next time */\tfor (i = 0; i &lt; ARRAY_SIZE(tp-&gt;recv_sack_cache) - used_sacks; i++) &#123;\t\ttp-&gt;recv_sack_cache[i].start_seq = 0;\t\ttp-&gt;recv_sack_cache[i].end_seq = 0;\t&#125;\tfor (j = 0; j &lt; used_sacks; j++)\t\ttp-&gt;recv_sack_cache[i++] = sp[j];\tif (inet_csk(sk)-&gt;icsk_ca_state != TCP_CA_Loss || tp-&gt;undo_marker)\t\ttcp_check_sack_reordering(sk, state-&gt;reord, 0);\ttcp_verify_left_out(tp);out:#if FASTRETRANS_DEBUG &gt; 0\tWARN_ON((int)tp-&gt;sacked_out &lt; 0);\tWARN_ON((int)tp-&gt;lost_out &lt; 0);\tWARN_ON((int)tp-&gt;retrans_out &lt; 0);\tWARN_ON((int)tcp_packets_in_flight(tp) &lt; 0);#endif\treturn state-&gt;flag;&#125;\n\ntcp_sacktag_write_queue首先从TCP选项头中提取SACK信息，然后计算当前数据包sack块的数量，如果当前连接还没有任何skb被ack过，则重置最高sack（可以加速找到需要标记的ack），之后调用tcp_check_dsack，检查本次sack中是否存在dsack（后续会因为是否为dsack执行不同的流程）具体代码如下所示：\n//检查dsack 好像rfc的规范是dsack的信息就在前两个块中static bool tcp_check_dsack(struct sock *sk, const struct sk_buff *ack_skb,\t\t\t    struct tcp_sack_block_wire *sp, int num_sacks,\t\t\t    u32 prior_snd_una, struct tcp_sacktag_state *state)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 start_seq_0 = get_unaligned_be32(&amp;sp[0].start_seq);//起始序列号\tu32 end_seq_0 = get_unaligned_be32(&amp;sp[0].end_seq);    //结束序列号\tu32 dup_segs;\t//第一个SACK块在ACK序号之前，标准的dsack\tif (before(start_seq_0, TCP_SKB_CB(ack_skb)-&gt;ack_seq)) &#123;\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKRECV);\t&#125; else if (num_sacks &gt; 1) &#123;\t//是否存在重叠\t\tu32 end_seq_1 = get_unaligned_be32(&amp;sp[1].end_seq);\t\tu32 start_seq_1 = get_unaligned_be32(&amp;sp[1].start_seq);\t\tif (after(end_seq_0, end_seq_1) || before(start_seq_0, start_seq_1))\t\t\treturn false;\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKOFORECV);\t&#125; else &#123;\t\treturn false;\t&#125;\t//走到这里表示一定发生了重复sack，基于mss计算重复的段数\tdup_segs = tcp_dsack_seen(tp, start_seq_0, end_seq_0, state);\tif (!dup_segs) &#123;\t/* Skip dubious DSACK */\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKIGNOREDDUBIOUS);\t\treturn false;\t&#125;\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPDSACKRECVSEGS, dup_segs);\t/* D-SACK for already forgotten data... Do dumb counting. */\tif (tp-&gt;undo_marker &amp;&amp; tp-&gt;undo_retrans &gt; 0 &amp;&amp;\t\t\t\t\t\t//需要undo，且重传过了\t    !after(end_seq_0, prior_snd_una) &amp;&amp;\t\t\t\t\t//dsack的数据是 una之前的数据 \t    after(end_seq_0, tp-&gt;undo_marker))\t\t\t\t\t//dsack的数据是 una之前的数据 \t\ttp-&gt;undo_retrans = max_t(int, 0, tp-&gt;undo_retrans - dup_segs);\t//修正计数\treturn true;&#125;\n\ntcp_check_dsack主要工作就是判断是否存在dsack，如果确实是 dsack，就更新 dsack统计、并调用tcp_dsack_seen计算重复段数，同时在必要时修相关的重传计数。计算dup_segs代码如下所示：\nstatic u32 tcp_dsack_seen(struct tcp_sock *tp, u32 start_seq,\t\t\t  u32 end_seq, struct tcp_sacktag_state *state)&#123;\tu32 seq_len, dup_segs = 1;\t//合法性检查\tif (!before(start_seq, end_seq))\t\treturn 0;\t//计算长度\tseq_len = end_seq - start_seq;\t/* Dubious DSACK: DSACKed range greater than maximum advertised rwnd */\t//合法性检查\tif (seq_len &gt; tp-&gt;max_window)\t\treturn 0;\tif (seq_len &gt; tp-&gt;mss_cache)\t\tdup_segs = DIV_ROUND_UP(seq_len, tp-&gt;mss_cache);\t//区分是TLP引起的重复还是真正的虚假重传\telse if (tp-&gt;tlp_high_seq &amp;&amp; tp-&gt;tlp_high_seq == end_seq)\t\tstate-&gt;flag |= FLAG_DSACK_TLP;\t//记录重复的段数\ttp-&gt;dsack_dups += dup_segs;\t/* Skip the DSACK if dup segs weren&#x27;t retransmitted by sender */\t//合法性检查吧\tif (tp-&gt;dsack_dups &gt; tp-&gt;total_retrans)\t\treturn 0;\t//收到dsack\ttp-&gt;rx_opt.sack_ok |= TCP_DSACK_SEEN;\t/* We increase the RACK ordering window in rounds where we receive\t * DSACKs that may have been due to reordering causing RACK to trigger\t * a spurious fast recovery. Thus RACK ignores DSACKs that happen\t * without having seen reordering, or that match TLP probes (TLP\t * is timer-driven, not triggered by RACK).\t */\t//如果不是tlp导致的重传，就告诉rack，这里会影响undoloss\tif (tp-&gt;reord_seen &amp;&amp; !(state-&gt;flag &amp; FLAG_DSACK_TLP))\t\ttp-&gt;rack.dsack_seen = 1;\t//标识sack块中包含dsack\tstate-&gt;flag |= FLAG_DSACKING_ACK;\t/* A spurious retransmission is delivered */\t//记录重复的段数\tstate-&gt;sack_delivered += dup_segs;\treturn dup_segs;&#125;\n\ntcp_dsack_seen的主要逻辑为估算重复段数（dup_segs），区分是否是 TLP 探测导致的重复（会影响后续tlp报文确认的处理），并把 DSACK 结果写回到 tp 和 state，供后续 tcp_ack_is_dubious等逻辑使用\n判断是否为dsack后，接下来会调用tcp_is_sackblock_valid把ack报文中的每个sack``block逐个校验**，并丢弃无用的block最终得到一个sp[]数组**，这个数组用于后面指导标记真正的重传队列，同时它还会维护一个first_sack_index标识原始第 0 块是否还能算 dsack\ntcp_is_sackblock_valid代码如下所示：\n//返回 true/false：这个 SACK block 对当前连接来说是不是“合理的、值得处理的”static bool tcp_is_sackblock_valid(struct tcp_sock *tp, bool is_dsack,\t\t\t\t   u32 start_seq, u32 end_seq)&#123;\t/* Too far in future, or reversed (interpretation is ambiguous) */\t//明显非法\tif (after(end_seq, tp-&gt;snd_nxt) || !before(start_seq, end_seq))\t\treturn false;\t/* Nasty start_seq wrap-around check (see comments above) */\t//明显非法\tif (!before(start_seq, tp-&gt;snd_nxt))\t\treturn false;\t/* In outstanding window? ...This is valid exit for D-SACKs too.\t * start_seq == snd_una is non-sensical (see comments above)\t */\t//正常 SACK 的合法范围\tif (after(start_seq, tp-&gt;snd_una))\t\treturn true;\t//这种情况就应直接判非法，但是dsack需要进一步考虑\tif (!is_dsack || !tp-&gt;undo_marker)\t\treturn false;\t/* ...Then it&#x27;s D-SACK, and must reside below snd_una completely */\t//endseq不合法\tif (after(end_seq, tp-&gt;snd_una))\t\treturn false;\t//这个块在undo_marker右边 虚假重传？ true\tif (!before(start_seq, tp-&gt;undo_marker))\t\treturn true;\t/* Too old */\t//太老\tif (!after(end_seq, tp-&gt;undo_marker))\t\treturn false;\t/* Undo_marker boundary crossing (overestimates a lot). Known already:\t *   start_seq &lt; undo_marker and end_seq &gt;= undo_marker.\t */\t//start和end跨度不是太大\treturn !before(start_seq, end_seq - tp-&gt;max_window);&#125;\n\ntcp_is_sackblock_valid 主要逻辑为根据发送端当前窗口状态（snd_una / snd_nxt / undo_marker）判断一个 sack block 是否在值得处理的序列区间内。例如sack确认未确认的数据直接返回fasle，如果是dsack且这个块的startseq在undo_marker右边则认为是虚假重传。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP确认"]},{"title":"TCP确认 tcp_sacktag_write_queue（三）","url":"/2025/12/22/TCPsack%E5%A4%84%E7%90%86tcp_sacktag_write_queue%EF%BC%883%EF%BC%89/","content":"前文TCP确认 tcp_sacktag_write_queue（二）中分析了tcp_sacktag_walk会调用tcp_match_skb_to_sack尝试合并dsack（其实就是拆分数据包，方便后续标记）。\n如果合并失败了（没有拆分）或者根本没有满足合并条件，则会调用tcp_shift_skb_data进行处理，tcp_shift_skb_data的核心思想也是对重传队列做合并，优化sack的数据，并尽量向前合并， 具体代码如下所示：\n//核心就是把skb往前一个挪动 前提是前一个skb必须是sack的。tcp_shift_skb_data不是必须的 这是优化策略// 前// prev: [#### SACK ####]// skb : [++++++SACK++++][----未SACK----]// next: [#### SACK ####]// 后// prev: [#### SACK ####][++++++SACK++++]// skb : [----未SACK----]// next: [#### SACK ####]//另外就是努力去填hole吧static struct sk_buff *tcp_shift_skb_data(struct sock *sk, struct sk_buff *skb,\t\t\t\t\t  struct tcp_sacktag_state *state,\t\t\t\t\t  u32 start_seq, u32 end_seq,\t\t\t\t\t  bool dup_sack)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *prev;\tint mss;\tint pcount = 0;\tint len;\tint in_sack;\t/* Normally R but no L won&#x27;t result in plain S */\t//如果数据包仅被标记为重传或者丢失则不处理只有 重传标记而没有丢失标记时，不太可能转成普通 SACK状态\tif (!dup_sack &amp;&amp;\t    (TCP_SKB_CB(skb)-&gt;sacked &amp; (TCPCB_LOST|TCPCB_SACKED_RETRANS)) == TCPCB_SACKED_RETRANS)\t\tgoto fallback;\t//SKB是否可合并\tif (!skb_can_shift(skb))\t\tgoto fallback;\t/* This frame is about to be dropped (was ACKed). */\t//已经确认过不处理\tif (!after(TCP_SKB_CB(skb)-&gt;end_seq, tp-&gt;snd_una))\t\tgoto fallback;\t/* Can only happen with delayed DSACK + discard craziness */\t//获取前一个skb\tprev = skb_rb_prev(skb);\tif (!prev)\t\tgoto fallback;\t//!!!!!!!!!!!!!前一个SKB必须且只能被标记为SACKED_ACKED!!!!!\tif ((TCP_SKB_CB(prev)-&gt;sacked &amp; TCPCB_TAGBITS) != TCPCB_SACKED_ACKED)\t\tgoto fallback;\t//大概率通过\tif (!tcp_skb_can_collapse(prev, skb))\t\tgoto fallback;\t//当前skb是否完全被sack块cover住了\tin_sack = !after(start_seq, TCP_SKB_CB(skb)-&gt;seq) &amp;&amp;\t\t  !before(end_seq, TCP_SKB_CB(skb)-&gt;end_seq);\t//完全cover住的情况\tif (in_sack) &#123;\t\tlen = skb-&gt;len; \t\t\t\t//记录数据包长度\t\tpcount = tcp_skb_pcount(skb);   //计算段数\t\tmss = tcp_skb_seglen(skb);\t\t//计算mss\t\t/* TODO: Fix DSACKs to not fragment already SACKed and we can\t\t * drop this restriction as unnecessary\t\t */\t\tif (mss != tcp_skb_seglen(prev)) //两个mss不同，不处理\t\t\tgoto fallback;\t&#125; else &#123; //检查SKB是否完全在SACK之前\t\tif (!after(TCP_SKB_CB(skb)-&gt;end_seq, start_seq))\t\t\tgoto noop;\t\t/* CHECKME: This is non-MSS split case only?, this will\t\t * cause skipped skbs due to advancing loop btw, original\t\t * has that feature too\t\t */\t\t//是否可以分割\t\tif (tcp_skb_pcount(skb) &lt;= 1)\t\t\tgoto noop;\t\t//SACK 的 start 在 skb 起始序号的左边或相等\t\tin_sack = !after(start_seq, TCP_SKB_CB(skb)-&gt;seq);\t\tif (!in_sack) &#123;\t\t\t/* TODO: head merge to next could be attempted here\t\t\t * if (!after(TCP_SKB_CB(skb)-&gt;end_seq, end_seq)),\t\t\t * though it might not be worth of the additional hassle\t\t\t *\t\t\t * ...we can probably just fallback to what was done\t\t\t * previously. We could try merging non-SACKed ones\t\t\t * as well but it probably isn&#x27;t going to buy off\t\t\t * because later SACKs might again split them, and\t\t\t * it would make skb timestamp tracking considerably\t\t\t * harder problem.\t\t\t */\t\t\tgoto fallback;\t\t&#125;\t\t//左侧或者完全cover住的情况 重叠长度 = SACK结束序列号 - SKB起始序列号\t\tlen = end_seq - TCP_SKB_CB(skb)-&gt;seq;\t\tBUG_ON(len &lt; 0);\t\tBUG_ON(len &gt; skb-&gt;len);\t\t/* MSS boundaries should be honoured or else pcount will\t\t * severely break even though it makes things bit trickier.\t\t * Optimize common case to avoid most of the divides\t\t */\t\t//计算mss\t\tmss = tcp_skb_mss(skb);\t\t/* TODO: Fix DSACKs to not fragment already SACKed and we can\t\t * drop this restriction as unnecessary\t\t */\t\t//MSS对齐 前面几个段必须mss\t\tif (mss != tcp_skb_seglen(prev))\t\t\tgoto fallback;\t\tif (len == mss) &#123;\t\t\tpcount = 1;\t\t&#125; else if (len &lt; mss) &#123;\t\t\tgoto noop;\t\t&#125; else &#123;\t\t\tpcount = len / mss;\t\t\tlen = pcount * mss;\t\t&#125;\t&#125;\t/* tcp_sacktag_one() won&#x27;t SACK-tag ranges below snd_una */\t// 检查合并部分是否已完全确认\tif (!after(TCP_SKB_CB(skb)-&gt;seq + len, tp-&gt;snd_una))\t\tgoto fallback;\t//将数据从skb移动到prev\tif (!tcp_skb_shift(prev, skb, pcount, len))\t\tgoto fallback;\t//更新数据包的字段，返回false表示数据包合并了一部分！ 就不会走下面的逻辑了\tif (!tcp_shifted_skb(sk, prev, skb, state, pcount, len, mss, dup_sack))\t\tgoto out;\t/* Hole filled allows collapsing with the next as well, this is very\t * useful when hole on every nth skb pattern happens\t */\t//看看后面的skb是否可以合并\tskb = skb_rb_next(prev);\tif (!skb)\t\tgoto out;\tif (!skb_can_shift(skb) ||\t\t\t\t\t\t\t\t\t\t\t\t//缓冲区不符合要求\t    ((TCP_SKB_CB(skb)-&gt;sacked &amp; TCPCB_TAGBITS) != TCPCB_SACKED_ACKED) || //不是纯 SACKED_ACKED！！ 其实是把hole给填上\t    (mss != tcp_skb_seglen(skb)))\t\t\t\t\t\t\t\t\t\t//mss不一致\t\tgoto out;\tif (!tcp_skb_can_collapse(prev, skb))\t\tgoto out;\tlen = skb-&gt;len;\tpcount = tcp_skb_pcount(skb);\t//和上面一样\tif (tcp_skb_shift(prev, skb, pcount, len))\t\ttcp_shifted_skb(sk, prev, skb, state, pcount,\t\t\t\tlen, mss, 0);out:\treturn prev;noop:\treturn skb;fallback:\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKSHIFTFALLBACK);\treturn NULL;&#125;\n\ntcp_shift_skb_data可以理解为处理sack的优化函数，发现当前skb的头部已经被sack覆盖并且前一个skb纯sack时，会把这个部分数据挪到前一个skb的尾部，让队列更紧凑，合并的前提需要满足一系列条件，例如不能是重复的sack，skb的物理层面可以合并，前一个数据包不能为空，最重要的是前一个skb必须是已经被完全sack的，如果不是这里直接fallback了，之后进一步skb的序号是否完全被sack的范围覆盖了，不论完全覆盖还是部分覆盖，最终会计算一个len，然后调用tcp_skb_shift在物理层面完成数据包的移动。\n如果合并成功了，则会进一步调用tcp_shifted_skb更新数据包的字段，相当于是对于上面物理合并完成之后的记账工作（核心是里面调用tcp_sacktag_one对tp的很多字段进行更新），tcp_shifted_skb具体代码如下所示：\nstatic bool tcp_shifted_skb(struct sock *sk, struct sk_buff *prev,\t\t\t    struct sk_buff *skb,\t\t\t    struct tcp_sacktag_state *state,\t\t\t    unsigned int pcount, int shifted, int mss,\t\t\t    bool dup_sack)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//这里计算的start和end是前面移动的范围\tu32 start_seq = TCP_SKB_CB(skb)-&gt;seq;\t/* start of newly-SACKed */\tu32 end_seq = start_seq + shifted;\t/* end of newly-SACKed */\tBUG_ON(!pcount);\t/* Adjust counters and hints for the newly sacked sequence\t * range but discard the return value since prev is already\t * marked. We must tag the range first because the seq\t * advancement below implicitly advances\t * tcp_highest_sack_seq() when skb is highest_sack.\t */\t//注意这里丢弃了返回值，调用这个函数的地方是skb的数据一部分已经移动到prev了 切记！\t//感觉这丢弃返回值原因是就想修正一下计数\ttcp_sacktag_one(sk, state, TCP_SKB_CB(skb)-&gt;sacked,\t\t\tstart_seq, end_seq, dup_sack, pcount,\t\t\ttcp_skb_timestamp_us(skb));\t//更新采样字段，拥塞算法可能会用到\ttcp_rate_skb_delivered(sk, skb, state-&gt;rate);\t//入如果是丢失的第一个包 修正lost_cnt_hint\tif (skb == tp-&gt;lost_skb_hint)\t\ttp-&gt;lost_cnt_hint += pcount;\t//注意这里修改了 挪到prev后的序列号\tTCP_SKB_CB(prev)-&gt;end_seq += shifted;\tTCP_SKB_CB(skb)-&gt;seq += shifted;\t//更新段数\ttcp_skb_pcount_add(prev, pcount);\tWARN_ON_ONCE(tcp_skb_pcount(skb) &lt; pcount);\ttcp_skb_pcount_add(skb, -pcount);\t/* When we&#x27;re adding to gso_segs == 1, gso_size will be zero,\t * in theory this shouldn&#x27;t be necessary but as long as DSACK\t * code can come after this skb later on it&#x27;s better to keep\t * setting gso_size to something.\t */\tif (!TCP_SKB_CB(prev)-&gt;tcp_gso_size)\t\tTCP_SKB_CB(prev)-&gt;tcp_gso_size = mss;\t/* CHECKME: To clear or not to clear? Mimics normal skb currently */\tif (tcp_skb_pcount(skb) &lt;= 1)\t\tTCP_SKB_CB(skb)-&gt;tcp_gso_size = 0;\t/* Difference in this won&#x27;t matter, both ACKed by the same cumul. ACK */\tTCP_SKB_CB(prev)-&gt;sacked |= (TCP_SKB_CB(skb)-&gt;sacked &amp; TCPCB_EVER_RETRANS);\t//这里很关键，决定了外面 tcp_shift_skb_data 是否能继续合并！！！\tif (skb-&gt;len &gt; 0) &#123;\t\tBUG_ON(!tcp_skb_pcount(skb));\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKSHIFTED);\t\treturn false;\t&#125;\t/* Whole SKB was eaten :-) */\t//这里当前的数据包都被eaten了  继续修正重传用到的字段\tif (skb == tp-&gt;retransmit_skb_hint)\t\ttp-&gt;retransmit_skb_hint = prev;\tif (skb == tp-&gt;lost_skb_hint) &#123;\t\ttp-&gt;lost_skb_hint = prev;\t\ttp-&gt;lost_cnt_hint -= tcp_skb_pcount(prev);\t&#125;\t//合并标志位\tTCP_SKB_CB(prev)-&gt;tcp_flags |= TCP_SKB_CB(skb)-&gt;tcp_flags;\tTCP_SKB_CB(prev)-&gt;eor = TCP_SKB_CB(skb)-&gt;eor;\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN)\t\tTCP_SKB_CB(prev)-&gt;end_seq++;\t//更新最高 SACK 指针\tif (skb == tcp_highest_sack(sk))\t\ttcp_advance_highest_sack(sk, skb);\t//处理时间戳\ttcp_skb_collapse_tstamp(prev, skb);\tif (unlikely(TCP_SKB_CB(prev)-&gt;tx.delivered_mstamp))\t\tTCP_SKB_CB(prev)-&gt;tx.delivered_mstamp = 0;\t//从重传队列删除并释放这个 skb\ttcp_rtx_queue_unlink_and_free(skb, sk);\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKMERGED);\treturn true;&#125;\n\ntcp_shifted_skb本质上就做两讲事情，一个是给刚刚被挪走的skb做 sack记账，之后修正skb的部分字段，例如重传用到的字段，合并标志位，更新sack最高指针，处理时间戳，从重传队列中删除skb上述代码也是sack的核心是调用tcp_sacktag_one做sack的记账，但是这里并未接收返回值，因为这里的上下文是把数据包的一部分移动到了前面，只是更新字段就ok。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP确认"]},{"title":"TCP确认 tcp_sacktag_write_queue（二）","url":"/2025/12/18/TCPsack%E5%A4%84%E7%90%86tcp_sacktag_write_queue%EF%BC%882%EF%BC%89/","content":"在TCP确认 tcp_sacktag_write_queue（一）中处理完判断是否为dsack块并丢弃无用的sack块后，接下来会把合法的sack block按startseq排序，为后续扫描发送队列提供优化。\n接下来进入tcp_sacktag_write_queue中的主循环，循环会遍历每个有效的sack block，首先尝试让 cache 指针指向相关的 cache block，如果 cache 可用且是普通 sack（不是dup）则进入关键的处理流程。\n首先判断当前当前确认的段是否在cache管理的前面（表示说明当前 SACK block 的左边一段是新的），如果条件满足则调用tcp_sacktag_skip二分找到发送队列中第一个 待处理的skb，调用tcp_sacktag_walk对 [start_seq, cache-&gt;start_seq) 这段序列范围的数据进行标记，tcp_sacktag_walk代码如下所示：\n//从给定的 skb 开始，沿着 TCP 发送队列（有序 rb-tree）遍历序列号在 [start_seq, end_seq) 的所有 skb//这个函数本质上才是sack的核心逻辑static struct sk_buff *tcp_sacktag_walk(struct sk_buff *skb, struct sock *sk,\t\t\t\t\tstruct tcp_sack_block *next_dup,\t\t\t\t\tstruct tcp_sacktag_state *state,\t\t\t\t\tu32 start_seq, u32 end_seq,\t\t\t\t\tbool dup_sack_in)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *tmp;\t//遍历重传队列\tskb_rbtree_walk_from(skb) &#123;\t\tint in_sack = 0; \t\t\t //标识skb是否被某个sack覆盖\t\tbool dup_sack = dup_sack_in; //这个块是否dsack语义\t\t/* queue is in-order =&gt; we can short-circuit the walk early */\t\t//这里开始后面的 skb 都不在 [start_seq, end_seq) 内  可以直接break\t\tif (!before(TCP_SKB_CB(skb)-&gt;seq, end_seq))\t\t\tbreak;\t\t//第一个条件是 考虑dsack的情况（两块组合编码） &amp;&amp; 当前 skb 起始序列号 &lt; DSACK块末尾序号 注意这里传入的的start和end是可能是dup的\t\tif (next_dup  &amp;&amp;\t\t    before(TCP_SKB_CB(skb)-&gt;seq, next_dup-&gt;end_seq)) &#123; \t\t\t//判断当前 skb 是否被 DSACK block 完整或部分覆盖,这里注意，里面会拆分数据包，如果dsack start在skb序号的右侧这里返回的是0\t\t\tin_sack = tcp_match_skb_to_sack(sk, skb,\t\t\t\t\t\t\tnext_dup-&gt;start_seq,\t\t\t\t\t\t\tnext_dup-&gt;end_seq);\t\t\tif (in_sack &gt; 0)\t\t\t\tdup_sack = true;\t\t&#125;\t\t/* skb reference here is a bit tricky to get right, since\t\t * shifting can eat and free both this skb and the next,\t\t * so not even _safe variant of the loop is enough.\t\t */\t\t//不是dsack 或者上面dsack start在skb序号的右侧？\t\tif (in_sack &lt;= 0) &#123;\t\t\t//优化队列结构，本质思想是合成一个大块，返回空表示无法合并\t\t\ttmp = tcp_shift_skb_data(sk, skb, state,\t\t\t\t\t\t start_seq, end_seq, dup_sack);\t\t\tif (tmp) &#123;\t\t\t\tif (tmp != skb) &#123;\t\t\t\t\tskb = tmp;\t\t\t\t\tcontinue;\t\t\t\t&#125;\t\t\t\tin_sack = 0;\t\t\t&#125; else &#123;\t\t\t\t//上面没有合并成功的情况，这里面是拆分数据包，如果这里返回0 表示没有命中sack这也是有可能的因为\t\t\t\tin_sack = tcp_match_skb_to_sack(sk, skb,\t\t\t\t\t\t\t\tstart_seq,\t\t\t\t\t\t\t\tend_seq);\t\t\t&#125;\t\t&#125;\t\tif (unlikely(in_sack &lt; 0))\t\t\tbreak;\t\t//真正的标记数据包，肯定是in_sack才能标记，注意这里接收了返回值\t\tif (in_sack) &#123;\t\t\tTCP_SKB_CB(skb)-&gt;sacked =\t\t\t\ttcp_sacktag_one(sk,\t\t\t\t\t\tstate,\t\t\t\t\t\tTCP_SKB_CB(skb)-&gt;sacked,\t\t\t\t\t\tTCP_SKB_CB(skb)-&gt;seq,\t\t\t\t\t\tTCP_SKB_CB(skb)-&gt;end_seq,\t\t\t\t\t\tdup_sack,\t\t\t\t\t\ttcp_skb_pcount(skb),\t\t\t\t\t\ttcp_skb_timestamp_us(skb));\t\t\t//更新拥塞算法用到的字段\t\t\ttcp_rate_skb_delivered(sk, skb, state-&gt;rate);\t\t\tif (TCP_SKB_CB(skb)-&gt;sacked &amp; TCPCB_SACKED_ACKED)\t\t\t\tlist_del_init(&amp;skb-&gt;tcp_tsorted_anchor);\t\t\tif (!before(TCP_SKB_CB(skb)-&gt;seq,\t\t\t\t    tcp_highest_sack_seq(tp)))\t\t\t\t//如果需要，更新最高的sack对应的数据包\t\t\t\ttcp_advance_highest_sack(sk, skb);\t\t&#125;\t&#125;\t//这个数据包貌似已经是不再 sack范围的数据包了\treturn skb;&#125;\n\ntcp_sacktag_walk的核心逻辑为把落在某个sack区间的skb找出来并作标记，它做的事不仅简单的判断在不在区间，会为了精确标记而 可能拆分 skb、可能合并 skb，具体逻辑如下：\ntcp_sacktag_walk中首先遍历重传队列，当数据包的seq已经超出了sack的右边界那就直接break，否则优先处理dsack双编码的场景，（这里注意如果使用的是cache这里就一定不会走吧），如果next_dup 非空且 当前 skb 起始序列号 &lt; dsack块末尾序号，则调用tcp_match_skb_to_sack可能拆分数据包，拆分数据包的目的是方便整整齐齐的标记：-）tcp_match_skb_to_sack具体代码如下所示：\n// 带 GSO：如果不刚好对齐 尝试 tcp_fragmentstatic int tcp_match_skb_to_sack(struct sock *sk, struct sk_buff *skb,\t\t\t\t  u32 start_seq, u32 end_seq)&#123;\tint err;\tbool in_sack;\tunsigned int pkt_len;\tunsigned int mss;\t//skb 完全被 SACK 覆盖\tin_sack = !after(start_seq, TCP_SKB_CB(skb)-&gt;seq) &amp;&amp;\t\t  !before(end_seq, TCP_SKB_CB(skb)-&gt;end_seq);\t// GSO &amp;&amp; 部分的情况\tif (tcp_skb_pcount(skb) &gt; 1 &amp;&amp; !in_sack &amp;&amp;\t////部分重叠 数据包的结束序列号小于块的起始序列号\t    after(TCP_SKB_CB(skb)-&gt;end_seq, start_seq)) &#123; \t\tmss = tcp_skb_mss(skb);\t\tin_sack = !after(start_seq, TCP_SKB_CB(skb)-&gt;seq); \t\t//SACK start在 skb 里面\t\tif (!in_sack) &#123;\t\t\tpkt_len = start_seq - TCP_SKB_CB(skb)-&gt;seq;\t\t\tif (pkt_len &lt; mss)\t\t\t\tpkt_len = mss;//对齐mss\t\t//SACK start &lt;= skb 起点\t\t&#125; else &#123;\t\t\tpkt_len = end_seq - TCP_SKB_CB(skb)-&gt;seq;\t\t\tif (pkt_len &lt; mss)\t\t\t\treturn -EINVAL;\t\t&#125;\t\t/* Round if necessary so that SACKs cover only full MSSes\t\t * and/or the remaining small portion (if present)\t\t */\t\tif (pkt_len &gt; mss) &#123;\t\t\tunsigned int new_len = (pkt_len / mss) * mss;\t\t\tif (!in_sack &amp;&amp; new_len &lt; pkt_len)\t\t\t\tnew_len += mss;\t\t\tpkt_len = new_len; //对齐成 N * MSS\t\t&#125;\t\tif (pkt_len &gt;= skb-&gt;len &amp;&amp; !in_sack)\t\t\treturn 0;\t\t//切分数据包，这里很关键这次返回0 下次应该就不会了\t\terr = tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb,\t\t\t\t   pkt_len, mss, GFP_ATOMIC);\t\tif (err &lt; 0)\t\t\treturn err;\t&#125;\treturn in_sack;&#125;\n\ntcp_match_skb_to_sack首秀判断当前sack的返回是否覆盖了这个skb如果满足条件则直接返回了，否则只有在是GSO报文的情况下同时可能存在重叠则会进一步处理，首先计算mss然后判断sack从skb的左侧开始还是sack的起点在skb的内部，并进行mss对齐处理，之后调用tcp_fragment切分数据包，具体代码如下所示：\nnt tcp_fragment(struct sock *sk, enum tcp_queue tcp_queue,\t\t struct sk_buff *skb, u32 len,\t\t unsigned int mss_now, gfp_t gfp)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *buff;\tint old_factor;\tlong limit;\tint nlen;\tu8 flags;\tif (WARN_ON(len &gt; skb-&gt;len))\t\treturn -EINVAL;\tDEBUG_NET_WARN_ON_ONCE(skb_headlen(skb));\t/* tcp_sendmsg() can overshoot sk_wmem_queued by one full size skb.\t * We need some allowance to not penalize applications setting small\t * SO_SNDBUF values.\t * Also allow first and last skb in retransmit queue to be split.\t */\t//发送缓冲区是否够大，是否超出了内存限制\tlimit = sk-&gt;sk_sndbuf + 2 * SKB_TRUESIZE(GSO_LEGACY_MAX_SIZE);\tif (unlikely((sk-&gt;sk_wmem_queued &gt;&gt; 1) &gt; limit &amp;&amp;\t\t     tcp_queue != TCP_FRAG_IN_WRITE_QUEUE &amp;&amp;\t\t     skb != tcp_rtx_queue_head(sk) &amp;&amp;\t\t     skb != tcp_rtx_queue_tail(sk))) &#123;\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPWQUEUETOOBIG);\t\treturn -ENOMEM;\t&#125;\tif (skb_unclone_keeptruesize(skb, gfp))\t\treturn -ENOMEM;\t/* Get a new skb... force flag on. */\t//申请一个新的skb\tbuff = tcp_stream_alloc_skb(sk, gfp, true);\tif (!buff)\t\treturn -ENOMEM; /* We&#x27;ll just try again later. */\tskb_copy_decrypted(buff, skb);\tmptcp_skb_ext_copy(buff, skb);\t//更新内存记账\tsk_wmem_queued_add(sk, buff-&gt;truesize);\tsk_mem_charge(sk, buff-&gt;truesize);\tnlen = skb-&gt;len - len;\t//新的skb\tbuff-&gt;truesize += nlen;\t//旧skb\tskb-&gt;truesize -= nlen;\t/* Correct the sequence numbers. */\t//更新序列号\tTCP_SKB_CB(buff)-&gt;seq = TCP_SKB_CB(skb)-&gt;seq + len;\tTCP_SKB_CB(buff)-&gt;end_seq = TCP_SKB_CB(skb)-&gt;end_seq;\tTCP_SKB_CB(skb)-&gt;end_seq = TCP_SKB_CB(buff)-&gt;seq;\t/* PSH and FIN should only be set in the second packet. */\t//更新标志位\tflags = TCP_SKB_CB(skb)-&gt;tcp_flags;\tTCP_SKB_CB(skb)-&gt;tcp_flags = flags &amp; ~(TCPHDR_FIN | TCPHDR_PSH);\tTCP_SKB_CB(buff)-&gt;tcp_flags = flags;\tTCP_SKB_CB(buff)-&gt;sacked = TCP_SKB_CB(skb)-&gt;sacked;\ttcp_skb_fragment_eor(skb, buff);\t//真正的数据分割\tskb_split(skb, buff, len);\tskb_set_delivery_time(buff, skb-&gt;tstamp, true);\ttcp_fragment_tstamp(skb, buff);\t//原本数据包的段数\told_factor = tcp_skb_pcount(skb);\t/* Fix up tso_factor for both original and new SKB.  */\t//调整GSO字段\ttcp_set_skb_tso_segs(skb, mss_now);\ttcp_set_skb_tso_segs(buff, mss_now);\t/* Update delivered info for the new segment */\tTCP_SKB_CB(buff)-&gt;tx = TCP_SKB_CB(skb)-&gt;tx;\t/* If this packet has been sent out already, we must\t * adjust the various packet counters.\t */\t//判断数据包是否已经发送过\tif (!before(tp-&gt;snd_nxt, TCP_SKB_CB(buff)-&gt;end_seq)) &#123;\t\tint diff = old_factor - tcp_skb_pcount(skb) -\t\t\ttcp_skb_pcount(buff);\t\t//为什么要调整这个已发送未确认的计数呢？？\t\tif (diff)\t\t\ttcp_adjust_pcount(sk, skb, diff);\t&#125;\t/* Link BUFF into the send queue. */\t__skb_header_release(buff);\t//将新SKB插入到原SKB之后\ttcp_insert_write_queue_after(skb, buff, sk, tcp_queue);\tif (tcp_queue == TCP_FRAG_IN_RTX_QUEUE)\t\tlist_add(&amp;buff-&gt;tcp_tsorted_anchor, &amp;skb-&gt;tcp_tsorted_anchor);\treturn 0;&#125;\n\ntcp_fragment核心逻辑为把发送队列里的 skb 按 len 位置切成两个 skb原来的在前面，新的在后面，之后修正 seq&#x2F;end_seq、TCP 标志位、GSO 分段信息、内存记账等。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP确认"]},{"title":"TCP确认 tcp_sacktag_write_queue（四）","url":"/2025/12/23/TCPsack%E5%A4%84%E7%90%86tcp_sacktag_write_queue%EF%BC%884%EF%BC%89/","content":"tcp_shifted_skb中会调用tcp_sacktag_one来更新sack的相关记账，注意tcp_shifted_skb没有接收返回值，因为做的工作就是只是将当前skb的sack确认的部分移动到前一个部分，因此不需要接收返回值，当真正需要标记sack时候会接收返回值，tcp_sacktag_one具体代码如下所示：\nstatic u8 tcp_sacktag_one(struct sock *sk,\t\t\t  struct tcp_sacktag_state *state, u8 sacked,\t\t\t  u32 start_seq, u32 end_seq,\t\t\t  int dup_sack, int pcount,\t\t\t  u64 xmit_time)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t/* Account D-SACK for retransmitted packet. */\t//如果是dsack且是重传包\tif (dup_sack &amp;&amp; (sacked &amp; TCPCB_RETRANS)) &#123;\t\tif (tp-&gt;undo_marker &amp;&amp; tp-&gt;undo_retrans &gt; 0 &amp;&amp; \t\t\t\t\t//存在重传\t\t    after(end_seq, tp-&gt;undo_marker))\t\t\ttp-&gt;undo_retrans = max_t(int, 0, tp-&gt;undo_retrans - pcount);//更新未确认重传包的数量\t\tif ((sacked &amp; TCPCB_SACKED_ACKED) &amp;&amp; \t\t    before(start_seq, state-&gt;reord))\t\t\t\tstate-&gt;reord = start_seq; //跟踪最早的重排序边界\t&#125;\t/* Nothing to do; acked frame is about to be dropped (was ACKed). */\t//如果要标记范围已经被确认了\tif (!after(end_seq, tp-&gt;snd_una))\t\treturn sacked;\t//这个数据包没有被sack过的情况，如果是tcp_shifted_skb调进来，大概率不进入这个分支吧？\tif (!(sacked &amp; TCPCB_SACKED_ACKED)) &#123; //\t\t//这里很关键，应该叫真正启动rack吧\t\ttcp_rack_advance(tp, sacked, end_seq, xmit_time);\t\t//这段数据曾经被重传过\t\tif (sacked &amp; TCPCB_SACKED_RETRANS) &#123;//\t\t\t/* If the segment is not tagged as lost,\t\t\t * we do not clear RETRANS, believing\t\t\t * that retransmission is still in flight.\t\t\t */\t\t\t//同时还有LOST标记\t\t\tif (sacked &amp; TCPCB_LOST) &#123;\t\t\t\tsacked &amp;= ~(TCPCB_LOST|TCPCB_SACKED_RETRANS);//清掉标志位\t\t\t\ttp-&gt;lost_out -= pcount; \t\t\t\ttp-&gt;retrans_out -= pcount;\t\t\t&#125;\t\t&#125; else &#123;//之前没有重传过新的sack，这是最常见的场景吧？\t\t\tif (!(sacked &amp; TCPCB_RETRANS)) &#123;\t\t\t\t/* New sack for not retransmitted frame,\t\t\t\t * which was in hole. It is reordering.\t\t\t\t */\t\t\t\t//这种情况就表示乱序\t\t\t\tif (before(start_seq,\t\t\t\t\t   tcp_highest_sack_seq(tp)) &amp;&amp;\t\t\t\t    before(start_seq, state-&gt;reord))\t\t\t\t\tstate-&gt;reord = start_seq; //更新乱序边界\t\t\t\t//SACK 的 end_seq 不超过 high_seq，是原始发送窗口里的数据\t\t\t\tif (!after(end_seq, tp-&gt;high_seq))\t\t\t\t\tstate-&gt;flag |= FLAG_ORIG_SACK_ACKED;\t\t\t\tif (state-&gt;first_sackt == 0)\t\t\t\t\tstate-&gt;first_sackt = xmit_time;\t\t\t\tstate-&gt;last_sackt = xmit_time;\t\t\t&#125;\t\t\t//如果之前带 LOST 撤销 LOSS\t\t\tif (sacked &amp; TCPCB_LOST) &#123;\t\t\t\tsacked &amp;= ~TCPCB_LOST;\t\t\t\ttp-&gt;lost_out -= pcount;\t\t\t&#125;\t\t&#125;\t\t//真正标记为 SACKED_ACKED\t\tsacked |= TCPCB_SACKED_ACKED;\t\tstate-&gt;flag |= FLAG_DATA_SACKED;\t\ttp-&gt;sacked_out += pcount;\t\t/* Out-of-order packets delivered */\t\tstate-&gt;sack_delivered += pcount;\t\t//修正 lost_skb_hint 对应的 hint 计数\t\t/* Lost marker hint past SACKed? Tweak RFC3517 cnt */\t\tif (tp-&gt;lost_skb_hint &amp;&amp;\t\t    before(start_seq, TCP_SKB_CB(tp-&gt;lost_skb_hint)-&gt;seq))\t\t\ttp-&gt;lost_cnt_hint += pcount;\t&#125;\t/* D-SACK. We can detect redundant retransmission in S|R and plain R\t * frames and clear it. undo_retrans is decreased above, L|R frames\t * are accounted above as well.\t */\tif (dup_sack &amp;&amp; (sacked &amp; TCPCB_SACKED_RETRANS)) &#123;\t\tsacked &amp;= ~TCPCB_SACKED_RETRANS;\t\ttp-&gt;retrans_out -= pcount;\t&#125;\treturn sacked;&#125;\n\ntcp_sacktag_one中如果当前数据包是dsack且重传过则证明重传是多余，则修正拥塞撤销的相关计数，同时更新乱序边界，合理。\n如果当前数据包已经被累计的ack确认过，则直接返回。之后对没有标记为sack的数据进行处理，如果是优化队列结构，这里应该大概率会进入！！！，然后调用tcp_rack_advance这个是启动rack定时器的关键，也就是说收到了部分的确认，因此需要启动rack，具体代码如下所示：\nvoid tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,\t\t      u64 xmit_time)&#123;\tu32 rtt_us;\t//计算当前数据包的rtt\trtt_us = tcp_stamp_us_delta(tp-&gt;tcp_mstamp, xmit_time);\t//小于最小rtt可能是乱续导致的，直接return\tif (rtt_us &lt; tcp_min_rtt(tp) &amp;&amp; (sacked &amp; TCPCB_RETRANS)) &#123;\t\t/* If the sacked packet was retransmitted, it&#x27;s ambiguous\t\t * whether the retransmission or the original (or the prior\t\t * retransmission) was sacked.\t\t *\t\t * If the original is lost, there is no ambiguity. Otherwise\t\t * we assume the original can be delayed up to aRTT + min_rtt.\t\t * the aRTT term is bounded by the fast recovery or timeout,\t\t * so it&#x27;s at least one RTT (i.e., retransmission is at least\t\t * an RTT later).\t\t */\t\treturn;\t&#125;\t//设置rack用到的标志，这个很关键\ttp-&gt;rack.advanced = 1;\t//保存rtt\ttp-&gt;rack.rtt_us = rtt_us;\t//更新当前数据包的时间戳和序列好到 rack使用的相关字段中\tif (tcp_skb_sent_after(xmit_time, tp-&gt;rack.mstamp,\t\t\t       end_seq, tp-&gt;rack.end_seq)) &#123;\t\ttp-&gt;rack.mstamp = xmit_time;\t\ttp-&gt;rack.end_seq = end_seq;\t&#125;&#125;\n\ntcp_rack_advance的主要工作是在每次确认部分数据时， 拿到RTT，如果rtt很小这直接忽略，否则设置rack.advanced这个是是否启动rack标记丢包的关键，也就是说通过收到sack就会启动。\n回到tcp_sacktag_one中，如果这段数据包被重传过同时还被标了 LOST，但是现在被sack了就不算丢失，也不算重传在飞，则修改对应计数。如果没有重传标志（最常见的场景吧），则判断是否认为是乱序，如果乱序则更新乱序的边界。如果原始发送窗口内的数据被 SACK 到则置 FLAG_ORIG_SACK_ACKED表示没有重传过的数据被确认了。最后给这个数据包打上已被 SACK 确认的永久标记，并更新sack累计数量。\n回调tcp_shifted_skb中处理完sack的记账相关信息后，会调用tcp_rate_skb_delivered设置采样信息，BBR拥塞算法用到的通过这里采样得到的信息进行拥塞控制。具体代码如下所示：\n//某个 skb 被 SACK 或被累计 ACK 时，就会调用它void tcp_rate_skb_delivered(struct sock *sk, struct sk_buff *skb,\t\t\t    struct rate_sample *rs)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct tcp_skb_cb *scb = TCP_SKB_CB(skb);\tu64 tx_tstamp;\tif (!scb-&gt;tx.delivered_mstamp)\t\treturn;\t//TCP_SKB_CB(skb)-&gt;tx 里存的是这段数据当时发出去时的快照\t//BBR就会用到这下面部分字段\ttx_tstamp = tcp_skb_timestamp_us(skb);\tif (!rs-&gt;prior_delivered ||\t    tcp_skb_sent_after(tx_tstamp, tp-&gt;first_tx_mstamp,\t\t\t       scb-&gt;end_seq, rs-&gt;last_end_seq)) &#123;\t\trs-&gt;prior_delivered_ce  = scb-&gt;tx.delivered_ce;\t\trs-&gt;prior_delivered  = scb-&gt;tx.delivered;\t\trs-&gt;prior_mstamp     = scb-&gt;tx.delivered_mstamp;\t\trs-&gt;is_app_limited   = scb-&gt;tx.is_app_limited;\t\trs-&gt;is_retrans\t     = scb-&gt;sacked &amp; TCPCB_RETRANS;\t\trs-&gt;last_end_seq     = scb-&gt;end_seq;\t\t/* Record send time of most recently ACKed packet: */\t\ttp-&gt;first_tx_mstamp  = tx_tstamp;\t\t/* Find the duration of the &quot;send phase&quot; of this window: */\t\trs-&gt;interval_us = tcp_stamp_us_delta(tp-&gt;first_tx_mstamp,\t\t\t\t\t\t     scb-&gt;tx.first_tx_mstamp);\t&#125;\t/* Mark off the skb delivered once it&#x27;s sacked to avoid being\t * used again when it&#x27;s cumulatively acked. For acked packets\t * we don&#x27;t need to reset since it&#x27;ll be freed soon.\t */\tif (scb-&gt;sacked &amp; TCPCB_SACKED_ACKED)\t\tscb-&gt;tx.delivered_mstamp = 0;&#125;\n\n上述代码主要工作汇总如下：\n\n发送 skb 时：TCP 在 skb 里埋下一个发送时 delivered 快照 + 时间戳。（tcp_rate_skb_sent对称的逻辑，__tcp_transmit_skb中调用）\n收到 ACK&#x2F;SACK 时：tcp_rate_skb_delivered() 从被确认的 skb挑一个最合适的数据包，把快照拷贝到 rate_sample。\n后续（在别的函数里）用当前 tp-&gt;delivered 和 rs-&gt;prior_delivered、以及时间差，算 delivery rate（给 BBR 等用）。\n如果是 SACK 确认：清零 delivered_mstamp 防重复采样。\n\n回到tcp_shift_skb_data中（之前说了tcp_shift_skb_data的主要作用是尽量把已经被确认的数据进行合并），如果合并成功还会继续尝试是否能够合并后面的数据包，这里不再重复。\n回到tcp_sacktag_walk中，调用完tcp_shift_skb_data后返回的结构表示是否合并成功了，如果合并成功则会continue继续遍历重传队列继续尝试合并，直到没有合并成功为止，如果合并失败则调用tcp_match_skb_to_sack尝试拆分数据包，如果拆分成功了则表示数据包完全在sack的范围内，会调用tcp_sacktag_one标记数据包，这里注意：此时接收的了该函数的返回值，表示真正的标记了数据包。之后调用tcp_rate_skb_delivered更新拥塞算法用到字段。并更新更新最高的sack对应的数据包。如果拆分失败，还会继续遍历重传队列。\n回到tcp_sacktag_write_queue中，有cache可用并调用 tcp_sacktag_walk完成后，会进一步判断当前数据包携带的sack结束序列号是否在缓存管理的范围内，如果在则直接跳出遍历每个sack块的循环，否则跳出循环（相当于不用cache加速直接走慢速路径）。\n如果无法用cache加速则直接调用tcp_sacktag_skip找到待标记的skb同时调用tcp_sacktag_walk遍历重传队列标记数据包。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP确认"]},{"title":"TCP三次握手-发送ack","url":"/2025/10/19/TCP%E4%B8%BB%E5%8A%A8%E6%89%93%E5%BC%80%E5%8F%91%E9%80%81ack/","content":"当在tcp_rcv_synsent_state_process中处理完成对端发送的synack报文后，会回复最后一个ack，这里注意，当收到对方的synack报文后，会唤醒用户阻塞的connect（如果是阻塞模式的话）此时还没有发送最后一个ack报文，就已经是TCP_ESTABLISHED状态了。具体是立即发送ack还是延迟发送ack取决于是否有待发送的数据，或者用户设置过选项，亦或是处于pingpong模式，如果处于上述三种情况则会延迟发送ack否则直接发送ack具体代码如下所示：\nif (sk-&gt;sk_write_pending ||  //有待发的数据这时候刚建立连接 通常不会有吧    READ_ONCE(icsk-&gt;icsk_accept_queue.rskq_defer_accept) ||  //setsockopt 设置    inet_csk_in_pingpong_mode(sk)) &#123; //pingpong模式\t//进入这个分支的思想是，先慢发一个ack然后快发\t/* Save one ACK. Data will be ready after\t * several ticks, if write_pending is set.\t *\t * It may be deleted, but with this feature tcpdumps\t * look so _wonderfully_ clever, that I was not able\t * to stand against the temptation 8)     --ANK\t */\tinet_csk_schedule_ack(sk); //设置ack pending位\ttcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);//设置后面快发ack\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK, //启动延迟ack定时器\t\t\t\t  TCP_DELACK_MAX, TCP_RTO_MAX);\t//注意这里直接返回了 没有发ack\tgoto consume;&#125;//发送最后一个acktcp_send_ack(sk);\n\n大多数情况会立即发送一个ack报文，然后返回-1，外层会针对这个返回值进行进一步处理，具体代码代码如下所示：\ncase TCP_SYN_SENT:\t\ttp-&gt;rx_opt.saw_tstamp = 0;\t\ttcp_mstamp_refresh(tp);//更新收包时间戳\t\t//真正的处理函数 正常时返回-1 返回1 会发送rst 返回0 什么也不做\t\tqueued = tcp_rcv_synsent_state_process(sk, skb, th);\t\tif (queued &gt;= 0)\t\t\treturn queued;\t\t/* Do step6 onward by hand. */\t\t//返回-1的情况\t\t//处理是否有urg 几乎不会有\t\ttcp_urg(sk, skb, th);\t\t__kfree_skb(skb);\t\t//是否需要扩充sndbuf 或者是否需发送可写事件\t\ttcp_data_snd_check(sk);\t\treturn 0;\t&#125;\n\n内层返回-1后，会首先判断是否需要处理urg数据，如果有则会处理紧急数据（这里几乎直接返回了），因此接下来会释放数据包，然后调用tcp_data_snd_check，判断是否有数据需要发送，或者是否需要通知应用层有可写事件，具体代码如下所示：\nstatic inline void tcp_data_snd_check(struct sock *sk)&#123;\t//检查是否有待发送的数据\ttcp_push_pending_frames(sk);\ttcp_check_space(sk);&#125;static inline void tcp_push_pending_frames(struct sock *sk)&#123;\t//是否有待发送的数据\tif (tcp_send_head(sk)) &#123;\t\tstruct tcp_sock *tp = tcp_sk(sk);\t\t//尝试发包\t\t__tcp_push_pending_frames(sk, tcp_current_mss(sk), tp-&gt;nonagle);\t&#125;&#125;\n\n上述tcp_data_snd_check调用tcp_push_pending_frames判断是否有发送的数据，如果有，则直接调用发送接口发送数据包，之后调用\ntcp_check_space检查是否检测发送缓冲区原本满了以及是否需要唤醒应用层具体代码如下所示：\nvoid tcp_check_space(struct sock *sk)&#123;\t/* pairs with tcp_poll() */\tsmp_mb();\t//是否没有空间\tif (sk-&gt;sk_socket &amp;&amp;\t    test_bit(SOCK_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags)) &#123;\t\ttcp_new_space(sk);\t\tif (!test_bit(SOCK_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags))\t\t\ttcp_chrono_stop(sk, TCP_CHRONO_SNDBUF_LIMITED);\t&#125;&#125;\n\ntcp_check_space首先判断当前sock是否处于NOSPACE状态，即是否之前标记为不可写状态，如果标记过则调用tcp_new_space进一步进行检查，如果有空间了则唤醒用户阻塞的进程。具体代码呢如下所示：\nstatic void tcp_new_space(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//是否可以扩大sndbuf，这里还有可能减小\tif (tcp_should_expand_sndbuf(sk)) &#123;\t\t//扩充\t\ttcp_sndbuf_expand(sk);\t\ttp-&gt;snd_cwnd_stamp = tcp_jiffies32;\t&#125;\t//是否唤醒阻塞的进程\tINDIRECT_CALL_1(sk-&gt;sk_write_space, sk_stream_write_space, sk);&#125;\n\ntcp_new_space中，首先调用tcp_should_expand_sndbuf判断是否可以扩大发送缓存区，如果可以则会扩大sndbuf否则还可能会减小，之后调用sk_stream_write_space判断是否可以唤醒阻塞的进程。具体代码如下所示\nstatic bool tcp_should_expand_sndbuf(struct sock *sk)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\t/* If the user specified a specific send buffer setting, do\t * not modify it.\t */\t//用户没有显示改过buf大小\tif (sk-&gt;sk_userlocks &amp; SOCK_SNDBUF_LOCK)\t\treturn false;\t/* If we are under global TCP memory pressure, do not expand.  */\t// //读全局变量判断是否内存压力，如果存在压力，可能主动调小\tif (tcp_under_memory_pressure(sk)) &#123;\t\t//如果用户没有保留内存 这里大概率是0  ，否则返回剩余的内存（也可能是0）\t\tint unused_mem = sk_unused_reserved_mem(sk); \t\t/* Adjust sndbuf according to reserved mem. But make sure\t\t * it never goes below SOCK_MIN_SNDBUF.\t\t * See sk_stream_moderate_sndbuf() for more details.\t\t */\t\t//大于两个包 主动调小\t\tif (unused_mem &gt; SOCK_MIN_SNDBUF)\t\t\tWRITE_ONCE(sk-&gt;sk_sndbuf, unused_mem);\t\treturn false;\t&#125;\t/* If we are under soft global TCP memory pressure, do not expand.  *///当前这个 socket 是否已经占用了过多内存\tif (sk_memory_allocated(sk) &gt;= sk_prot_mem_limits(sk, 0))\t\treturn false;\t/* If we filled the congestion window, do not expand.  */\t//途中的数据超过拥塞窗口，已经发出去的都大于最大并发包数了 别扩了\tif (tcp_packets_in_flight(tp) &gt;= tcp_snd_cwnd(tp))\t\treturn false;\treturn true;&#125;\n\ntcp_should_expand_sndbuf中首先判断用户是否显示设置过发送缓冲区的大小，这里可以发现，如果用户显示设置发送缓存区的大小，就失去了自动扩大发送缓冲区大小的逻辑，之后读取全局内存压力的变量，如果存在内存压力的情况，可能会进一步缩小发送缓冲区的小，并且返回false表示不能扩大缓冲区，如果不存在内存压力，但是当前的sock已经占用了过多内存（跟tcp内存参数0比较）或者&#x2F;途中的数据超过拥塞窗口，已经发出去的都大于最大并发包数，也返回false ，否则返回true表示可以扩大发送缓冲区。如果返回true外层会调用tcp_sndbuf_expand扩大发送缓冲区，具体代码如下所示：\nstatic void tcp_sndbuf_expand(struct sock *sk)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)-&gt;icsk_ca_ops;\tint sndmem, per_mss;\tu32 nr_segs;\t//最坏的情况 不用gso 每个mss对应一个skb 且head用2的幂kmalloc\t/* Worst case is non GSO/TSO : each frame consumes one skb\t * and skb-&gt;head is kmalloced using power of two area of memory\t */\tper_mss = max_t(u32, tp-&gt;rx_opt.mss_clamp, tp-&gt;mss_cache) +\t\t  MAX_TCP_HEADER +\t\t  SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\tper_mss = roundup_pow_of_two(per_mss) +\t\t  SKB_DATA_ALIGN(sizeof(struct sk_buff));\t//考虑冷启动情况\tnr_segs = max_t(u32, TCP_INIT_CWND, tcp_snd_cwnd(tp));\t//乱序\tnr_segs = max_t(u32, nr_segs, tp-&gt;reordering + 1);\t/* Fast Recovery (RFC 5681 3.2) :\t * Cubic needs 1.7 factor, rounded to 2 to include\t * extra cushion (application might react slowly to EPOLLOUT)\t */\t//是否用塞算法的钩子扩大sndbuf\tsndmem = ca_ops-&gt;sndbuf_expand ? ca_ops-&gt;sndbuf_expand(sk) : 2;\t//计算一下扩容后的内存\tsndmem *= nr_segs * per_mss;\t//是否能扩容，如果能，不要超过第二个大小！！！\tif (sk-&gt;sk_sndbuf &lt; sndmem)\t\tWRITE_ONCE(sk-&gt;sk_sndbuf,\t\t\t   min(sndmem, READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_wmem[2])));&#125;\n\ntcp_sndbuf_expand() 会按 最坏内存开销 × 需要并发的段数 × 拥塞控制放大因子 估算的发送缓冲大小，如果当前 sk_sndbuf 太小，就把它提升（但不超过 tcp_wmem[2]）。\n回到tcp_new_space中，处理完是否需要扩大发送缓冲区大小后，会进一步调用sk_stream_write_space决定是否需要唤醒阻塞的进程具体代码如下所示：\nvoid sk_stream_write_space(struct sock *sk)&#123;\tstruct socket *sock = sk-&gt;sk_socket;\tstruct socket_wq *wq;\t//是否可以写 剩余空间大于已发送的一半的时候可以发送！\tif (__sk_stream_is_writeable(sk, 1) &amp;&amp; sock) &#123;\t\tclear_bit(SOCK_NOSPACE, &amp;sock-&gt;flags);\t\trcu_read_lock();\t\twq = rcu_dereference(sk-&gt;sk_wq);\t\tif (skwq_has_sleeper(wq))\t\t\twake_up_interruptible_poll(&amp;wq-&gt;wait, EPOLLOUT | //唤醒 poll/epoll/select 等待的线程\t\t\t\t\t\tEPOLLWRNORM | EPOLLWRBAND); \t\tif (wq &amp;&amp; wq-&gt;fasync_list &amp;&amp; !(sk-&gt;sk_shutdown &amp; SEND_SHUTDOWN)) //如果注册了异步通知 (SIGIO)\t\t\tsock_wake_async(wq, SOCK_WAKE_SPACE, POLL_OUT);\t\trcu_read_unlock();\t&#125;&#125;\n\nsk_stream_write_space中首先调用__sk_stream_is_writeable判断是否可以写，如果可写则唤醒poll/epoll上的等待者。__sk_stream_is_writeable具体实现如下所示：\nstatic inline bool __sk_stream_is_writeable(const struct sock *sk, int wake)&#123;\t//剩余空间大于已发送的一半 同时比较内核没发送的字节数是否小于系统默认参数\treturn sk_stream_wspace(sk) &gt;= sk_stream_min_wspace(sk) &amp;&amp;\t       __sk_stream_memory_free(sk, wake);&#125;\n\n__sk_stream_is_writeable中首先判断&#x2F;剩余空间大于已发送的一半(否则认为发送缓冲区太小，不值得发送)，同时内核中未发送的字节数不能太多（小于系统参数）。这表示可以唤醒可写事件，上述具体代码如下所示：\nstatic inline int sk_stream_wspace(const struct sock *sk)&#123;\treturn READ_ONCE(sk-&gt;sk_sndbuf) - READ_ONCE(sk-&gt;sk_wmem_queued);&#125; static inline int sk_stream_min_wspace(const struct sock *sk)&#123;\treturn READ_ONCE(sk-&gt;sk_wmem_queued) &gt;&gt; 1;&#125;static inline bool __sk_stream_memory_free(const struct sock *sk, int wake)&#123;\t//发送队列大小比发送缓冲区大\tif (READ_ONCE(sk-&gt;sk_wmem_queued) &gt;= READ_ONCE(sk-&gt;sk_sndbuf))\t\treturn false;\t//进一步判断是否可以唤醒\treturn sk-&gt;sk_prot-&gt;stream_memory_free ?\t\tINDIRECT_CALL_INET_1(sk-&gt;sk_prot-&gt;stream_memory_free,\t\t\t\t     tcp_stream_memory_free, sk, wake) : true;&#125;//发送缓冲区是否有足够空间可写 __sk_stream_memory_free 调用 wake 为1 bool tcp_stream_memory_free(const struct sock *sk, int wake)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\tu32 notsent_bytes = READ_ONCE(tp-&gt;write_seq) - //用户写入了但还没发出去的字节数 也就是内核缓存里已经排队的字节数\t\t\t    READ_ONCE(tp-&gt;snd_nxt);\t//排队的字节数左移两位必须系统参数或者用户配置的小，就返回ture 这里注意 后面的值很大\treturn (notsent_bytes &lt;&lt; wake) &lt; tcp_notsent_lowat(tp);&#125;\n\n回到tcp_check_space中如果tcp_new_space中清除了SOCK_NOSPACE这个标志，则会调用tcp_chrono_stop停止该状态的计时，具体代码如下所示：\nvoid tcp_chrono_stop(struct sock *sk, const enum tcp_chrono type)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t/* There are multiple conditions worthy of tracking in a\t * chronograph, so that the highest priority enum takes\t * precedence over the other conditions (see tcp_chrono_start).\t * If a condition stops, we only stop chrono tracking if\t * it&#x27;s the &quot;most interesting&quot; or current chrono we are\t * tracking and starts busy chrono if we have pending data.\t */\t//重传队列和发送队列为空\tif (tcp_rtx_and_write_queues_empty(sk))\t\ttcp_chrono_set(tp, TCP_CHRONO_UNSPEC);//空闲不计时\telse if (type == tp-&gt;chrono_type) //如果当前状态和之前是一样的 就设置为TCP_CHRONO_BUSY\t\ttcp_chrono_set(tp, TCP_CHRONO_BUSY); //正常发送阶段&#125;\n\ntcp_chrono_set用于切换状态并记录某种状态持续的时间，用户可以通过setsockopt获取，具体代码如下所示：\nstatic void tcp_chrono_set(struct tcp_sock *tp, const enum tcp_chrono new)&#123;\tconst u32 now = tcp_jiffies32;\tenum tcp_chrono old = tp-&gt;chrono_type;//当前的状态\t//只有当上一个状态不是未计时UNSPEC时，才需要统计时间。\tif (old &gt; TCP_CHRONO_UNSPEC)\t\ttp-&gt;chrono_stat[old - 1] += now - tp-&gt;chrono_start; //就是上一个状态持续的时间长度 //getsetopt可以获取\ttp-&gt;chrono_start = now; //当前时间\ttp-&gt;chrono_type = new;\t//当前状态类型&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP三次握手-发送syn (二)","url":"/2025/10/12/TCP%E4%B8%BB%E5%8A%A8%E6%89%93%E5%BC%80%E5%8F%91%E9%80%81syn(%E4%BA%8C)/","content":"回到__inet_hash_connect中，可以发现，如果用户在调用connect之前调用过bind了则__inet_hash_connect中的工作就是把当前sk插入ehash中直接返回，否则会bind一个port，具体逻辑与bind系统调用类似，首先在可用的端口范围之间确定一个随机端口，然后计算hash确定hash桶，遍历挂在桶下的条目，判断是否有匹配的，如果匹配同时开启了reuseport的话这里直接会找下一个port，貌似因为客户端通常要bind一个完全没有被使用的port？如果条目匹配且不是reuse的情况下进一步判断是否是tw类型的（这里可能复用tw）。如果没有匹配的话则会创建一个条目，之后插入到ehash和bhash和bhash2中。\nint __inet_hash_connect(struct inet_timewait_death_row *death_row,\t\tstruct sock *sk, u64 port_offset,\t\tint (*check_established)(struct inet_timewait_death_row *,\t\t\tstruct sock *, __u16, struct inet_timewait_sock **))&#123;\tstruct inet_hashinfo *hinfo = death_row-&gt;hashinfo;\tstruct inet_bind_hashbucket *head, *head2;\tstruct inet_timewait_sock *tw = NULL;\tint port = inet_sk(sk)-&gt;inet_num;\tstruct net *net = sock_net(sk);\tstruct inet_bind2_bucket *tb2;\tstruct inet_bind_bucket *tb;\tbool tb_created = false;\tu32 remaining, offset;\tint ret, i, low, high;\tint l3mdev;\tu32 index;\t//如果port不为空(用户已经bind过了)，这里直接返回了\tif (port) &#123;\t\tlocal_bh_disable();\t\tret = check_established(death_row, sk, port, NULL);\t\tlocal_bh_enable();\t\treturn ret;\t&#125;\tl3mdev = inet_sk_bound_l3mdev(sk);\t//根据系统参数和用户配置确定扫描范围\tinet_sk_get_local_port_range(sk, &amp;low, &amp;high);\thigh++; /* [32768, 60999] -&gt; [32768, 61000[ */\tremaining = high - low;\tif (likely(remaining &gt; 1))\t\tremaining &amp;= ~1U;\t//为确定端口混入一个随机值\tget_random_sleepable_once(table_perturb,\t\t\t\t  INET_TABLE_PERTURB_SIZE * sizeof(*table_perturb));\tindex = port_offset &amp; (INET_TABLE_PERTURB_SIZE - 1);\toffset = READ_ONCE(table_perturb[index]) + (port_offset &gt;&gt; 32);\toffset %= remaining;\t/* In first pass we try ports of @low parity.\t * inet_csk_get_port() does the opposite choice.\t */\t//默认是从奇数开始扫描\toffset &amp;= ~1U;other_parity_scan:\tport = low + offset;\t//步长为2\tfor (i = 0; i &lt; remaining; i += 2, port += 2) &#123;\t\tif (unlikely(port &gt;= high))\t\t\tport -= remaining;\t\t//跳过保留端口\t\tif (inet_is_local_reserved_port(net, port))\t\t\tcontinue;\t\t//找到桶\t\thead = &amp;hinfo-&gt;bhash[inet_bhashfn(net, port,\t\t\t\t\t\t  hinfo-&gt;bhash_size)];\t\tspin_lock_bh(&amp;head-&gt;lock);\t\t/* Does not bother with rcv_saddr checks, because\t\t * the established check is already unique enough.\t\t */\t\t//遍历bhash桶下挂的条目\t\tinet_bind_bucket_for_each(tb, &amp;head-&gt;chain) &#123;\t\t\t//端口是否匹配\t\t\tif (inet_bind_bucket_match(tb, net, port, l3mdev)) &#123;\t\t\t\t//这里需要注意：如果这个条目是可以fastresues的这里直接跳过了\t\t\t\t//因为客户端通常要bind一个没有被使用的(或者不是建连的状态)？\t\t\t\tif (tb-&gt;fastreuse &gt;= 0 ||\t\t\t\t    tb-&gt;fastreuseport &gt;= 0)\t\t\t\t\tgoto next_port;\t\t\t\tWARN_ON(hlist_empty(&amp;tb-&gt;owners));\t\t\t\t//注意这里是判断是否可以重用tw的sock，如果走到了这里貌似一定会把新的sock插入到ehahs中!\t\t\t\tif (!check_established(death_row, sk,\t\t\t\t\t\t       port, &amp;tw))\t\t\t\t\tgoto ok;\t\t\t\tgoto next_port;\t\t\t&#125;\t\t&#125;\t\t//走到这里表示上面没有找到条目，那这里就要创建一个\t\ttb = inet_bind_bucket_create(hinfo-&gt;bind_bucket_cachep,\t\t\t\t\t     net, head, port, l3mdev);\t\tif (!tb) &#123;\t\t\tspin_unlock_bh(&amp;head-&gt;lock);\t\t\treturn -ENOMEM;\t\t&#125;\t\ttb_created = true;\t\ttb-&gt;fastreuse = -1;\t\ttb-&gt;fastreuseport = -1;\t\tgoto ok;next_port:\t\tspin_unlock_bh(&amp;head-&gt;lock);\t\tcond_resched();\t&#125;\toffset++;\tif ((offset &amp; 1) &amp;&amp; remaining &gt; 1)\t\tgoto other_parity_scan;\treturn -EADDRNOTAVAIL;ok:\t/* Find the corresponding tb2 bucket since we need to\t * add the socket to the bhash2 table as well\t */\t//走到这里分两种情况一种是没有在bhahs中找到条目，或者是找到了可以重用的tw。\t//这里根据port和addr找到bhash2的桶\thead2 = inet_bhashfn_portaddr(hinfo, sk, net, port);\tspin_lock(&amp;head2-&gt;lock);\t//找具体的条目，没有找到就直接创建\ttb2 = inet_bind2_bucket_find(head2, net, port, l3mdev, sk);\tif (!tb2) &#123;\t\ttb2 = inet_bind2_bucket_create(hinfo-&gt;bind2_bucket_cachep, net,\t\t\t\t\t       head2, port, l3mdev, sk);\t\tif (!tb2)\t\t\tgoto error;\t&#125;\t/* Here we want to add a little bit of randomness to the next source\t * port that will be chosen. We use a max() with a random here so that\t * on low contention the randomness is maximal and on high contention\t * it may be inexistent.\t */\ti = max_t(int, i, get_random_u32_below(8) * 2);\tWRITE_ONCE(table_perturb[index], READ_ONCE(table_perturb[index]) + i + 2);\t/* Head lock still held and bh&#x27;s disabled */\tinet_bind_hash(sk, tb, tb2, port);\t//判断是否加入到ehahs中过，比如说上面是有可能插入ehash\tif (sk_unhashed(sk)) &#123;\t\tinet_sk(sk)-&gt;inet_sport = htons(port);\t\t//插入到ehash中\t\tinet_ehash_nolisten(sk, (struct sock *)tw, NULL);\t&#125;\tif (tw)\t//如果是tw重用，这里把tw从bhahs中移除\t\tinet_twsk_bind_unhash(tw, hinfo);\tspin_unlock(&amp;head2-&gt;lock);\tspin_unlock(&amp;head-&gt;lock);\tif (tw)\t//释放tw的资源\t\tinet_twsk_deschedule_put(tw);\tlocal_bh_enable();\treturn 0;error:\tspin_unlock(&amp;head2-&gt;lock);\tif (tb_created)\t\tinet_bind_bucket_destroy(hinfo-&gt;bind_bucket_cachep, tb);\tspin_unlock_bh(&amp;head-&gt;lock);\treturn -ENOMEM;&#125;\n\n调用完inet_hash_connect后，回到tcp_v4_connect中由于刚刚可能改变了port所以这里需要重新查找一次路由。查找路由完成后，设置GSO标志位，之后调用secure_tcp_seq和secure_tcp_ts_off计算发送序列号以及时间戳的偏移量，之后调用tcp_connect申请一个skb并完成发送。tcp_connect具体逻辑如下所示：\n* Build a SYN and send it off. */int tcp_connect(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *buff;\tint err;\t//调用bpf钩子\ttcp_call_bpf(sk, BPF_SOCK_OPS_TCP_CONNECT_CB, 0, NULL);\t//查一下路由\tif (inet_csk(sk)-&gt;icsk_af_ops-&gt;rebuild_header(sk))\t\treturn -EHOSTUNREACH; /* Routing failure or similar. */\t//设置mss 窗口大小，窗口缩放因子， 管理窗口的相关字段\ttcp_connect_init(sk);\tif (unlikely(tp-&gt;repair)) &#123;\t\ttcp_finish_connect(sk, NULL);\t\treturn 0;\t&#125;\t//申请一个skb\tbuff = tcp_stream_alloc_skb(sk, sk-&gt;sk_allocation, true);\tif (unlikely(!buff))\t\treturn -ENOBUFS;\t//构造syn包字段，注意这里tp-&gt;write_seq++\ttcp_init_nondata_skb(buff, tp-&gt;write_seq++, TCPHDR_SYN);\ttcp_mstamp_refresh(tp);//更新时间戳\ttp-&gt;retrans_stamp = tcp_time_stamp(tp);//记录第一次发包时间戳\ttcp_connect_queue_skb(sk, buff); //更新序列号，更新内存使用相关字段\ttcp_ecn_send_syn(sk, buff);\ttcp_rbtree_insert(&amp;sk-&gt;tcp_rtx_queue, buff); //放入重传队列中\t/* Send off SYN; include data in Fast Open. */\t//发送数据包\terr = tp-&gt;fastopen_req ? tcp_send_syn_data(sk, buff) :\t      tcp_transmit_skb(sk, buff, 1, sk-&gt;sk_allocation);\tif (err == -ECONNREFUSED)\t\treturn err;\t/* We change tp-&gt;snd_nxt after the tcp_transmit_skb() call\t * in order to make this packet get counted in tcpOutSegs.\t */\tWRITE_ONCE(tp-&gt;snd_nxt, tp-&gt;write_seq); //下一个待发送的序列号\ttp-&gt;pushed_seq = tp-&gt;write_seq;\tbuff = tcp_send_head(sk);//peek一下 这里大概率返回空\tif (unlikely(buff)) &#123;\t\t//如果发送队列中还有没发的包，那下一个要发的序号应该对齐到那个包的seq\t\tWRITE_ONCE(tp-&gt;snd_nxt, TCP_SKB_CB(buff)-&gt;seq);\t\ttp-&gt;pushed_seq\t= TCP_SKB_CB(buff)-&gt;seq;\t&#125;\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ACTIVEOPENS);\t/* Timer for repeating the SYN until an answer. */\t//这里启动了超时重传定时器\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\t\t\t\t  inet_csk(sk)-&gt;icsk_rto, TCP_RTO_MAX);\treturn 0;&#125;\n\ntcp_connect中主要工作如下，首先重新查找一下路由，然后设置mss，窗口大小，窗口缩放因子等字段，之后申请一个skb并设置好一系列序列号，并更新内存管理相关的字段，然后调用发包接口完成发包，最后启动超时重传定时器。\n首先调用tcp_connect_init设置mss cache，通告mss，接收窗口大小，窗口缩放因子， 管理窗口的一系列序列号相关的字段以及rto等，具体代码如下所示：\n/* Do all connect socket setups that can be done AF independent. */static void tcp_connect_init(struct sock *sk)&#123;\tconst struct dst_entry *dst = __sk_dst_get(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\t__u8 rcv_wscale;\tu32 rcv_wnd;\t/* We&#x27;ll fix this up when we get a response from the other end.\t * See tcp_input.c:tcp_rcv_state_process case TCP_SYN_SENT.\t */\ttp-&gt;tcp_header_len = sizeof(struct tcphdr);\t//有时间戳选项加上时间戳选项的长度\tif (READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_timestamps))\t\ttp-&gt;tcp_header_len += TCPOLEN_TSTAMP_ALIGNED;\t/* If user gave his TCP_MAXSEG, record it to clamp */\t//用户是否设置mss\tif (tp-&gt;rx_opt.user_mss)\t\ttp-&gt;rx_opt.mss_clamp = tp-&gt;rx_opt.user_mss;\ttp-&gt;max_window = 0;\ttcp_mtup_init(sk);//初始化mtu探测的函数\t//更新tp 的mss cache\ttcp_sync_mss(sk, dst_mtu(dst));\t//用户是否设拥塞算法\ttcp_ca_dst_init(sk, dst);\tif (!tp-&gt;window_clamp)\t\ttp-&gt;window_clamp = dst_metric(dst, RTAX_WINDOW);//这里还是零吧\t//设置通告mss，这里大概率返回值就是基于mtu计算的mss\ttp-&gt;advmss = tcp_mss_clamp(tp, dst_metric_advmss(dst));\ttcp_initialize_rcv_mss(sk);\t/* limit the window selection if the user enforce a smaller rx buffer */\t//用户是否显示设置了缓冲区大小\tif (sk-&gt;sk_userlocks &amp; SOCK_RCVBUF_LOCK &amp;&amp;\t    (tp-&gt;window_clamp &gt; tcp_full_space(sk) || tp-&gt;window_clamp == 0))\t\ttp-&gt;window_clamp = tcp_full_space(sk);\t//bpf钩子\trcv_wnd = tcp_rwnd_init_bpf(sk);\tif (rcv_wnd == 0)\t\trcv_wnd = dst_metric(dst, RTAX_INITRWND);\t//设置接收窗口大小，以及窗口缩放银子\ttcp_select_initial_window(sk, tcp_full_space(sk),\t\t\t\t  tp-&gt;advmss - (tp-&gt;rx_opt.ts_recent_stamp ? tp-&gt;tcp_header_len - sizeof(struct tcphdr) : 0),\t\t\t\t  &amp;tp-&gt;rcv_wnd,\t\t\t\t  &amp;tp-&gt;window_clamp,\t\t\t\t  READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_window_scaling),\t\t\t\t  &amp;rcv_wscale,\t\t\t\t  rcv_wnd);\ttp-&gt;rx_opt.rcv_wscale = rcv_wscale;\ttp-&gt;rcv_ssthresh = tp-&gt;rcv_wnd;\tWRITE_ONCE(sk-&gt;sk_err, 0);\tsock_reset_flag(sk, SOCK_DONE);\ttp-&gt;snd_wnd = 0;\ttcp_init_wl(tp, 0);//初始化发送窗口更新时候的序列号\ttcp_write_queue_purge(sk); //清空发送队列？？\ttp-&gt;snd_una = tp-&gt;write_seq; //发送出去未确认的序号\ttp-&gt;snd_sml = tp-&gt;write_seq; //发送小于mss的数据包的最后一个字节的序列号\ttp-&gt;snd_up = tp-&gt;write_seq;\tWRITE_ONCE(tp-&gt;snd_nxt, tp-&gt;write_seq); //下一个待发送的序列号\tif (likely(!tp-&gt;repair))\t\ttp-&gt;rcv_nxt = 0;\telse\t\ttp-&gt;rcv_tstamp = tcp_jiffies32;\ttp-&gt;rcv_wup = tp-&gt;rcv_nxt; //上一次更新的rcv_nxt\tWRITE_ONCE(tp-&gt;copied_seq, tp-&gt;rcv_nxt); //设置用户进程已经读到的位置\tinet_csk(sk)-&gt;icsk_rto = tcp_timeout_init(sk); //这里应该是初始为1s\tinet_csk(sk)-&gt;icsk_retransmits = 0; //重传次数设置为0\ttcp_clear_retrans(tp); //清楚重传相关字段&#125;\n\ntcp_connect_init完成后，会调用tcp_stream_alloc_skb申请一个skb，这个申请skb的接口和发送正常tcp数据包的使用的接口是相同的，这里先不做分析，申请skb后会调用tcp_init_nondata_skb（注意这里调用完函数后对write_seq++）进一步完成对数据包的初始化(发送ack也会调用这个函数)，具体代码如下所示：\n/* Constructs common control bits of non-data skb. If SYN/FIN is present, * auto increment end seqno. */static void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)&#123;\tskb-&gt;ip_summed = CHECKSUM_PARTIAL; //checksum标志\tTCP_SKB_CB(skb)-&gt;tcp_flags = flags; //设置ack  syn flag \ttcp_skb_pcount_set(skb, 1); //设置gso_segs\tTCP_SKB_CB(skb)-&gt;seq = seq;\t//设置数据包的序号\tif (flags &amp; (TCPHDR_SYN | TCPHDR_FIN)) //协议规范要加1？\t\tseq++;\t//设置endseq 因为是纯数据包\tTCP_SKB_CB(skb)-&gt;end_seq = seq;&#125;\n\n完成数据包的初始化后，更新tp管理的时间戳字段，同时记录第一次发包的时间戳，后面重传可能会用到这个字段，之后调用tcp_connect_queue_skb记录内存使用量，更新发送出去没有收到ack的数量，以及发送序列号，具体代码如下所示：\nstatic void tcp_connect_queue_skb(struct sock *sk, struct sk_buff *skb)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\ttcb-&gt;end_seq += skb-&gt;len;\t__skb_header_release(skb);\tsk_wmem_queued_add(sk, skb-&gt;truesize); //更新整个协议栈用到的使用量\tsk_mem_charge(sk, skb-&gt;truesize); //扣除预分配的内存额度，\tWRITE_ONCE(tp-&gt;write_seq, tcb-&gt;end_seq); //更新发送序列号\ttp-&gt;packets_out += tcp_skb_pcount(skb); //更新发送出去还没有收到ack的数量&#125;\n\n 之后根据系统配置以及用户配置决定是否启用ECN，并把数据包放入重传队列中，之后调用tcp_transmit_skb完成数据包的发送，发送完成后更新会更新snd_nxt字段，同时调用inet_csk_reset_xmit_timer启动超时重传定时器.\n至此，发送syn包的流程到此结束了:-)\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP三次握手-发送syn (一)","url":"/2025/10/10/TCP%E4%B8%BB%E5%8A%A8%E6%89%93%E5%BC%80%E5%8F%91%E9%80%81syn%EF%BC%88%E4%B8%80%EF%BC%89/","content":" __sys_connect() 为主动打开阶段，发送syn包的入口函数，具体代码如下所示：\nint __sys_connect(int fd, struct sockaddr __user *uservaddr, int addrlen)&#123;\tint ret = -EBADF;\tstruct fd f;\tf = fdget(fd);\t//找到file\tif (f.file) &#123;\t\tstruct sockaddr_storage address;\t\t//copy_from_usr\t\tret = move_addr_to_kernel(uservaddr, addrlen, &amp;address);\t\tif (!ret)\t\t\tret = __sys_connect_file(f.file, &amp;address, addrlen, 0);\t\tfdput(f);\t&#125;\treturn ret;&#125;\n\n和其他系统调用类似，也是根据fd找到file同时将用户传入的地址信息拷贝到内核态，然后调用__sys_connect_file进一步处理，具体代码如下：\nint __sys_connect_file(struct file *file, struct sockaddr_storage *address,\t\t       int addrlen, int file_flags)&#123;\tstruct socket *sock;\tint err;\t//通过file的私有结构找到sock\tsock = sock_from_file(file);\tif (!sock) &#123;\t\terr = -ENOTSOCK;\t\tgoto out;\t&#125;\t//安全相关的钩子\terr =\t    security_socket_connect(sock, (struct sockaddr *)address, addrlen);\tif (err)\t\tgoto out;\t//调用套接字层 connect__inet_stream_connect\terr = READ_ONCE(sock-&gt;ops)-&gt;connect(sock, (struct sockaddr *)address,\t\t\t\taddrlen, sock-&gt;file-&gt;f_flags | file_flags);out:\treturn err;&#125;\n\n__sys_connect_file中通过file的私有结构获取sock后进一步调用套接字层的connect回调，也就是__inet_stream_connect具体代码如下所示：\nint __inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,\t\t\t  int addr_len, int flags, int is_sendmsg)&#123;\tstruct sock *sk = sock-&gt;sk;\tint err;\tlong timeo;\t/*\t * uaddr can be NULL and addr_len can be 0 if:\t * sk is a TCP fastopen active socket and\t * TCP_FASTOPEN_CONNECT sockopt is set and\t * we already have a valid cookie for this socket.\t * In this case, user can call write() after connect().\t * write() will invoke tcp_sendmsg_fastopen() which calls\t * __inet_stream_connect().\t */\tif (uaddr) &#123;\t\t//长度不合法直接返回\t\tif (addr_len &lt; sizeof(uaddr-&gt;sa_family))\t\t\treturn -EINVAL;\t\t//指定特殊协议族，直接断开连接\t\tif (uaddr-&gt;sa_family == AF_UNSPEC) &#123;\t\t\tsk-&gt;sk_disconnects++;\t\t\terr = sk-&gt;sk_prot-&gt;disconnect(sk, flags);\t\t\tsock-&gt;state = err ? SS_DISCONNECTING : SS_UNCONNECTED;\t\t\tgoto out;\t\t&#125;\t&#125;\t//正常流程走到这里\tswitch (sock-&gt;state) &#123;\tdefault:\t\terr = -EINVAL;\t\tgoto out;\tcase SS_CONNECTED:\t\terr = -EISCONN;\t\tgoto out;\tcase SS_CONNECTING:\t\tif (inet_test_bit(DEFER_CONNECT, sk))\t\t\terr = is_sendmsg ? -EINPROGRESS : -EISCONN;\t\telse\t\t\terr = -EALREADY;\t\t/* Fall out of switch with err, set for this state */\t\tbreak;\t//通常是这个状态\tcase SS_UNCONNECTED:\t\terr = -EISCONN;\t\tif (sk-&gt;sk_state != TCP_CLOSE)\t\t\tgoto out;\t\t//BPF相关的钩子\t\tif (BPF_CGROUP_PRE_CONNECT_ENABLED(sk)) &#123;\t\t\terr = sk-&gt;sk_prot-&gt;pre_connect(sk, uaddr, addr_len);\t\t\tif (err)\t\t\t\tgoto out;\t\t&#125;\t\t//调用具体协议的connect 对于tcp则是tcp_v4_connect\t\terr = sk-&gt;sk_prot-&gt;connect(sk, uaddr, addr_len);\t\tif (err &lt; 0)\t\t\tgoto out;\t\t//正在建立连接的状态\t\tsock-&gt;state = SS_CONNECTING;\t\tif (!err &amp;&amp; inet_test_bit(DEFER_CONNECT, sk))\t\t\tgoto out;\t\t/* Just entered SS_CONNECTING state; the only\t\t * difference is that return value in non-blocking\t\t * case is EINPROGRESS, rather than EALREADY.\t\t */\t\terr = -EINPROGRESS;\t\tbreak;\t&#125;\ttimeo = sock_sndtimeo(sk, flags &amp; O_NONBLOCK);\tif ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_SYN_SENT | TCPF_SYN_RECV)) &#123;\t\tint writebias = (sk-&gt;sk_protocol == IPPROTO_TCP) &amp;&amp;\t\t\t\ttcp_sk(sk)-&gt;fastopen_req &amp;&amp;\t\t\t\ttcp_sk(sk)-&gt;fastopen_req-&gt;data ? 1 : 0;\t\tint dis = sk-&gt;sk_disconnects;\t\t/* Error code is set above */\t\tif (!timeo || !inet_wait_for_connect(sk, timeo, writebias))\t\t\tgoto out;\t\terr = sock_intr_errno(timeo);\t\tif (signal_pending(current))\t\t\tgoto out;\t\tif (dis != sk-&gt;sk_disconnects) &#123;\t\t\terr = -EPIPE;\t\t\tgoto out;\t\t&#125;\t&#125;\t/* Connection was closed by RST, timeout, ICMP error\t * or another process disconnected us.\t */\tif (sk-&gt;sk_state == TCP_CLOSE)\t\tgoto sock_error;\t/* sk-&gt;sk_err may be not zero now, if RECVERR was ordered by user\t * and error was received after socket entered established state.\t * Hence, it is handled normally after connect() return successfully.\t */\tsock-&gt;state = SS_CONNECTED;\terr = 0;out:\treturn err;sock_error:\terr = sock_error(sk) ? : -ECONNABORTED;\tsock-&gt;state = SS_UNCONNECTED;\tsk-&gt;sk_disconnects++;\tif (sk-&gt;sk_prot-&gt;disconnect(sk, flags))\t\tsock-&gt;state = SS_DISCONNECTING;\tgoto out;&#125;\n\n__inet_stream_connect中首先进合法性校验，然后根据socket的状态执行不同case，这里通常走SS_UNCONNECTED状态，也就是回调用具体协议的prot的connect回调，这里需要注意，调用完套机字层的回调后，此时应该是SYN_SENT状态，接下来的行为则取决于connect是阻塞还是非阻塞，如果是阻 的，它会在 SYN 发出后通过 inet_wait_for_connect() 睡眠等待握手完成，若是非阻塞的（设置 O_NONBLOCK），则不会等待，直接返回 -EINPROGRESS。后续貌似需要getsockopt来判断是否建立连接成功。接下来看上述具体协议的connect函数，对于tcp则是tcp_v4_connect\n/* This will initiate an outgoing connection. */int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)&#123;\tstruct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\tstruct inet_timewait_death_row *tcp_death_row;\tstruct inet_sock *inet = inet_sk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct ip_options_rcu *inet_opt;\tstruct net *net = sock_net(sk);\t__be16 orig_sport, orig_dport;\t__be32 daddr, nexthop;\tstruct flowi4 *fl4;\tstruct rtable *rt;\tint err;\t//检查床读是否合法\tif (addr_len &lt; sizeof(struct sockaddr_in))\t\treturn -EINVAL;\tif (usin-&gt;sin_family != AF_INET)\t\treturn -EAFNOSUPPORT;\tnexthop = daddr = usin-&gt;sin_addr.s_addr;\t//拿到ip opt\tinet_opt = rcu_dereference_protected(inet-&gt;inet_opt,\t\t\t\t\t     lockdep_sock_is_held(sk));\t//如果设置了源路由选项，就把选项中的地址拿到，下面查路由的时候要用到\tif (inet_opt &amp;&amp; inet_opt-&gt;opt.srr) &#123;\t\tif (!daddr)\t\t\treturn -EINVAL;\t\tnexthop = inet_opt-&gt;opt.faddr;\t&#125;\torig_sport = inet-&gt;inet_sport; //这里用户如果没有调用bind的话是0吧\torig_dport = usin-&gt;sin_port;//用户设置port\tfl4 = &amp;inet-&gt;cork.fl.u.ip4;\t//这里调用查路由的接口\trt = ip_route_connect(fl4, nexthop, inet-&gt;inet_saddr,\t\t\t      sk-&gt;sk_bound_dev_if, IPPROTO_TCP, orig_sport,\t\t\t      orig_dport, sk);\tif (IS_ERR(rt)) &#123;\t\terr = PTR_ERR(rt);\t\tif (err == -ENETUNREACH)\t\t\tIP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\t\treturn err;\t&#125;\t//目的地址多播\tif (rt-&gt;rt_flags &amp; (RTCF_MULTICAST | RTCF_BROADCAST)) &#123;\t\tip_rt_put(rt);\t\treturn -ENETUNREACH;\t&#125;\t//没有启用严格路由选项\tif (!inet_opt || !inet_opt-&gt;opt.srr)\t\tdaddr = fl4-&gt;daddr;\t//这个叫管理timewait套接字的结构\ttcp_death_row = &amp;sock_net(sk)-&gt;ipv4.tcp_death_row;\t//如果用户没有指定源ip地址（也就是没有bind？）通常都不会指定吧\tif (!inet-&gt;inet_saddr) &#123;\t\t//如果没有显示指定源ip地址，进入这里后把路由查找后得到的源ip设置到sock上就返回了，不立马的返回的逻辑在disconnect中\t\terr = inet_bhash2_update_saddr(sk,  &amp;fl4-&gt;saddr, AF_INET);\t\tif (err) &#123;\t\t\tip_rt_put(rt);\t\t\treturn err;\t\t&#125;\t&#125; else &#123;\t\t//sk-&gt;sk_rcv_saddr = addr\t\tsk_rcv_saddr_set(sk, inet-&gt;inet_saddr);\t&#125;\t//如果有历史连接，这个时间戳就不为空吧，且两次地址不同？\tif (tp-&gt;rx_opt.ts_recent_stamp &amp;&amp; inet-&gt;inet_daddr != daddr) &#123;\t\t/* Reset inherited state */\t\t//复位时间戳信息\t\ttp-&gt;rx_opt.ts_recent\t   = 0;\t\ttp-&gt;rx_opt.ts_recent_stamp = 0;\t\tif (likely(!tp-&gt;repair))\t\t\tWRITE_ONCE(tp-&gt;write_seq, 0);\t&#125;\t//设置port\tinet-&gt;inet_dport = usin-&gt;sin_port;\t//设置sk-&gt;sk_daddr = addr; \tsk_daddr_set(sk, daddr);\t//设置ip选项长度\tinet_csk(sk)-&gt;icsk_ext_hdr_len = 0;\tif (inet_opt)\t\tinet_csk(sk)-&gt;icsk_ext_hdr_len = inet_opt-&gt;opt.optlen;\t//mss钳制到536\ttp-&gt;rx_opt.mss_clamp = TCP_MSS_DEFAULT;\t/* Socket identity is still unknown (sport may be zero).\t * However we set state to SYN-SENT and not releasing socket\t * lock select source port, enter ourselves into the hash tables and\t * complete initialization after this.\t */\t//这里设置为了SYN_SENT\ttcp_set_state(sk, TCP_SYN_SENT);\t//bind一个port并加入hash 表（查入ehash和bhash），如果用户调用过bind这里直接返回了\terr = inet_hash_connect(tcp_death_row, sk);\tif (err)\t\tgoto failure;\t//设置txhash\tsk_set_txhash(sk);\t//这里等于可能改变了新的port所以要在查一次路由\trt = ip_route_newports(fl4, rt, orig_sport, orig_dport,\t\t\t       inet-&gt;inet_sport, inet-&gt;inet_dport, sk);\tif (IS_ERR(rt)) &#123;\t\terr = PTR_ERR(rt);\t\trt = NULL;\t\tgoto failure;\t&#125;\t/* OK, now commit destination to socket.  */\tsk-&gt;sk_gso_type = SKB_GSO_TCPV4;\t//设置GSO能力 TCP这里默认置位TSO\tsk_setup_caps(sk, &amp;rt-&gt;dst);\trt = NULL;\tif (likely(!tp-&gt;repair)) &#123;\t\tif (!tp-&gt;write_seq)\t\t\tWRITE_ONCE(tp-&gt;write_seq,\t\t\t\t   secure_tcp_seq(inet-&gt;inet_saddr,\t\t\t\t\t\t  inet-&gt;inet_daddr,\t\t\t\t\t\t  inet-&gt;inet_sport,\t\t\t\t\t\t  usin-&gt;sin_port));\t\tWRITE_ONCE(tp-&gt;tsoffset,\t\t\t   secure_tcp_ts_off(net, inet-&gt;inet_saddr,\t\t\t\t\t     inet-&gt;inet_daddr));\t&#125;\tatomic_set(&amp;inet-&gt;inet_id, get_random_u16());\tif (tcp_fastopen_defer_connect(sk, &amp;err))\t\treturn err;\tif (err)\t\tgoto failure;\terr = tcp_connect(sk);\tif (err)\t\tgoto failure;\treturn 0;failure:\t/*\t * This unhashes the socket and releases the local port,\t * if necessary.\t */\ttcp_set_state(sk, TCP_CLOSE);\tinet_bhash2_reset_saddr(sk);\tip_rt_put(rt);\tsk-&gt;sk_route_caps = 0;\tinet-&gt;inet_dport = 0;\treturn err;&#125;\n\ntcp_v4_connect中主要做了如下几件事情，首先获取ip option，入果有严格路由选项就用选项中的ip去进行路由，否则就用用户指定的ip进行路由，之后进行路由查找，如果是法网多播或者广播的地址，则直接返回，之后根据路由查找到的结构设置源IP地址（前提是没有指定源ip地址的情况下），之后设置目的port，addr等字段，然后将sk的状态设置为TCP_SYN_SENT，之后会调用inet_hash_connect这个很关键，主要工作就是bind一个port然后就将套接子插入ehash这里注意，如果用户显示调用过bind这里直接就返回了，下面看具体实现\nint inet_hash_connect(struct inet_timewait_death_row *death_row,\t\t      struct sock *sk)&#123;\tu64 port_offset = 0;\tif (!inet_sk(sk)-&gt;inet_num)\t\tport_offset = inet_sk_port_offset(sk);\treturn __inet_hash_connect(death_row, sk, port_offset,\t\t\t\t   __inet_check_established);&#125;int __inet_hash_connect(struct inet_timewait_death_row *death_row,\t\tstruct sock *sk, u64 port_offset,\t\tint (*check_established)(struct inet_timewait_death_row *,\t\t\tstruct sock *, __u16, struct inet_timewait_sock **))&#123;\tstruct inet_hashinfo *hinfo = death_row-&gt;hashinfo;\tstruct inet_bind_hashbucket *head, *head2;\tstruct inet_timewait_sock *tw = NULL;\tint port = inet_sk(sk)-&gt;inet_num;\tstruct net *net = sock_net(sk);\tstruct inet_bind2_bucket *tb2;\tstruct inet_bind_bucket *tb;\tbool tb_created = false;\tu32 remaining, offset;\tint ret, i, low, high;\tint l3mdev;\tu32 index;\t//如果port不为空(用户已经bind过了)，这里直接返回了\tif (port) &#123;\t\tlocal_bh_disable();\t\tret = check_established(death_row, sk, port, NULL);\t\tlocal_bh_enable();\t\treturn ret;\t&#125;···&#125;\n\n上述__inet_hash_connect中，首先判断用户在connect之前是否已经bind过了，入如果bind过，则调用__inet_check_established把sk插入ehash 中(也可能会复用tw状态的套接字)，具体代码如下所示：\n/* called with local bh disabled *///检查端口是否可用的核心函数，主要用于判断一个端口是否已经被 established 或 TIME_WAIT 状态的连接占用static int __inet_check_established(struct inet_timewait_death_row *death_row,\t\t\t\t    struct sock *sk, __u16 lport,\t\t\t\t    struct inet_timewait_sock **twp)&#123;\tstruct inet_hashinfo *hinfo = death_row-&gt;hashinfo;\tstruct inet_sock *inet = inet_sk(sk);\t__be32 daddr = inet-&gt;inet_rcv_saddr;\t__be32 saddr = inet-&gt;inet_daddr;\tint dif = sk-&gt;sk_bound_dev_if;\tstruct net *net = sock_net(sk);\tint sdif = l3mdev_master_ifindex_by_index(net, dif);\tINET_ADDR_COOKIE(acookie, saddr, daddr);\tconst __portpair ports = INET_COMBINED_PORTS(inet-&gt;inet_dport, lport);\tunsigned int hash = inet_ehashfn(net, daddr, lport,\t\t\t\t\t saddr, inet-&gt;inet_dport);\tstruct inet_ehash_bucket *head = inet_ehash_bucket(hinfo, hash);\tspinlock_t *lock = inet_ehash_lockp(hinfo, hash);\tstruct sock *sk2;\tconst struct hlist_nulls_node *node;\tstruct inet_timewait_sock *tw = NULL;\tspin_lock(lock);\t//这里是在遍历ehash的一个桶\tsk_nulls_for_each(sk2, node, &amp;head-&gt;chain) &#123;\t\tif (sk2-&gt;sk_hash != hash)\t\t\tcontinue;\t\t//遍历桶下的条目，判断是否有匹配的\t\tif (likely(inet_match(net, sk2, acookie, ports, dif, sdif))) &#123;\t\t\tif (sk2-&gt;sk_state == TCP_TIME_WAIT) &#123; //如果有匹配的且是tw状态\t\t\t\ttw = inet_twsk(sk2);\t\t\t\t//这里只要系统开启可以重用tw的套接子这里大概率返回1 \t\t\t\tif (twsk_unique(sk, sk2, twp))\t\t\t\t\tbreak;//tw 类型的sock 可以重用\t\t\t&#125;\t\t\tgoto not_unique;\t\t&#125;\t&#125;\t/* Must record num and sport now. Otherwise we will see\t * in hash table socket with a funny identity.\t */\t//走到这里表示没有冲突 或者可以重用\tinet-&gt;inet_num = lport;\tinet-&gt;inet_sport = htons(lport);\tsk-&gt;sk_hash = hash;//找hahs表的hash值\tWARN_ON(!sk_unhashed(sk));\t//将新的sock插入ehahs\t__sk_nulls_add_node_rcu(sk, &amp;head-&gt;chain);\tif (tw) &#123;//如果是重用的那这里就要把旧的给移除\t\tsk_nulls_del_node_init_rcu((struct sock *)tw);\t\t__NET_INC_STATS(net, LINUX_MIB_TIMEWAITRECYCLED);\t&#125;\tspin_unlock(lock);\tsock_prot_inuse_add(sock_net(sk), sk-&gt;sk_prot, 1);\t//这里是空\tif (twp) &#123;\t\t*twp = tw;\t&#125; else if (tw) &#123;\t\t/* Silly. Should hash-dance instead... */\t\tinet_twsk_deschedule_put(tw);//释放twsock的资源\t&#125;\treturn 0;not_unique:\tspin_unlock(lock);\treturn -EADDRNOTAVAIL;&#125;\n\n__inet_check_established首先根据四元组计算一个hash勇于索引ehash表中的桶，然后遍历这个桶下挂的所有sock判断是否有匹配的，这里大概率不会有有吧，如果有的话进一步判断是否是tw类型的sk如果是的话则进一步判断是否可以重用（根据系统参数，如果可以重用的话会重新设置发送序列号），如果可以重用则直接break，否则返回err，如果没有冲突 或者可以重用则会把当前的sk插入到ehash中，同时如果是复用tw的话会移除这个tw。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP三次握手-接收synack","url":"/2025/10/14/TCP%E4%B8%BB%E5%8A%A8%E6%89%93%E5%BC%80%E6%8E%A5%E6%94%B6synack/","content":"TCP主动打开的一方发送syn包后，此时状态为SYN_SENT，该状态下收到数据包后会交由tcp_v4_do_rcv 中的tcp_rcv_state_process处理，具体代码如下所示：\nint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)&#123;\t...\tcase TCP_SYN_SENT:\t\ttp-&gt;rx_opt.saw_tstamp = 0;\t\ttcp_mstamp_refresh(tp);//更新收包时间戳\t\t//真正的处理函数 正常时返回0 返回1 会发送rst\t\tqueued = tcp_rcv_synsent_state_process(sk, skb, th);\t\tif (queued &gt;= 0)\t\t\treturn queued;\t\t/* Do step6 onward by hand. */\t\t//返回-1的情况，通常不会返回-1\t\ttcp_urg(sk, skb, th);\t\t__kfree_skb(skb);\t\ttcp_data_snd_check(sk);\t\treturn 0;\t...&#125;\n\n上述代码是针对TCP_SYN_SENT状态下收到数据包的处理，其实就是调用tcp_rcv_synsent_state_process完成进一步处理，返回值如果为1则会发送rst报文，tcp_rcv_synsent_state_process具体代码如下所示：\nstatic int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,\t\t\t\t\t const struct tcphdr *th)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct tcp_fastopen_cookie foc = &#123; .len = -1 &#125;;\tint saved_clamp = tp-&gt;rx_opt.mss_clamp;\tbool fastopen_fail;\tSKB_DR(reason);\t//解析tcp选项，收到和收到syn包类似，这里时收到synack\ttcp_parse_options(sock_net(sk), skb, &amp;tp-&gt;rx_opt, 0, &amp;foc);\tif (tp-&gt;rx_opt.saw_tstamp &amp;&amp; tp-&gt;rx_opt.rcv_tsecr)\t\ttp-&gt;rx_opt.rcv_tsecr -= tp-&gt;tsoffset;\t//报文有ack\tif (th-&gt;ack) &#123;\t\t/* rfc793:\t\t * &quot;If the state is SYN-SENT then\t\t *    first check the ACK bit\t\t *      If the ACK bit is set\t\t *\t  If SEG.ACK =&lt; ISS, or SEG.ACK &gt; SND.NXT, send\t\t *        a reset (unless the RST bit is set, if so drop\t\t *        the segment and return)&quot;\t\t */\t\t//如果 ack_seq 不在 (snd_una, snd_nxt]进入这个分支，属于异常情况 返回1\t\tif (!after(TCP_SKB_CB(skb)-&gt;ack_seq, tp-&gt;snd_una) ||\t\t    after(TCP_SKB_CB(skb)-&gt;ack_seq, tp-&gt;snd_nxt)) &#123;\t\t\t/* Previous FIN/ACK or RST/ACK might be ignored. */\t\t\tif (icsk-&gt;icsk_retransmits == 0)\t\t\t\tinet_csk_reset_xmit_timer(sk,\t\t\t\t\t\tICSK_TIME_RETRANS,\t\t\t\t\t\tTCP_TIMEOUT_MIN, TCP_RTO_MAX);\t\t\tgoto reset_and_undo;\t\t&#125;\t\t//带时间戳且 rcv_tsecr 不在回显时间戳和now之间 返回1\t\tif (tp-&gt;rx_opt.saw_tstamp &amp;&amp; tp-&gt;rx_opt.rcv_tsecr &amp;&amp;\t\t    !between(tp-&gt;rx_opt.rcv_tsecr, tp-&gt;retrans_stamp,\t\t\t     tcp_time_stamp(tp))) &#123;\t\t\tNET_INC_STATS(sock_net(sk),\t\t\t\t\tLINUX_MIB_PAWSACTIVEREJECTED);\t\t\tgoto reset_and_undo;\t\t&#125;\t\t/* Now ACK is acceptable.\t\t *\t\t * &quot;If the RST bit is set\t\t *    If the ACK was acceptable then signal the user &quot;error:\t\t *    connection reset&quot;, drop the segment, enter CLOSED state,\t\t *    delete TCB, and return.&quot;\t\t */\t\t//rst 直接释放数据包后返回\t\tif (th-&gt;rst) &#123;\t\t\ttcp_reset(sk, skb);consume:\t\t\t__kfree_skb(skb);\t\t\treturn 0;\t\t&#125;\t\t/* rfc793:\t\t *   &quot;fifth, if neither of the SYN or RST bits is set then\t\t *    drop the segment and return.&quot;\t\t *\t\t *    See note below!\t\t *                                        --ANK(990513)\t\t */\t\t//不带 SYN 的 ACK 直接丢弃\t\tif (!th-&gt;syn) &#123;\t\t\tSKB_DR_SET(reason, TCP_FLAGS);\t\t\tgoto discard_and_undo;\t\t&#125;\t\t/* rfc793:\t\t *   &quot;If the SYN bit is on ...\t\t *    are acceptable then ...\t\t *    (our SYN has been ACKed), change the connection\t\t *    state to ESTABLISHED...&quot;\t\t */\t\t//设置ecn标志位\t\ttcp_ecn_rcv_synack(tp, th);\t\t//发送窗口更新时候的序列号\t\ttcp_init_wl(tp, TCP_SKB_CB(skb)-&gt;seq);\t\t//可能撤销超时重传的判断\t\ttcp_try_undo_spurious_syn(sk);\t\ttcp_ack(sk, skb, FLAG_SLOWPATH);\t\t/* Ok.. it&#x27;s good. Set up sequence numbers and\t\t * move to established.\t\t */\t\tWRITE_ONCE(tp-&gt;rcv_nxt, TCP_SKB_CB(skb)-&gt;seq + 1); //更新下一个希望接收的序列号\t\ttp-&gt;rcv_wup = TCP_SKB_CB(skb)-&gt;seq + 1; //记录上一次更新的rcv_nxt\t\t/* RFC1323: The window in SYN &amp; SYN/ACK segments is\t\t * never scaled.\t\t */\t\t//注意：这里是发送窗口的大小，从接收的报文中获取的\t\ttp-&gt;snd_wnd = ntohs(th-&gt;window);\t\t//不支持窗口扩大因子\t\tif (!tp-&gt;rx_opt.wscale_ok) &#123;\t\t\t//窗口缩放因子设置为0\t\t\ttp-&gt;rx_opt.snd_wscale = tp-&gt;rx_opt.rcv_wscale = 0;\t\t\ttp-&gt;window_clamp = min(tp-&gt;window_clamp, 65535U);//这里大概率时65535左右\t\t&#125;\t\t//有时间戳选项\t\tif (tp-&gt;rx_opt.saw_tstamp) &#123;\t\t\ttp-&gt;rx_opt.tstamp_ok\t   = 1;\t\t\ttp-&gt;tcp_header_len =\t\t\t\tsizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;\t\t\ttp-&gt;advmss\t    -= TCPOLEN_TSTAMP_ALIGNED;\t\t\t//记录时间戳，注意这里PAWS机制可能会用到\t\t\ttcp_store_ts_recent(tp);\t\t&#125; else &#123;\t\t\ttp-&gt;tcp_header_len = sizeof(struct tcphdr);\t\t&#125;\t\t//SYN 阶段协商的 MSS 只是对端的建议值 这里要重新计算msscache 这个icsk_pmtu_cookie 大概概率是发送syn包时候设置的\t\ttcp_sync_mss(sk, icsk-&gt;icsk_pmtu_cookie);\t\ttcp_initialize_rcv_mss(sk);\t\t/* Remember, tcp_poll() does not lock socket!\t\t * Change state from SYN-SENT only after copied_seq\t\t * is initialized. */\t\t//用户进程已经读取到的位置\t\tWRITE_ONCE(tp-&gt;copied_seq, tp-&gt;rcv_nxt);\t\tsmc_check_reset_syn(tp);\t\tsmp_mb();\t\t//注意：这里设置了establish状态\t\ttcp_finish_connect(sk, skb);\t\tfastopen_fail = (tp-&gt;syn_fastopen || tp-&gt;syn_data) &amp;&amp;\t\t\t\ttcp_rcv_fastopen_synack(sk, skb, &amp;foc);\t\t//不是dead状态\t\t//！！！！！也就是说connect 返回的时候是establish状态 但是还没有发送ack （阻塞模式）,如果是非阻塞模式  connect的时候返回的是synsent状态\t\tif (!sock_flag(sk, SOCK_DEAD)) &#123;\t\t\tsk-&gt;sk_state_change(sk); //阻塞在 connect() 的进程被唤醒。\t\t\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT);//异步 I/O 事件通知 epoll?\t\t&#125;\t\tif (fastopen_fail)\t\t\treturn -1;\t\tif (sk-&gt;sk_write_pending ||  //有待发的数据这时候刚建立连接 通常不会有吧\t\t    READ_ONCE(icsk-&gt;icsk_accept_queue.rskq_defer_accept) ||  //setsockopt 设置\t\t    inet_csk_in_pingpong_mode(sk)) &#123; //pingpong模式\t\t\t//进入这个分支的思想是，先慢发一个ack然后快发\t\t\t/* Save one ACK. Data will be ready after\t\t\t * several ticks, if write_pending is set.\t\t\t *\t\t\t * It may be deleted, but with this feature tcpdumps\t\t\t * look so _wonderfully_ clever, that I was not able\t\t\t * to stand against the temptation 8)     --ANK\t\t\t */\t\t\tinet_csk_schedule_ack(sk); //设置ack pending位\t\t\ttcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);//设置后面快发ack\t\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK, //启动延迟ack定时器\t\t\t\t\t\t  TCP_DELACK_MAX, TCP_RTO_MAX);\t\t\t//注意这里直接返回了 没有发ack\t\t\tgoto consume;\t\t&#125;\t\t//发送最后一个ack\t\ttcp_send_ack(sk);\t\treturn -1;\t&#125;\t/* No ACK in the segment */\t//没有syn sent状态下收到没有ack报文的情况\t//收到rst 直接丢弃\tif (th-&gt;rst) &#123;\t\t/* rfc793:\t\t * &quot;If the RST bit is set\t\t *\t\t *      Otherwise (no ACK) drop the segment and return.&quot;\t\t */\t\tSKB_DR_SET(reason, TCP_RESET);\t\tgoto discard_and_undo;\t&#125;\t/* PAWS check. */\t//序号不合法直接丢弃\tif (tp-&gt;rx_opt.ts_recent_stamp &amp;&amp; tp-&gt;rx_opt.saw_tstamp &amp;&amp;\t    tcp_paws_reject(&amp;tp-&gt;rx_opt, 0)) &#123;\t\tSKB_DR_SET(reason, TCP_RFC7323_PAWS);\t\tgoto discard_and_undo;\t&#125;\t//收到一个syn包\tif (th-&gt;syn) &#123;\t\t...\t&#125;\t/* &quot;fifth, if neither of the SYN or RST bits is set then\t * drop the segment and return.&quot;\t */discard_and_undo:\ttcp_clear_options(&amp;tp-&gt;rx_opt);\ttp-&gt;rx_opt.mss_clamp = saved_clamp;\ttcp_drop_reason(sk, skb, reason);\treturn 0;reset_and_undo:\ttcp_clear_options(&amp;tp-&gt;rx_opt);\ttp-&gt;rx_opt.mss_clamp = saved_clamp;\treturn 1;&#125;\n\n上述处理逻辑中，首先先解析tcp选项字段，这里和被动打开的一方收到syn包是一样的逻辑，保存服务端所支持的选项，之后针对报文中是否携带ack标志位进行处理，这里如果没有携带ack的话，那就只有两种情况，rst和syn，如果有syn但是没有ack可能是同时打开的情况。首先看一下报文中携带ack的处理逻辑。\n首先检查ack的序列号，如果序列号不在未确认的序号和下一个待发送的序号之间，或者数据包回显的时间戳，不在记录的第一次发包时间和当前时间之间，会直接回复rst。\n然后判断是否数据包是否有rst如果存在释放这个数据包后直接返回，如果是不带syn的ack报文，这里也直接丢弃。\n上述如果都放行了的话则进入正常处理逻辑，调用tcp_ack，然后更新rcv_nxt和记录rcv_wup，从数据包提取发送窗口，处理窗口缩放因子，时间戳选项，更新mss，设置用户进程已经读到的序号(这里设置为rcv_next合理！) 然后调用tcp_finish_connect，具体代码如下所示：\nvoid tcp_finish_connect(struct sock *sk, struct sk_buff *skb)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct inet_connection_sock *icsk = inet_csk(sk);\t//设置establish状态\ttcp_set_state(sk, TCP_ESTABLISHED);\ticsk-&gt;icsk_ack.lrcvtime = tcp_jiffies32;\tif (skb) &#123;\t\t//将sk关联上路由信息\t\ticsk-&gt;icsk_af_ops-&gt;sk_rx_dst_set(sk, skb);\t\tsecurity_inet_conn_established(sk, skb);\t\tsk_mark_napi_id(sk, skb);\t&#125;\t//设置拥塞窗口，接收窗口阈值，tcpmtu探测的范围，调用初始化拥塞算法的钩子（如果有） RTT、ssthresh、乱序阈值\ttcp_init_transfer(sk, BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB, skb);\t/* Prevent spurious tcp_cwnd_restart() on first data\t * packet.\t */\ttp-&gt;lsndtime = tcp_jiffies32;\t//是否需要启动保活定时器\tif (sock_flag(sk, SOCK_KEEPOPEN))\t\tinet_csk_reset_keepalive_timer(sk, keepalive_time_when(tp));\t//如果没有设置窗口扩大因子\tif (!tp-&gt;rx_opt.snd_wscale)\t//设置收包快速路径的标志\t\t__tcp_fast_path_on(tp, tp-&gt;snd_wnd);\telse\t\ttp-&gt;pred_flags = 0;&#125;\n\ntcp_finish_connect中将套接字设置为了TCP_ESTABLISHED状态，同时还初始化了一系列关键字段，例如拥塞窗口，接收窗口阈值，rtt，慢启动阈值乱序阈值，设置pastpath的标志等等，然后调用tcp_init_transfer。具体代码如下所示：\n//接收synack后会调用 三次握手完成后也会调用void tcp_init_transfer(struct sock *sk, int bpf_op, struct sk_buff *skb)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\t//初始化pmtu探测的范围\ttcp_mtup_init(sk);\t//查一次路由，超时重传中也会用到\ticsk-&gt;icsk_af_ops-&gt;rebuild_header(sk);\t//尝试利用 历史缓存的网络性能参数（RTT、ssthresh、乱序阈值等）\ttcp_init_metrics(sk);\t/* Initialize the congestion window to start the transfer.\t * Cut cwnd down to 1 per RFC5681 if SYN or SYN-ACK has been\t * retransmitted. In light of RFC6298 more aggressive 1sec\t * initRTO, we only reset cwnd when more than 1 SYN/SYN-ACK\t * retransmission has occurred.\t */\t//如果三次握手阶段丢包，要降低cwnd所以设置为了1\tif (tp-&gt;total_retrans &gt; 1 &amp;&amp; tp-&gt;undo_marker)\t\ttcp_snd_cwnd_set(tp, 1);\telse\t//这里有用户没有显示配置的话大概率就是10个mss\t\ttcp_snd_cwnd_set(tp, tcp_init_cwnd(tp, __sk_dst_get(sk)));\t//每次调整拥塞窗口的时间戳\ttp-&gt;snd_cwnd_stamp = tcp_jiffies32;\t//bpf相关\tbpf_skops_established(sk, bpf_op, skb);\t/* Initialize congestion control unless BPF initialized it already: */\tif (!icsk-&gt;icsk_ca_initialized) //调用拥塞算法初始化的钩子\t\ttcp_init_congestion_control(sk);\t//设置接收窗口阈值\ttcp_init_buffer_space(sk);&#125;\n\ntcp_init_transfer中首先初始化了pmtu探测需要用到的字段，然后尝试利用 历史缓存的网络性能参数（RTT、ssthresh、乱序阈值等\n之后根据是否丢包设置拥塞窗口大小，最后调用tcp_init_buffer_space设置通告的窗口大小的钳制值(这里主要的工作就是让出一个mss)具体代码如下所示：\nstatic void tcp_init_buffer_space(struct sock *sk)&#123;\tint tcp_app_win = READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_app_win); //默认是31\tstruct tcp_sock *tp = tcp_sk(sk);\tint maxwin;\t//用户是否setsockopt显示设置缓冲大小，默认是没有的\tif (!(sk-&gt;sk_userlocks &amp; SOCK_SNDBUF_LOCK))\t//这里会扩大snd_buf\t\ttcp_sndbuf_expand(sk);\ttcp_mstamp_refresh(tp);\ttp-&gt;rcvq_space.time = tp-&gt;tcp_mstamp;\ttp-&gt;rcvq_space.seq = tp-&gt;copied_seq;\t //这个在三次握手的中间也调用过，用于告诉对端本端窗口的大小，是根据rcv_buf计算出来的\tmaxwin = tcp_full_space(sk);\tif (tp-&gt;window_clamp &gt;= maxwin) &#123;\t\ttp-&gt;window_clamp = maxwin; //限制到最大缓存 这个可以理解为真实的缓存\t\t//最终大概率window_clamp = maxwin左右\t\tif (tcp_app_win &amp;&amp; maxwin &gt; 4 * tp-&gt;advmss)\t\t\ttp-&gt;window_clamp = max(maxwin -\t\t\t\t\t       (maxwin &gt;&gt; tcp_app_win),\t\t\t\t\t       4 * tp-&gt;advmss);\t&#125;\t/* Force reservation of one segment. */\t//接收窗口要让出一个mss\tif (tcp_app_win &amp;&amp;\t    tp-&gt;window_clamp &gt; 2 * tp-&gt;advmss &amp;&amp;\t    tp-&gt;window_clamp + tp-&gt;advmss &gt; maxwin)\t\ttp-&gt;window_clamp = max(2 * tp-&gt;advmss, maxwin - tp-&gt;advmss);\t//tp-&gt;rcv_ssthresh  这个值是发送synack的时候计算出来的(根据rcv_buf)\t///这个叫什么接收慢启动阈值？，就应该叫接受窗口阈值吧\ttp-&gt;rcv_ssthresh = min(tp-&gt;rcv_ssthresh, tp-&gt;window_clamp);\ttp-&gt;snd_cwnd_stamp = tcp_jiffies32;\ttp-&gt;rcvq_space.space = min3(tp-&gt;rcv_ssthresh, tp-&gt;rcv_wnd,   //预测未来的接收缓冲区状态？？\t\t\t\t    (u32)TCP_INIT_CWND * tp-&gt;advmss);&#125;\n\n回到tcp_finish_connect中tcp_init_transfer调用完成后，根据用户配置，决定是否开启保活功能，然后设置需要走快速路径的标志位。\n回到tcp_rcv_synsent_state_process中，上述工作完成后，会唤醒阻塞在connect的进程（如果是阻塞模式的话），这里注意，此时还没有发送ack就已经建立连接状态了。\n最后根据是否需要立即发送ack，如果当前套接字有待发送的数据包，或者用显示设置了不立即发送，亦或是在pingpong模式下都会延迟发送ack，否则立即发送一个ack \n至此，正常逻辑下主动打开的一方收到synack的处理逻辑到此结束：）\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP保活定时器","url":"/2025/08/14/TCP%E4%BF%9D%E6%B4%BB%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"TCP保活定时器TCP 保活定时器（TCP Keepalive Timer）是 TCP 协议栈里用来检测长时间空闲连接是否还存活的一个定时机制。它不是强制标准功能，而是一个可选功能。通过setsockopt设置\n相关内核参数（Linux）这些值可以通过 /proc/sys/net/ipv4/ 目录下的文件查看和调整：\n\ntcp_keepalive_time（默认 7200 秒 &#x3D; 2 小时） 空闲多久后开始发送第一个保活探测包。\ntcp_keepalive_intvl（默认 75 秒） 如果没有收到响应，下一次保活探测的间隔。\ntcp_keepalive_probes（默认 9 次） 如果连续 N 次探测都无响应，内核认为连接已断开，关闭连接。\n\nTCP保活定时器到期的回调tcp_keepalive_timer为注册保活定时器到期的回调函数，该函数中还有针对fin_wait2定时器的逻辑，这里只关心保活定时器的逻辑，tcp_keepalive_timer中的核心逻辑就是计算当前已经空闲的时间，如果达到了超时时间，首先判断是不是已经超过最大的保活探测次数或者超过用户配置的超时时间了，如果是，则会发送rst报文来关闭连接，否则调用tcp_write_wakeup发送一个探测报文，注意这里和零窗口探测调用的是一个函数。具体代码如下：\nstatic void tcp_keepalive_timer (struct timer_list *t)&#123;\tstruct sock *sk = from_timer(sk, t, sk_timer);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 elapsed;\t/* Only process if socket is not in use. */\tbh_lock_sock(sk);\t//用户正在持有这个套接字\tif (sock_owned_by_user(sk)) &#123;\t\t/* Try again later. */\t\t//当前基础上加50ms后调度\t\tinet_csk_reset_keepalive_timer (sk, HZ/20);\t\tgoto out;\t&#125;\tif (sk-&gt;sk_state == TCP_LISTEN) &#123;\t\tpr_err(&quot;Hmm... keepalive on a LISTEN ???\\n&quot;);\t\tgoto out;\t&#125;\t//更新时间戳\ttcp_mstamp_refresh(tp);\t//  这个应该是fin_wait2 定时器，处于fin_wait2状态，且sock 为dead(这里的dead是tcp close中阻塞一个linger时间后（也可能不阻塞）设置的)\t//\t只有两个地方激活定时器后才会走到这个分支，一个是tcp_close 另一个是tcp_rcv_state_process()\tif (sk-&gt;sk_state == TCP_FIN_WAIT2 &amp;&amp; sock_flag(sk, SOCK_DEAD)) &#123;\t\t//用户是否设置linger2\t\tif (READ_ONCE(tp-&gt;linger2) &gt;= 0) &#123;\t\t\t//这里减去了timewait时间为什么？ 没太理解 因为linger2 是fin_wait2到结束的时间？？？ \t\t\tconst int tmo = tcp_fin_time(sk) - TCP_TIMEWAIT_LEN;\t\t\tif (tmo &gt; 0) &#123;\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\t\t\t\tgoto out;\t\t\t&#125;\t\t&#125;\t\t//如果没有设置linger2，因为定时器到期了，这里直接发rst\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\t\tgoto death;\t&#125;\t//是否开启了保活(通过setsockopt设置) 如果没开启保活或者是close或者syn sent状态，直接退出了 \t//！！！这个判断条件在下面的原因应该是不影响到fin_wait2定时器\tif (!sock_flag(sk, SOCK_KEEPOPEN) ||\t    ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_SYN_SENT)))\t\tgoto out;\telapsed = keepalive_time_when(tp);\t/* It is alive without keepalive 8) */\t//如果有发送出去未确认的数据包，或者发送队列不为空\tif (tp-&gt;packets_out || !tcp_write_queue_empty(sk))\t\tgoto resched;\t//拿到空闲的时间\telapsed = keepalive_time_elapsed(tp);\t//是否到达保活时间的阈值了\tif (elapsed &gt;= keepalive_time_when(tp)) &#123;\t\t//用户配置的时间\t\tu32 user_timeout = READ_ONCE(icsk-&gt;icsk_user_timeout);\t\t/* If the TCP_USER_TIMEOUT option is enabled, use that\t\t * to determine when to timeout instead.\t\t */\t\t//这里分为两种情况进入到这个分支\t\t//1. 用户配置了超时时间超过了保活探测时间，同时不是第一次探测\t\t//2. 用户没有配置超时时间，但是超过了保活探测的次数（9次）\t\tif ((user_timeout != 0 &amp;&amp;\t\t    elapsed &gt;= msecs_to_jiffies(user_timeout) &amp;&amp;\t\t    icsk-&gt;icsk_probes_out &gt; 0) ||\t\t    (user_timeout == 0 &amp;&amp;\t\t    icsk-&gt;icsk_probes_out &gt;= keepalive_probes(tp))) &#123;\t\t\t//发送rst报文\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\t\t\t//直接返回err\t\t\ttcp_write_err(sk);\t\t\tgoto out;\t\t&#125;\t\tif (tcp_write_wakeup(sk, LINUX_MIB_TCPKEEPALIVE) &lt;= 0) &#123;\t\t\t//增加探测次数\t\t\ticsk-&gt;icsk_probes_out++;\t\t\t//设置下次保活探测的时间\t\t\telapsed = keepalive_intvl_when(tp);\t\t&#125; else &#123;\t\t\t/* If keepalive was lost due to local congestion,\t\t\t * try harder.\t\t\t */\t\t\telapsed = TCP_RESOURCE_PROBE_INTERVAL; //75s\t\t&#125;\t&#125; else &#123;\t\t/* It is tp-&gt;rcv_tstamp + keepalive_time_when(tp) */\t\t//如果还没有达到发送保活数据包的时间，就重新计算时间\t\telapsed = keepalive_time_when(tp) - elapsed;\t&#125;resched:\t//更新定时器的到期时间\tinet_csk_reset_keepalive_timer (sk, elapsed);\tgoto out;death: //fin_wait2定时器使用\ttcp_done(sk);out:\tbh_unlock_sock(sk);\tsock_put(sk);&#125;\n\nkeepalive_time_when为空闲多久后发送探测报文，如果用户没有设置则为2个小时，keepalive_time_elapsed用来计算当前距离上次收到数据包的时间，具体代码如下：\nstatic inline int keepalive_time_when(const struct tcp_sock *tp)&#123;\tstruct net *net = sock_net((struct sock *)tp);\tint val;\t/* Paired with WRITE_ONCE() in tcp_sock_set_keepidle_locked() */\t//读取用户的配置\tval = READ_ONCE(tp-&gt;keepalive_time);\t//使用系统的配置\treturn val ? : READ_ONCE(net-&gt;ipv4.sysctl_tcp_keepalive_time);&#125;static inline u32 keepalive_time_elapsed(const struct tcp_sock *tp)&#123;\tconst struct inet_connection_sock *icsk = &amp;tp-&gt;inet_conn;\t//返回的是一个当前时间减去最后一个ack或者收到数据包的时间\treturn min_t(u32, tcp_jiffies32 - icsk-&gt;icsk_ack.lrcvtime,\t\t\t  tcp_jiffies32 - tp-&gt;rcv_tstamp);&#125;\n\n如果没有超过次数限制或者用户配置的阈值则会调用tcp_write_wakeup 发送一个保活探测报文，这里其实是和零窗口探测使用的是一个函数\n/* Initiate keepalive or window probe from timer. */int tcp_write_wakeup(struct sock *sk, int mib)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb;\tif (sk-&gt;sk_state == TCP_CLOSE)\t\treturn -1;\t//取出一个skb\tskb = tcp_send_head(sk);\t//数据包非空，且序列号落在右边界内\tif (skb &amp;&amp; before(TCP_SKB_CB(skb)-&gt;seq, tcp_wnd_end(tp))) &#123;\t\tint err;\t\t//拿到mss\t\t\t\tunsigned int mss = tcp_current_mss(sk);\t\t//计算可以用多少字节\t\tunsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)-&gt;seq;\t\t//下一个要push的序列号如果在endseq的前面\t\tif (before(tp-&gt;pushed_seq, TCP_SKB_CB(skb)-&gt;end_seq))\t\t\ttp-&gt;pushed_seq = TCP_SKB_CB(skb)-&gt;end_seq; //更新push seq，表示这学数据要尽快交付\t\t/* We are probing the opening of a window\t\t * but the window size is != 0\t\t * must have been a result SWS avoidance ( sender )\t\t */\t\t//如果可用的空间不够了，或者skb的len大于mss,就需要分段了\t\tif (seg_size &lt; TCP_SKB_CB(skb)-&gt;end_seq - TCP_SKB_CB(skb)-&gt;seq ||\t\t    skb-&gt;len &gt; mss) &#123;\t\t\tseg_size = min(seg_size, mss);\t\t\tTCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_PSH;\t\t\tif (tcp_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\t\t\t\t\t skb, seg_size, mss, GFP_ATOMIC))\t\t\t\treturn -1;\t\t&#125; else if (!tcp_skb_pcount(skb))//如果没有设置这个skb的segs数量\t\t\t//设置segs的数量\t\t\t\t\t\ttcp_set_skb_tso_segs(skb, mss);\t\t//设置push的标志位\t\tTCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_PSH;\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\t\tif (!err)\t\t\ttcp_event_new_data_sent(sk, skb);\t\treturn err;\t&#125; else &#123;\t\t//大概率应该走这个分支？\t\tif (between(tp-&gt;snd_up, tp-&gt;snd_una + 1, tp-&gt;snd_una + 0xFFFF))\t\t\ttcp_xmit_probe_skb(sk, 1, mib);   //有紧急数据的情况，发送数据包的序列号为snd_una\t\treturn tcp_xmit_probe_skb(sk, 0, mib);//发送数据包的序列号为 una - 1 真正的探测包，数据是无法交付给应用层的\t&#125;&#125;\n\nTCP保活定时器到期的激活用户需要setsockopt显示启用tcp保活\nint sk_setsockopt(struct sock *sk, int level, int optname,\t\t  sockptr_t optval, unsigned int optlen)&#123;...\tcase SO_KEEPALIVE:\t\tif (sk-&gt;sk_prot-&gt;keepalive)\t\t\tsk-&gt;sk_prot-&gt;keepalive(sk, valbool);\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\t\tbreak;...&#125;\n\n对于tcp4 则sk-&gt;sk_prot-&gt;keepalive(sk, valbool); 最终调用到tcp_set_keepalive\nvoid tcp_set_keepalive(struct sock *sk, int val)&#123;\tif ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_LISTEN))\t\treturn;\t\t//保活定时器\tif (val &amp;&amp; !sock_flag(sk, SOCK_KEEPOPEN))\t\tinet_csk_reset_keepalive_timer(sk, keepalive_time_when(tcp_sk(sk)));\telse if (!val)\t\tinet_csk_delete_keepalive_timer(sk);&#125;\n\n三次握手阶段，会启动保活定时器\nvoid tcp_finish_connect(struct sock *sk, struct sk_buff *skb)&#123;...\t//保活定时器\tif (sock_flag(sk, SOCK_KEEPOPEN))\t\tinet_csk_reset_keepalive_timer(sk, keepalive_time_when(tp));...&#125;//客户端\n\nstruct sock *tcp_create_openreq_child(const struct sock *sk,\t\t\t\t      struct request_sock *req,\t\t\t\t      struct sk_buff *skb)&#123;...\t//保活定时器\tif (sock_flag(newsk, SOCK_KEEPOPEN))\t\tinet_csk_reset_keepalive_timer(newsk,\t\t\t\t\t       keepalive_time_when(newtp));...&#125;//服务端\n\n\n\n","categories":["网络协议栈源码学习"],"tags":["TCP定时器","TCP"]},{"title":"TCP协议的初始化","url":"/2025/07/24/TCP%E5%8D%8F%E8%AE%AE%E5%88%9D%E5%A7%8B%E5%8C%96/","content":"TCP协议的初始化tcp_init() 是 Linux 内核中 TCP 协议的全局初始化函数，在inet_init中被调用，主要完成了以下工作\n初始化listen哈希表，两个bind哈希表和ehash表，创建bind用到的slab分配器，根据cpu的数量来确定ehash表的锁的数量，hash 桶的大小是更具当前系统内存大小计算得来的 比如8G就是64k个桶，桶的大小是不会改变的，可以通过参数指定范围。\n根据系统内存大小设置tcp的可以使用页数的内存水位线，包括全局的tcp和单个的tcp。\n**调用tcp_v4_init**为每个cpu创建一个rawsocket套接字用来发送rst报文，为每个namespace注册init和exit的ops\n具体逻辑如下：\nvoid __init tcp_init(void)&#123;\tint max_rshare, max_wshare, cnt;\tunsigned long limit;\tunsigned int i;\t//最小的mss不小于tcp选项的长度\tBUILD_BUG_ON(TCP_MIN_SND_MSS &lt;= MAX_TCP_OPTION_SPACE);\tBUILD_BUG_ON(sizeof(struct tcp_skb_cb) &gt;\t\t     sizeof_field(struct sk_buff, cb));\t//初始化一个tcp数量的计数器\tpercpu_counter_init(&amp;tcp_sockets_allocated, 0, GFP_KERNEL);\t//设置一个定时器，不启动，处理孤儿连接\ttimer_setup(&amp;tcp_orphan_timer, tcp_orphan_update, TIMER_DEFERRABLE);\tmod_timer(&amp;tcp_orphan_timer, jiffies + TCP_ORPHAN_TIMER_PERIOD);\t//初始化listen hash表\tinet_hashinfo2_init(&amp;tcp_hashinfo, &quot;tcp_listen_portaddr_hash&quot;,\t\t\t    thash_entries, 21,  /* one slot per 2 MB*/\t\t\t    0, 64 * 1024);\t//创建两个bind用到的slab\ttcp_hashinfo.bind_bucket_cachep =\t\tkmem_cache_create(&quot;tcp_bind_bucket&quot;,\t\t\t\t  sizeof(struct inet_bind_bucket), 0,\t\t\t\t  SLAB_HWCACHE_ALIGN | SLAB_PANIC |\t\t\t\t  SLAB_ACCOUNT,\t\t\t\t  NULL);\ttcp_hashinfo.bind2_bucket_cachep =\t\tkmem_cache_create(&quot;tcp_bind2_bucket&quot;,\t\t\t\t  sizeof(struct inet_bind2_bucket), 0,\t\t\t\t  SLAB_HWCACHE_ALIGN | SLAB_PANIC |\t\t\t\t  SLAB_ACCOUNT,\t\t\t\t  NULL);\t/* Size and allocate the main established and bind bucket\t * hash tables.\t *\t * The methodology is similar to that of the buffer cache.\t */\t //创建ehash表 并初始化桶和锁，bhash是一个bucket一个桶\t //第三个参数的本质上就是根据系统的内存去算有多少个桶 比如8G就是64k个桶\ttcp_hashinfo.ehash =\t\talloc_large_system_hash(&quot;TCP established&quot;,\t\t\t\t\tsizeof(struct inet_ehash_bucket),\t\t\t\t\tthash_entries,\t\t\t\t\t17, /* one slot per 128 KB of memory */ \t\t\t\t\t0,\t\t\t\t\tNULL,\t\t\t\t\t&amp;tcp_hashinfo.ehash_mask,\t\t\t\t\t0,\t\t\t\t\tthash_entries ? 0 : 512 * 1024);\tfor (i = 0; i &lt;= tcp_hashinfo.ehash_mask; i++)\t\tINIT_HLIST_NULLS_HEAD(&amp;tcp_hashinfo.ehash[i].chain, i);\t//根据cpu的数量计算 自旋锁的数量，\tif (inet_ehash_locks_alloc(&amp;tcp_hashinfo))\t\tpanic(&quot;TCP: failed to alloc ehash_locks&quot;);\t//创建bhash表  注意，根据第二个参数这里实际申请了bhash和bhash2 他们在内存上是连续的\ttcp_hashinfo.bhash =\t\talloc_large_system_hash(&quot;TCP bind&quot;,\t\t\t\t\t2 * sizeof(struct inet_bind_hashbucket),\t\t\t\t\ttcp_hashinfo.ehash_mask + 1, //这里其实让ehash和bhash的数目保持一致了\t\t\t\t\t17, /* one slot per 128 KB of memory */\t\t\t\t\t0,\t\t\t\t\t&amp;tcp_hashinfo.bhash_size,\t\t\t\t\tNULL,\t\t\t\t\t0,\t\t\t\t\t64 * 1024);\t//左移 bhash_size 位变成真正的桶数 比如 1 &lt;&lt;12 = 4096\ttcp_hashinfo.bhash_size = 1U &lt;&lt; tcp_hashinfo.bhash_size; \t//设置bhash2的索引\ttcp_hashinfo.bhash2 = tcp_hashinfo.bhash + tcp_hashinfo.bhash_size; \tfor (i = 0; i &lt; tcp_hashinfo.bhash_size; i++) &#123;\t\tspin_lock_init(&amp;tcp_hashinfo.bhash[i].lock);\t\tINIT_HLIST_HEAD(&amp;tcp_hashinfo.bhash[i].chain);\t\tspin_lock_init(&amp;tcp_hashinfo.bhash2[i].lock);\t\tINIT_HLIST_HEAD(&amp;tcp_hashinfo.bhash2[i].chain);\t&#125;\ttcp_hashinfo.pernet = false;\t//根据ehash的桶的数目，计算了一个容许的最大孤儿连接的数量？，在tcp_close 中会用到\tcnt = tcp_hashinfo.ehash_mask + 1;\tsysctl_tcp_max_orphans = cnt / 2;\t//根据系统当前空闲的页数来确定tcp可以使用页数的水位线有三个值，注意这个是全局的\t//net.ipv4.tcp_mem \ttcp_init_mem();\t/* Set per-socket limits to no more than 1/128 the pressure threshold */\t//设置单个tcp的发送和接收使用的字节数，tcp_init_mem的是页数！\tlimit = nr_free_buffer_pages() &lt;&lt; (PAGE_SHIFT - 7);\tmax_wshare = min(4UL*1024*1024, limit);\tmax_rshare = min(6UL*1024*1024, limit);\tinit_net.ipv4.sysctl_tcp_wmem[0] = PAGE_SIZE;\tinit_net.ipv4.sysctl_tcp_wmem[1] = 16*1024;\tinit_net.ipv4.sysctl_tcp_wmem[2] = max(64*1024, max_wshare);\tinit_net.ipv4.sysctl_tcp_rmem[0] = PAGE_SIZE;\tinit_net.ipv4.sysctl_tcp_rmem[1] = 131072;\tinit_net.ipv4.sysctl_tcp_rmem[2] = max(131072, max_rshare);\tpr_info(&quot;Hash tables configured (established %u bind %u)\\n&quot;,\t\ttcp_hashinfo.ehash_mask + 1, tcp_hashinfo.bhash_size);\t//创建一个rawsocket然后初始化系统参数 sysctl -a \ttcp_v4_init();\ttcp_metrics_init();\tBUG_ON(tcp_register_congestion_control(&amp;tcp_reno) != 0);\t//注册tsq的软中端处理函数\ttcp_tasklet_init();\tmptcp_init();&#125;\n\n上述tcp_init_mem();中设置了tcp 内存的水位线具体如下所示，这里的limit的单位是页。\nstatic void __init tcp_init_mem(void)&#123;\tunsigned long limit = nr_free_buffer_pages() / 16;\t//最少用128个页\tlimit = max(limit, 128UL);\t//低阈值\tsysctl_tcp_mem[0] = limit / 4 * 3;\t\t/* 4.68 % */\t//中阈值\tsysctl_tcp_mem[1] = limit;\t\t\t/* 6.25 % */\t//高阈值\tsysctl_tcp_mem[2] = sysctl_tcp_mem[0] * 2;\t/* 9.37 % */&#125;\n\n在tcp_v4_init中创建了一个rawsocket套接字，当tcp需要回复rst报文的时候就是用的这个套接字。然后调用register_pernet_subsys(&amp;tcp_sk_ops)来注册初始化的系统的参数的回调函数。具体代码如下：\nvoid __init tcp_v4_init(void)&#123;\tint cpu, res;\t//这里为每个cpu创建了一个内核的rawsocekt套接字，当tcp发送reset报文的时候貌似就会用这个这个套件字，目的是让用户无感知？\tfor_each_possible_cpu(cpu) &#123;\t\tstruct sock *sk;\t\tres = inet_ctl_sock_create(&amp;sk, PF_INET, SOCK_RAW,\t\t\t\t\t   IPPROTO_TCP, &amp;init_net);\t\tif (res)\t\t\tpanic(&quot;Failed to create the TCP control socket.\\n&quot;);\t\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\t\t//rst报文规范\t\t/* Please enforce IP_DF and IPID==0 for RST and\t\t * ACK sent in SYN-RECV and TIME-WAIT state.\t\t */\t\t\tinet_sk(sk)-&gt;pmtudisc = IP_PMTUDISC_DO;\t\tper_cpu(ipv4_tcp_sk, cpu) = sk;\t&#125;\t//注册tcp协议栈，每个网络空间一个\tif (register_pernet_subsys(&amp;tcp_sk_ops))\t\tpanic(&quot;Failed to create the TCP control socket.\\n&quot;);//bpf相关#if defined(CONFIG_BPF_SYSCALL) &amp;&amp; defined(CONFIG_PROC_FS)\tbpf_iter_register();#endif&#125;\n\n上述tcp_sk_ops的具体结构如下所示：\nstatic struct pernet_operations __net_initdata tcp_sk_ops = &#123;       .init\t   = tcp_sk_init,       .exit\t   = tcp_sk_exit,       .exit_batch = tcp_sk_exit_batch,&#125;;\n\n上述tcp_sk_init中完成了对sysctl -a | grep tcp中绝大多数参数的初始化，以及注册拥塞算法，默认是reno，具体代码如下：\nstatic int __net_init tcp_sk_init(struct net *net)&#123;\tnet-&gt;ipv4.sysctl_tcp_ecn = 2;  //启用ecn\tnet-&gt;ipv4.sysctl_tcp_ecn_fallback = 1;\tnet-&gt;ipv4.sysctl_tcp_base_mss = TCP_BASE_MSS;//1024\tnet-&gt;ipv4.sysctl_tcp_min_snd_mss = TCP_MIN_SND_MSS;  //发送端最小的mss\tnet-&gt;ipv4.sysctl_tcp_probe_threshold = TCP_PROBE_THRESHOLD; //零窗口探测次数 8\tnet-&gt;ipv4.sysctl_tcp_probe_interval = TCP_PROBE_INTERVAL;  //两次探测的最小间隔 600\tnet-&gt;ipv4.sysctl_tcp_mtu_probe_floor = TCP_MIN_SND_MSS; //mtu探测的最小mss\tnet-&gt;ipv4.sysctl_tcp_keepalive_time = TCP_KEEPALIVE_TIME; //在空闲多少秒后启动这个保活探测\tnet-&gt;ipv4.sysctl_tcp_keepalive_probes = TCP_KEEPALIVE_PROBES;//保活探测次数 9\tnet-&gt;ipv4.sysctl_tcp_keepalive_intvl = TCP_KEEPALIVE_INTVL;\t//零窗口探测时间间隔\tnet-&gt;ipv4.sysctl_tcp_syn_retries = TCP_SYN_RETRIES;\t\t\t//syn包重传次数 6\tnet-&gt;ipv4.sysctl_tcp_synack_retries = TCP_SYNACK_RETRIES;\t//syn ack 重传次数 5\tnet-&gt;ipv4.sysctl_tcp_syncookies = 1;\t\t\t\t\t\t//开启syncookie\tnet-&gt;ipv4.sysctl_tcp_reordering = TCP_FASTRETRANS_THRESH;\t//乱续的阈值\tnet-&gt;ipv4.sysctl_tcp_retries1 = TCP_RETR1;\t\t\t\t\t//超过这个数量之后，好险\tnet-&gt;ipv4.sysctl_tcp_retries2 = TCP_RETR2;\t\t\t\t\t//已经建立连接尝试重传的最大次数同上\tnet-&gt;ipv4.sysctl_tcp_orphan_retries = 0;\t\t\t\t\t//孤儿连接的重传次数\tnet-&gt;ipv4.sysctl_tcp_fin_timeout = TCP_FIN_TIMEOUT;\t\t\t//timeout的时间\tnet-&gt;ipv4.sysctl_tcp_notsent_lowat = UINT_MAX;\t\t\t\t//当未发送数据量低于此值时，内核会通知应用程序可以继续写入数据\tnet-&gt;ipv4.sysctl_tcp_tw_reuse = 2;\t\t\t\t\t\t\t//timeout中是否可可以reuseport\tnet-&gt;ipv4.sysctl_tcp_no_ssthresh_metrics_save = 1;\t\t\t//设置为1 表示每次新建立连接的时候重新计算慢启动阈值\trefcount_set(&amp;net-&gt;ipv4.tcp_death_row.tw_refcount, 1);\ttcp_set_hashinfo(net);\tnet-&gt;ipv4.sysctl_tcp_sack = 1;\t\t\t\t\t\t\t\t//sack\tnet-&gt;ipv4.sysctl_tcp_window_scaling = 1;\t\t\t\t\t//窗口缩放\tnet-&gt;ipv4.sysctl_tcp_timestamps = 1;\t\t\t\t\t\t//tcp时间戳选项\tnet-&gt;ipv4.sysctl_tcp_early_retrans = 3;\t\t\t\t\t\t//TLP会用到\tnet-&gt;ipv4.sysctl_tcp_recovery = TCP_RACK_LOSS_DETECTION;\t//RACK\t用到\tnet-&gt;ipv4.sysctl_tcp_slow_start_after_idle = 1; /* By default, RFC2861 behavior.  */  //从idel开始发包后是是否经历慢启动\tnet-&gt;ipv4.sysctl_tcp_retrans_collapse = 1;\t\t\t\t\t//合并重传数据包\tnet-&gt;ipv4.sysctl_tcp_max_reordering = 300;\t\t\t\t\t//容许的乱续数量，怎么计算的？ 拥塞控制中会用到\tnet-&gt;ipv4.sysctl_tcp_dsack = 1;\t\t\t\t\t\t\t\t//启用dsack\t\tnet-&gt;ipv4.sysctl_tcp_app_win = 31;\t\t\t\t\t\t\t//限制 TCP 接收窗口不要占满整个buffer\tnet-&gt;ipv4.sysctl_tcp_adv_win_scale = 1;\t\t\t\t\t\t//好像没有地方用到\tnet-&gt;ipv4.sysctl_tcp_frto = 2;\t\t\t\t\t\t\t\t//FRTO\tnet-&gt;ipv4.sysctl_tcp_moderate_rcvbuf = 1;\t\t\t\t\t//可以动态调整接收缓冲区和接收窗口大小\t/* This limits the percentage of the congestion window which we\t * will allow a single TSO frame to consume.  Building TSO frames\t * which are too large can cause TCP streams to be bursty.\t */\tnet-&gt;ipv4.sysctl_tcp_tso_win_divisor = 3;\t\t\t\t\t//限制tso报文的最大大小\t/* Default TSQ limit of 16 TSO segments */\tnet-&gt;ipv4.sysctl_tcp_limit_output_bytes = 16 * 65536;\t\t//tsq用到！防止当个socket占用整个qdisc队列\t/* rfc5961 challenge ack rate limiting, per net-ns, disabled by default. */\tnet-&gt;ipv4.sysctl_tcp_challenge_ack_limit = INT_MAX;\t\t\t\tnet-&gt;ipv4.sysctl_tcp_min_tso_segs = 2;\t\t\t\t\t\t//最少几个tcp段\tnet-&gt;ipv4.sysctl_tcp_tso_rtt_log = 9;  /* 2^9 = 512 usec */ //计算tso的size会用到\tnet-&gt;ipv4.sysctl_tcp_min_rtt_wlen = 300;\t\t\t\t\t//计算多少轮中rtt的最小值 这里就是300\tnet-&gt;ipv4.sysctl_tcp_autocorking = 1;\t\t\t\t\t\t//是否开启corking\tnet-&gt;ipv4.sysctl_tcp_invalid_ratelimit = HZ/2;\t\t\t\t//参与处理不合法ack的速率的计算\tnet-&gt;ipv4.sysctl_tcp_pacing_ss_ratio = 200;\t\t\t\t\t//ss表示慢启动\tnet-&gt;ipv4.sysctl_tcp_pacing_ca_ratio = 120;\t\t\t\t\t//参与tcppacing的计算，上面的也是ca表示拥塞避免\tif (net != &amp;init_net) &#123;\t\tmemcpy(net-&gt;ipv4.sysctl_tcp_rmem,\t\t       init_net.ipv4.sysctl_tcp_rmem,\t\t       sizeof(init_net.ipv4.sysctl_tcp_rmem));\t\tmemcpy(net-&gt;ipv4.sysctl_tcp_wmem,\t\t       init_net.ipv4.sysctl_tcp_wmem,\t\t       sizeof(init_net.ipv4.sysctl_tcp_wmem));\t&#125;\tnet-&gt;ipv4.sysctl_tcp_comp_sack_delay_ns = NSEC_PER_MSEC;\t\t//压缩sack的时间1ms\tnet-&gt;ipv4.sysctl_tcp_comp_sack_slack_ns = 100 * NSEC_PER_USEC;\t//和上面的配合，提前的时间\tnet-&gt;ipv4.sysctl_tcp_comp_sack_nr = 44;\t\t\t\t\t\t\t//和上面的类似， 收到乱序数据包的数量 超过这个值也发送ack\tnet-&gt;ipv4.sysctl_tcp_fastopen = TFO_CLIENT_ENABLE;\t\t\t\t//是否开启TFO\tnet-&gt;ipv4.sysctl_tcp_fastopen_blackhole_timeout = 0;\t\t\t//TFO 失败后 放入黑名单持续的时间\tatomic_set(&amp;net-&gt;ipv4.tfo_active_disable_times, 0);\t/* Set default values for PLB */\t\t\t\t\t\t\t\t\tnet-&gt;ipv4.sysctl_tcp_plb_enabled = 0; /* Disabled by default */\tnet-&gt;ipv4.sysctl_tcp_plb_idle_rehash_rounds = 3;\tnet-&gt;ipv4.sysctl_tcp_plb_rehash_rounds = 12;\tnet-&gt;ipv4.sysctl_tcp_plb_suspend_rto_sec = 60;\t/* Default congestion threshold for PLB to mark a round is 50% */\tnet-&gt;ipv4.sysctl_tcp_plb_cong_thresh = (1 &lt;&lt; TCP_PLB_SCALE) / 2;\t/* Reno is always built in */\tif (!net_eq(net, &amp;init_net) &amp;&amp;\t    bpf_try_module_get(init_net.ipv4.tcp_congestion_control,\t\t\t       init_net.ipv4.tcp_congestion_control-&gt;owner))\t\tnet-&gt;ipv4.tcp_congestion_control = init_net.ipv4.tcp_congestion_control;\telse\t\tnet-&gt;ipv4.tcp_congestion_control = &amp;tcp_reno;  \tnet-&gt;ipv4.sysctl_tcp_syn_linear_timeouts = 4;\t\t\t\t//线性退避加指数退避\tnet-&gt;ipv4.sysctl_tcp_shrink_window = 0;\t\t\t\t\t\t//是否允许收缩接窗口\treturn 0;&#125;\n\ntcp_init中还会创建  tcp_metrics hash表，缓存rtt和拥塞窗口等信息，用于下次连接的时候参考？\n最终还会调用tcp_tasklet_init来注册tsq的软中断处理函数\nvoid __init tcp_tasklet_init(void)&#123;\tint i;\tfor_each_possible_cpu(i) &#123;\t\tstruct tsq_tasklet *tsq = &amp;per_cpu(tsq_tasklet, i);\t\tINIT_LIST_HEAD(&amp;tsq-&gt;head);\t\t//给tsq关联一个软中断处理函数\t\ttasklet_setup(&amp;tsq-&gt;tasklet, tcp_tasklet_func);\t&#125;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","sock"]},{"title":"TCP超时重传定时器","url":"/2025/08/03/TCP%E5%AE%9A%E6%97%B6%E5%99%A8-%E8%B6%85%E6%97%B6%E9%87%8D%E4%BC%A0%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"TCP的超时重传定时器TCP的超时重传定时器，是TCP最最基本，最重要的定时器。\nTCP超时重传定时器的初始化在用户调用socket() 后，对于AF_INET协议族，会调用inet_create 而inet_create 中会调用具体协议的init函数，对于TCP来说就是tcp_v4_init_sock 在tcp_v4_init_sock最终中会调用tcp_init_xmit_timers 会对多个定时器进行初始化，其中管理超时重传的的定时器为icsk-&gt;icsk_retransmit_timer如下所示：\nvoid inet_csk_init_xmit_timers(struct sock *sk,\t\t\t       void (*retransmit_handler)(struct timer_list *t),\t\t\t       void (*delack_handler)(struct timer_list *t),\t\t\t       void (*keepalive_handler)(struct timer_list *t))&#123;\t...\ttimer_setup(&amp;icsk-&gt;icsk_retransmit_timer, retransmit_handler, 0);//初始化重传定时器\t\t...&#125;\n\n注意:上述其实只是注册了一个定时器并没有真正的启动，定时器对应的超时处理函数为retransmit_handler , 该函数其实使用一个定时器实现了多个定时任务，超时重传定时器只是其中的一个定时任务。retransmit_handler中最终会调用tcp_write_timer_handler来处理不同的定时器任务，具体代码如下：\n/* Called with bottom-half processing disabled.   Called by tcp_write_timer() */void tcp_write_timer_handler(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tint event;\t//判断状态是否有效\tif (((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_LISTEN)) ||\t    !icsk-&gt;icsk_pending)\t\treturn;\tif (time_after(icsk-&gt;icsk_timeout, jiffies)) &#123;\t\tsk_reset_timer(sk, &amp;icsk-&gt;icsk_retransmit_timer, icsk-&gt;icsk_timeout);\t\treturn;\t&#125;\ttcp_mstamp_refresh(tcp_sk(sk));\tevent = icsk-&gt;icsk_pending;\tswitch (event) &#123;\tcase ICSK_TIME_REO_TIMEOUT:\t\ttcp_rack_reo_timeout(sk);\t\tbreak;\tcase ICSK_TIME_LOSS_PROBE:\t\ttcp_send_loss_probe(sk);\t\tbreak;\tcase ICSK_TIME_RETRANS:\t\t/*启动重传定时器的地方1.fastopen 2.正常发送syn包 3.建连后发送数据包 4.tcp_retransmit_timer中再次调度 5.sack撤销\t\t6.tcp_xmit_retransmit_queue中 什么时候会调用tcp_xmit_retransmit_queue？ 检测到丢包或者恢复传输的时候，基本都在tcp_ack中*/\t\t//重置重传定时器的地方 tcp_ack中\t\ticsk-&gt;icsk_pending = 0;\t\ttcp_retransmit_timer(sk);\t\tbreak;\tcase ICSK_TIME_PROBE0:\t\ticsk-&gt;icsk_pending = 0;\t\ttcp_probe_timer(sk);\t\tbreak;\t&#125;&#125;\n\n由上述代码可以看到，TCP的超时重传只是其中的一个‘case’，当设置ICSK_TIME_RETRANS pending位的时候才会真正的处理超时重传！\nTCP超时重传定时器的启动启动TCP定时器定时器的地方有如下几个位置：\n1.TFO 相关static void tcp_rcv_synrecv_state_fastopen(struct sock *sk)&#123;\t...\ttcp_rearm_rto(sk);//fastopen\t...&#125;\n\n\n\n2.TCP 第一次握手发送syn报int tcp_connect(struct sock *sk)&#123;\t...\t/* Timer for repeating the SYN until an answer. */\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\t\t\t\t  inet_csk(sk)-&gt;icsk_rto, TCP_RTO_MAX);\t...&#125;\n\n3. 建连后发送数据包建连后发送数据包若没有未确认的数据，就会启动超时重传定时器下面的tcp_event_new_data_sent 在tcp的发包函数tcp_write_xmit 中会被调用。\nstatic void tcp_event_new_data_sent(struct sock *sk, struct sk_buff *skb)&#123;\t...\tif (!prior_packets || icsk-&gt;icsk_pending == ICSK_TIME_LOSS_PROBE)\t\ttcp_rearm_rto(sk);//没有在途中的数据包\t...&#125;\n\n4.超时重传的回调函数中重新启动定时器void tcp_retransmit_timer(struct sock *sk)&#123;\t...\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\t\t\t\t  tcp_clamp_rto_to_user_timeout(sk), TCP_RTO_MAX);\t...&#125;\n\n5.sack撤销后启动重传定时器6.tcp_ack中重新启动重传定时器TCP  超时重传定时器的关闭当上述定时器被启动后，如果数据包正常到达（也就是数据都被ack掉了），则需要clear掉已经启动的定时器，具体位置在tcp_rearm_rto 中（有多个地方会调用tcp_rearm_rto ，比如tcp_ack ，发送数据包的时候）。\n */void tcp_rearm_rto(struct sock *sk)&#123;\t...\tif (!tp-&gt;packets_out) &#123;\t\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_RETRANS);\t...&#125;\n\nTCP超时重传定时器的回调函数tcp_retransmit_timer为定时器到期的处理函数，该函数主要做了如下工作：\n1.针对TFO或者零窗口情况做特殊处理\n2.从TCP管理的重传队列中取出待重传的数据包\n3.调用tcp_write_timeout判断是否到超过容许的超时时间(根据系统参数计算，超过retry1次 需要进行路由的黑洞检测。)，超过retry2 次则直接返回用户err（本质上是否需要返回err 是根据retry2计算得到的一个时间和当前数据包实际重传所耗费的时间进行比较！）\n4.进入拥塞控制的loss状态，因为已经超时了\n5.调用真正的重传函数tcp_retransmit_skb\n6.如果没有返回用户err, 计算定时器下次触发回调函数的时间(如果是thin flow 就是根据 rtt 计算超时时间，否则就是指数退避）来复位超时重传定时器\n具体代码如下所示：\nvoid tcp_retransmit_timer(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct net *net = sock_net(sk);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct request_sock *req;\tstruct sk_buff *skb;\t//处理TFO的逻辑\treq = rcu_dereference_protected(tp-&gt;fastopen_rsk,\t\t\t\t\tlockdep_sock_is_held(sk));\tif (req) &#123;\t\tWARN_ON_ONCE(sk-&gt;sk_state != TCP_SYN_RECV &amp;&amp;\t\t\t     sk-&gt;sk_state != TCP_FIN_WAIT1);\t\t//针对TFO的处理\t\ttcp_fastopen_synack_timer(sk, req);\t\t/* Before we receive ACK to our SYN-ACK don&#x27;t retransmit\t\t * anything else (e.g., data or FIN segments).\t\t */\t\treturn;\t&#125;\t//如果没有未确认的数据包，就直接返回\tif (!tp-&gt;packets_out)\t\treturn;\t//从重传队列中取出一个数据包\tskb = tcp_rtx_queue_head(sk);\tif (WARN_ON_ONCE(!skb))\t\treturn;\t//tlp探测管理的序号直接置位0 ， 因为都已经超时重传了，等于回退到最原始的重传方式了\ttp-&gt;tlp_high_seq = 0;\t//如果接收窗口为0， socket没有关闭，且不是三次握手阶段进入这个分支\t//注意！通常不会走到这个分支， 当接收端变成零窗口的时候，在超时时间段内通常就恢复了 大概率不会等到超时时间的时候还是零窗口\tif (!tp-&gt;snd_wnd &amp;&amp; !sock_flag(sk, SOCK_DEAD) &amp;&amp;\t    !((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_SYN_SENT | TCPF_SYN_RECV))) &#123;\t\t/* Receiver dastardly shrinks window. Our retransmits\t\t * become zero probes, but we should not timeout this\t\t * connection. If the socket is an orphan, time it out,\t\t * we cannot allow such beasts to hang infinitely.\t\t */\t\tstruct inet_sock *inet = inet_sk(sk);\t\tu32 rtx_delta;\t\t//这里是当前tcp的时间戳 减去数据包或者重传的时间戳，用于下面的打印 ，注意这里用到了ratelimit打印\t\trtx_delta = tcp_time_stamp(tp) - (tp-&gt;retrans_stamp ?: tcp_skb_timestamp(skb));\t\tif (sk-&gt;sk_family == AF_INET) &#123;\t\t\tnet_dbg_ratelimited(&quot;Probing zero-window on %pI4:%u/%u, seq=%u:%u, recv %ums ago, lasting %ums\\n&quot;,\t\t\t\t&amp;inet-&gt;inet_daddr, ntohs(inet-&gt;inet_dport),\t\t\t\tinet-&gt;inet_num, tp-&gt;snd_una, tp-&gt;snd_nxt,\t\t\t\tjiffies_to_msecs(jiffies - tp-&gt;rcv_tstamp),\t\t\t\trtx_delta);\t\t&#125;#if IS_ENABLED(CONFIG_IPV6)\t\telse if (sk-&gt;sk_family == AF_INET6) &#123;\t\t\tnet_dbg_ratelimited(&quot;Probing zero-window on %pI6:%u/%u, seq=%u:%u, recv %ums ago, lasting %ums\\n&quot;,\t\t\t\t&amp;sk-&gt;sk_v6_daddr, ntohs(inet-&gt;inet_dport),\t\t\t\tinet-&gt;inet_num, tp-&gt;snd_una, tp-&gt;snd_nxt,\t\t\t\tjiffies_to_msecs(jiffies - tp-&gt;rcv_tstamp),\t\t\t\trtx_delta);\t\t&#125;#endif\t\t//上面判断了是否是0窗口， 这里需要判断0探测是否超时\t\tif (tcp_rtx_probe0_timed_out(sk, skb)) &#123;\t\t\ttcp_write_err(sk);\t\t\tgoto out;\t\t&#125;\t\t//进入拥塞算法的loss状态\t\ttcp_enter_loss(sk);\t\t//重传数据包\t\ttcp_retransmit_skb(sk, skb, 1);\t\t//这个把路由信息给复位了！\t\t__sk_dst_reset(sk);\t\tgoto out_reset_timer;\t&#125;\t//增加timeout的统计计数\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPTIMEOUTS);\t//这里很关键！！！计算是否达已经超时了 ，如果超过了，里面会调用tcp_err\tif (tcp_write_timeout(sk))\t\tgoto out;\t//如果是一次丢包进入这个分支，增加统计计数，但是第一次感觉不会走任何一个分支，enterloss在下面啊，拥塞状态到这里已经被修改了吗？？可能是受到其他机制的影响\t//比如在tcpack中会改状态？因为icsk_retransmits的值只在超时重传的逻辑中被增加\tif (icsk-&gt;icsk_retransmits == 0) &#123;\t\tint mib_idx = 0;\t\tif (icsk-&gt;icsk_ca_state == TCP_CA_Recovery) &#123;\t\t\tif (tcp_is_sack(tp))\t\t\t\tmib_idx = LINUX_MIB_TCPSACKRECOVERYFAIL;\t\t\telse\t\t\t\tmib_idx = LINUX_MIB_TCPRENORECOVERYFAIL;\t\t&#125; else if (icsk-&gt;icsk_ca_state == TCP_CA_Loss) &#123;\t\t\tmib_idx = LINUX_MIB_TCPLOSSFAILURES;\t\t&#125; else if ((icsk-&gt;icsk_ca_state == TCP_CA_Disorder) ||\t\t\t   tp-&gt;sacked_out) &#123;\t\t\tif (tcp_is_sack(tp))\t\t\t\tmib_idx = LINUX_MIB_TCPSACKFAILURES;\t\t\telse\t\t\t\tmib_idx = LINUX_MIB_TCPRENOFAILURES;\t\t&#125;\t\tif (mib_idx)\t\t\t__NET_INC_STATS(sock_net(sk), mib_idx);\t&#125;\t//进入拥塞的loss状态\ttcp_enter_loss(sk);\ticsk-&gt;icsk_retransmits++;\t//真正的重传\tif (tcp_retransmit_skb(sk, tcp_rtx_queue_head(sk), 1) &gt; 0) &#123;\t\t/* Retransmission failed because of local congestion,\t\t * Let senders fight for local resources conservatively.\t\t */\t\t//走到这里是上面重传失败了，原因可能是本地资源不足，重新设置一下定时器\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\t\t\t\t\t  TCP_RESOURCE_PROBE_INTERVAL,\t\t\t\t\t  TCP_RTO_MAX);\t\t//直接退出\t\tgoto out;\t&#125;\t/* Increase the timeout each time we retransmit.  Note that\t * we do not increase the rtt estimate.  rto is initialized\t * from rtt, but increases here.  Jacobson (SIGCOMM 88) suggests\t * that doubling rto each time is the least we can get away with.\t * In KA9Q, Karn uses this for the first few times, and then\t * goes to quadratic.  netBSD doubles, but only goes up to *64,\t * and clamps at 1 to 64 sec afterwards.  Note that 120 sec is\t * defined in the protocol as the maximum possible RTT.  I guess\t * we&#x27;ll have to use something other than TCP to talk to the\t * University of Mars.\t *\t * PAWS allows us longer timeouts and large windows, so once\t * implemented ftp to mars will work nicely. We will have to fix\t * the 120 second clamps though!\t */\ticsk-&gt;icsk_backoff++;out_reset_timer:\t/* If stream is thin, use linear timeouts. Since &#x27;icsk_backoff&#x27; is\t * used to reset timer, set to 0. Recalculate &#x27;icsk_rto&#x27; as this\t * might be increased if the stream oscillates between thin and thick,\t * thus the old value might already be too high compared to the value\t * set by &#x27;tcp_set_rto&#x27; in tcp_input.c which resets the rto without\t * backoff. Limit to TCP_THIN_LINEAR_RETRIES before initiating\t * exponential backoff behaviour to avoid continue hammering\t * linear-timeout retransmissions into a black hole\t */\t //建连状态下，当前sock是瘦流（发出去的未确认的包很少就叫瘦流）\tif (sk-&gt;sk_state == TCP_ESTABLISHED &amp;&amp;\t    (tp-&gt;thin_lto || READ_ONCE(net-&gt;ipv4.sysctl_tcp_thin_linear_timeouts)) &amp;&amp;\t    tcp_stream_is_thin(tp) &amp;&amp;\t    icsk-&gt;icsk_retransmits &lt;= TCP_THIN_LINEAR_RETRIES) &#123;\t\ticsk-&gt;icsk_backoff = 0; //这里不使用指数退避\t\t//这个clamp可以理解为在rtt和minrto之间选择一个值\t\ticsk-&gt;icsk_rto = clamp(__tcp_set_rto(tp),\t\t//rtt\t\t\t\t       tcp_rto_min(sk),\t\t\t\t\t//tcp初始化的时候设置的200ms\t\t\t\t       TCP_RTO_MAX);\t\t\t\t\t//120s\t&#125; else if (sk-&gt;sk_state != TCP_SYN_SENT ||\t\t   icsk-&gt;icsk_backoff &gt;\t\t   READ_ONCE(net-&gt;ipv4.sysctl_tcp_syn_linear_timeouts)) &#123;\t\t/* Use normal (exponential) backoff unless linear timeouts are\t\t * activated.\t\t */\t\t//建连状态下不是瘦流的话就是指数退避\t\ticsk-&gt;icsk_rto = min(icsk-&gt;icsk_rto &lt;&lt; 1, TCP_RTO_MAX);\t&#125;\t//三个参数，超时重传的的定时器标志 ，定时器的时间， 定时器最大时间 \tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\t\t\t\t  tcp_clamp_rto_to_user_timeout(sk), TCP_RTO_MAX);\t//如果超过retry1 就复位路由 这里为啥+1呢\tif (retransmits_timed_out(sk, READ_ONCE(net-&gt;ipv4.sysctl_tcp_retries1) + 1, 0))\t\t__sk_dst_reset(sk);out:;&#125;\n\n上述代码中会用tcp_write_timeout 来判断是否发生真正的超时，这里的超时指的是需要直接返回给用户err了!\n/* A write timeout has occurred. Process the after effects. */static int tcp_write_timeout(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct net *net = sock_net(sk);\tbool expired = false, do_reset;\tint retry_until, max_retransmits;\t//处理建立建联过程中的重传\tif ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_SYN_SENT | TCPF_SYN_RECV)) &#123;\t\t////第一次进来不走这里，这个值是在外面加的\t\tif (icsk-&gt;icsk_retransmits)  \t\t\t__dst_negative_advice(sk);\t\t/* Paired with WRITE_ONCE() in tcp_sock_set_syncnt() */\t\tretry_until = READ_ONCE(icsk-&gt;icsk_syn_retries) ? : //这个值是通过是setsockopt设置\t\t\tREAD_ONCE(net-&gt;ipv4.sysctl_tcp_syn_retries);\t//初始化的时候设置的\t\tmax_retransmits = retry_until;\t\tif (sk-&gt;sk_state == TCP_SYN_SENT)\t\t\t//注意：这里在默认次数的基础上 又加了sysctl_tcp_syn_linear_timeouts 次！！！\t\t\tmax_retransmits += READ_ONCE(net-&gt;ipv4.sysctl_tcp_syn_linear_timeouts);\t\t//大于 max 就失效了\t\texpired = icsk-&gt;icsk_retransmits &gt;= max_retransmits;\t&#125; else &#123;\t\t//不是建立建连的阶段  这里第二个参数就是retray1 默认是3 \t\t//这里很关键 这里第二个参数retray1传入进入后其实就是判断是否超过了三次指数退避时间，如果超过了，返回true那就需要进行mtu探测了, 防止mtu黑洞导致的丢包。\t\tif (retransmits_timed_out(sk, READ_ONCE(net-&gt;ipv4.sysctl_tcp_retries1), 0)) &#123;\t\t\t/* Black hole detection */\t\t\t//怀疑存在mtu黑洞，!!!\t\t\ttcp_mtu_probing(icsk, sk);\t\t\t__dst_negative_advice(sk);\t\t&#125;\t\tretry_until = READ_ONCE(net-&gt;ipv4.sysctl_tcp_retries2);\t\tif (sock_flag(sk, SOCK_DEAD)) &#123; //如果是孤儿套接字，走这里\t\t\t//最后一次重传的超时时间 是否小于rtomax\t\t\tconst bool alive = icsk-&gt;icsk_rto &lt; TCP_RTO_MAX;\t\t\tretry_until = tcp_orphan_retries(sk, alive);\t\t\tdo_reset = alive ||\t\t\t\t!retransmits_timed_out(sk, retry_until, 0);\t\t\tif (tcp_out_of_resources(sk, do_reset))\t\t\t\treturn 1;\t\t&#125;\t&#125;\t//这里判断是否 expired（到期）了 注意这里 第二个参数是retry_until\tif (!expired)\t\texpired = retransmits_timed_out(sk, retry_until,\t\t\t\t\t\tREAD_ONCE(icsk-&gt;icsk_user_timeout));\t//tfo相关\ttcp_fastopen_active_detect_blackhole(sk, expired);\t//bpf相关\tif (BPF_SOCK_OPS_TEST_FLAG(tp, BPF_SOCK_OPS_RTO_CB_FLAG))\t\ttcp_call_bpf_3arg(sk, BPF_SOCK_OPS_RTO_CB,\t\t\t\t  icsk-&gt;icsk_retransmits,\t\t\t\t  icsk-&gt;icsk_rto, (int)expired);\t//超过重传次数了。直接返回用户err\tif (expired) &#123;\t\t/* Has it gone just too far? */\t\ttcp_write_err(sk);\t\treturn 1;\t&#125;\t//如果开启了rehash 就rehash一次，进而影响队列的选择\tif (sk_rethink_txhash(sk)) &#123;\t\ttp-&gt;timeout_rehash++;\t\t//增加rehash的次数\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPTIMEOUTREHASH);\t&#125;\treturn 0;&#125;\n\ntcp_write_timeout中其实就做了两件事，1.是否需要进行黑洞检测（根据retry1），2.调用retransmits_timed_out是否发生真正的超时（根据retry2计算）。计算是否真正超时就是根据系统参数计算一个容许最大的超时时间，如果当前重传的数据包（多次线性退避加指数退避之后）所消耗的时间大于根据系统参数计算出来的超时时间，就认为是超时了。\n\n如果当前的重传所用的时间超过了根据retry1这个系统参数计算的超时时间，则需要调用tcp_mtu_probing进行黑洞检测，也就是缩小数据包的mss，具体代码如下：\n\nstatic void tcp_mtu_probing(struct inet_connection_sock *icsk, struct sock *sk)&#123;\tconst struct net *net = sock_net(sk);\tint mss;\t/* Black hole detection */\t//如果没有使能mtu探测直接返回\tif (!READ_ONCE(net-&gt;ipv4.sysctl_tcp_mtu_probing))\t\treturn;\t//使能mtu探测\tif (!icsk-&gt;icsk_mtup.enabled) &#123;\t\ticsk-&gt;icsk_mtup.enabled = 1;\t\ticsk-&gt;icsk_mtup.probe_timestamp = tcp_jiffies32;\t&#125; else &#123;\t\t\t\t//这里直接把icsk-&gt;icsk_mtup.search_low 设置成一半了\t\tmss = tcp_mtu_to_mss(sk, icsk-&gt;icsk_mtup.search_low) &gt;&gt; 1;//tcp_mtup_init中设置了icsk-&gt;icsk_mtup.search_low为basemss（初始时1024） tcp_mtup_init 在connet中被调用的\t\t//找一个更小的\t\tmss = min(READ_ONCE(net-&gt;ipv4.sysctl_tcp_base_mss), mss);\t\t//但是别小于最小的，默认48\t\tmss = max(mss, READ_ONCE(net-&gt;ipv4.sysctl_tcp_mtu_probe_floor));\t\t//和发送端最小mss在选一个最大的\t\tmss = max(mss, READ_ONCE(net-&gt;ipv4.sysctl_tcp_min_snd_mss));\t\t//根据mss 和包头长度计算mtu\t\ticsk-&gt;icsk_mtup.search_low = tcp_mss_to_mtu(sk, mss);\t&#125;\t//调用这个的目的应该就是想设置sock的mss 保证最新的mss   上面设置low会影响这个函数计算mss\ttcp_sync_mss(sk, icsk-&gt;icsk_pmtu_cookie);&#125;\n\n\n调用retransmits_timed_out 计算当前耗费的时间是否超过了容许的最大传输时间（这个最大时间是根据retry2计算得到的）\n\n上述retransmits_timed_out 实现如下：\nstatic bool retransmits_timed_out(struct sock *sk,\t\t\t\t  unsigned int boundary,\t\t\t\t  unsigned int timeout)&#123;\tunsigned int start_ts;\t//第一次进来(也就是第一次重传)直接return\tif (!inet_csk(sk)-&gt;icsk_retransmits)\t\treturn false;\t//保存第一次重传的时间\tstart_ts = tcp_sk(sk)-&gt;retrans_stamp;\t//几乎走这里，用户如果setsockopt设置了就不走\tif (likely(timeout == 0)) &#123; \t\t\t\t\tunsigned int rto_base = TCP_RTO_MIN; //默认应该是200ms\t\tif ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_SYN_SENT | TCPF_SYN_RECV))\t\t //如果是建立连接阶段就是1s！！\t\t\trto_base = tcp_timeout_init(sk);\t\t //这里就是根据boundary和rto_base 计算线性+ 指数退避boundary次的timeout时间\t\ttimeout = tcp_model_timeout(sk, boundary, rto_base); \t&#125;\t//这里是根据当前时间 减去首次重传的时间 减去超时时间 如果 &gt;= 0 表示就是超时了！ 也就是超过计算的时间了。\treturn (s32)(tcp_time_stamp(tcp_sk(sk)) - start_ts - timeout) &gt;= 0;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP定时器","TCP"]},{"title":"TCP三次握手-同时打开","url":"/2025/10/21/TCP%E5%90%8C%E6%97%B6%E6%89%93%E5%BC%80/","content":"同时打开的场景出现于双方同时发送syn，因此双方都处于 SYN_SENT 状态，此时如果收到对端的syn包，从tcp_v4_rcv中收到数据包后，根据状态会交由tcp_rcv_synsent_state_process处理，（注意和正常握手接收synack是一个路径），tcp_rcv_synsent_state_process中对于纯syn包的处理如下所示：\nstatic int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,\t\t\t\t\t const struct tcphdr *th)&#123;   ...consume:\t\t\t__kfree_skb(skb);\t\t\treturn 0;if (th-&gt;syn) &#123;\t\t/* We see SYN without ACK. It is attempt of\t\t * simultaneous connect with crossed SYNs.\t\t * Particularly, it can be connect to self.\t\t */\t\t//注意这里设置成了syn recv  正常第三次握手短暂存活的状态！！！\t\ttcp_set_state(sk, TCP_SYN_RECV);\t\t//有时间戳选项，保存时间戳 修正tcp头长度\t\tif (tp-&gt;rx_opt.saw_tstamp) &#123;\t\t\ttp-&gt;rx_opt.tstamp_ok = 1;\t\t\ttcp_store_ts_recent(tp);\t\t\ttp-&gt;tcp_header_len =\t\t\t\tsizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;\t\t&#125; else &#123;\t\t\ttp-&gt;tcp_header_len = sizeof(struct tcphdr);\t\t&#125;\t\t//更新接收序号与用户已读指针\t\tWRITE_ONCE(tp-&gt;rcv_nxt, TCP_SKB_CB(skb)-&gt;seq + 1);\t\tWRITE_ONCE(tp-&gt;copied_seq, tp-&gt;rcv_nxt);\t\ttp-&gt;rcv_wup = TCP_SKB_CB(skb)-&gt;seq + 1;\t\t/* RFC1323: The window in SYN &amp; SYN/ACK segments is\t\t * never scaled.\t\t */\t\t//握手报文不使用窗口缩放\t\ttp-&gt;snd_wnd    = ntohs(th-&gt;window);\t\ttp-&gt;snd_wl1    = TCP_SKB_CB(skb)-&gt;seq;\t\ttp-&gt;max_window = tp-&gt;snd_wnd;\t\ttcp_ecn_rcv_syn(tp, th);\t\t//初始化PMTU相关的字段\t\ttcp_mtup_init(sk);\t\t//重新计算mss cache  这第二个参数是发syn包的时候设置的\t\ttcp_sync_mss(sk, icsk-&gt;icsk_pmtu_cookie);\t\t//设置接收mss\t\ttcp_initialize_rcv_mss(sk);\t\t//发送syn ack\t\ttcp_send_synack(sk);#if 0\t\t/* Note, we could accept data and URG from this segment.\t\t * There are no obstacles to make this (except that we must\t\t * either change tcp_recvmsg() to prevent it from returning data\t\t * before 3WHS completes per RFC793, or employ TCP Fast Open).\t\t *\t\t * However, if we ignore data in ACKless segments sometimes,\t\t * we have no reasons to accept it sometimes.\t\t * Also, seems the code doing it in step6 of tcp_rcv_state_process\t\t * is not flawless. So, discard packet for sanity.\t\t * Uncomment this return to process the data.\t\t */\t\treturn -1;#else\t\tgoto consume;#endif\t&#125;    ...&#125;\n\n如上述代码所示，SYN_SENT状态下如果收到一个合法的syn包会直接将状态设置为TCP_SYN_RECV，注意，这里和第三次握手收到ack后短暂存活的状态是一样的，因此下面发送synack的时候就走了第三次握手的分支，并设置为建立连接状态。\n接下来处理时间戳选项，更新接收序号与用户已读指针，初始化PMTU相关的字段，重新计算mss相关参数，之后调用tcp_send_synack\n发送syn_ack发送synack具体代码如下所示：\n//注意 只有同时打开调用这个函数int tcp_send_synack(struct sock *sk)&#123;\tstruct sk_buff *skb;\t//取出重传队列中的syn  很巧妙 应为没有被ack\tskb = tcp_rtx_queue_head(sk);\tif (!skb || !(TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_SYN)) &#123;\t\tpr_err(&quot;%s: wrong queue state\\n&quot;, __func__);\t\treturn -EFAULT;\t&#125;\t//如果这个 SYN 还没带 ACK，就给它加上 ACK 标志\tif (!(TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_ACK)) &#123;\t\tif (skb_cloned(skb)) &#123; //处理克隆的情况\t\t\tstruct sk_buff *nskb;\t\t\ttcp_skb_tsorted_save(skb) &#123;\t\t\t\tnskb = skb_copy(skb, GFP_ATOMIC); //深copy\t\t\t&#125; tcp_skb_tsorted_restore(skb);\t\t\tif (!nskb)\t\t\t\treturn -ENOMEM;\t\t\tINIT_LIST_HEAD(&amp;nskb-&gt;tcp_tsorted_anchor);\t\t\ttcp_highest_sack_replace(sk, skb, nskb);//如果是最高 SACK包，更新为新的 nskb\t\t\ttcp_rtx_queue_unlink_and_free(skb, sk); \t\t\t__skb_header_release(nskb);\t\t\ttcp_rbtree_insert(&amp;sk-&gt;tcp_rtx_queue, nskb);//替换原 skb\t\t\tsk_wmem_queued_add(sk, nskb-&gt;truesize);  //更新队列使用量 注意上面会uncharge\t\t\tsk_mem_charge(sk, nskb-&gt;truesize); //正式向内存管理系统申请这部分内存\t\t\tskb = nskb;\t\t&#125;\t\t//给新包加上 ACK 位\t\tTCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_ACK;\t\t//如果协商了 ECN（显式拥塞通知），在 SYN+ACK 上打相应的标志\t\ttcp_ecn_send_synack(sk, skb);\t&#125;\t//发送数据包\treturn tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);&#125;\n\n首先从传出队列中取出skb由于此时数据包一定没有被ack所以一定能获取，之后深拷贝获取一个skb，将flag加上ack标志，并插入重传队列中，更新内存使用量，之后调用tcp_transmit_skb发送数据包。之后，当收到对端synack后会重走一便第三次握手的逻辑，这里不再重复了：）\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP拥塞控制-BBR算法（一）","url":"/2025/11/09/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6-BBR%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89/","content":"BBR算法注册的回调函数如下所示：\nstatic struct tcp_congestion_ops tcp_bbr_cong_ops __read_mostly = &#123;\t.flags\t\t= TCP_CONG_NON_RESTRICTED,\t// 不受限的算法类型，可被任意 socket 使用（非专用）\t.name\t\t= &quot;bbr&quot;,\t\t\t\t\t\t.owner\t\t= THIS_MODULE,\t\t\t\t.init\t\t= bbr_init,\t\t\t\t\t\t// 初始化函数：每条连接建立时调用，初始化 BBR 的内部状态（min_rtt、bw 等）\t.cong_control\t= bbr_main,\t\t\t\t\t// bbr控制核心函数！！！！！！！！！！！！,每次收到tcp_ack中会调用\t.sndbuf_expand\t= bbr_sndbuf_expand,\t\t// 发送缓存扩展函数：三次握手建立连接时调用\t.undo_cwnd\t= bbr_undo_cwnd,\t\t\t\t// 撤销 cwnd 缩减：如果丢包判断错误（如 DSACK 撤销），恢复 cwnd 到丢包前的值\t.cwnd_event\t= bbr_cwnd_event,\t\t\t\t// cwnd 事件处理这里bbr只处理用户发包太慢\t.ssthresh\t= bbr_ssthresh,\t\t\t\t\t// 计算慢启动阈值：进入丢包恢复时调用，BBR 不使用传统 ssthresh，通常直接返回当前值\t.min_tso_segs\t= bbr_min_tso_segs,\t\t\t// 设置最小 TSO 分段数\t.get_info\t= bbr_get_info,\t\t\t\t\t// 导出 BBR 运行时信息\t.set_state\t= bbr_set_state,\t\t\t\t// 状态切换回调&#125;;\n\n注意BBR算法的回调中没有实现拥塞避免的回调函数，而是实现了cong_control接口，原因是BBR不是基于丢包的算法。因此在收到ack后会调用cong_control，而不是传统的cong_avoid。\nbbr_init三次握手建立完成后会调用拥塞算法的初始化函数，如果此时是bbr算法，则会调用bbr_init\n__bpf_kfunc static void bbr_init(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\tbbr-&gt;prior_cwnd = 0;\ttp-&gt;snd_ssthresh = TCP_INFINITE_SSTHRESH; \t//这里是无限大\tbbr-&gt;rtt_cnt = 0;\t\t\t\t\t\t\t//包计时轮次计数器\tbbr-&gt;next_rtt_delivered = tp-&gt;delivered; \t//当前轮的结束 delivered 值\tbbr-&gt;prev_ca_state = TCP_CA_Open;\t\t\t//Open\tbbr-&gt;packet_conservation = 0;\t\t\t\t//包守恒标志\tbbr-&gt;probe_rtt_done_stamp = 0; \t\t\t\t//probe结束的时间戳\tbbr-&gt;probe_rtt_round_done = 0;\t\t\t\t//probe完成一轮\tbbr-&gt;min_rtt_us = tcp_min_rtt(tp); \t\t\t//最小rtt\tbbr-&gt;min_rtt_stamp = tcp_jiffies32; \t\t//记录rtt的时间\tminmax_reset(&amp;bbr-&gt;bw, bbr-&gt;rtt_cnt, 0);  /* init max bw to 0 */\tbbr-&gt;has_seen_rtt = 0;\tbbr_init_pacing_rate_from_rtt(sk); \t\t\t//计算发包速率 pacingrate！\tbbr-&gt;round_start = 0;\t\t\t\t\t\t// 是否是新一轮的ack\tbbr-&gt;idle_restart = 0;\t \t\t\t\t\t// 是否处于应用空闲后重启\tbbr-&gt;full_bw_reached = 0;  \t\t\t\t\t//是否达到最大带宽\tbbr-&gt;full_bw = 0;\t\t\t\t\t\t\t//最大带宽\tbbr-&gt;full_bw_cnt = 0;\t\t\t\t\t\t//连续未显著增长的轮次\tbbr-&gt;cycle_mstamp = 0;\t\t\t\t\t\t//PROBE_BW 8 相位循环的起点时间清零\tbbr-&gt;cycle_idx = 0;\t\t\t\t\t\t\t//相位索引\tbbr_reset_lt_bw_sampling(sk);  \t\t\t\t//长期带宽采样\tbbr_reset_startup_mode(sk);  \t\t\t\t//初始状态 STARTUP 模式\tbbr-&gt;ack_epoch_mstamp = tp-&gt;tcp_mstamp; \t//ACK 聚合统计周期的起点时间\tbbr-&gt;ack_epoch_acked = 0;\t\t\t\t\t//累计收到的 ACK 数量\tbbr-&gt;extra_acked_win_rtts = 0;\t\t\t\t //管理ack聚合的情况\tbbr-&gt;extra_acked_win_idx = 0;\tbbr-&gt;extra_acked[0] = 0;\tbbr-&gt;extra_acked[1] = 0;\tcmpxchg(&amp;sk-&gt;sk_pacing_status, SK_PACING_NONE, SK_PACING_NEEDED); //发包通路上会判断这个标志&#125;\n\nbbr_init中复位了算法使用的多个字段，例如rtt，最大带宽，管理探测周期的字段，相位的索引，长期带宽采样（丢包会用到），聚合ack数量，其中调用了bbr_init_pacing_rate_from_rtt设置了初始的发包速率，之后调用bbr_reset_startup_mode设置为STARTUP状态。\n上述bbr_init_pacing_rate_from_rtt代码如下所示：\nstatic void bbr_init_pacing_rate_from_rtt(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\tu64 bw;\tu32 rtt_us;\tif (tp-&gt;srtt_us) &#123;\t\t/* any RTT sample yet? *///平滑rtt\t\trtt_us = max(tp-&gt;srtt_us &gt;&gt; 3, 1U);\t\tbbr-&gt;has_seen_rtt = 1;\t&#125; else &#123;\t\t\t /* no RTT sample yet */\t\trtt_us = USEC_PER_MSEC;\t /* use nominal default RTT *///1ms\t&#125;\tbw = (u64)tcp_snd_cwnd(tp) * BW_UNIT;\tdo_div(bw, rtt_us);////计算bw\tsk-&gt;sk_pacing_rate = bbr_bw_to_pacing_rate(sk, bw, bbr_high_gain);&#125;\n\nbbr_init_pacing_rate_from_rtt中首先判断是否能拿到平滑的rtt（这里大概率可以拿到），之后获取当前的拥塞窗口，两者运算后获取到bw注意这里的单位是mss&#x2F;us，之后调用bbr_bw_to_pacing_rate转换成发包速率，具体调用代码如下所示：\nstatic u64 bbr_rate_bytes_per_sec(struct sock *sk, u64 rate, int gain)&#123;\tunsigned int mss = tcp_sk(sk)-&gt;mss_cache;\trate *= mss;// pkt/µs → bytes/µs\trate *= gain;\trate &gt;&gt;= BBR_SCALE; //去掉缩放\trate *= USEC_PER_SEC / 100 * (100 - bbr_pacing_margin_percent); //把 µs 换算成秒乘 0.99\treturn rate &gt;&gt; BW_SCALE; //去掉缩放&#125;/* Convert a BBR bw and gain factor to a pacing rate in bytes per second. */static unsigned long bbr_bw_to_pacing_rate(struct sock *sk, u32 bw, int gain)&#123;\tu64 rate = bw;\trate = bbr_rate_bytes_per_sec(sk, rate, gain);// 转为 bytes/s\trate = min_t(u64, rate, sk-&gt;sk_max_pacing_rate);\treturn rate;&#125;\n\n从上述代码可以看到sk_pacing_rate的单位为，bytes&#x2F;sec,  这里注意，tcp发包通路中会用到sk_pacing_rate来控制发送速率。\n回到bbr_init中，计算初始化的发包速率之后会调用bbr_reset_lt_bw_sampling复位长期带宽采样的字段，具体代码如下所示:\n/* Completely reset long-term bandwidth sampling. */static void bbr_reset_lt_bw_sampling(struct sock *sk)&#123;\tstruct bbr *bbr = inet_csk_ca(sk);\tbbr-&gt;lt_bw = 0; \t\t\t//长期平均带宽\tbbr-&gt;lt_use_bw = 0;\t\t    //表示当前不使用 lt_bw 来替代普通的带宽估计\tbbr-&gt;lt_is_sampling = false; //当前没有在进行长期采样\tbbr_reset_lt_bw_sampling_interval(sk);&#125;static void bbr_reset_lt_bw_sampling_interval(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\tbbr-&gt;lt_last_stamp = div_u64(tp-&gt;delivered_mstamp, USEC_PER_MSEC);\tbbr-&gt;lt_last_delivered = tp-&gt;delivered;\tbbr-&gt;lt_last_lost = tp-&gt;lost; \t//丢包数量\tbbr-&gt;lt_rtt_cnt = 0; \t\t\t//清零长期采样周期内的 RTT 计数&#125;/* Start a new long-term sampling interval. */static void bbr_reset_lt_bw_sampling_interval(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\tbbr-&gt;lt_last_stamp = div_u64(tp-&gt;delivered_mstamp, USEC_PER_MSEC);\tbbr-&gt;lt_last_delivered = tp-&gt;delivered;\tbbr-&gt;lt_last_lost = tp-&gt;lost; \t//丢包数量\tbbr-&gt;lt_rtt_cnt = 0; \t\t\t//清零长期采样周期内的 RTT 计数&#125;\n\n复位长期带宽采样的相关字段后，会将当前bbr状态设置为STARTUP模式，具体代码如下所示：\nstatic void bbr_reset_startup_mode(struct sock *sk)&#123;\tstruct bbr *bbr = inet_csk_ca(sk);\tbbr-&gt;mode = BBR_STARTUP;&#125;\n\n\n\nbbr_mainbbr_main为bbr拥塞算法的核心，具体工作包括计算bw，完成不同状态之间的转换，是否需要长期采样带宽，完成发包速率和cwnd的计算，具体代码如下所示：\n__bpf_kfunc static void bbr_main(struct sock *sk, const struct rate_sample *rs)&#123;\tstruct bbr *bbr = inet_csk_ca(sk);\tu32 bw;\t//核心\tbbr_update_model(sk, rs);\t//拿到当前的带宽估计，最大或者是长期的\tbw = bbr_bw(sk);\t//设置发包速率\tbbr_set_pacing_rate(sk, bw, bbr-&gt;pacing_gain);\t//计算cwnd\tbbr_set_cwnd(sk, rs, rs-&gt;acked_sacked, bw, bbr-&gt;cwnd_gain);&#125;\n\nbbr_update_model为测量与状态机更新的关键函数，具体代码如下所示：\nstatic void bbr_update_model(struct sock *sk, const struct rate_sample *rs)&#123;\t//更新bw\tbbr_update_bw(sk, rs);\t//计算聚合ack的数量\tbbr_update_ack_aggregation(sk, rs);\t//相位变换\tbbr_update_cycle_phase(sk, rs);\t//是否达到最大速率\tbbr_check_full_bw_reached(sk, rs);\t//是否需要排空\tbbr_check_drain(sk, rs);\t//更新最小rtt 是否进入到probe rtt\tbbr_update_min_rtt(sk, rs);\t//根据不同状态设置影响pacing 和cwnd的参数\tbbr_update_gains(sk);&#125;\n\nbbr_update_model中首先调用bbr_update_bw 更新带宽估计\n调用bbr_update_ack_aggregation统计ack聚合情况(后面会补偿cwnd)\n调用bbr_update_cycle_phase处理相位的变化(8个)\n调用bbr_check_full_bw_reached检查是否已经到达最大bw(到达之后通常会进入probe_bw状态)\n调用bbr_check_drain是否需要排空\n调用bbr_update_min_rtt更新最小rtt，以及是否需要切换到probe rtt状态。\n调用bbr_update_gains 设置 pacing_gain 与 cwnd_gain\n上述bbr_update_bw代码如下所示：\n/* Estimate the bandwidth based on how fast packets are delivered */static void bbr_update_bw(struct sock *sk, const struct rate_sample *rs)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\tu64 bw;\t//这里上来就设置为0 \tbbr-&gt;round_start = 0;\t//不合法检查\tif (rs-&gt;delivered &lt; 0 || rs-&gt;interval_us &lt;= 0)\t\treturn; /* Not a valid observation */\t/* See if we&#x27;ve reached the next RTT */\t//是否可以进入到新的一轮，本次速率采样区间开始时的确认包数 每个rtt进入一次\tif (!before(rs-&gt;prior_delivered, bbr-&gt;next_rtt_delivered)) &#123;\t\tbbr-&gt;next_rtt_delivered = tp-&gt;delivered; //\t\tbbr-&gt;rtt_cnt++;  //轮次++\t\tbbr-&gt;round_start = 1; //标记新一轮开始\t\tbbr-&gt;packet_conservation = 0; //清掉包守恒\t&#125;\t//长期带宽估计，被限速的时候会启用长期bw？\tbbr_lt_bw_sampling(sk, rs);\t/* Divide delivered by the interval to find a (lower bound) bottleneck\t * bandwidth sample. Delivered is in packets and interval_us in uS and\t * ratio will be &lt;&lt;1 for most connections. So delivered is first scaled.\t */\t//计算本次ack的样本的bw\tbw = div64_long((u64)rs-&gt;delivered * BW_UNIT, rs-&gt;interval_us);\t/* If this sample is application-limited, it is likely to have a very\t * low delivered count that represents application behavior rather than\t * the available network rate. Such a sample could drag down estimated\t * bw, causing needless slow-down. Thus, to continue to send at the\t * last measured network rate, we filter out app-limited samples unless\t * they describe the path bw at least as well as our bw model.\t *\t * So the goal during app-limited phase is to proceed with the best\t * network rate no matter how long. We automatically leave this\t * phase when app writes faster than the network can deliver :)\t */\t//用户没有发送的太慢\tif (!rs-&gt;is_app_limited || bw &gt;= bbr_max_bw(sk)) &#123;\t\t/* Incorporate new sample into our max bw filter. */\t\t//min_max rtt测量也用到， 这里是的窗口是10 个rtt，记录的是最大带宽\t\tminmax_running_max(&amp;bbr-&gt;bw, bbr_bw_rtts, bbr-&gt;rtt_cnt, bw);\t&#125;&#125;\n\nbbr_update_bw中首先判断当前是否计入到了一个新的轮次(取决于本次速率采样区间开始时的确认包数是否超过了被确认的ack总数)。\n如果是则设置round_start为1（后面多个地方会用到），同时更新轮次统计计数（更新bw会用到），之后调用bbr_lt_bw_sampling监测是否出现了令牌桶限速，并决定是否启用长期带宽估计，具体代码如下所示：\n//检测是否被令牌桶限速，并估计长期带宽static void bbr_lt_bw_sampling(struct sock *sk, const struct rate_sample *rs)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\tu32 lost, delivered;\tu64 bw;\tu32 t;\t//已经使用长期带宽了\tif (bbr-&gt;lt_use_bw) &#123;\t/* already using long-term rate, lt_bw? */\t\tif (bbr-&gt;mode == BBR_PROBE_BW &amp;&amp; bbr-&gt;round_start &amp;&amp;  //处于稳态，且当前的ack是新的计时轮次中的\t\t    ++bbr-&gt;lt_rtt_cnt &gt;= bbr_lt_bw_max_rtts) &#123;  //最多维持多少个rtt使用lt_bw\t\t\tbbr_reset_lt_bw_sampling(sk);    /* stop using lt_bw */ //复位使用到的字段\t\t\tbbr_reset_probe_bw_mode(sk);  /* restart gain cycling */ //进入带宽探测模式\t\t&#125;\t\treturn;\t&#125;\t/* Wait for the first loss before sampling, to let the policer exhaust\t * its tokens and estimate the steady-state rate allowed by the policer.\t * Starting samples earlier includes bursts that over-estimate the bw.\t */\t//没有开始长期采样\tif (!bbr-&gt;lt_is_sampling) &#123;\t\tif (!rs-&gt;losses) //没有丢包，直接返回\t\t\treturn;\t\tbbr_reset_lt_bw_sampling_interval(sk);\t\tbbr-&gt;lt_is_sampling = true;\t&#125;\t/* To avoid underestimates, reset sampling if we run out of data. */\tif (rs-&gt;is_app_limited) &#123; //用户发的太慢直接返回\t\tbbr_reset_lt_bw_sampling(sk);\t\treturn;\t&#125;\t//当前ACK属于一个新的RTT轮次的起点\tif (bbr-&gt;round_start)\t\tbbr-&gt;lt_rtt_cnt++;\t//一个新的RTT周期完成\tif (bbr-&gt;lt_rtt_cnt &lt; bbr_lt_intvl_min_rtts)//太短，不统计 至少要经过4个rtt！\t\treturn;\t\t/* sampling interval needs to be longer */\tif (bbr-&gt;lt_rtt_cnt &gt; 4 * bbr_lt_intvl_min_rtts) &#123; //太长也不考虑\t\tbbr_reset_lt_bw_sampling(sk);  /* interval is too long */\t\treturn;\t&#125;\t/* End sampling interval when a packet is lost, so we estimate the\t * policer tokens were exhausted. Stopping the sampling before the\t * tokens are exhausted under-estimates the policed rate.\t */\tif (!rs-&gt;losses) //没有丢包直接返回\t\treturn;\t/* Calculate packets lost and delivered in sampling interval. */\tlost = tp-&gt;lost - bbr-&gt;lt_last_lost; //采样期间丢失的数据包\tdelivered = tp-&gt;delivered - bbr-&gt;lt_last_delivered; //采样期间发送的数据包\t/* Is loss rate (lost/delivered) &gt;= lt_loss_thresh? If not, wait. */\tif (!delivered || (lost &lt;&lt; BBR_SCALE) &lt; bbr_lt_loss_thresh * delivered)//20%\t\treturn;\t/* Find average delivery rate in this sampling interval. */\t//上述采样所用的时间 ms\tt = div_u64(tp-&gt;delivered_mstamp, USEC_PER_MSEC) - bbr-&gt;lt_last_stamp;\tif ((s32)t &lt; 1) //时间太短 返回\t\treturn;\t\t/* interval is less than one ms, so wait */\t/* Check if can multiply without overflow */\tif (t &gt;= ~0U / USEC_PER_MSEC) &#123; //太长也不行\t\tbbr_reset_lt_bw_sampling(sk);  /* interval too long; reset */\t\treturn;\t&#125;\tt *= USEC_PER_MSEC;\tbw = (u64)delivered * BW_UNIT;\tdo_div(bw, t);\tbbr_lt_bw_interval_done(sk, bw);&#125;\n\nbbr_lt_bw_sampling() 是 BBR 用来在 被限速网络中估算长期有效带宽 的机制。当链路出现持续丢包时，说明可能触发了限速器，这时 BBR 会启动一次长期带宽采样。具体来说bbr_lt_bw_sampling中首先判断是否已经启用了长期带宽估计，如果在48个轮次之外，且当前状态为稳态，会直接复位长期带宽采样的信息，否则直接返回。\n如果没有开启长期采样，且没有丢包发生，或者用户发送数据包太慢，这里也直接返回。如果有丢包发生，则开启长期采样，接下进入计算长期采样带宽的逻辑。注意这里有个前置条件，就是rtt的轮次不能太短(要超过4个rtt)，也不能太长(16个rtt)，这个条件下才会进行进一步计算，计算这段时间内丢失的数据包数**，如果低于百分之20这里也直接返回！**之后根据采样期间发送的数据包数，和采样持续的时间计算bw。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP拥塞控制"]},{"title":"TCP拥塞控制-BBR算法（三）","url":"/2025/11/16/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6-BBR%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89/","content":"bbr_main中调用bbr_update_model完成后，会调用bbr_bw获取瓶颈带宽（取决于是否开启长期带宽检测）具体代码如下所示：\nstatic u32 bbr_bw(const struct sock *sk)&#123;\tstruct bbr *bbr = inet_csk_ca(sk);\treturn bbr-&gt;lt_use_bw ? bbr-&gt;lt_bw : bbr_max_bw(sk);&#125;\n\n拿到bw后调用bbr_set_pacing_rate设置发包速率，具体代码如下所示：\nstatic void bbr_set_pacing_rate(struct sock *sk, u32 bw, int gain)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\tunsigned long rate = bbr_bw_to_pacing_rate(sk, bw, gain);  \t   //计算pacing速率\tif (unlikely(!bbr-&gt;has_seen_rtt &amp;&amp; tp-&gt;srtt_us)) \t\t\t  //没有rtt?\t\tbbr_init_pacing_rate_from_rtt(sk); \t\t\t\t\t\t //没有记录rtt的情况，设置rtt 计算bw\tif (bbr_full_bw_reached(sk) || rate &gt; sk-&gt;sk_pacing_rate)\t   //达到最大带宽，或者大于pacing速率\t\tsk-&gt;sk_pacing_rate = rate;&#125;\n\n更新发包速率后，接下调用bbr_set_cwnd设置拥塞窗口的大小，具体代码如下所示：\nstatic void bbr_set_cwnd(struct sock *sk, const struct rate_sample *rs,\t\t\t u32 acked, u32 bw, int gain)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\tu32 cwnd = tcp_snd_cwnd(tp), target_cwnd = 0;\t//确认报文段的个数\tif (!acked)\t\tgoto done;  /* no packet fully ACKed; just apply caps */\t//设置cwnd 如果是包守恒（第一轮进入recovery）就就直接返回了\tif (bbr_set_cwnd_to_recover_or_restore(sk, rs, acked, &amp;cwnd))\t\tgoto done;\t//根据bw(段数/us)计算bdp\ttarget_cwnd = bbr_bdp(sk, bw, gain);\t/* Increment the cwnd to account for excess ACKed data that seems\t * due to aggregation (of data and/or ACKs) visible in the ACK stream.\t */\t//考虑聚合后的cwnd\ttarget_cwnd += bbr_ack_aggregation_cwnd(sk);\t//考虑tc 和tso/gso等\ttarget_cwnd = bbr_quantization_budget(sk, target_cwnd);\t/* If we&#x27;re below target cwnd, slow start cwnd toward target cwnd. */\t//是否达到最大带宽\tif (bbr_full_bw_reached(sk))  /* only cut cwnd if we filled the pipe */\t//每次按 +acked 线性涨，但不超过 target_cwnd（BDP×gain 得到的目标）。\t\tcwnd = min(cwnd + acked, target_cwnd);\telse if (cwnd &lt; target_cwnd || tp-&gt;delivered &lt; TCP_INIT_CWND) //Startup 阶段\t\tcwnd = cwnd + acked;\tcwnd = max(cwnd, bbr_cwnd_min_target);//确保大于4done:\ttcp_snd_cwnd_set(tp, min(cwnd, tp-&gt;snd_cwnd_clamp));\t/* apply global cap */ //设置拥塞窗口\tif (bbr-&gt;mode == BBR_PROBE_RTT)  /* drain queue, refresh min_rtt */ //probe状态的特殊处理\t\ttcp_snd_cwnd_set(tp, min(tcp_snd_cwnd(tp), bbr_cwnd_min_target));&#125;\n\nbbr_set_cwnd中首先判断是否确认了数据包，如果没有确认这里直接返回。\n否则调用bbr_set_cwnd_to_recover_or_restore决定是进入包守恒模式、恢复旧 cwnd 还是继续按正常 BBR 计算，具体代码如下所示：\nstatic bool bbr_set_cwnd_to_recover_or_restore(\tstruct sock *sk, const struct rate_sample *rs, u32 acked, u32 *new_cwnd)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\tu8 prev_state = bbr-&gt;prev_ca_state, state = inet_csk(sk)-&gt;icsk_ca_state;\tu32 cwnd = tcp_snd_cwnd(tp);\t/* An ACK for P pkts should release at most 2*P packets. We do this\t * in two steps. First, here we deduct the number of lost packets.\t * Then, in bbr_set_cwnd() we slow start up toward the target cwnd.\t */\t//减去丢包数\tif (rs-&gt;losses &gt; 0)\t\tcwnd = max_t(s32, cwnd - rs-&gt;losses, 1);\t//进入recovery状态，且上次不是这个状态，表示刚进入这个状态\tif (state == TCP_CA_Recovery &amp;&amp; prev_state != TCP_CA_Recovery) &#123;\t\t/* Starting 1st round of Recovery, so do packet conservation. */\t\tbbr-&gt;packet_conservation = 1; //包守恒\t\tbbr-&gt;next_rtt_delivered = tp-&gt;delivered;  /* start round now */ //当前确认的数据包数\t\t/* Cut unused cwnd from app behavior, TSQ, or TSO deferral: */\t\tcwnd = tcp_packets_in_flight(tp) + acked;\t\t//从更严重的状态到 正常状态或者乱序\t&#125; else if (prev_state &gt;= TCP_CA_Recovery &amp;&amp; state &lt; TCP_CA_Recovery) &#123; \t\t/* Exiting loss recovery; restore cwnd saved before recovery. */\t\tcwnd = max(cwnd, bbr-&gt;prior_cwnd); //相当于撤销\t\tbbr-&gt;packet_conservation = 0; //关闭包守恒\t&#125;\tbbr-&gt;prev_ca_state = state;//记录本次的状态\t//在包守恒模式下\tif (bbr-&gt;packet_conservation) &#123;\t\t*new_cwnd = max(cwnd, tcp_packets_in_flight(tp) + acked);\t\treturn true;\t/* yes, using packet conservation */\t&#125;\t*new_cwnd = cwnd;\treturn false;&#125;\n\nbbr_set_cwnd_to_recover_or_restore首先判断是否丢包了，如果丢包了会将拥塞窗口减去丢包的数量，之后判断是否首次进入Recovery，如果是，则进入包守恒模式，把cwnd设置为in_flight + acked\n如果刚从 Recovery&#x2F;Loss 状态退出，则恢复旧 cwnd，关闭包守恒。\n 如果当前处于包守恒模式 ，直接用包守恒规则算 cwnd，并返回 true\n回到bbr_set_cwnd中如果在保守恒模式下，调用bbr_set_cwnd_to_recover_or_restore后就直接返回了。否则调用bbr_bdp计算bdp\n之后考虑聚合ack和tso，gso等情况修正cwnd，具体代码如下所示：\nstatic u32 bbr_ack_aggregation_cwnd(struct sock *sk)&#123;\tu32 max_aggr_cwnd, aggr_cwnd = 0;\t//表示连接已经完成启动阶段（Startup），管道带宽已探测完成\tif (bbr_extra_acked_gain &amp;&amp; bbr_full_bw_reached(sk)) &#123;\t\tmax_aggr_cwnd = ((u64)bbr_bw(sk) * bbr_extra_acked_max_us) //钳制\t\t\t\t/ BW_UNIT;\t\taggr_cwnd = (bbr_extra_acked_gain * bbr_extra_acked(sk)) //计算额外的cwnd\t\t\t     &gt;&gt; BBR_SCALE;\t\taggr_cwnd = min(aggr_cwnd, max_aggr_cwnd); //取一个最小值\t&#125;\treturn aggr_cwnd;&#125;static u32 bbr_quantization_budget(struct sock *sk, u32 cwnd)&#123;\tstruct bbr *bbr = inet_csk_ca(sk);\t/* Allow enough full-sized skbs in flight to utilize end systems. */\tcwnd += 3 * bbr_tso_segs_goal(sk); //qdisc tso/gso LRG/GRO\t/* Reduce delayed ACKs by rounding up cwnd to the next even number. */\tcwnd = (cwnd + 1) &amp; ~1U;// 偶数？\t/* Ensure gain cycling gets inflight above BDP even for small BDPs. */\tif (bbr-&gt;mode == BBR_PROBE_BW &amp;&amp; bbr-&gt;cycle_idx == 0) //第一个相位\t\tcwnd += 2;\treturn cwnd; //增加后的cwnd&#125;\n\n修正cwnd之后，接下来会判断是否达到了最大带宽，如果达到了最大带宽，就慢慢涨 cwnd（线性增长），否则不做上限设置。\n最后如果是在BBR_PROBE_RTT模式下，这设置拥塞窗口最大为4。\n至此，bbr_main中所有逻辑就结束了，接下来分析其他钩子函数。\nbbr_undo_cwnd为拥塞撤销时候的回调函数，具体代码如下所示：\n__bpf_kfunc static u32 bbr_undo_cwnd(struct sock *sk)&#123;\tstruct bbr *bbr = inet_csk_ca(sk);\t//复位最大带宽\tbbr-&gt;full_bw = 0;   /* spurious slow-down; reset full pipe detection */\tbbr-&gt;full_bw_cnt = 0;\tbbr_reset_lt_bw_sampling(sk); //重置长期带宽采样\treturn tcp_snd_cwnd(tcp_sk(sk));//设置回之前的窗口大小&#125;\n\nbbr_undo_cwnd() 是 BBR 在“误判丢包”撤销阶段的恢复函数， 它不改变 cwnd，只重置带宽采样状态（full_bw、lt_bw）\nbbr_cwnd_event为刚恢复发送（应用层之前停了）场景下的回调函数，具体代码如下所示：\n__bpf_kfunc static void bbr_cwnd_event(struct sock *sk, enum tcp_ca_event event)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\tif (event == CA_EVENT_TX_START &amp;&amp; tp-&gt;app_limited) &#123; //开始发送数据，且应用发送的慢\t\tbbr-&gt;idle_restart = 1;\t\tbbr-&gt;ack_epoch_mstamp = tp-&gt;tcp_mstamp;\t\t\t//重置 ACK 统计周期的起始时间戳 聚合ack会用到\t\tbbr-&gt;ack_epoch_acked = 0;\t\t\t\t\t\t//清零本周期累计 ACK 的字节数\t\t/* Avoid pointless buffer overflows: pace at est. bw if we don&#x27;t\t\t * need more speed (we&#x27;re restarting from idle and app-limited).\t\t */\t\tif (bbr-&gt;mode == BBR_PROBE_BW)\t\t\t\t\t//稳态模式下设置gain为1\t\t\tbbr_set_pacing_rate(sk, bbr_bw(sk), BBR_UNIT);\t\telse if (bbr-&gt;mode == BBR_PROBE_RTT)\t\t\t//rtt探测模式下\t\t\tbbr_check_probe_rtt_done(sk);\t\t\t\t//可能切换到start  或者 稳态\t&#125;&#125;\n\n上述段代码用于处理 TCP从空闲重新开始发送数据时的窗口调整行为。当事件类型为 CA_EVENT_TX_START 且当前连接处于 **应用限速状态 ，BBR 会重置 ACK 聚合统计周期。如果处在带宽探测稳），则将 pacing 增益设置为 **1，让发送速率与带宽估计保持一致，如果处在 PROBE_RTT（测 RTT 阶段），则调用 bbr_check_probe_rtt_done() 判断是否应退出测 RTT 状态并恢复正常模式。这里注意，应用层的限速标记可能是bbr算法中探测最小rtt时候设设置的。\nbbr_ssthresh为设置慢启动阈值的回调函数，注意，因为BBR算法不依靠丢包，因此这里这里没有改变慢启动阈值，具体代码如下所示:\n__bpf_kfunc static u32 bbr_ssthresh(struct sock *sk)&#123;\tbbr_save_cwnd(sk);\t\t\t\t //保存之前的cwnd\treturn tcp_sk(sk)-&gt;snd_ssthresh; //注意，这里没有改变慢启动阈值，因为不依靠丢包&#125;\n\nbbr_set_state为设置拥塞状态的回调函数，与cubic类似，bbr也只实现了针对loss状态的处理，将记录最大带宽复位，（会影响是否达到最大速率判断），将长期带宽采用信息复位，同时标记为新一轮bbr算法的开始。具体代码如下所示：\n__bpf_kfunc static void bbr_set_state(struct sock *sk, u8 new_state)&#123;\tstruct bbr *bbr = inet_csk_ca(sk);\tif (new_state == TCP_CA_Loss) &#123;\t\tstruct rate_sample rs = &#123; .losses = 1 &#125;;\t\tbbr-&gt;prev_ca_state = TCP_CA_Loss;\t\tbbr-&gt;full_bw = 0;\t\t//复位带宽信息\t\tbbr-&gt;round_start = 1;\t/* treat RTO like end of a round */ //新的一轮\t\tbbr_lt_bw_sampling(sk, &amp;rs);\t&#125;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP拥塞控制"]},{"title":"TCP拥塞控制-CUBIC算法(一)","url":"/2025/11/02/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6-cubic%E6%8B%A5%E5%A1%9E%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89/","content":" CUBIC 是为高带宽-时延积设计的拥塞控制算法，也是目前 Linux系统的默认 TCP 算法与传统的 Reno 算法（线性增大窗口）不同，CUBIC 采用随时间增长拥塞窗口的策略，使得窗口先快速上升接近上次拥塞前的大小（凹形增长），在达到“拐点”时短暂平稳期，然后再以更快的速度进行凸形增长来探测更多带宽。这样设计可以使窗口增长独立于 RTT（基于时间驱动）。\n CUBIC 的核心公式如下所示$$W(t) &#x3D; C \\cdot (t - K)^3 + W_{\\max}$$\n\n\n\n参数\n含义\n典型值 &#x2F; 单位\n说明\n\n\n\nW(t)\n当前时间 t 下的目标拥塞窗口（Congestion Window）\n单位：MSS\n算法根据时间推算出的理想发送窗口。实际发送窗口 cwnd 会慢慢靠近它。\n\n\nC\n曲线增长系数（Cubic Scaling Factor）\n通常取 0.4\n控制窗口增长的“速度”和“陡峭程度”。C 越大，增长越快；越小，增长越平缓。\n\n\nt\n自上次丢包事件以来的时间\n秒（或 RTT 为单位）\n从上次检测到拥塞开始计时，用于计算曲线位置。\n\n\nK\n到达上次最大窗口 Wmax 所需的时间偏移\n单位：秒（或 RTT）\n表示多久后窗口恢复到上次丢包时的最大值。由下式计算：\n\n\n拥塞算法的注册注册拥塞算法的代码如下所示：\nstatic int __init cubictcp_register(void)&#123;\tint ret;\tBUILD_BUG_ON(sizeof(struct bictcp) &gt; ICSK_CA_PRIV_SIZE);\t/* Precompute a bunch of the scaling factors that are used per-packet\t * based on SRTT of 100ms\t */\t// TCP友好性用到 beta_scale 用来把 CUBIC 的增长速度调成和 Reno 相当\tbeta_scale = 8*(BICTCP_BETA_SCALE+beta) / 3\t\t/ (BICTCP_BETA_SCALE - beta);\t//算C用到\tcube_rtt_scale = (bic_scale * 10);\t/* 1024*c/rtt */\t/* calculate the &quot;K&quot; for (wmax-cwnd) = c/rtt * K^3\t *  so K = cubic_root( (wmax-cwnd)*rtt/c )\t * the unit of K is bictcp_HZ=2^10, not HZ\t *\t *  c = bic_scale &gt;&gt; 10\t *  rtt = 100ms\t *\t * the following code has been designed and tested for\t * cwnd &lt; 1 million packets\t * RTT &lt; 100 seconds\t * HZ &lt; 1,000,00  (corresponding to 10 nano-second)\t */\t/* 1/c * 2^2*bictcp_HZ * srtt */\tcube_factor = 1ull &lt;&lt; (10+3*BICTCP_HZ); /* 2^40 */\t//算K的时候用到\t/* divide by bic_scale and by constant Srtt (100ms) */\tdo_div(cube_factor, bic_scale * 10);\t//bfp相关\tret = register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS, &amp;tcp_cubic_kfunc_set);\tif (ret &lt; 0)\t\treturn ret;\treturn tcp_register_congestion_control(&amp;cubictcp);&#125;\n\ncubictcp_register中首先预计算了几个常量，计算C和K的时候会用到，用于减少运算的复杂度，之后调用tcp_register_congestion_control注册CUBIC的一系列ops，具体的回调函数集合如下所示以及管理CUBIC结构的字段如下所示：\nstatic struct tcp_congestion_ops cubictcp __read_mostly = &#123;\t.init\t\t= cubictcp_init,\t\t//三次握手完成的时候会调用\t.ssthresh\t= cubictcp_recalc_ssthresh,  //进入恢复状态的时候会调用，重新计算慢启动阈值\t.cong_avoid\t= cubictcp_cong_avoid,\t//拥塞避免阶段会调用\t.set_state\t= cubictcp_state,\t\t//状态转换的时候会调用\t.undo_cwnd\t= tcp_reno_undo_cwnd,  //拥塞窗口撤销的时候调用\t.cwnd_event\t= cubictcp_cwnd_event, //多个地方会调用，在发送的通路上回调用cubic注册的这个钩子\t.pkts_acked     = cubictcp_acked,\t//清理重传队列回调用，是否需要提前退出慢启动\t.owner\t\t= THIS_MODULE,\t.name\t\t= &quot;cubic&quot;,&#125;;\n\nstruct bictcp &#123;\tu32\tcnt;\t\t        // 每收到 cnt 个 ACK 后 cwnd 增加 1，控制增长速度\tu32\tlast_max_cwnd;\t    // 上次丢包前的最大 cwnd，用于确定下一轮 cubic 峰值\tu32\tlast_cwnd;\t\t    // 上次调用 bictcp_update() 时的 cwnd\tu32\tlast_time;\t\t    // 上次更新 cwnd 的时间戳（jiffies）\tu32\tbic_origin_point;\t// cubic 曲线原点（通常为 Wmax 或当前 cwnd）\tu32\tbic_K;\t\t        // 当前周期起点到原点的时间间隔，用于计算 cubic 曲线\tu32\tdelay_min;\t\t    // 观测到的最小 RTT（微秒），表示基础传播延迟\tu32\tepoch_start;\t    // 当前增长周期起始时间戳（检测到拥塞后重置）\tu32\tack_cnt;\t\t    // 当前周期累计 ACK 数量，用于控制 cwnd 增长步频\tu32\ttcp_cwnd;\t\t    // 模拟 Reno 下的 cwnd，用于保持 TCP 友好性\tu16\tunused;\t\t        // 保留字段（未使用）\tu8\tsample_cnt;\t\t    // 当前 RTT 轮采样的 RTT 数量，用于 HyStart 延迟判断\tu8\tfound;\t\t        // 是否已找到退出慢启动的时机（HyStart 标志）\tu32\tround_start;\t    // 当前轮发送开始时间，用于 ACK train 检测\tu32\tend_seq;\t\t    // 当前轮发送数据的最后一个序列号\tu32\tlast_ack;\t\t    // 最近一次收到密集 ACK 的时间戳（ACK train）\tu32\tcurr_rtt;\t\t    // 当前轮最小 RTT，用于判断排队是否出现&#125;;\n\n如上述代码所示，在协议栈的不同阶段会调用上述注册的一系列钩子，具体功能如下所示\ncubictcp_init三次握手完成时候会调用，作用是完成拥塞算法的初始化，具体包括复位管理拥塞控制的结构体，以及如果开启了hystart也会复位相关字段。此外，如果显示设置了慢启动阈值，则使用设置的慢启动阈值，具体代码如下所示：\n__bpf_kfunc static void cubictcp_init(struct sock *sk)&#123;\tstruct bictcp *ca = inet_csk_ca(sk);\t//memset\tbictcp_reset(ca);\t//默认是1 ，检测到拥塞迹象时提前退出慢启动\tif (hystart)\t\tbictcp_hystart_reset(sk);\t//是否设置了初始化了慢启动阈值\tif (!hystart &amp;&amp; initial_ssthresh) \t\ttcp_sk(sk)-&gt;snd_ssthresh = initial_ssthresh;&#125;static inline void bictcp_reset(struct bictcp *ca)&#123;\tmemset(ca, 0, offsetof(struct bictcp, unused));\tca-&gt;found = 0;&#125;static inline void bictcp_hystart_reset(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bictcp *ca = inet_csk_ca(sk);\tca-&gt;round_start = ca-&gt;last_ack = bictcp_clock_us(sk); //复位时间戳\tca-&gt;end_seq = tp-&gt;snd_nxt; //记录当前发送的下一个序列号\tca-&gt;curr_rtt = ~0U;  // 重置 RTT 测量\tca-&gt;sample_cnt = 0;&#125;\n\ncubictcp_recalc_ssthresh用于计算慢启动阈值，超时进入loss 或者收到显示拥塞，或者进入recovery都会调用这个函数重新计算慢启动阈值，具体代码如下所示：\n__bpf_kfunc static u32 cubictcp_recalc_ssthresh(struct sock *sk)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\tstruct bictcp *ca = inet_csk_ca(sk);\t//表示当前的 CUBIC 增长周期结束，拥塞避免会用到\tca-&gt;epoch_start = 0;\t/* end of epoch */\t/* Wmax and fast convergence */\t//当前 cwnd 比上一次的最大窗口小, 并且启用了快速收敛 就说明这次丢包发生在带宽下降的情况下\t//如果开启 fast_convergence，新的历史最大窗口 = 当前窗口*0.85 即比上一次最大值更小一点，防止继续过冲\t//last_max_cwnd影响拥塞避免阶段的计算\tif (tcp_snd_cwnd(tp) &lt; ca-&gt;last_max_cwnd &amp;&amp; fast_convergence)\t\tca-&gt;last_max_cwnd = (tcp_snd_cwnd(tp) * (BICTCP_BETA_SCALE + beta))\t\t\t/ (2 * BICTCP_BETA_SCALE);\telse //带宽没变差\t\tca-&gt;last_max_cwnd = tcp_snd_cwnd(tp); //就直接把当前 cwnd 当成新的最大窗口 Wmax\t//cwnd减少到70%\treturn max((tcp_snd_cwnd(tp) * beta) / BICTCP_BETA_SCALE, 2U);&#125;\n\n cubictcp_recalc_ssthresh用来计算新的慢启动阈值。首先把当前的 CUBIC 增长周期结束掉，这样下一次拥塞避免时会重新按立方曲线计算。接着它会根据当前的 cwnd 和历史最大窗口 last_max_cwnd 来更新 Wmax：如果发现这次拥塞发生在比上次更小的窗口上，并且开启了 fast convergence，就把记录的 Wmax 稍微往下压一截，。最后它返回 cwnd * 0.7 作为新的 ssthresh，这也是 CUBIC 相比 Reno 更“温和”的地方——Reno 是减半，CUBIC 是减到 70%。\ncubictcp_cong_avoid为拥塞避免的回调函数，如果处于open状态，在tcp_ack中会被调用，用于计算新的拥塞窗口大小，具体代码如下所示：\n__bpf_kfunc static void cubictcp_cong_avoid(struct sock *sk, u32 ack, u32 acked)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bictcp *ca = inet_csk_ca(sk);\t//是否被拥塞窗口限制，该字段在发包的时候被设置\tif (!tcp_is_cwnd_limited(sk))\t\treturn;\t//当前拥塞窗口小于慢启动阈值\tif (tcp_in_slow_start(tp)) &#123;\t\tacked = tcp_slow_start(tp, acked);\t\tif (!acked)\t\t\treturn;\t&#125;\t//传入剩余ack的数量\tbictcp_update(ca, tcp_snd_cwnd(tp), acked);\t//更新拥塞窗口\ttcp_cong_avoid_ai(tp, ca-&gt;cnt, acked);&#125;\n\ncubictcp_cong_avoid中首先判断是否需要扩大拥塞窗口(飞的数据包比拥塞窗口大，就需要扩大拥塞窗口)如果不需要，就直接返回了，否则进一步判断是否在慢启动阶段内（当前拥塞窗口小于慢启动阈值），如果在慢启动阶段，调用tcp_slow_start,也就是收到一个段，增加一个拥塞窗口，具体代码如下所示：\n__bpf_kfunc u32 tcp_slow_start(struct tcp_sock *tp, u32 acked)&#123;\t//拥塞窗口阈值加上新确认的段数的最小值\tu32 cwnd = min(tcp_snd_cwnd(tp) + acked, tp-&gt;snd_ssthresh);\t//计算剩余的ack数量\tacked -= cwnd - tcp_snd_cwnd(tp);\ttcp_snd_cwnd_set(tp, min(cwnd, tp-&gt;snd_cwnd_clamp));//这个钳制应该是无限大？\treturn acked; //给拥塞避免使用&#125;\n\n可以看到返回的ack可以理解为超出慢启动阈值，留给拥塞避免的数量。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP拥塞控制"]},{"title":"TCP拥塞控制-BBR算法（二）","url":"/2025/11/11/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6-BBR%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"bbr_update_bw中调用bbr_lt_bw_sampling会检测是否需要开启长期采样机制（是否稳定丢包），是否使用长期bw由bbr_lt_bw_interval_done决定，具体代码如下所示：\nstatic void bbr_lt_bw_interval_done(struct sock *sk, u32 bw)&#123;\tstruct bbr *bbr = inet_csk_ca(sk);\tu32 diff;\tif (bbr-&gt;lt_bw) &#123;  /* do we have bw from a previous interval? */\t\t/* Is new bw close to the lt_bw from the previous interval? */\t\tdiff = abs(bw - bbr-&gt;lt_bw);\t\tif ((diff * BBR_UNIT &lt;= bbr_lt_bw_ratio * bbr-&gt;lt_bw) ||   //和上次比相差不大\t\t    (bbr_rate_bytes_per_sec(sk, diff, BBR_UNIT) &lt;= //两次带宽样本的绝对差值\t\t     bbr_lt_bw_diff)) &#123; //4kb per sec\t\t\t/* All criteria are met; estimate we&#x27;re policed. */\t\t\tbbr-&gt;lt_bw = (bw + bbr-&gt;lt_bw) &gt;&gt; 1;  /* avg 2 intvls */ //平均\t\t\tbbr-&gt;lt_use_bw = 1; //使用长期bw\t\t\tbbr-&gt;pacing_gain = BBR_UNIT;  /* try to avoid drops */\t\t\tbbr-&gt;lt_rtt_cnt = 0;\t\t\treturn;\t\t&#125;\t&#125;\tbbr-&gt;lt_bw = bw; //包/us\tbbr_reset_lt_bw_sampling_interval(sk); //reset 长期采样&#125;\n\nbbr_lt_bw_interval_done中核心逻辑为比较当前周期测得的带宽 bw 与上一次 bbr-&gt;lt_bw判断带宽变化是否稳定\n如果稳定，则启用长期带宽模式，否则保存bw后复位长期采样信息。\n回到bbr_update_bw中，计算完成是否使用长期采样带宽后，根据本采样的信息，计算bw，注意这里bw的单位是packet&#x2F;us，接下来根据is_app_limited和bbr_max_bw判断是否需要更新最大带宽，如果需要则调用minmax_running_max更新最大的的带宽。至此，bbr_update_model中计算bw的工作就完成了。\n接下来是调用bbr_update_ack_aggregation计算聚合ack的数量，计算的目的是动态补偿 cwnd。具体代码如下所示：\n//计算可能的聚合的ack数量static void bbr_update_ack_aggregation(struct sock *sk,\t\t\t\t       const struct rate_sample *rs)&#123;\tu32 epoch_us, expected_acked, extra_acked;\tstruct bbr *bbr = inet_csk_ca(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\t//合法性检查\tif (!bbr_extra_acked_gain || rs-&gt;acked_sacked &lt;= 0 ||\t    rs-&gt;delivered &lt; 0 || rs-&gt;interval_us &lt;= 0)\t\treturn;\t//新的rtt轮次\tif (bbr-&gt;round_start) &#123;\t\tbbr-&gt;extra_acked_win_rtts = min(0x1F,\t\t\t\t\t\tbbr-&gt;extra_acked_win_rtts + 1);\t\tif (bbr-&gt;extra_acked_win_rtts &gt;= bbr_extra_acked_win_rtts) &#123; //5轮新的rtt就切换窗口槽\t\t\tbbr-&gt;extra_acked_win_rtts = 0;\t\t\tbbr-&gt;extra_acked_win_idx = bbr-&gt;extra_acked_win_idx ? //设置使用的是哪个窗口 这里是交替使用？ \t\t\t\t\t\t   0 : 1;\t\t\tbbr-&gt;extra_acked[bbr-&gt;extra_acked_win_idx] = 0;\t\t&#125;\t&#125;\t/* Compute how many packets we expected to be delivered over epoch. */\t////收报时间戳 - 统计ack聚合开始的时间戳\tepoch_us = tcp_stamp_us_delta(tp-&gt;delivered_mstamp,  \t\t\t\t      bbr-&gt;ack_epoch_mstamp);\t\t\t\texpected_acked = ((u64)bbr_bw(sk) * epoch_us) / BW_UNIT; //计算期望的包数\t/* Reset the aggregation epoch if ACK rate is below expected rate or\t * significantly large no. of ack received since epoch (potentially\t * quite old epoch).\t */\tif (bbr-&gt;ack_epoch_acked &lt;= expected_acked ||   //实际 ACK 的包数 ≤ 理论应当 ACK 的包数。\t    (bbr-&gt;ack_epoch_acked + rs-&gt;acked_sacked &gt;=   //如果自从本 epoch 开始累计收到的 ACK 数太多\t     bbr_ack_epoch_acked_reset_thresh)) &#123;\t\tbbr-&gt;ack_epoch_acked = 0;\t\t\t\t\t//清除计数\t\tbbr-&gt;ack_epoch_mstamp = tp-&gt;delivered_mstamp; //重新开始\t\texpected_acked = 0;\t&#125;\t/* Compute excess data delivered, beyond what was expected. */    //累计的ack计数\tbbr-&gt;ack_epoch_acked = min_t(u32, 0xFFFFF,\t\t\t\t     bbr-&gt;ack_epoch_acked + rs-&gt;acked_sacked); \textra_acked = bbr-&gt;ack_epoch_acked - expected_acked; \t//累计的减去期望的，是聚合的\textra_acked = min(extra_acked, tcp_snd_cwnd(tp));\t\t//钳制一下\tif (extra_acked &gt; bbr-&gt;extra_acked[bbr-&gt;extra_acked_win_idx])\t\tbbr-&gt;extra_acked[bbr-&gt;extra_acked_win_idx] = extra_acked;  ///更新聚合ack的窗口&#125;\n\n上述bbr_update_ack_aggregation中通过计算某一统计周期内实际收到的 ACK 数量与按带宽预期应收到的数量之差，得到聚合ack的数量，表示当发现 ACK 被集中返回时，BBR 会记录历史最大聚合量，并在计算cwnd时加上这部分补偿，以便在 ACK 间隔期间仍能继续发送数据，避免发送停顿和带宽利用率下降。\n计算完bw和聚合ack的数量之后，会处理稳态阶段下相位的变换，具体代码如下所示：\nstatic void bbr_update_cycle_phase(struct sock *sk,\t\t\t\t   const struct rate_sample *rs)&#123;\tstruct bbr *bbr = inet_csk_ca(sk);\t//稳态阶段，且需要进入到下一个\tif (bbr-&gt;mode == BBR_PROBE_BW &amp;&amp; bbr_is_next_cycle_phase(sk, rs))\t\tbbr_advance_cycle_phase(sk);&#125;\n\n上述代码首先判断当前bbr的状态是否时BBR_PROBE_BW，如果是调用bbr_is_next_cycle_phase怕段是否需要切换相位，具体代码如下所示：\n/* End cycle phase if it&#x27;s time and/or we hit the phase&#x27;s in-flight target. */static bool bbr_is_next_cycle_phase(struct sock *sk,\t\t\t\t    const struct rate_sample *rs)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\t//从本相位开始的时间（cycle_mstamp）到现在（delivered_mstamp）是否已超过一个 min_rtt\tbool is_full_length =\t\ttcp_stamp_us_delta(tp-&gt;delivered_mstamp, bbr-&gt;cycle_mstamp) &gt;\t\tbbr-&gt;min_rtt_us;\tu32 inflight, bw;\t/* The pacing_gain of 1.0 paces at the estimated bw to try to fully\t * use the pipe without increasing the queue.\t */\t//正常速率 ，貌似使用ltbw的时候才会设置\tif (bbr-&gt;pacing_gain == BBR_UNIT)\t\treturn is_full_length;\t\t/* just use wall clock time */\t//实际在途中的数据包\tinflight = bbr_packets_in_net_at_edt(sk, rs-&gt;prior_in_flight);\tbw = bbr_max_bw(sk); //获取bw\t/* A pacing_gain &gt; 1.0 probes for bw by trying to raise inflight to at\t * least pacing_gain*BDP; this may take more than min_rtt if min_rtt is\t * small (e.g. on a LAN). We do not persist if packets are lost, since\t * a path with small buffers may not hold that much.\t */\tif (bbr-&gt;pacing_gain &gt; BBR_UNIT)\t\t\t\t\t\t//速率增长阶段\t\treturn is_full_length &amp;&amp;\t\t\t\t\t\t\t//到切换周期了\t\t\t(rs-&gt;losses ||  /* perhaps pacing_gain*BDP won&#x27;t fit */ //出现丢包\t\t\t inflight &gt;= bbr_inflight(sk, bw, bbr-&gt;pacing_gain)); \t/* A pacing_gain &lt; 1.0 tries to drain extra queue we added if bw\t * probing didn&#x27;t find more bw. If inflight falls to match BDP then we\t * estimate queue is drained; persisting would underutilize the pipe.\t */\treturn is_full_length ||\t\t//相位时间到了，或者没到但是当前在途数据量已经回落到或低于正常BDP水平\t\tinflight &lt;= bbr_inflight(sk, bw, BBR_UNIT);&#125;\n\nbbr_is_next_cycle_phase首先判断当前相位持续的时间是否已经持续了一个最小rtt，接下来判断pacing_gain是否为1，貌似只有使用长期带宽才会设置，接下来调用bbr_packets_in_net_at_edt计算实际实际在途中数据包的数量，具体代码如下所示：\nstatic u32 bbr_packets_in_net_at_edt(struct sock *sk, u32 inflight_now)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\tu64 now_ns, edt_ns, interval_us;\tu32 interval_delivered, inflight_at_edt;\tnow_ns = tp-&gt;tcp_clock_cache; \t\t\t\t\t\t\t\t\t\t\t//缓存的时间\tedt_ns = max(tp-&gt;tcp_wstamp_ns, now_ns); \t\t\t\t\t\t\t\t//发包的时间\tinterval_us = div_u64(edt_ns - now_ns, NSEC_PER_USEC);//发包的时间减去缓存的时间\tinterval_delivered = (u64)bbr_bw(sk) * interval_us &gt;&gt; BW_SCALE;\t\t\t //数据包应该发出去的数量\tinflight_at_edt = inflight_now;\t\t\t\t\t\t\t\t\t\t\t //保存一下当前在途中的段数\tif (bbr-&gt;pacing_gain &gt; BBR_UNIT)              /* increasing inflight */ //处于探测带宽阶段\t\tinflight_at_edt += bbr_tso_segs_goal(sk);  /* include EDT skb */ //\tif (interval_delivered &gt;= inflight_at_edt) \t\t\t\t\t\t\t\t//这里类似是排空\t\treturn 0;\treturn inflight_at_edt - interval_delivered;\t\t\t\t\t\t\t//这个应该是网络中实际在飞行的包数&#125;\n\nbbr_packets_in_net_at_edt核心思想就是通过时间的差值（从之前缓存的时间到包发出时刻）与bw计算间隔时间的应该发出去的包的数量，与网络中当前在途中的数目相减计算实际在途中的数目(排除的就是在协议栈没有真正发出去的数据包吧)。\n回到bbr_is_next_cycle_phase中计算完实际在途的数据包后，获取当前的bbr的最大bw，接下来判断如果处于探测带宽阶段，且时间到且满足 出现丢包 或 在途量已达到目标（根据最小rtt计算得来）则需要切换相位。\n否则的话，如果时间到了或者在途量≤1BDP，也会切换相位。\n切换相位的代码如下所示：\nstatic void bbr_advance_cycle_phase(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\t//这里和理论不太一样，这里有可能随机的 这个更合理！\tbbr-&gt;cycle_idx = (bbr-&gt;cycle_idx + 1) &amp; (CYCLE_LEN - 1);\tbbr-&gt;cycle_mstamp = tp-&gt;delivered_mstamp; //当前 gain 周期（phase）的开始时间&#125;\n\nbbr_update_model判定是否需要完成相位变换后，会调用bbr_check_full_bw_reached判断链路是否已经被填满，通过比较当前带宽采样值与过去最大带宽 full_bw 的变化趋势：若连续 3 个 RTT 都未超过 25% 增长，则认为已经探测到带宽上线，后续会进入排空阶段。\nbbr_check_drain为上述判断是否达到上限带宽之后调用的函数，用于判断是否需要切换到排空阶段，或者在排空阶段下是否需要切换回稳态。具体代码如下所示：\n/* If pipe is probably full, drain the queue and then enter steady-state. */static void bbr_check_drain(struct sock *sk, const struct rate_sample *rs)&#123;\tstruct bbr *bbr = inet_csk_ca(sk);\t//BBR_STARTUP 且已经达到最大带宽\tif (bbr-&gt;mode == BBR_STARTUP &amp;&amp; bbr_full_bw_reached(sk)) &#123;\t\tbbr-&gt;mode = BBR_DRAIN;\t/* drain queue we created */\t\ttcp_sk(sk)-&gt;snd_ssthresh =  //设置慢启动阈值，就为最大带宽\t\t\t\tbbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT);\t&#125;\t/* fall through to check if in-flight is already small: */\tif (bbr-&gt;mode == BBR_DRAIN &amp;&amp; //网络中实际的数据包，小于正常管道的容量\t    bbr_packets_in_net_at_edt(sk, tcp_packets_in_flight(tcp_sk(sk))) &lt;=\t    bbr_inflight(sk, bbr_max_bw(sk), BBR_UNIT))\t\tbbr_reset_probe_bw_mode(sk);  /* we estimate queue is drained */ //这里设置为为了稳态&#125;\n\nbbr_check_drain() 负责检测 BBR 是否需要从 Startup 切换到 Drain 阶段，以及何时从 Drain 切换到稳态 ProbeBW。 当 BBR 判断已经到达了上限带宽，就会进入 Drain 阶段。当在Drain检测到在途数据量下降到等于或低于 1×BDP 时，说明管道已恢复到正常水位，BBR 便进入稳态。\n之后在bbr_update_model中会进一步调用bbr_update_min_rtt，更新最小rtt和决定是否进入BBR_PROBE_RTT状态，具体代码如下所示：\nstatic void bbr_update_min_rtt(struct sock *sk, const struct rate_sample *rs)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\tbool filter_expired;\t/* Track min RTT seen in the min_rtt_win_sec filter window: */\t//记录的最小rtt的时间是否已经过期了  这里阈值是10s\tfilter_expired = after(tcp_jiffies32,\t\t\t       bbr-&gt;min_rtt_stamp + bbr_min_rtt_win_sec * HZ);\tif (rs-&gt;rtt_us &gt;= 0 &amp;&amp;\t    (rs-&gt;rtt_us &lt; bbr-&gt;min_rtt_us ||\t     (filter_expired &amp;&amp; !rs-&gt;is_ack_delayed))) &#123;\t\tbbr-&gt;min_rtt_us = rs-&gt;rtt_us;  \t\t\t\t\t//更新最小rtt\t\tbbr-&gt;min_rtt_stamp = tcp_jiffies32; \t&#125;\t//进入PROBE_RTT模式\tif (bbr_probe_rtt_mode_ms &gt; 0 &amp;&amp; filter_expired &amp;&amp;\t    !bbr-&gt;idle_restart &amp;&amp; bbr-&gt;mode != BBR_PROBE_RTT) &#123;\t\tbbr-&gt;mode = BBR_PROBE_RTT;  /* dip, drain queue */ \t\t\t//设置为BBR_PROBE_RTT\t\tbbr_save_cwnd(sk);  /* note cwnd so we can restore it */ \t//保存拥塞窗口\t\tbbr-&gt;probe_rtt_done_stamp = 0;  \t\t\t\t\t\t\t//表示 PROBE_RTT模式刚开始\t&#125;\tif (bbr-&gt;mode == BBR_PROBE_RTT) &#123;\t\t/* Ignore low rate samples during this mode. */\t\t//这里直接标记为应用受限\t\ttp-&gt;app_limited = \t\t\t(tp-&gt;delivered + tcp_packets_in_flight(tp)) ? : 1;\t\t/* Maintain min packets in flight for max(200 ms, 1 round). */\t\tif (!bbr-&gt;probe_rtt_done_stamp &amp;&amp;  //刚开始probe阶段\t\t    tcp_packets_in_flight(tp) &lt;= bbr_cwnd_min_target) &#123; \t//在途数据包小于四个\t\t\tbbr-&gt;probe_rtt_done_stamp = tcp_jiffies32 +  \t\t\t//设置结束时间\t\t\t\tmsecs_to_jiffies(bbr_probe_rtt_mode_ms);\t\t\tbbr-&gt;probe_rtt_round_done = 0;   \t\t\t\t\t\t// 重置回合完成标志\t\t\tbbr-&gt;next_rtt_delivered = tp-&gt;delivered;  \t\t\t\t//用于检测rtt round\t\t&#125; else if (bbr-&gt;probe_rtt_done_stamp) &#123;\t\t\tif (bbr-&gt;round_start)  \t\t\t\t\t\t\t\t\t//新一轮的开始\t\t\t\tbbr-&gt;probe_rtt_round_done = 1; \t\t\t\t\t    //标记新一轮的结束\t\t\tif (bbr-&gt;probe_rtt_round_done)  \t\t\t\t\t\t//如果一个rtt结束了\t\t\t\tbbr_check_probe_rtt_done(sk);\t\t&#125;\t&#125;\t/* Restart after idle ends only once we process a new S/ACK for data */\tif (rs-&gt;delivered &gt; 0)\t\tbbr-&gt;idle_restart = 0;&#125;\n\nbbr_update_min_rtt首先判断最小rtt是否已经过期，如果最小rtt已经过期了，或者rrt更小，且不是延迟ack则更新最小的rtt，接下来判断如果rtt已经过期，且当前不是BBR_PROBE_RTT模式，则设置当前状态为BBR_PROBE_RTT并保存当前的cwnd\n如果设置为BBR_PROBE_RTT状态，则直接置为app_limited标记为应用受限，之后当在途量降到 4个 包后开始计时（约 200ms）并等待至少 1 个包计时 RTT，满足后通过 bbr_check_probe_rtt_done() 退出 PROBE_RTT，回到原模式，或者回到start up阶段。bbr_check_probe_rtt_done代码如下所示：\nstatic void bbr_check_probe_rtt_done(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bbr *bbr = inet_csk_ca(sk);\t//已设置结束时间戳，但是时间还诶到\tif (!(bbr-&gt;probe_rtt_done_stamp &amp;&amp;\t      after(tcp_jiffies32, bbr-&gt;probe_rtt_done_stamp)))\t\treturn;\t//重置最小RTT的更新时间戳，确保10秒内不会再次进入PROBE_RTT模式。\tbbr-&gt;min_rtt_stamp = tcp_jiffies32;  /* wait a while until PROBE_RTT */\ttcp_snd_cwnd_set(tp, max(tcp_snd_cwnd(tp), bbr-&gt;prior_cwnd)); //还原回去原来的窗口\tbbr_reset_mode(sk); //切换到稳态或者startup&#125;//rtt probe调用static void bbr_reset_mode(struct sock *sk)&#123;\tif (!bbr_full_bw_reached(sk)) //通常是true吧\t\tbbr_reset_startup_mode(sk); //不是ture 进入到BBR_STARTUP状态\telse\t\tbbr_reset_probe_bw_mode(sk); //通常走这个分支&#125;\n\n最后，会根据当前处于哪个状态来设置具体的pacing_gain和cwnd_gain，进而影响发包速率和窗口的大小。具体代码如下所示：\nstatic void bbr_update_gains(struct sock *sk)&#123;\tstruct bbr *bbr = inet_csk_ca(sk);\tswitch (bbr-&gt;mode) &#123;\tcase BBR_STARTUP:\t\tbbr-&gt;pacing_gain = bbr_high_gain; //2.885\t\tbbr-&gt;cwnd_gain\t = bbr_high_gain; //2.885\t\tbreak;\tcase BBR_DRAIN:\t\tbbr-&gt;pacing_gain = bbr_drain_gain;\t/* slow, to drain */ //上面的倒数\t\tbbr-&gt;cwnd_gain\t = bbr_high_gain;\t/* keep cwnd */\t\tbreak;\tcase BBR_PROBE_BW:\t\tbbr-&gt;pacing_gain = (bbr-&gt;lt_use_bw ?\t\t\t\t    BBR_UNIT :\t\t\t\t\t//1\t\t\t\t    bbr_pacing_gain[bbr-&gt;cycle_idx]); //8个相位的值\t\tbbr-&gt;cwnd_gain\t = bbr_cwnd_gain;\t\tbreak;\tcase BBR_PROBE_RTT:\t\tbbr-&gt;pacing_gain = BBR_UNIT; //1\t\tbbr-&gt;cwnd_gain\t = BBR_UNIT;\t\tbreak;\tdefault:\t\tWARN_ONCE(1, &quot;BBR bad mode: %u\\n&quot;, bbr-&gt;mode);\t\tbreak;\t&#125;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP拥塞控制"]},{"title":"TCP拥塞控制-BBR（理论）","url":"/2025/11/06/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6-BBR%E7%AE%97%E6%B3%95%EF%BC%88%E7%90%86%E8%AE%BA%EF%BC%89/","content":"背景大多数TCP拥塞控制算法(Congestion Control Algorithms)都是基于丢包拥塞控制，运行方式可以分为以下几个阶段：\n刚开始建立连接时进入 slow start 阶段，指数级别地成倍增加发送数据的速率,试探网络的带宽上限增长，到达一定限度 ssthresh 之后，进入线性增长 Congestion Avoidance的阶段，慢慢增加发送的速率。\n当检测到可能丢包（例如收到重复ack）时, 认为网络发生了拥塞，会大幅度减少滑动窗口的大小，超时的情况下还会重新回到slow start\n基于丢包的拥塞控制算法不足之处但是当今的网络环境，链路带宽高达数十Gbps，无法避免丢包事件的发生，吞吐量(Throughput)会随着丢包的发生反复震荡，不够稳定，同时也意味着对链路的利用率不够高。\nBBR ( Bottleneck Bandwidth and Round-trip propagation time，BBR) 应运而生，试图用一种全新的方式来控制拥塞。\nBBR算法的原理介绍要解决拥塞是如何发生的，首先要对以下两点有深入理解：\n拥塞与瓶颈在 TCP 网络传输过程中，数据从发送端经过一系列链路和路由器到达接收端。路径上的每一段链路都有自己的带宽限制，而路径中可用带宽最小的那一段链路，就被称为 瓶颈带宽（Bottleneck Bandwidth）。\n当发送方的速率超过瓶颈链路所能承载的速率时，瓶颈处的路由器缓存队列会开始积压数据包。随着积压的增多，队列延迟（Queueing Delay） 不断上升，最终缓冲区被占满后，多余的数据包只能被丢弃。 这时，TCP 协议的发送端会通过 丢包信号 或 延迟增加（RTT 变大） 等现象感知到网络拥塞，从而减小发送速率。\n因此，拥塞的本质是：\n\n网络中存在数据的积压（队列膨胀），导致延迟增加甚至丢包，吞吐量无法再提高。\n\n换句话说，网络的最大吞吐量由瓶颈带宽和传播时延（RTT）共同决定。 在理想状态下，发送端以一个稳定的速率发送数据，使得网络中的**在途数据量（in-flight data）**恰好等于：$$\\text{BDP} &#x3D; \\text{Bottleneck Bandwidth} \\times \\text{Round Trip Time}$$\n这就是 带宽时延积（BDP）。当发送窗口大小等于 BDP 时，网络利用率最高、队列最小、延迟最低。\n当一个连接满足以下两个条件时，它将运行在最高吞吐和最低延迟状态\n\n速率平衡：瓶颈链路的数据包到达速率**刚好等于瓶颈带宽 。\n管道填满：传输中的总数据（inflight）等于 &#96;BDP。\n\n因此，在链路瓶颈以及整条路径上最小化积压的唯一方式是同时满足以上两个条件：\n\nBBR通过控制数据发送的速率（pacing rate) 来保证速率平衡\n\n同时通过控制拥塞滑动窗口(cwnd)的大小来控制inflight数据量等于BDP\n\n\nBBR算法的具体实现\nBBR如何测量当前网络的Bottleneck Bandwidth和RTT？\n\n在得知以上两个参数后，如何控制发送的速率？\n\nBBR的四个不同阶段分别是什么，有什么样的作用？\n\n\n如何测量Bottleneck Bandwidth与RTT为了控制发送的速率，我们首先需要计算出当前网络的Bottleneck Bandwidth和RTT，同时由于网络环境的动态变化，这两个参数并不是固定的，而是会随着时间而变化，所以需要进行实时的采样测量。\n得益于TCP协议的Send-ACK设计，其实每一次发送方发送数据，接收方接收到数据后发送一个ACK信号给发送方，而每一次这样的交互都可以视为一次独立的数据采样那么此时我们就可以计算出两个属性：\n\n这个数据包的 RTT ： 只需要将当前的到达时间减去发送时间就得到了数据包一次往返的总时长，就得到了RTT \n\n过去一个RTT时间内的带宽 bw ：过去一个 RTT 时间内成功发送并被确认的数据量 ÷ RTT 时间\n\n\n这样以来，每次有一个ACK信号到达时，我们就可以根据上述公式，更新相应的Bandwidth和RTT数据，每一个数据包都是一个采样估计，或许偶有偏差，但是大量数据的计算就可以实现对这两个物理属性的无偏估计。\n理论上这个算法没有问题，但是实际上RTT和瓶颈带宽会不断变化，究竟如何确定哪一个才是网络的真实属性呢？BBR算法里通过这种方式来去确定：\n\n一段时间内测量到的最小RTT，就是这条路径在这一段时间内的往返传输延迟。在实际BBR算法中，选择10s作为这个窗口时间，因此一段时间内测量到的链路最大传输带宽，就是这条链路的瓶颈带宽 。\n\n控制发送的数据（如何均匀地发送数据）为了让数据包的到达速率能匹配到 瓶颈链路的离开速率, BBR 会对每个数据进行 pace （在每个 RTT 窗口内均匀发送数据）。总的来说BBR有两个控制参数：\npacing_gain（速率增益系数）\n控制数据包**发送速率（pacing rate）**的放大或收缩倍数。\n\ncwnd_gain（窗口增益系数）\n控制发送窗口大小（Congestion Window, cwnd）的放大倍数。\n\nBBR的四个不同阶段\n\n\n阶段\n名称\n主要目标\npacing_gain\ncwnd_gain\n\n\n\n1️⃣\nStartup（启动阶段）\n快速探测瓶颈带宽上限\n2.885\n2.0\n\n\n2️⃣\nDrain（排空阶段）\n清空探测阶段产生的队列积压\n1&#x2F;2.885\n2.0\n\n\n3️⃣\nProbeBW（探测带宽阶段）\n稳态运行中周期性探测更高带宽\n循环切换 {1.25, 0.75, 1, 1, 1, 1, 1, 1}\n2.0\n\n\n4️⃣\nProbeRTT（探测最小RTT阶段）\n周期性降低速率以测量最小 RTT\n0.5\n1.0\n\n\n我们已经知道了 瓶颈带宽和 RTT 是怎么来的， 但是 pacing_gain 和 cwnd_gain 又是怎么来的呢？后两个参数是BBR控制网络速率的关键，他们是BBR算法自身设定好的，或者更具体来说，BBR算法在建立TCP连接后会进入四个不同的阶段，每个阶段都有对应的 pacing_gain 和 cwnd_gain ，来实现不同阶段的目的。\nStartUp 阶段StartUp 阶段类似于CUBIC算法的slow start阶段，发生在连接刚刚建立的时候，目的是快速找到网络路径的最大可用带宽。 会指数级增长 cwnd 和 pacing rate 快速地把水管注满，探测到当前网络环境的天花板，记录下此时的 RTT 和 BW 。\n当最近采样的 BW 没有再增加（连续几次不变），BBR 认为带宽已探测到上限，进入下一阶段。\nDrain 阶段在 Startup 结束后，把上阶段“过快发送”造成的队列膨胀排掉。所以 DRAIN 阶段的目标就是把 StartUp **阶段中注入过多的inflight数据排空，**使得 inflight=BDP , 这样延迟才会降低，同时吞吐量也足够大。\nProbeBW 阶段当 BBR 结束了 Startup 和 Drain 阶段后，就进入稳定传输阶段，也就是 ProbeBW（Probe Bandwidth）阶段。 在这个阶段，BBR 认为自己已经大致探测到了网络的瓶颈带宽 (BtlBw) 和最小往返时延 (min_rtt)，接下来要做的事情不是盲目加速，而是在保持高吞吐的同时，周期性地探测网络是否有更多可用带宽。\nProbeBW 的核心目标有两个：\n\n维持稳定的发送速率，让链路始终“填满但不过载”；\n周期性地探测带宽变化，在网络条件变好时快速提升速率，在变差时平滑收敛。\n\n在 ProbeBW 阶段，BBR 不再像 Startup 那样指数级地加速，而是通过周期性调整发送速率（pacing rate） 来“轻微扰动”网络，观察 ACK 反馈中的变化。\n整个周期由 8 个 RTT 周期（rounds） 组成，每个 RTT 对应一个固定的 pacing_gain 值：\n\n\n\nRTT 周期序号\npacing_gain\n含义\n\n\n\n1\n1.25\n略微加速，尝试探测更高带宽\n\n\n2\n0.75\n放慢速度，排空队列、验证是否过载\n\n\n3~8\n1.0\n稳定传输，维持估计的 BtlBw\n\n\nBBR 会在 ProbeBW 和 ProbeRTT 之间周期性切换：\n\n默认持续时间： ProbeBW 阶段持续时间是大部分连接时间（约 98% 的时间都处于此阶段）；\n切换条件： 当超过一定时间（默认 10 秒）没有更新到新的最小 RTT 时， BBR 会暂时进入 ProbeRTT 阶段，重新测量 RTT，然后再回到 ProbeBW。\n\nProbeRTT 阶段BBR 默认每 10 秒 检查一次 min_rtt 是否过期（这个周期可以调）。 如果 min_rtt 的观测时间超过阈值，则进入 ProbeRTT 阶段，\n在 ProbeRTT 中：\n\n\n\n参数\n值\n含义\n\n\n\npacing_gain\n0.5\n降低发送速率\n\n\ncwnd_gain\n1.0\n仅保留 1×BDP 的窗口（约等于 1 个 RTT 的数据量）\n\n\n所以 ProbeRTT 是一种 健康检查机制：BBR 需要定期刷新「网络最低延迟」的基准线。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP拥塞控制"]},{"title":"TCP拥塞控制-CUBIC算法(二)","url":"/2025/11/04/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6-cubic%E6%8B%A5%E5%A1%9E%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"cubictcp_cong_avoid中如果当前不在慢启动阶段，则会调用bictcp_update计算拥塞窗口，具体代如下所示：\n */static inline void bictcp_update(struct bictcp *ca, u32 cwnd, u32 acked)&#123;\tu32 delta, bic_target, max_cnt;\tu64 offs, t;\t//ack的总数\tca-&gt;ack_cnt += acked;\t/* count the number of ACKed packets */\t//拥塞窗口没有变化，且时间太短了\tif (ca-&gt;last_cwnd == cwnd &amp;&amp;\t    (s32)(tcp_jiffies32 - ca-&gt;last_time) &lt;= HZ / 32)\t\treturn;\t/* The CUBIC function can update ca-&gt;cnt at most once per jiffy.\t * On all cwnd reduction events, ca-&gt;epoch_start is set to 0,\t * which will force a recalculation of ca-&gt;cnt.\t */\t//是否处于一个增长周期 且在同一个时间片内，意思就是每个jiffy最多计算一次Cubic\tif (ca-&gt;epoch_start &amp;&amp; tcp_jiffies32 == ca-&gt;last_time)\t\tgoto tcp_friendliness;\tca-&gt;last_cwnd = cwnd;  //记录当前拥塞窗口\tca-&gt;last_time = tcp_jiffies32; //记录当前时间戳\t//== 0表示上次发生过拥塞\tif (ca-&gt;epoch_start == 0) &#123;\t\tca-&gt;epoch_start = tcp_jiffies32;\t/* record beginning */ //记录新周期开始的时间戳\t\tca-&gt;ack_cnt = acked;\t\t\t/* start counting */ //传入的ack数目\t\tca-&gt;tcp_cwnd = cwnd;\t\t\t/* syn with cubic */ //保存当前的拥塞窗口\t\tif (ca-&gt;last_max_cwnd &lt;= cwnd) &#123;  //历史最大窗口是否小于本次的拥塞窗口\t\t\tca-&gt;bic_K = 0;       \t\t//立即开始快速增长？\t\t\tca-&gt;bic_origin_point = cwnd; //当前窗口为起点\t\t&#125; else &#123; //低于最大的拥塞窗口\t\t\t/* Compute new K based on\t\t\t * (wmax-cwnd) * (srtt&gt;&gt;3 / HZ) / c * 2^(3*bictcp_HZ)\t\t\t */\t\t\t//从当前窗口恢复到历史最大窗口所需的预估时间\t\t\tca-&gt;bic_K = cubic_root(cube_factor\t\t\t\t\t       * (ca-&gt;last_max_cwnd - cwnd));\t\t\tca-&gt;bic_origin_point = ca-&gt;last_max_cwnd;\t\t&#125;\t&#125;\t/* cubic function - calc*/\t/* calculate c * time^3 / rtt,\t *  while considering overflow in calculation of time^3\t * (so time^3 is done by using 64 bit)\t * and without the support of division of 64bit numbers\t * (so all divisions are done by using 32 bit)\t *  also NOTE the unit of those veriables\t *\t  time  = (t - K) / 2^bictcp_HZ\t *\t  c = bic_scale &gt;&gt; 10\t * rtt  = (srtt &gt;&gt; 3) / HZ\t * !!! The following code does not have overflow problems,\t * if the cwnd &lt; 1 million packets !!!\t */\t//当前增长周期开始到现在经过的时间\tt = (s32)(tcp_jiffies32 - ca-&gt;epoch_start);\t//加一个最小rtt 是干什么的\tt += usecs_to_jiffies(ca-&gt;delay_min);\t/* change the unit from HZ to bictcp_HZ */\tt &lt;&lt;= BICTCP_HZ;\tdo_div(t, HZ);\tif (t &lt; ca-&gt;bic_K)\t\t//计算/* t - K */\t\toffs = ca-&gt;bic_K - t;\telse\t\toffs = t - ca-&gt;bic_K;\t/* c/rtt * (t-K)^3 */\t//相当于delta = C × (t - K)³\tdelta = (cube_rtt_scale * offs * offs * offs) &gt;&gt; (10+3*BICTCP_HZ);  //这个是计算窗口差值\tif (t &lt; ca-&gt;bic_K)                            /* below origin*/\t\tbic_target = ca-&gt;bic_origin_point - delta; //恢复\telse                                          /* above origin*/\t\tbic_target = ca-&gt;bic_origin_point + delta; //超越\t/* cubic function - calc bictcp_cnt*/\tif (bic_target &gt; cwnd) &#123;\t\t//计算多少个ack增加拥塞窗口\t\tca-&gt;cnt = cwnd / (bic_target - cwnd);\t&#125; else &#123; //不正常的情况，极慢\t\tca-&gt;cnt = 100 * cwnd;              /* very small increment*/\t&#125;\t/*\t * The initial growth of cubic function may be too conservative\t * when the available bandwidth is still unknown.\t */\t //如果刚开始，确保增长的别太慢\tif (ca-&gt;last_max_cwnd == 0 &amp;&amp; ca-&gt;cnt &gt; 20)\t\tca-&gt;cnt = 20;\t/* increase cwnd 5% per RTT */tcp_friendliness:\t/* TCP Friendly */\tif (tcp_friendliness) &#123;\t\tu32 scale = beta_scale;\t\t\t//计算reno 多少ack加拥塞窗口\t\tdelta = (cwnd * scale) &gt;&gt; 3; //每 RTT +1 cwnd？ 这里不太懂\t\twhile (ca-&gt;ack_cnt &gt; delta) &#123;\t\t/* update tcp cwnd */\t\t\tca-&gt;ack_cnt -= delta;\t\t\tca-&gt;tcp_cwnd++;\t\t&#125;\t\t//比reno还慢，用reno的\t\tif (ca-&gt;tcp_cwnd &gt; cwnd) &#123;\t/* if bic is slower than tcp */\t\t\tdelta = ca-&gt;tcp_cwnd - cwnd;\t\t\tmax_cnt = cwnd / delta;\t\t\tif (ca-&gt;cnt &gt; max_cnt)\t\t\t\tca-&gt;cnt = max_cnt;\t\t&#125;\t&#125;\t/* The maximum rate of cwnd increase CUBIC allows is 1 packet per\t * 2 packets ACKed, meaning cwnd grows at 1.5x per RTT.\t */    //至少收到两个ack 窗口才能加1 \tca-&gt;cnt = max(ca-&gt;cnt, 2U);&#125;\n\nbictcp_update为CUBIC算法的核心，核心思想为：根据当前时间、上一次的最大窗口、现在的 cwnd，把 CUBIC 曲线算出来，得到“下一段时间 cwnd 应该以多快的速度涨”，这个“多快”就是 ca-&gt;cnt（拥塞窗口增长的速率）。\nbictcp_update中首先判断是否需要进行计算，如果距离上次计算拥塞窗口的时间很短，就直接返回。如果当前已经在一个增长周期里，且这次时间戳跟上次完全一样， 那这次就不算立方曲线部分，直接走 TCP 兼容。\n否则的话判断根据epoch_start 是否为0，判断是否需要计算新的cubic曲线，也就是发生过拥塞（或者第一次进入拥塞避免），这里分两种情况进行处理，如果当前窗口已经达到或超过历史峰值，\tK为 0，立方曲线从当前时刻开始。如果历史最大值 &gt; 当前窗口，则计算K的值， 表示预计要经过多久（时间偏移）才能恢复到上次最大窗口。\n得到了K之后，会根据t - K以及C来计算delta这个delta就是窗口的变化量(是超越还是恢复)，接下来计算多少个ack增加拥塞窗口（如果处于刚开阶段，还会修正一下确保增长不过于缓慢）。\n接下来进入tcp的友好处理逻辑：\n算法会同时维护一个虚 Reno 拥塞窗口（tcp_cwnd）来模拟传统 Reno 的线性增长过程计算出在 Reno 模型下，大约需要多少个 ACK 才能让 cwnd 增加 1。\n如果发现 CUBIC 的实际窗口 (cwnd) 比 Reno 模拟窗口 (tcp_cwnd) 小，说明 CUBIC 增长太慢，于是计算两者差距并调低 ca-&gt;cnt\n接下来回到bictcp_update中 根据上面计算出的ca-&gt;cnt调用tcp_cong_avoid_ai计算新的拥塞窗口，具体代码如下所示:\n_bpf_kfunc void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w, u32 acked)&#123;\t/* If credits accumulated at a higher w, apply them gently now. */\t//这个是收到ack数量大于一个w 表示需要加一个拥塞窗口了\tif (tp-&gt;snd_cwnd_cnt &gt;= w) &#123; \t\ttp-&gt;snd_cwnd_cnt = 0;\t\ttcp_snd_cwnd_set(tp, tcp_snd_cwnd(tp) + 1);\t&#125;\t//更新累计ack的数量\ttp-&gt;snd_cwnd_cnt += acked;\t//需要更新了\tif (tp-&gt;snd_cwnd_cnt &gt;= w) &#123;\t\tu32 delta = tp-&gt;snd_cwnd_cnt / w;\t\t//重新更新snd_cwnd_cnt\t\ttp-&gt;snd_cwnd_cnt -= delta * w;\t\ttcp_snd_cwnd_set(tp, tcp_snd_cwnd(tp) + delta);\t&#125;\t//钳制一下\ttcp_snd_cwnd_set(tp, min(tcp_snd_cwnd(tp), tp-&gt;snd_cwnd_clamp));&#125;\n\ncubictcp_state为拥塞状态改变的时候被调用的回调函数，对于CUBIC来说，只针对LOSS状态进行处理，具体代码如下所示：\n__bpf_kfunc static void cubictcp_state(struct sock *sk, u8 new_state)&#123;\tif (new_state == TCP_CA_Loss) &#123;\t\tbictcp_reset(inet_csk_ca(sk));\t\tbictcp_hystart_reset(sk);\t&#125;&#125;static inline void bictcp_reset(struct bictcp *ca)&#123;\tmemset(ca, 0, offsetof(struct bictcp, unused));\tca-&gt;found = 0;&#125;static inline void bictcp_hystart_reset(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bictcp *ca = inet_csk_ca(sk);\tca-&gt;round_start = ca-&gt;last_ack = bictcp_clock_us(sk); //复位时间戳\tca-&gt;end_seq = tp-&gt;snd_nxt; //记录当前发送的下一个序列号\tca-&gt;curr_rtt = ~0U;  // 重置 RTT 测量\tca-&gt;sample_cnt = 0;&#125;\n\ncubictcp_state用于当 TCP 进入丢包状态（Loss）时，重置 CUBIC 拥塞控制算法的内部状态（ 清除所有历史增长参数、时间点、窗口记录等信息），重置慢启动阶段探测。\ntcp_reno_undo_cwnd在拥塞窗口撤销的时候被调用，把先前保存的拥塞窗口重新设置回去，具体代码如下所示：\n__bpf_kfunc u32 tcp_reno_undo_cwnd(struct sock *sk)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\treturn max(tcp_snd_cwnd(tp), tp-&gt;prior_cwnd);&#125;\n\ncubictcp_cwnd_event在空闲状态重新开始发送数据包的时候被调用（通过inflight判断），用于修正epoch_start的值，将空闲的时间排除掉，具体代码如下所示\n__bpf_kfunc static void cubictcp_cwnd_event(struct sock *sk, enum tcp_ca_event event)&#123;\tif (event == CA_EVENT_TX_START) &#123;\t\tstruct bictcp *ca = inet_csk_ca(sk);\t\tu32 now = tcp_jiffies32;\t\ts32 delta;\t\tdelta = now - tcp_sk(sk)-&gt;lsndtime;\t\t/* We were application limited (idle) for a while.\t\t * Shift epoch_start to keep cwnd growth to cubic curve.\t\t */\t\tif (ca-&gt;epoch_start &amp;&amp; delta &gt; 0) &#123;\t\t\tca-&gt;epoch_start += delta;\t\t\tif (after(ca-&gt;epoch_start, now))\t\t\t\tca-&gt;epoch_start = now;\t\t&#125;\t\treturn;\t&#125;&#125;\n\ncubictcp_acked在清理重传队列的时候被调用，用于判断是否需要提前退出慢启动，进入拥塞避免（根据ack的行为）。具体代码如下所示\n//每次收到ACK时， RTT 采样给HyStart提供数据__bpf_kfunc static void cubictcp_acked(struct sock *sk, const struct ack_sample *sample)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\tstruct bictcp *ca = inet_csk_ca(sk);\tu32 delay;\t/* Some calls are for duplicates without timetamps */\t//直接返回\tif (sample-&gt;rtt_us &lt; 0)\t\treturn;\t/* Discard delay samples right after fast recovery */\t//快恢复一秒内不采样\tif (ca-&gt;epoch_start &amp;&amp; (s32)(tcp_jiffies32 - ca-&gt;epoch_start) &lt; HZ)\t\treturn;\t//更新当前 RTT\tdelay = sample-&gt;rtt_us;\tif (delay == 0)\t\tdelay = 1;\t/* first time call or link delay decreases */\t//记录最小rtt\tif (ca-&gt;delay_min == 0 || ca-&gt;delay_min &gt; delay)\t\tca-&gt;delay_min = delay;\t/* hystart triggers when cwnd is larger than some threshold */\t//在慢启动状态是否可以提前结束慢启动，进入拥塞避免\tif (!ca-&gt;found &amp;&amp; tcp_in_slow_start(tp) &amp;&amp; hystart &amp;&amp;\t    tcp_snd_cwnd(tp) &gt;= hystart_low_window)\t\thystart_update(sk, delay);&#125;\n\ncubictcp_acked() 用来处理当收到一个 ACK 时，首先检查该 ACK 是否有效（有 RTT 时间戳），如果是重复 ACK 或没有时间戳则直接返回。其次，如果刚刚经历过快速恢复也会跳过。随后用当前 ACK 的 RTT 更新 delay_min，记录连接最小的 RTT 值。最后，如果当前还处于慢启动阶段，且启用了 Hystart 机制并且 cwnd 已超过设定阈值 hystart_low_window（默认16），就调用 hystart_update() 判断是否可以提前结束慢启动、进入拥塞避免阶段。\nhystart_update的具体代码如下所示：\nstatic void hystart_update(struct sock *sk, u32 delay)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct bictcp *ca = inet_csk_ca(sk);\tu32 threshold;\t//una 在最后一个序号后面，更新end\tif (after(tp-&gt;snd_una, ca-&gt;end_seq))\t\tbictcp_hystart_reset(sk);\tif (hystart_detect &amp; HYSTART_ACK_TRAIN) &#123;\t\tu32 now = bictcp_clock_us(sk); //拿到当前时间\t\t/* first detection parameter - ack-train detection */\t\t//两个ack的时间的差值小于阈值的话，表示的是ack很密集？，很密集表示网络要出现拥塞了？\t\t//因为ack可能在链路中被挤积压了？？\t\tif ((s32)(now - ca-&gt;last_ack) &lt;= hystart_ack_delta_us) &#123;\t\t\tca-&gt;last_ack = now;\t\t\t//最小rtt加上 一个1ms以下的时间（qdisc, TSO ,GSO考虑进来）\t\t\tthreshold = ca-&gt;delay_min + hystart_ack_delay(sk);\t\t\t/* Hystart ack train triggers if we get ack past\t\t\t * ca-&gt;delay_min/2.\t\t\t * Pacing might have delayed packets up to RTT/2\t\t\t * during slow start.\t\t\t */\t\t\t//没有用pacing 除2\t\t\tif (sk-&gt;sk_pacing_status == SK_PACING_NONE)\t\t\t\tthreshold &gt;&gt;= 1;\t\t\t//这一轮（到end——seq之前叫做一轮） ACK 已经持续得比认为正常的时间更久\t\t\tif ((s32)(now - ca-&gt;round_start) &gt; threshold) &#123;\t\t\t\tca-&gt;found = 1;\t\t\t\tpr_debug(&quot;hystart_ack_train (%u &gt; %u) delay_min %u (+ ack_delay %u) cwnd %u\\n&quot;,\t\t\t\t\t now - ca-&gt;round_start, threshold,\t\t\t\t\t ca-&gt;delay_min, hystart_ack_delay(sk), tcp_snd_cwnd(tp));\t\t\t\tNET_INC_STATS(sock_net(sk),\t\t\t\t\t      LINUX_MIB_TCPHYSTARTTRAINDETECT);\t\t\t\tNET_ADD_STATS(sock_net(sk),\t\t\t\t\t      LINUX_MIB_TCPHYSTARTTRAINCWND,\t\t\t\t\t      tcp_snd_cwnd(tp));\t\t\t\ttp-&gt;snd_ssthresh = tcp_snd_cwnd(tp); //设置慢启动阈值\t\t\t&#125;\t\t&#125;\t&#125;\tif (hystart_detect &amp; HYSTART_DELAY) &#123;\t\t/* obtain the minimum delay of more than sampling packets */\t\tif (ca-&gt;curr_rtt &gt; delay)\t\t\tca-&gt;curr_rtt = delay; //更新一轮中rtt的最小值\t\tif (ca-&gt;sample_cnt &lt; HYSTART_MIN_SAMPLES) &#123;\t\t\tca-&gt;sample_cnt++;  //先采样几次\t\t&#125; else &#123;\t\t\t//如果这一轮中rtt明显高于历史最高rtt 直接设置慢启动阈值\t\t\tif (ca-&gt;curr_rtt &gt; ca-&gt;delay_min +\t\t\t    HYSTART_DELAY_THRESH(ca-&gt;delay_min &gt;&gt; 3)) &#123;\t\t\t\tca-&gt;found = 1;\t\t\t\tNET_INC_STATS(sock_net(sk),\t\t\t\t\t      LINUX_MIB_TCPHYSTARTDELAYDETECT);\t\t\t\tNET_ADD_STATS(sock_net(sk),\t\t\t\t\t      LINUX_MIB_TCPHYSTARTDELAYCWND,\t\t\t\t\t      tcp_snd_cwnd(tp));\t\t\t\ttp-&gt;snd_ssthresh = tcp_snd_cwnd(tp);\t\t\t&#125;\t\t&#125;\t&#125;&#125;\n\nhystart_update根据两个信息决定是否提前退出慢启动，一是如果ack非常密集（小于阈值），同时ack的时间变长了！就认为网络中可能已经开始出现排队。二是统计当前轮次最小 RTT。如果这一轮的最小 RTT 明显高于历史最小 RTT加上一个阈值（通常是最小 RTT 的 1&#x2F;8 左右），就说明链路中已经开始排队、延迟上升，同样会触发提前结束慢启动。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP拥塞控制"]},{"title":"TCP拥塞控制-拥塞控制状态机","url":"/2025/10/23/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6-%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%8A%B6%E6%80%81%E6%9C%BA/","content":"TCP 拥塞控制的核心思想是： 用 ACK 驱动拥塞状态机，根据网络反馈改变发送窗口。\n在内核中，TCP 的拥塞控制状态用枚举 enum tcp_ca_state 表示：\nenum tcp_ca_state &#123;    TCP_CA_Open = 0,      // 正常状态    TCP_CA_Disorder = 1,  // 乱序 / 可疑丢包    TCP_CA_CWR = 2,       // ECN 拥塞反馈    TCP_CA_Recovery = 3,  // 快速重传恢复中    TCP_CA_Loss = 4       // 超时丢包恢复&#125;;\n\n\n\n\n状态名\n含义\n常见触发\n\n\n\nOpen\n网络顺畅\n正常 ACK\n\n\nDisorder\n出现乱序或重复 ACK\nDUPACK&#x2F;SACK\n\n\nCWR\n网络拥塞反馈（ECN）\nECE 标志或队列丢包\n\n\nRecovery\n检测到丢包，快速恢复中\n多个 DUPACK\n\n\nLoss\n超时丢包恢复\nRTO 超时\n\n\n上述表格为常见的拥塞状态的触发机制。下图为具体状态转移图：\n\n首先分析进入各个状态后的需要处理的逻辑：\n设置拥塞状态为TCP_CA_Losstcp_enter_loss在超时重传定时器到期的时候会被调用会将状态设置为TCP_CA_Loss具体代码如下所示：\nvoid tcp_enter_loss(struct sock *sk)&#123;\tconst struct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct net *net = sock_net(sk);\tbool new_recovery = icsk-&gt;icsk_ca_state &lt; TCP_CA_Recovery; //是否在recovery状态之前\tu8 reordering;\t//基于rack 批量标记丢失数据包\ttcp_timeout_mark_lost(sk);\t/* Reduce ssthresh if it has not yet been made inside this window. */\t//保存撤销用到的值\tif (icsk-&gt;icsk_ca_state &lt;= TCP_CA_Disorder ||  //如果是乱序或者open\t    !after(tp-&gt;high_seq, tp-&gt;snd_una) ||  //上一次拥塞恢复范围（high_seq）已经被完全 ACK ， 下面会重新设置\t    (icsk-&gt;icsk_ca_state == TCP_CA_Loss &amp;&amp; !icsk-&gt;icsk_retransmits)) &#123;\t\ttp-&gt;prior_ssthresh = tcp_current_ssthresh(sk);  //慢启动阈值\t\ttp-&gt;prior_cwnd = tcp_snd_cwnd(tp);    //enterloss前保存拥塞窗口\t\ttp-&gt;snd_ssthresh = icsk-&gt;icsk_ca_ops-&gt;ssthresh(sk);\t\ttcp_ca_event(sk, CA_EVENT_LOSS); //调用拥塞算法的钩子\t\ttcp_init_undo(tp); //初始化撤销用到的信息\t&#125;\ttcp_snd_cwnd_set(tp, tcp_packets_in_flight(tp) + 1); //设置拥塞窗口，飞包数+1\ttp-&gt;snd_cwnd_cnt   = 0; //清零 cwnd 计数器\ttp-&gt;snd_cwnd_stamp = tcp_jiffies32;  //记录时间戳\t/* Timeout in disordered state after receiving substantial DUPACKs\t * suggests that the degree of reordering is over-estimated.\t */\treordering = READ_ONCE(net-&gt;ipv4.sysctl_tcp_reordering); //乱序容忍阈值\tif (icsk-&gt;icsk_ca_state &lt;= TCP_CA_Disorder &amp;&amp;\t    tp-&gt;sacked_out &gt;= reordering)    \t\t//\t\ttp-&gt;reordering = min_t(unsigned int, tp-&gt;reordering,  //缩小乱序容忍阈值,相当于容忍度太宽了，都丢包了，所以要缩小\t\t\t\t       reordering);\ttcp_set_ca_state(sk, TCP_CA_Loss); //设置为loss\ttp-&gt;high_seq = tp-&gt;snd_nxt;  //进入拥塞控制状态后下一个待发送的序列号\ttcp_ecn_queue_cwr(tp); //设置cwr标志位\t/* F-RTO RFC5682 sec 3.1 step 1: retransmit SND.UNA if no previous\t * loss recovery is underway except recurring timeout(s) on\t * the same SND.UNA (sec 3.2). Disable F-RTO on path MTU probing\t */\t//F-RTO 在 RTO 之后先只重传 SND.UNA 再观察后续 ACK 的模式来判定是否真丢包\ttp-&gt;frto = READ_ONCE(net-&gt;ipv4.sysctl_tcp_frto) &amp;&amp;  //是否开启frto\t\t   (new_recovery || icsk-&gt;icsk_retransmits) &amp;&amp;\t\t   !inet_csk(sk)-&gt;icsk_mtup.probe_size;&#125;\n\ntcp_enter_loss中首先调用tcp_timeout_mark_lost批量标记待重传的数据包（基于rack，重传队列中的数据包是否超过了平均rtt）具体代码如下所示：\n */static void tcp_timeout_mark_lost(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb, *head;\tbool is_reneg;\t\t\t/* is receiver reneging on SACKs? */\t//重传队列最老的数据包\thead = tcp_rtx_queue_head(sk);\t//被sack过，但是还在重传队列中，认为是反悔的\tis_reneg = head &amp;&amp; (TCP_SKB_CB(head)-&gt;sacked &amp; TCPCB_SACKED_ACKED);\tif (is_reneg) &#123; //处理 SACK 反悔情况\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSACKRENEGING);\t\ttp-&gt;sacked_out = 0;\t\t/* Mark SACK reneging until we recover from this loss event. */\t\t//置位 is_sack_reneg\t\ttp-&gt;is_sack_reneg = 1;  \t&#125; else if (tcp_is_reno(tp)) &#123;\t\ttcp_reset_reno_sack(tp);\t&#125;\tskb = head;\t//遍历重传队列\tskb_rbtree_walk_from(skb) &#123;\t\tif (is_reneg)//处理 SACK 反悔情况\t\t\tTCP_SKB_CB(skb)-&gt;sacked &amp;= ~TCPCB_SACKED_ACKED; //反悔了，清掉\t\telse if (tcp_is_rack(sk) &amp;&amp; skb != head &amp;&amp;  //否则如果启用了 RACK（基于时间的丢包检测\t\t\t tcp_rack_skb_timeout(tp, skb, 0) &gt; 0)  //这个数据包的时间戳是否已经超过了一个平均rtt时间\t\t\tcontinue; /* Don&#x27;t mark recently sent ones lost yet */\t\ttcp_mark_skb_lost(sk, skb); //标记丢失\t&#125;\t//重置 TCP 的重传辅助指针\ttcp_verify_left_out(tp);\ttcp_clear_all_retrans_hints(tp);&#125;\n\ntcp_timeout_mark_lost中，首先取出重传队列的第一个数据包，判断是否是sack反悔的情况，如果是，则设置标志位，下面会把所有数据包标记位loss并清除sack过的标志，否则的话根据数据包是否在队列中的超过平均rtt来标记是否丢失。\n回到tcp_enter_loss中，标记丢失的数据包之后，保存当前的拥塞窗口，慢启动阈值（为撤销使用），同时调用拥塞算法的钩子，之后更新乱序容忍的阈值，因为已经超时重传了，所以可能是乱序阈值太大了。这里可能会缩小\n之后设置状态为TCP_CA_Loss并调用拥塞算法的钩子。同时记录进入拥塞状态后下一个待发送的序列号。最后根据重传次和系统参数，以及超时重传确定是否开启frto。这里的frto会影响loss的撤销！\n设置拥塞状态为TCP_CA_Recoverytcp_enter_recovery在rack定时到期和tcp_fastretrans_alert中被调用具体代码如下所示：\nvoid tcp_enter_recovery(struct sock *sk, bool ece_ack)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tint mib_idx;\t//通常不会走这个\tif (tcp_is_reno(tp))\t\tmib_idx = LINUX_MIB_TCPRENORECOVERY;\telse\t\tmib_idx = LINUX_MIB_TCPSACKRECOVERY;\tNET_INC_STATS(sock_net(sk), mib_idx);\t//重置之前的慢启动阈值\ttp-&gt;prior_ssthresh = 0;\t//记录进入恢复时候的una和重传的包数\ttcp_init_undo(tp);\t//是否在恢复或者减少拥塞窗口的状态\tif (!tcp_in_cwnd_reduction(sk)) &#123;\t\tif (!ece_ack) //如果不是现实拥塞通知，保存慢启动阈值\t\t\ttp-&gt;prior_ssthresh = tcp_current_ssthresh(sk);\t\ttcp_init_cwnd_reduction(sk);\t&#125;\ttcp_set_ca_state(sk, TCP_CA_Recovery); //设置recovery&#125;\n\ntcp_enter_recovery的工作很简单，首先重置了之前的慢启动阈值，之后记录撤销可能用到的参数，之后判断是否是是TCPF_CA_CWR | TCPF_CA_Recovery 状态，如果不是同时不是显示拥塞的情况下，重新计算之前的慢启动阈值（大概率当前拥塞 窗口的0.75），之后调用\ntcp_init_cwnd_reduction，**tcp_init_cwnd_reduction中会调用拥塞算法的钩子，重新计算慢启动阈值，**具体代码如下所示：\n&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\ttp-&gt;high_seq = tp-&gt;snd_nxt; //标记恢复开始的位置\ttp-&gt;tlp_high_seq = 0; //TLP（尾部丢包探测）相关参数清零\ttp-&gt;snd_cwnd_cnt = 0; //这个有什么用？？累加之后影响拥塞窗口？\ttp-&gt;prior_cwnd = tcp_snd_cwnd(tp); //保存拥塞窗口\ttp-&gt;prr_delivered = 0;\ttp-&gt;prr_out = 0;\t////拥塞算法钩子重新计算慢启动阈值\ttp-&gt;snd_ssthresh = inet_csk(sk)-&gt;icsk_ca_ops-&gt;ssthresh(sk); \ttcp_ecn_queue_cwr(tp);&#125;\n\n设置拥塞状态为TCP_CA_CWR当 TCP 检测到发生 ECN 拥塞或本地主机丢包（比如qdisc）时会调用tcp_enter_cwr\nvoid tcp_enter_cwr(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\ttp-&gt;prior_ssthresh = 0;\tif (inet_csk(sk)-&gt;icsk_ca_state &lt; TCP_CA_CWR) &#123;\t\ttp-&gt;undo_marker = 0;\t\ttcp_init_cwnd_reduction(sk);//初始化拥塞窗口减小\t\ttcp_set_ca_state(sk, TCP_CA_CWR); //设置cwr状态\t&#125;&#125;\n\ntcp_enter_cwr中将待设置为TCP_CA_CWR因为是显示拥塞，所以是不会撤销的！\n设置拥塞状态为TCP_CA_DisorderTLP ACK的确认以及tcp_fastretrans_alert中会调用tcp_try_keep_open来设置TCP_CA_Disorder状态，具体代码如下所示：\nstatic void tcp_try_keep_open(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tint state = TCP_CA_Open;\t//计算sack和丢包的数量  或者是否正在重传的包或者重传队列中有数据包\tif (tcp_left_out(tp) || tcp_any_retrans_done(sk))\t\tstate = TCP_CA_Disorder;  //设置为乱需状态\tif (inet_csk(sk)-&gt;icsk_ca_state != state) &#123;  //如果不是open状态\t\ttcp_set_ca_state(sk, state); //设置乱需或者open状态\t\ttp-&gt;high_seq = tp-&gt;snd_nxt;  //保存进入拥塞状态的的序号\t&#125;&#125;\n\n这里的tcp_try_keep_open可以理解为一个健康检查，当存在sack或者存在可能丢失的数据包（sack和dupack导致的？），或者重传队列的最早的数据包以及重传过则设置为TCP_CA_Disorder状态。\n设置拥塞状态为TCP_CA_OpenTCP_CA_Open在多个地方会设置，这里不粘贴代码了，当从各种异常状态撤销时，会转变为open状态，拥塞算法刚初始化时，也会设置为open状态。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP拥塞控制"]},{"title":"TCP拥塞控制-拥塞状态转换","url":"/2025/10/28/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6-%E6%8B%A5%E5%A1%9E%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2/","content":"TCP的收包路径会调用tcp_ack()对携带ack的数据包进行处理，如果是一个“可疑”的ack（例如重复ack，sack，dsack纯ack等）会调用tcp_fastretrans_alert进行拥塞状态的处理，具体代码如下所示：\nstatic int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)&#123;...\t//当前ack是否可疑（\t没有确认数据，窗口更新 纯ack sack 或者dsack ）\tif (tcp_ack_is_dubious(sk, flag)) &#123;\t\t//是否是一个纯粹的重复ack（没有确认新数据）\t\tif (!(flag &amp; (FLAG_SND_UNA_ADVANCED | \t\t\t      FLAG_NOT_DUP | FLAG_DSACKING_ACK))) &#123;\t\t\tnum_dupack = 1;\t\t\t/* Consider if pure acks were aggregated in tcp_add_backlog() */\t\t\t//统计纯ack的计数，注意这里协议站可能会聚合纯ack\t\t\tif (!(flag &amp; FLAG_DATA)) \t\t\t\tnum_dupack = max_t(u16, 1, skb_shinfo(skb)-&gt;gso_segs);\t\t&#125;\t\t//传入的是snd_una, 重复ack的数量，ack的标志位 ，传入传出rexmit会指导下面的重传\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &amp;flag,\t\t\t\t      &amp;rexmit);\t&#125;...&#125;\n\n上述代码首先调用tcp_ack_is_dubious判断ack是否可疑，传入的flag为修饰ack 等信息，快速路径，慢速路径，清理重传队列等，均会设置flag影响tcp_fastretrans_alert的处理。tcp_ack_is_dubious的实现具体代码如下所示:\nstatic inline bool tcp_ack_is_dubious(const struct sock *sk, const int flag)&#123;\t//没有确认数据，窗口更新 ，是纯ack            //是sack 或者dsack \treturn !(flag &amp; FLAG_NOT_DUP) || (flag &amp; FLAG_CA_ALERT) ||\t\tinet_csk(sk)-&gt;icsk_ca_state != TCP_CA_Open; //不是open状态&#125;\n\n如果tcp_ack_is_dubious中返回true则会根据是否是一个纯粹的重复ack（没有确认新数据）进一步统计重复ack的数目，这里需要注意，传入的重复的ack的数目如果是基于传统的拥塞控制算法（依靠重复ack，取决于是否开启sack）才会起作用，否则传入的这个参数是不会影响tcp_fastretrans_alert内部行为的。\ntcp_fastretrans_alert是处理拥塞状态的转换的关键，包括进入TCP_CA_Recovery，TCP_CA_CWR，TCP_CA_Disorder状态，或者从当前拥塞状态进行撤销，标记数据包丢失，调用拥塞算法的钩子等，tcp_fastretrans_alert具体代码如下所示：\nstatic void tcp_fastretrans_alert(struct sock *sk, const u32 prior_snd_una,\t\t\t\t  int num_dupack, int *ack_flag, int *rexmit)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tint fast_rexmit = 0, flag = *ack_flag;\tbool ece_ack = flag &amp; FLAG_ECE;\t//收到重复ack 或者有sack且乱续超过阈值，  这个值如果开启了rack好像就没什么用了?\tbool do_lost = num_dupack || ((flag &amp; FLAG_DATA_SACKED) &amp;&amp;\t\t\t\t      tcp_force_fast_retransmit(sk));\t//错误处理， 发出去没有被ack的为0 且sack确认的不为0\tif (!tp-&gt;packets_out &amp;&amp; tp-&gt;sacked_out)\t\ttp-&gt;sacked_out = 0;\t/* Now state machine starts.\t * A. ECE, hence prohibit cwnd undoing, the reduction is required. */\t //显示拥塞的情况，那就禁止撤销了\tif (ece_ack)\t\ttp-&gt;prior_ssthresh = 0;\t/* B. In all the states check for reneging SACKs. */\t//如果存在sack撤销且推进了una，则重新设置超时定时器，给发送放机会\tif (tcp_check_sack_reneging(sk, ack_flag))\t\treturn;\t/* C. Check consistency of the current state. */\ttcp_verify_left_out(tp);\t/* D. Check state exit conditions. State can be terminated\t *    when high_seq is ACKed. */\tif (icsk-&gt;icsk_ca_state == TCP_CA_Open) &#123;\t\tWARN_ON(tp-&gt;retrans_out != 0 &amp;&amp; !tp-&gt;syn_data);\t\ttp-&gt;retrans_stamp = 0;  //因为是open所以设置为0 合理\t//不是open的情况\t&#125; else if (!before(tp-&gt;snd_una, tp-&gt;high_seq)) &#123; //una已经推进到了high_seq的后面，那肯定尝试撤销了\t\tswitch (icsk-&gt;icsk_ca_state) &#123;\t\tcase TCP_CA_CWR:\t\t\t/* CWR is to be held something *above* high_seq\t\t\t * is ACKed for CWR bit to reach receiver. */\t\t\tif (tp-&gt;snd_una != tp-&gt;high_seq) &#123; //确认指针超过了进入CWR时的序列号\t\t\t\t//传统TCP算法中，这个函数会将拥塞窗口重置到慢启动阈值，bbr例外\t\t\t\ttcp_end_cwnd_reduction(sk); \t\t\t\ttcp_set_ca_state(sk, TCP_CA_Open); //TCP_CA_CWR -&gt;OPEN\t\t\t&#125;\t\t\tbreak;\t\tcase TCP_CA_Recovery:\t\t\tif (tcp_is_reno(tp))\t\t\t\ttcp_reset_reno_sack(tp);\t\t\t//只要不是reno就返回false,如果满足撤销的标记（有撤销的标记，没有重传过数据包？且当前确认的数据包的时间戳比重传的数据包要早）\t\t\t//会更新慢启动阈值和拥塞窗口\t\t\tif (tcp_try_undo_recovery(sk))\t\t\t\treturn;\t\t\t//设置拥塞窗口大小为慢启动阈值\t\t\ttcp_end_cwnd_reduction(sk);\t\t\tbreak;\t\t&#125;\t&#125;\t/* E. Process state. */\tswitch (icsk-&gt;icsk_ca_state) &#123;\tcase TCP_CA_Recovery:\t\t//只是重复的ack 或者sack ？，如果是reno算法，相当于什么也不做，继续走\t\tif (!(flag &amp; FLAG_SND_UNA_ADVANCED)) &#123;\t\t\tif (tcp_is_reno(tp))\t\t\t\ttcp_add_reno_sack(sk, num_dupack, ece_ack);\t\t\t//走到这里表示确认了部分数据，那就要尝试是否可以撤销了不能撤销返回ture ！\t\t\t//这里返回fasle的话有可能变成open或者disorder 或者仍然是recovery\t\t&#125; else if (tcp_try_undo_partial(sk, prior_snd_una, &amp;do_lost)) \t\t\treturn;\t\t//收到部分数据包尝试是否可以撤销（重新设置慢启动阈值和拥塞窗口），如果返回ture，下面会变成open或者乱续状态\t\t//不太理解为什么叫基于dsack？\t\tif (tcp_try_undo_dsack(sk))\t\t//变成open或者乱续状态\t\t\ttcp_try_keep_open(sk);\t\t//通过rack标记丢包\t\ttcp_identify_packet_loss(sk, ack_flag);\t\tif (icsk-&gt;icsk_ca_state != TCP_CA_Recovery) &#123;\t\t\t//如果有被标记丢失的数据包就会进入这个分支，比如上面撤销会清楚标记，rack检测可能重新打上标记\t\t\tif (!tcp_time_to_recover(sk, flag))//是否有被mark为loss的数据包\t\t\t\treturn;\t\t\t/* Undo reverts the recovery state. If loss is evident,\t\t\t * starts a new recovery (e.g. reordering then loss);\t\t\t */\t\t\t//进入recovery状态，调用拥塞算法钩子计算慢启动阈值\t\t\ttcp_enter_recovery(sk, ece_ack);\t\t&#125;\t\tbreak;\tcase TCP_CA_Loss:\t\t//loss状态的处理，如果una被推进，可能恢复到open或者走frto流程\t\ttcp_process_loss(sk, flag, num_dupack, rexmit);\t\ttcp_identify_packet_loss(sk, ack_flag); //rack处理\t\tif (!(icsk-&gt;icsk_ca_state == TCP_CA_Open ||  //如果不是open状态 就直接return了\t\t      (*ack_flag &amp; FLAG_LOST_RETRANS))) //rack逻辑会设置这个标记\t\t\treturn;\t\t/* Change state if cwnd is undone or retransmits are lost */\t\tfallthrough;\tdefault:\t\tif (tcp_is_reno(tp)) &#123;\t\t\tif (flag &amp; FLAG_SND_UNA_ADVANCED)\t\t\t\ttcp_reset_reno_sack(tp);\t\t\ttcp_add_reno_sack(sk, num_dupack, ece_ack);\t\t&#125;\t\t//如果是open或者乱序状态\t\tif (icsk-&gt;icsk_ca_state &lt;= TCP_CA_Disorder)\t\t\ttcp_try_undo_dsack(sk);\t\t//基于rack标记是否丢失\t\ttcp_identify_packet_loss(sk, ack_flag);\t\tif (!tcp_time_to_recover(sk, flag)) &#123; //是否需要进入recover状态！上面rack没有标记就会尝试回到open\t\t\t//如果这里没有返走，那就一定进入恢复状态了\t\t\ttcp_try_to_open(sk, flag);\t\t\treturn;\t\t&#125;\t\t/* MTU probe failure: don&#x27;t reduce cwnd */\t\tif (icsk-&gt;icsk_ca_state &lt; TCP_CA_CWR &amp;&amp;   //open或者乱续的情况\t\t    icsk-&gt;icsk_mtup.probe_size &amp;&amp;\t\t\t//正在进行mtup\t\t    tp-&gt;snd_una == tp-&gt;mtu_probe.probe_seq_start) &#123;  //探测的数据包还没有被u确认\t\t\ttcp_mtup_probe_failed(sk);   //记录MTU探测失败\t\t\t/* Restores the reduction we did in tcp_mtup_probe() */\t\t\ttcp_snd_cwnd_set(tp, tcp_snd_cwnd(tp) + 1);  //恢复一个探测窗口，合理\t\t\ttcp_simple_retransmit(sk); //基于mss，标记丢失，并重传 ，直接返回\t\t\treturn;\t\t&#125;\t\t/* Otherwise enter Recovery state */\t\t//如果不是TCP_CA_Recovery或者 TCP_CA_Loss 会进入\t\ttcp_enter_recovery(sk, ece_ack);\t\tfast_rexmit = 1;\t&#125;\tif (!tcp_is_rack(sk) &amp;&amp; do_lost)\t\ttcp_update_scoreboard(sk, fast_rexmit);\t*rexmit = REXMIT_LOST;&#125;\n\ntcp_fastretrans_alert中，首先计算do_lost的值，该值取决于传入参数重复ack的数量，以及tcp_force_fast_retransmit（判断当已被 SACK 确认的最高序号是否超出当前一定乱序容忍度），注意，如果开启了sack这里的``do_lost相当于没有使用到。\n接下来调用tcp_check_sack_reneging判断接收端是否出现sack返回的情况，具体代码如下所示：\nstatic bool tcp_check_sack_reneging(struct sock *sk, int *ack_flag)&#123;\t//存在sack撤销，且发送窗口前进了\tif (*ack_flag &amp; FLAG_SACK_RENEGING &amp;&amp;\t    *ack_flag &amp; FLAG_SND_UNA_ADVANCED) &#123;\t\tstruct tcp_sock *tp = tcp_sk(sk);\t\tunsigned long delay = max(usecs_to_jiffies(tp-&gt;srtt_us &gt;&gt; 4),\t\t\t\t\t  msecs_to_jiffies(10));\t\t//重新设置重传定时器\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\t\t\t\t\t  delay, TCP_RTO_MAX);\t\t*ack_flag &amp;= ~FLAG_SET_XMIT_TIMER;\t\treturn true;\t&#125;\treturn false;&#125;\n\n如果存在sack撤销，且发送窗口前进了则会重新设置重传定时器的超时时间 为(max(RTT&#x2F;2, 10ms) 并返回true外层也会直接返回，目的是给一点机会，不立即改变状态或者重传吧，之后处理TCP_CA_Open状态，清空了重传的时间戳，如果不是不处于open状态，则会判断当前的una是否已经在high_seq的后面了，表示进入拥塞控制状态的序列号已经被确认了，如果此时在TCP_CA_CWR状态则会直接撤销，并将拥塞窗口设置为慢启动阈值，如果处于TCP_CA_Recovery则会调用tcp_try_undo_recovery尝试撤销。\n之后进入下一个状态的处理中，如果上述处理后，仍处于TCP_CA_Recovery状态，则会判断当前的ack是否推进了部分窗口，如果没有，则什么也不做，否则会调用tcp_try_undo_partial尝试部分撤销的动作，如果返回true表示正在有数据包进行传输，因此不能撤销，这里就直接返回了，否则进一步调用tcp_try_undo_dsack判断是否可以清除重传队列中的丢包标志（取决于是否没有重传果数据包），如果成功撤销则进一步调用tcp_try_keep_open看看是否可以设置为open或者disorder状态。\n之后调用tcp_identify_packet_loss通过rack机制标记丢失的数据包，具体代码如下所示：\nstatic void tcp_identify_packet_loss(struct sock *sk, int *ack_flag)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tif (tcp_rtx_queue_empty(sk))\t\treturn;\tif (unlikely(tcp_is_reno(tp))) &#123;\t\ttcp_newreno_mark_lost(sk, *ack_flag &amp; FLAG_SND_UNA_ADVANCED);\t&#125; else if (tcp_is_rack(sk)) &#123;\t\tu32 prior_retrans = tp-&gt;retrans_out;\t\t//返回true表示通过rack标记了丢包\t\tif (tcp_rack_mark_lost(sk)) //注意这里会减少retrans_out\t\t//表示RACK已经处理了定时器逻辑，不需要再设置\t\t\t*ack_flag &amp;= ~FLAG_SET_XMIT_TIMER;\t\tif (prior_retrans &gt; tp-&gt;retrans_out)\t\t\t*ack_flag |= FLAG_LOST_RETRANS; //表示有重传包丢失\t&#125;&#125;\n\ntcp_identify_packet_loss主要就是调用tcp_rack_mark_lost基于时间戳来标记数据包是否丢失，这个函数在rack定时器中分析过，如果数据包在重传队列中超过rtt左右的时间（注意：会根据乱序阈值调制），就会被标记为丢失。\n之后在调用tcp_identify_packet_loss完成后的下一个函数tcp_time_to_recover会判断是否有被标记丢失的数据包，如果有则直接进入\ntcp_enter_recovery状态，否则直接返回了。\n如果当前状态为TCP_CA_Loss则会调用tcp_process_loss进一步处理，注意这里是有可能将状态恢复到open具体代码如下所示：\nstatic void tcp_process_loss(struct sock *sk, int flag, int num_dupack,\t\t\t     int *rexmit)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//recovered = true表示确认指针超过了进入Loss时的序列号\tbool recovered = !before(tp-&gt;snd_una, tp-&gt;high_seq);\t//如果更新了una，尝试是否可以撤销loss，如果撤销这里直接就返回了\tif ((flag &amp; FLAG_SND_UNA_ADVANCED || rcu_access_pointer(tp-&gt;fastopen_rsk)) &amp;&amp;\t    tcp_try_undo_loss(sk, false))\t\treturn;\t//tcp_enter_loss 会设置该字段\tif (tp-&gt;frto) &#123; /* F-RTO RFC5682 sec 3.1 (sack enhanced version). */\t\t/* Step 3.b. A timeout is spurious if not all data are\t\t * lost, i.e., never-retransmitted data are (s)acked.\t\t */\t\t//收到了从未重传过的原始数据的SACK确认，比如345但是3丢了？ 这里直接会变成open然后返回\t\tif ((flag &amp; FLAG_ORIG_SACK_ACKED) &amp;&amp;\t\t    tcp_try_undo_loss(sk, true))\t\t\treturn;\t\t//已经发送了新数据\t\tif (after(tp-&gt;snd_nxt, tp-&gt;high_seq)) &#123; //应该是第二次才会进入这个分支？ 发现是真丢包，关闭了frto\t\t\t// 确认是真实丢包，关闭F-RTO\t\t\tif (flag &amp; FLAG_DATA_SACKED || num_dupack)\t\t\t\ttp-&gt;frto = 0; /* Step 3.a. loss was real */  \t\t//确认了新数据，但是还没有完全恢复\t\t&#125; else if (flag &amp; FLAG_SND_UNA_ADVANCED &amp;&amp; !recovered) &#123; \t\t\ttp-&gt;high_seq = tp-&gt;snd_nxt; //将恢复序列号更新为当前发送序列号\t\t\t/* Step 2.b. Try send new data (but deferred until cwnd\t\t\t * is updated in tcp_ack()). Otherwise fall back to\t\t\t * the conventional recovery.\t\t\t */\t\t\t//有数据可发 &amp;&amp; 有窗口空间\t\t\tif (!tcp_write_queue_empty(sk) &amp;&amp;\t\t\t    after(tcp_wnd_end(tp), tp-&gt;snd_nxt)) &#123;\t\t\t\t*rexmit = REXMIT_NEW; //这个应该是标识发送新数据，貌似是frto的核心?\t\t\t\treturn;\t\t\t&#125;\t\t\ttp-&gt;frto = 0;\t\t&#125;\t&#125;\tif (recovered) &#123;\t\t/* F-RTO RFC5682 sec 3.1 step 2.a and 1st part of step 3.a */\t//尝试恢复到open\t\ttcp_try_undo_recovery(sk);\t\treturn;\t&#125;\tif (tcp_is_reno(tp)) &#123;\t\t/* A Reno DUPACK means new data in F-RTO step 2.b above are\t\t * delivered. Lower inflight to clock out (re)transmissions.\t\t */\t\tif (after(tp-&gt;snd_nxt, tp-&gt;high_seq) &amp;&amp; num_dupack)\t\t\ttcp_add_reno_sack(sk, num_dupack, flag &amp; FLAG_ECE);\t\telse if (flag &amp; FLAG_SND_UNA_ADVANCED)\t\t\ttcp_reset_reno_sack(tp);\t&#125;\t*rexmit = REXMIT_LOST;&#125;\n\ntcp_process_loss中首先判断当前的una是否已经在high_seq的后面了，如果满足条件，后面会尝试恢复到open状态。\n之后判断ack是否推进了una且撤销成功，如果成功了则这里直接变成open状态直接返回了。\n否则根据是否开启了tp-&gt;frto进一步判断，这里的tp-&gt;frto是在tcp_enter_loss中设置的，大概率会设置为true但是在下面的处理逻辑中可能会置为false。frto的处理逻辑中，首先判断是否在loss状态中收到了没有重传过的原始数据包，如果满足，这直接尝试撤销并返回（如果成功），否则进一步判断是否已经发送了新数据但是还是收到了sack（表示真丢包了）这里关闭了frto，如果确认了新数据，但是还没有完全恢复同时有有数据可发 &amp;&amp; 有窗口空间，这里会标识外层发送新数据包。\n这里可以看到frto&#96;的核心思想是进入loss状态下根据条件也会尝试发送新数据包。\n回到tcp_fastretrans_alert中，调用完tcp_process_loss后会进一步调用tcp_identify_packet_loss基于rack标记是否丢失数据包。因为上述tcp_process_loss可能会转换为open状态，如果这里状态没有发生转换或者rack又标记了丢包就直接返回了。也就是说open之后没有被重新标记丢包就直接返回了。\n最后是default处理流程，如果不处于TCP_CA_Recovery或者TCP_CA_Loss状态，会调用tcp_identify_packet_loss标记数据包是否丢失，如果没有标记丢失则会尝试保存open状态，否则会进入TCP_CA_Recovery状态。\n这里也考虑了mtup探测失败的情况，即探测数据包的序列号没有被确认，此时会增加一个拥塞窗口调用tcp_simple_retransmit基于mss把数据包标记为丢失并立即重传，具体代码如下所示：\nvoid tcp_simple_retransmit(struct sock *sk)&#123;\tconst struct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb;\tint mss;\t/* A fastopen SYN request is stored as two separate packets within\t * the retransmit queue, this is done by tcp_send_syn_data().\t * As a result simply checking the MSS of the frames in the queue\t * will not work for the SYN packet.\t *\t * Us being here is an indication of a path MTU issue so we can\t * assume that the fastopen SYN was lost and just mark all the\t * frames in the retransmit queue as lost. We will use an MSS of\t * -1 to mark all frames as lost, otherwise compute the current MSS.\t */\t//因为是可能路径探测导致的丢包，因此肯定是先计算一个mss\tif (tp-&gt;syn_data &amp;&amp; sk-&gt;sk_state == TCP_SYN_SENT)\t\tmss = -1;\telse\t\tmss = tcp_current_mss(sk);\t//遍历重传队列，把超过mss的包标记为丢失\tskb_rbtree_walk(skb, &amp;sk-&gt;tcp_rtx_queue) &#123;\t\tif (tcp_skb_seglen(skb) &gt; mss)\t\t\ttcp_mark_skb_lost(sk, skb);\t&#125;\t//清空重传用到的辅助字段\ttcp_clear_retrans_hints_partial(tp);\t//当前待重传的丢包数量\tif (!tp-&gt;lost_out)\t\treturn;\tif (tcp_is_reno(tp))\t\ttcp_limit_reno_sacked(tp);\ttcp_verify_left_out(tp);\t/* Don&#x27;t muck with the congestion window here.\t * Reason is that we do not increase amount of _data_\t * in network, but units changed and effective\t * cwnd/ssthresh really reduced now.\t */\t//设置 loss状态，注意这里没有缩小拥塞窗口\tif (icsk-&gt;icsk_ca_state != TCP_CA_Loss) &#123;\t\ttp-&gt;high_seq = tp-&gt;snd_nxt;\t\ttp-&gt;snd_ssthresh = tcp_current_ssthresh(sk);\t\ttp-&gt;prior_ssthresh = 0;\t\ttp-&gt;undo_marker = 0;\t\ttcp_set_ca_state(sk, TCP_CA_Loss);\t&#125;\t//重传数据包\ttcp_xmit_retransmit_queue(sk);&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP拥塞控制"]},{"title":"TCP确认 tcp_clean_rtx_queue","url":"/2025/12/29/TCP%E6%B8%85%E7%90%86%E9%87%8D%E4%BC%A0%E9%98%9F%E5%88%97/","content":"tcp_ack中处理完数据包的ack信息后，会调用tcp_clean_rtx_queue把已经确认的数据从重传队列中移除，并维护一系列的状态，整体工作可以总结为以下四点：\n\n清理：把确认了的 skb（或 skb 的一部分）从 tcp_rtx_queue 移除 \n计数修正：更新 packets_out / sacked_out / lost_out / retrans_out 等 in-flight 相关计数。\n交付与测量：更新 delivered 统计，为 BBR&#x2F;RACK&#x2F;RTT 采样提供输入。\n输出 flag：告诉上层 这次 ACK 发生了什么（是否需要重置重传计时器？是否出现 SACK 反悔等）。\n\ntcp_clean_rtx_queue具体代码如下所示：\nstatic int tcp_clean_rtx_queue(struct sock *sk, const struct sk_buff *ack_skb,\t\t\t       u32 prior_fack, u32 prior_snd_una,\t\t\t       struct tcp_sacktag_state *sack, bool ece_ack)&#123;\tconst struct inet_connection_sock *icsk = inet_csk(sk);\tu64 first_ackt, last_ackt;\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 prior_sacked = tp-&gt;sacked_out;\tu32 reord = tp-&gt;snd_nxt; /* lowest acked un-retx un-sacked seq */\tstruct sk_buff *skb, *next;\tbool fully_acked = true;\tlong sack_rtt_us = -1L;\tlong seq_rtt_us = -1L;\tlong ca_rtt_us = -1L;\tu32 pkts_acked = 0;\tbool rtt_update;\tint flag = 0;\tfirst_ackt = 0;\t//遍历重传队列\tfor (skb = skb_rb_first(&amp;sk-&gt;tcp_rtx_queue); skb; skb = next) &#123;\t\tstruct tcp_skb_cb *scb = TCP_SKB_CB(skb);\t\tconst u32 start_seq = scb-&gt;seq;\t\tu8 sacked = scb-&gt;sacked;\t\tu32 acked_pcount;\t\t/* Determine how many packets and what bytes were acked, tso and else */\t\t//结束序列号大于una 可能是部分确认\t\tif (after(scb-&gt;end_seq, tp-&gt;snd_una)) &#123;\t\t\t//只有一个段，或者una在seq前面则直接break\t\t\tif (tcp_skb_pcount(skb) == 1 ||\t\t\t    !after(tp-&gt;snd_una, scb-&gt;seq))\t\t\t\tbreak;\t\t\t//部分确认的情况，直接trim数据包 ,返回0表示无法trim直接break\t\t\tacked_pcount = tcp_tso_acked(sk, skb);\t\t\tif (!acked_pcount)\t\t\t\tbreak;\t\t\tfully_acked = false;\t\t&#125; else &#123; //整个包都被确认过，直接计算ack计数\t\t\tacked_pcount = tcp_skb_pcount(skb);\t\t&#125;\t\t//这个数据包被重传过\t\tif (unlikely(sacked &amp; TCPCB_RETRANS)) &#123;\t\t\tif (sacked &amp; TCPCB_SACKED_RETRANS)\t//也是表示重传过，有这个表示明确加过下面的计数\t\t\t\ttp-&gt;retrans_out -= acked_pcount;//更新重传出去的包数 \t\t\tflag |= FLAG_RETRANS_DATA_ACKED;\t\t&#125; else if (!(sacked &amp; TCPCB_SACKED_ACKED)) &#123;//没被重传过，也没有被sack确认过\t\t\tlast_ackt = tcp_skb_timestamp_us(skb); //获取数据包的发送时间戳\t\t\tWARN_ON_ONCE(last_ackt == 0);\t\t\tif (!first_ackt)\t\t\t//记录本轮清理的第一个被确认的skb\t\t\t\tfirst_ackt = last_ackt;\t\t\t//记录本次被确认的原始数据里，最靠前的起始 seq （\t\t\t// 后面如果发现 reord 比之前的 prior_fack 还靠前，通常用来推断 发生了乱序）\t\t\tif (before(start_seq, reord))\t\t\t\treord = start_seq;\t\t\t//其实就是确认了拥塞发送时候之前的数据，表示确认了原始的数据？\t\t\tif (!after(scb-&gt;end_seq, tp-&gt;high_seq))\t\t\t\tflag |= FLAG_ORIG_SACK_ACKED;\t\t&#125;\t\t//被sack确认过\t\tif (sacked &amp; TCPCB_SACKED_ACKED) &#123;\t\t\ttp-&gt;sacked_out -= acked_pcount;\t\t//更新交付的数量，同时如果不是虚假重传的话，\t\t&#125; else if (tcp_is_sack(tp)) &#123;\t\t\ttcp_count_delivered(tp, acked_pcount, ece_ack);\t\t\tif (!tcp_skb_spurious_retrans(tp, skb))\t\t\t\t//决定是否启用rack，最后一个参数表示发送出去的时间戳\t\t\t\ttcp_rack_advance(tp, sacked, scb-&gt;end_seq,\t\t\t\t\t\t tcp_skb_timestamp_us(skb));\t\t&#125;\t\t//之前认为丢失但是现在确认了，修正待重传的丢包数量\t\tif (sacked &amp; TCPCB_LOST)\t\t\ttp-&gt;lost_out -= acked_pcount;\t\t//更新发送出去还没有接收到ack的数量\t\ttp-&gt;packets_out -= acked_pcount;\t\t//累计确认了多少个段\t\tpkts_acked += acked_pcount;\t\t//bbr算法使用的字段\t\ttcp_rate_skb_delivered(sk, skb, sack-&gt;rate);\t\t/* Initial outgoing SYN&#x27;s get put onto the write_queue\t\t * just like anything else we transmit.  It is not\t\t * true data, and if we misinform our callers that\t\t * this ACK acks real data, we will erroneously exit\t\t * connection startup slow start one packet too\t\t * quickly.  This is severely frowned upon behavior.\t\t */\t\t//不是syn包\t\tif (likely(!(scb-&gt;tcp_flags &amp; TCPHDR_SYN))) &#123;\t\t\tflag |= FLAG_DATA_ACKED;\t\t&#125; else &#123;\t\t\tflag |= FLAG_SYN_ACKED;\t\t\ttp-&gt;retrans_stamp = 0;\t\t&#125;\t\t//注意如果不是整个包确认这里直接break\t\tif (!fully_acked)\t\t\tbreak;\t\t//用户态是否需要拿到整个数据包的时间戳？\t\ttcp_ack_tstamp(sk, skb, ack_skb, prior_snd_una);\t\tnext = skb_rb_next(skb);\t\tif (unlikely(skb == tp-&gt;retransmit_skb_hint))\t\t\ttp-&gt;retransmit_skb_hint = NULL;\t\tif (unlikely(skb == tp-&gt;lost_skb_hint))\t\t\ttp-&gt;lost_skb_hint = NULL;\t\ttcp_highest_sack_replace(sk, skb, next);\t\t//从重传队列中移除\t\ttcp_rtx_queue_unlink_and_free(skb, sk);\t&#125;\tif (!skb)\t\t//状态的计时器\t\ttcp_chrono_stop(sk, TCP_CHRONO_BUSY);\t//更新紧急指针发送边界，没什么用\tif (likely(between(tp-&gt;snd_up, prior_snd_una, tp-&gt;snd_una)))\t\ttp-&gt;snd_up = tp-&gt;snd_una;\tif (skb) &#123;\t\ttcp_ack_tstamp(sk, skb, ack_skb, prior_snd_una);\t\t//这里处理sack 反悔的情况，没太理解\t\tif (TCP_SKB_CB(skb)-&gt;sacked &amp; TCPCB_SACKED_ACKED)\t\t\tflag |= FLAG_SACK_RENEGING;\t&#125;\t//确认了原始发送的数据，并且没有重传过的数据\tif (likely(first_ackt) &amp;&amp; !(flag &amp; FLAG_RETRANS_DATA_ACKED)) &#123;\t\t//这里是获取两个计算rtt的样本，分别用当前的时间减去，第一个被确认的skb和最后一个被确认的skb\t\tseq_rtt_us = tcp_stamp_us_delta(tp-&gt;tcp_mstamp, first_ackt);\t\tca_rtt_us = tcp_stamp_us_delta(tp-&gt;tcp_mstamp, last_ackt);\t\tif (pkts_acked == 1 &amp;&amp; fully_acked &amp;&amp; !prior_sacked &amp;&amp;\t\t    (tp-&gt;snd_una - prior_snd_una) &lt; tp-&gt;mss_cache &amp;&amp;\t\t    sack-&gt;rate-&gt;prior_delivered + 1 == tp-&gt;delivered &amp;&amp;\t\t    !(flag &amp; (FLAG_CA_ALERT | FLAG_SYN_ACKED))) &#123;\t\t\t/* Conservatively mark a delayed ACK. It&#x27;s typically\t\t\t * from a lone runt packet over the round trip to\t\t\t * a receiver w/o out-of-order or CE events.\t\t\t */\t\t\tflag |= FLAG_ACK_MAYBE_DELAYED;\t\t&#125;\t&#125;\t//和上面类似，只不过这里是用sack的信息生成rtt样本\tif (sack-&gt;first_sackt) &#123; //tcp_sacktag_one 中设置的\t\tsack_rtt_us = tcp_stamp_us_delta(tp-&gt;tcp_mstamp, sack-&gt;first_sackt);\t\tca_rtt_us = tcp_stamp_us_delta(tp-&gt;tcp_mstamp, sack-&gt;last_sackt);\t&#125;\t//计算平滑rtt和rto 和三次握手一样注意这里参数不一样\trtt_update = tcp_ack_update_rtt(sk, flag, seq_rtt_us, sack_rtt_us,\t\t\t\t\tca_rtt_us, sack-&gt;rate);\t//如果ack确认了新数据\tif (flag &amp; FLAG_ACKED) &#123;\t\tflag |= FLAG_SET_XMIT_TIMER;  /* set TLP or RTO timer */\t\t//mtu探测成功！\t\tif (unlikely(icsk-&gt;icsk_mtup.probe_size &amp;&amp;\t\t\t     !after(tp-&gt;mtu_probe.probe_seq_end, tp-&gt;snd_una))) &#123;\t\t\ttcp_mtup_probe_success(sk);\t\t&#125;\t\t//这里通常不进入\t\tif (tcp_is_reno(tp)) &#123;\t\t\ttcp_remove_reno_sacks(sk, pkts_acked, ece_ack);\t\t\t/* If any of the cumulatively ACKed segments was\t\t\t * retransmitted, non-SACK case cannot confirm that\t\t\t * progress was due to original transmission due to\t\t\t * lack of TCPCB_SACKED_ACKED bits even if some of\t\t\t * the packets may have been never retransmitted.\t\t\t */\t\t\tif (flag &amp; FLAG_RETRANS_DATA_ACKED)\t\t\t\tflag &amp;= ~FLAG_ORIG_SACK_ACKED;\t\t&#125; else &#123;\t\t\tint delta;\t\t\t/* Non-retransmitted hole got filled? That&#x27;s reordering */\t\t//之前 SACK 认为后面已经收到，现在累计 ACK 却回过头来确认了一个更靠前的洞，就是更新乱序阈值！！\t\t\tif (before(reord, prior_fack))\t\t\t\ttcp_check_sack_reordering(sk, reord, 0);\t\t\tdelta = prior_sacked - tp-&gt;sacked_out;\t\t\ttp-&gt;lost_cnt_hint -= min(tp-&gt;lost_cnt_hint, delta);\t\t&#125;\t//如果ack没有确认新数据\t&#125; else if (skb &amp;&amp; rtt_update &amp;&amp; sack_rtt_us &gt;= 0 &amp;&amp;\t\t   sack_rtt_us &gt; tcp_stamp_us_delta(tp-&gt;tcp_mstamp,\t\t\t\t\t\t    tcp_skb_timestamp_us(skb))) &#123;\t\t/* Do not re-arm RTO if the sack RTT is measured from data sent\t\t * after when the head was last (re)transmitted. Otherwise the\t\t * timeout may continue to extend in loss recovery.\t\t */\t\tflag |= FLAG_SET_XMIT_TIMER;  /* set TLP or RTO timer */\t&#125;\t//是否有拥塞算法的钩子，vegas 有这个钩子\tif (icsk-&gt;icsk_ca_ops-&gt;pkts_acked) &#123;\t\tstruct ack_sample sample = &#123; .pkts_acked = pkts_acked,\t\t\t\t\t     .rtt_us = sack-&gt;rate-&gt;rtt_us &#125;;\t\tsample.in_flight = tp-&gt;mss_cache *\t\t\t(tp-&gt;delivered - sack-&gt;rate-&gt;prior_delivered);\t\ticsk-&gt;icsk_ca_ops-&gt;pkts_acked(sk, &amp;sample);\t&#125;#if FASTRETRANS_DEBUG &gt; 0\tWARN_ON((int)tp-&gt;sacked_out &lt; 0);\tWARN_ON((int)tp-&gt;lost_out &lt; 0);\tWARN_ON((int)tp-&gt;retrans_out &lt; 0);\tif (!tp-&gt;packets_out &amp;&amp; tcp_is_sack(tp)) &#123;\t\ticsk = inet_csk(sk);\t\tif (tp-&gt;lost_out) &#123;\t\t\tpr_debug(&quot;Leak l=%u %d\\n&quot;,\t\t\t\t tp-&gt;lost_out, icsk-&gt;icsk_ca_state);\t\t\ttp-&gt;lost_out = 0;\t\t&#125;\t\tif (tp-&gt;sacked_out) &#123;\t\t\tpr_debug(&quot;Leak s=%u %d\\n&quot;,\t\t\t\t tp-&gt;sacked_out, icsk-&gt;icsk_ca_state);\t\t\ttp-&gt;sacked_out = 0;\t\t&#125;\t\tif (tp-&gt;retrans_out) &#123;\t\t\tpr_debug(&quot;Leak r=%u %d\\n&quot;,\t\t\t\t tp-&gt;retrans_out, icsk-&gt;icsk_ca_state);\t\t\ttp-&gt;retrans_out = 0;\t\t&#125;\t&#125;#endif\treturn flag;&#125;\n\ntcp_clean_rtx_queue中首先从重传队列头部开始，尽可能的去清理skb，循环内部首先判断这个skb是否已经被snd_una覆盖，或者是否被部分确认了，如果是部分确认则后续数据包肯定不用继续走了，会直接break。\n如果确认了原始发送的数据（最常见的情况）则会记录本轮第一个skb（后面计算rtt会用到），同时更新乱序开始的序号并设置确认原始数据的标志。\n接下来判断数据包是否被sack确认过，如果确认过则更新确认的累计技计数，如果没有被sack确认过则会进一步判断当前是否是一个虚假的重传（通过比较时间戳选项），如果不是虚假重传则会根据这个数据包的rtt决定是否启用rack\n之后更新更新发送出去还没有接收到ack的数量，累计确认的总段数等信息，同时为bbr算法提供必要的字段。之后调用tcp_rtx_queue_unlink_and_free将当前数据包从重传队列中移除，进入下一次循环。\n重传队列清理完成后（也就是退出循环），进入收尾工作，最关键的就是计算rtt，同时决定是否启动重传相关定时器首先调用tcp_chrono_stop记录本次状态持续的时间（三次握手中有介绍过）之后判断当前重传队列是否被清空了，如果没有被清空，则会判断是否存在sack反悔，这里不太理解怎么判断的。\n之后计算rtt，生成rtt的样本来源两处，一个是累计ack清掉的原始数据另一个是sack选项确认的数据，这里使用哪个是不一定的。\n拿到样本之后调用tcp_ack_update_rtt（三次握手中有过分析，这里注意优先优先使用数据包的发送的时间做rtt估计），计算平滑rtt和rto\n接下来判断本次ack是否确认了新数据，如果确认了新数据同时启用了MTUP同时被确认了话会调用tcp_mtup_probe_success处理mtup具体代码如下所示：\nstatic void tcp_mtup_probe_success(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tu64 val;\t//慢启动阈值\ttp-&gt;prior_ssthresh = tcp_current_ssthresh(sk);\t//根据mss计算新的cwnd，合理\tval = (u64)tcp_snd_cwnd(tp) * tcp_mss_to_mtu(sk, tp-&gt;mss_cache);\tdo_div(val, icsk-&gt;icsk_mtup.probe_size);\tDEBUG_NET_WARN_ON_ONCE((u32)val != val);\ttcp_snd_cwnd_set(tp, max_t(u32, 1U, val));\ttp-&gt;snd_cwnd_cnt = 0;\ttp-&gt;snd_cwnd_stamp = tcp_jiffies32;\ttp-&gt;snd_ssthresh = tcp_current_ssthresh(sk);//重新设置慢启动阈值\t//抬高下界，并结束本轮探测\ticsk-&gt;icsk_mtup.search_low = icsk-&gt;icsk_mtup.probe_size;\ticsk-&gt;icsk_mtup.probe_size = 0;\ttcp_sync_mss(sk, icsk-&gt;icsk_pmtu_cookie);\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMTUPSUCCESS);&#125;\n\ntcp_mtup_probe_success中按照新的mtu重新计算cwnd和慢启动阈值，同时把search_low设置为probe_size，并把并把 probe_size=0 表示本轮探测结束。\n回到tcp_clean_rtx_queue中，处理完mtup后会判断本次ack是否确认了一个靠前的序号，如果是则认为是乱序严重，调用tcp_check_sack_reordering更新乱序阈值（拥塞控制中介绍过），进而会影响标记丢包逻辑的判断。\n如果本次ack没有确认新数据，但是rtt样有效的话会设置启动超时重传定时器或TLP重传定时器。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP确认"]},{"title":"TCP拥塞控制-拥塞窗口调整撤销","url":"/2025/10/26/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6-%E6%8B%A5%E5%A1%9E%E7%AA%97%E5%8F%A3%E8%B0%83%E6%95%B4%E6%92%A4%E9%94%80/","content":"TCP 在网络发生丢包时，会进入拥塞控制阶段（比如进入 Recovery 或 Loss 状态），此时通常会减小拥塞窗口（cwnd）以降低发送速率。 但有时候，丢包判断是误判的 —— 实际上网络并没有拥塞，只是出现了乱序或延迟。 这时，如果后续收到的 ACK 能证明之前的“丢包”判断是错误的，TCP 会撤销之前的 cwnd 减小，也就是 cwnd undo。\n从Loss拥塞状态撤销tcp_process_loss中调用\nstatic bool tcp_try_undo_loss(struct sock *sk, bool frto_undo)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//frto明确指出要撤销 或者有可能需要撤销的标记，且当前确认的数据包的时间戳比重传的数据包要早\tif (frto_undo || tcp_may_undo(tp)) &#123;\t\ttcp_undo_cwnd_reduction(sk, true);\t\tDBGUNDO(sk, &quot;partial loss&quot;);\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPLOSSUNDO);\t\tif (frto_undo)\t\t\tNET_INC_STATS(sock_net(sk),\t\t\t\t\tLINUX_MIB_TCPSPURIOUSRTOS);\t\tinet_csk(sk)-&gt;icsk_retransmits = 0;\t\t//要是支持sack 这里直接false\t\tif (tcp_is_non_sack_preventing_reopen(sk))\t\t\treturn true;\t\t//设置状态为open\t\tif (frto_undo || tcp_is_sack(tp)) &#123;\t\t\ttcp_set_ca_state(sk, TCP_CA_Open); //从loss 变成 open\t\t\ttp-&gt;is_sack_reneg = 0;\t\t&#125;\t\treturn true;\t&#125;\treturn false;&#125;\n\ntcp_try_undo_loss() 在 Loss 状态收到“误判证据”时，负责恢复 cwnd、清零重传计数，并在合适条件下把拥塞状态切回 Open\n启用 SACK 或命中 F-RTO 时会更果断地 reopen。上述tcp_may_undo会根据重传标记和时间戳判断是否可以撤销，具体代码如下所示：\nstatic inline bool tcp_may_undo(const struct tcp_sock *tp)&#123;\treturn tp-&gt;undo_marker &amp;&amp; (!tp-&gt;undo_retrans || tcp_packet_delayed(tp));&#125;static inline bool tcp_packet_delayed(const struct tcp_sock *tp)&#123;\treturn tp-&gt;retrans_stamp &amp;&amp; //首次重传的时间戳\t\t       tcp_tsopt_ecr_before(tp, tp-&gt;retrans_stamp);&#125;//有时间戳选项，且时间戳选项早于whenstatic bool tcp_tsopt_ecr_before(const struct tcp_sock *tp, u32 when)&#123;\treturn tp-&gt;rx_opt.saw_tstamp &amp;&amp; tp-&gt;rx_opt.rcv_tsecr &amp;&amp;\t       before(tp-&gt;rx_opt.rcv_tsecr, when);&#125;\n\n如果满足撤销的条件后，会进一步调用tcp_undo_cwnd_reduction完成实际的撤销动作，具体代码如下所示:\nstatic void tcp_undo_cwnd_reduction(struct sock *sk, bool unmark_loss)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tif (unmark_loss) &#123;\t\tstruct sk_buff *skb;\t\t//遍历重传队列清除重传标志\t\tskb_rbtree_walk(skb, &amp;sk-&gt;tcp_rtx_queue) &#123;\t\t\tTCP_SKB_CB(skb)-&gt;sacked &amp;= ~TCPCB_LOST;\t\t&#125;\t\ttp-&gt;lost_out = 0;\t\t//清空辅助数据\t\ttcp_clear_all_retrans_hints(tp);\t&#125;\t//之前保存过慢启动阈值\tif (tp-&gt;prior_ssthresh) &#123;\t\tconst struct inet_connection_sock *icsk = inet_csk(sk);\t\t//调用拥塞算法的钩子重新设置窗口大小\t\ttcp_snd_cwnd_set(tp, icsk-&gt;icsk_ca_ops-&gt;undo_cwnd(sk));\t\t//更新慢启动阈值\t\tif (tp-&gt;prior_ssthresh &gt; tp-&gt;snd_ssthresh) &#123;\t\t\ttp-&gt;snd_ssthresh = tp-&gt;prior_ssthresh;\t\t\t//清楚拥塞标记\t\t\ttcp_ecn_withdraw_cwr(tp);\t\t&#125;\t&#125;\ttp-&gt;snd_cwnd_stamp = tcp_jiffies32; //记录时间戳\ttp-&gt;undo_marker = 0; //重置undo\t//rack 相关字段需要更新？？\ttp-&gt;rack.advanced = 1; /* Force RACK to re-exam losses */&#125;\n\ntcp_undo_cwnd_reduction中首先清除重传的队列中的重传标记，并清空快速定位重传数据包的辅助数据，之后调用具体拥塞算法的钩子重新计算拥塞窗口，并重新设置慢启动阈值，之后重置撤销标记用到的字段。\n回到tcp_try_undo_loss中，如果支持sack或者启用的frto则直接设置拥塞状态为open（大概率会被设置为open吧）。\n从recovery拥塞状态撤销loss状态可能调用这个函数 recovery状态下可能也会调用\nstatic bool tcp_try_undo_recovery(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//有撤销的标记，且当前确认的数据包的时间戳比重传的数据包要早\tif (tcp_may_undo(tp)) &#123;\t\tint mib_idx;\t\t/* Happy end! We did not retransmit anything\t\t * or our original transmission succeeded.\t\t */\t\tDBGUNDO(sk, inet_csk(sk)-&gt;icsk_ca_state == TCP_CA_Loss ? &quot;loss&quot; : &quot;retrans&quot;);\t\t//清除重传队列中的标志，更新慢启动阈值\t\ttcp_undo_cwnd_reduction(sk, false);\t\tif (inet_csk(sk)-&gt;icsk_ca_state == TCP_CA_Loss)\t\t\tmib_idx = LINUX_MIB_TCPLOSSUNDO;\t\telse\t\t\tmib_idx = LINUX_MIB_TCPFULLUNDO;\t\tNET_INC_STATS(sock_net(sk), mib_idx);\t&#125; else if (tp-&gt;rack.reo_wnd_persist) &#123; //如果真是丢包，就要减少这个值，这个是误判丢包的？\t\ttp-&gt;rack.reo_wnd_persist--;\t&#125;\t//reno算法的处理\tif (tcp_is_non_sack_preventing_reopen(sk))\t\treturn true;\ttcp_set_ca_state(sk, TCP_CA_Open); //从recovery 到 open\ttp-&gt;is_sack_reneg = 0;\treturn false;&#125;\n\ntcp_try_undo_recovery中首先根据tcp_may_undo判单是否可以撤销，如果可以，则会调用tcp_undo_cwnd_reduction更新重传队列中数据包的标记，跟新慢启动阈值等。之后设置状态为open，这里貌似只要支持sack就会将状态设置为open，即使tcp_may_undo返回false？\n除了上述tcp_try_undo_recovery会在恢复状态下撤销外，如果&#x2F;在恢复期间收到了部分数据包，会调用tcp_try_undo_partial判断是否可以撤销。具体代码如下所示：\nstatic bool tcp_try_undo_partial(struct sock *sk, u32 prior_snd_una,\t\t\t\t bool *do_lost)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//有拥塞撤销的标志，且数据包是延时到达的\tif (tp-&gt;undo_marker &amp;&amp; tcp_packet_delayed(tp)) &#123;\t\t/* Plain luck! Hole if filled with delayed\t\t * packet, rather than with a retransmit. Check reordering.\t\t */\t\t//更新乱续容忍\t\ttcp_check_sack_reordering(sk, prior_snd_una, 1);\t\t/* We are getting evidence that the reordering degree is higher\t\t * than we realized. If there are no retransmits out then we\t\t * can undo. Otherwise we clock out new packets but do not\t\t * mark more packets lost or retransmit more.\t\t */\t\t//存在重传的数据包，不能撤销\t\tif (tp-&gt;retrans_out)\t\t\treturn true;\t\t//重传队列是否为空\t\tif (!tcp_any_retrans_done(sk))\t\t\ttp-&gt;retrans_stamp = 0;\t\tDBGUNDO(sk, &quot;partial recovery&quot;);\t\ttcp_undo_cwnd_reduction(sk, true);\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPARTIALUNDO);\t\t//看看是否可以变成open状态\t\ttcp_try_keep_open(sk);\t&#125; else &#123;\t\t/* Partial ACK arrived. Force fast retransmit. */\t\t*do_lost = tcp_force_fast_retransmit(sk);\t&#125;\treturn false;&#125;\n\ntcp_try_undo_partial中，首先确定有拥塞标志，且数据包时因为延时到达的，则会调用tcp_check_sack_reordering更新乱序容忍度\n具体代码如下所示:\n//low_seq 为待检查的最低序列号static void tcp_check_sack_reordering(struct sock *sk, const u32 low_seq,\t\t\t\t      const int ts)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tconst u32 mss = tp-&gt;mss_cache;\tu32 fack, metric;\t//fack 为当前选择确认中最大的序列号\tfack = tcp_highest_sack_seq(tp);\t//如果待检查的序列号已经在选择确认最高的后面，则直接返回\tif (!before(low_seq, fack))\t\treturn;\t//可能丢包的范围？\tmetric = fack - low_seq;\tif ((metric &gt; tp-&gt;reordering * mss) &amp;&amp; mss) &#123;#if FASTRETRANS_DEBUG &gt; 1\t\tpr_debug(&quot;Disorder%d %d %u f%u s%u rr%d\\n&quot;,\t\t\t tp-&gt;rx_opt.sack_ok, inet_csk(sk)-&gt;icsk_ca\t\t\t _state,\t\t\t tp-&gt;reordering,\t\t\t 0,\t\t\t tp-&gt;sacked_out,\t\t\t tp-&gt;undo_marker ? tp-&gt;undo_retrans : 0);#endif\t\t//算一下有几个mss和默认值300取一个最小值，reordering的单位为最大乱续包的数量，会决定收到几个ack快速重传吗？\t\ttp-&gt;reordering = min_t(u32, (metric + mss - 1) / mss,\t\t\t\t       READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_max_reordering));\t&#125;\t/* This exciting event is worth to be remembered. 8) */\t//乱续事件计数\ttp-&gt;reord_seen++;\tNET_INC_STATS(sock_net(sk),\t\t      ts ? LINUX_MIB_TCPTSREORDER : LINUX_MIB_TCPSACKREORDER);&#125;\n\ntcp_check_sack_reordering中主要的工作就是更新乱序容忍阈值，fack 是当前SACK 选择确认覆盖到的最高序号，通过和low_seq计算右侧确认点到低序号之间的距离也就是metric，进一步计算乱了多少个mss的段，如果若 metric 超过 目前的容忍阈值tp-&gt;reordering * mss，说明真实乱序比我们以前估的更严重，会提升乱序容忍阈值但不超过系统参数.\n回到tcp_try_undo_partial中更新乱序阈值之后，判断存在重传的数据包，不能撤销，直接返回，否则调用tcp_undo_cwnd_reduction完成撤销，之后调用tcp_try_keep_open将新的状态设置为open或者disorder\n如果数据包不是延迟到达，或者没有拥塞撤销的标记的话会调用tcp_force_fast_retransmit设置lost这个传入传出参数，之后会强制触发快速重传。tcp_force_fast_retransmit实现如下所示：\nstatic bool tcp_force_fast_retransmit(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//当已被 SACK 确认的最高序号超出当前累计确认序号 (snd_una) 超过一定乱序容忍度 (reordering * MSS) 时\treturn after(tcp_highest_sack_seq(tp),\t\t     tp-&gt;snd_una + tp-&gt;reordering * tp-&gt;mss_cache);&#125;\n\n除tcp_try_undo_partial和 tcp_try_undo_recovery会对拥塞状态进行撤销外，在TCP_CA_Recovery状态下，如果此次 ACK 周期里的ack是dsack的话，会调用tcp_try_undo_dsack尝试恢复之前的拥塞窗口，具体代码如下所示：\n//recover 状态下会调用这个函数 为什么叫撤销dsack呢，因为最最外层处理过？static bool tcp_try_undo_dsack(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tif (tp-&gt;undo_marker &amp;&amp; !tp-&gt;undo_retrans) &#123;\t\ttp-&gt;rack.reo_wnd_persist = min(TCP_RACK_RECOVERY_THRESH,\t\t\t\t\t       tp-&gt;rack.reo_wnd_persist + 1);\t\tDBGUNDO(sk, &quot;D-SACK&quot;);\t\t//，清楚重传队列中的丢包标志，以及重新计算慢启动阈值\t\ttcp_undo_cwnd_reduction(sk, false);\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKUNDO);\t\treturn true;\t&#125;\treturn false;&#125;\n\ntcp_try_undo_dsack首先判断当前处于可撤销的状态，同时没有真正的重传在进行，也就是没有真正的丢包？会调用tcp_undo_cwnd_reduction 重新计算慢启动阈值和拥塞窗口。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP拥塞控制"]},{"title":"TCP三次握手-发送synack","url":"/2025/09/16/TCP%E8%A2%AB%E5%8A%A8%E6%89%93%E5%BC%80%E5%8F%91%E9%80%81synack/","content":"在 Linux 内核中，服务端在LISTEN状态收到客户端的 SYN 报文时才会发送SYN+ACK 作为响应，具体的发送位置为收到客户端发来的syn包，创建req结构后会调用req的ops 来发送synack具体代码如下:\nint tcp_conn_request(struct request_sock_ops *rsk_ops,\t\t     const struct tcp_request_sock_ops *af_ops,\t\t     struct sock *sk, struct sk_buff *skb)&#123;...\t\t//发送syn_ack tcp_v4_send_synack\t\taf_ops-&gt;send_synack(sk, dst, &amp;fl, req, &amp;foc,...\treturn 0;&#125;\n\n上述send_synack IPv4 来说，调用的是 tcp_v4_send_synack() 具体代码如下所示：\nstatic int tcp_v4_send_synack(const struct sock *sk, struct dst_entry *dst,\t\t\t      struct flowi *fl,\t\t\t      struct request_sock *req,\t\t\t      struct tcp_fastopen_cookie *foc,\t\t\t      enum tcp_synack_type synack_type,\t\t\t      struct sk_buff *syn_skb)&#123;\tconst struct inet_request_sock *ireq = inet_rsk(req);\tstruct flowi4 fl4;\tint err = -1;\tstruct sk_buff *skb;\tu8 tos;\t/* First, grab a route. */\t//是否有路由信息，没有的话调用查路由的接口\tif (!dst &amp;&amp; (dst = inet_csk_route_req(sk, &amp;fl4, req)) == NULL)\t\treturn -1;\t//构造synack报文，这里传入了req，synack的类型（正常，cookie，TFO），和收到的syn包\tskb = tcp_make_synack(sk, dst, req, foc, synack_type, syn_skb);\tif (skb) &#123;\t\t__tcp_v4_send_check(skb, ireq-&gt;ir_loc_addr, ireq-&gt;ir_rmt_addr);\t\ttos = READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_reflect_tos) ?//叫做反射tos? 默认是不启用的\t\t\t\t(tcp_rsk(req)-&gt;syn_tos &amp; ~INET_ECN_MASK) |\t\t\t\t(inet_sk(sk)-&gt;tos &amp; INET_ECN_MASK) :\t\t\t\tinet_sk(sk)-&gt;tos; //直接使用服务器端的 TOS 值\t\t//bpf相关\t\tif (!INET_ECN_is_capable(tos) &amp;&amp;\t\t    tcp_bpf_ca_needs_ecn((struct sock *)req))\t\t\ttos |= INET_ECN_ECT_0;\t\trcu_read_lock();\t\terr = ip_build_and_send_pkt(skb, sk, ireq-&gt;ir_loc_addr,\t\t\t\t\t    ireq-&gt;ir_rmt_addr,\t\t\t\t\t    rcu_dereference(ireq-&gt;ireq_opt),//ip_option\t\t\t\t\t    tos);\t\trcu_read_unlock();\t\terr = net_xmit_eval(err);\t&#125;\treturn err;&#125;\n\ntcp_v4_send_synack() 的逻辑很简单，先根据请求块 req 查找或确认路由信息，然后调用 tcp_make_synack() 构造一个 SYN+ACK 报文，接着计算校验和、确定 TOS&#x2F;ECN 设置，最后通过 ip_build_and_send_pkt() 把这个报文发出去\n构造synack报文的逻辑如下所示：\nstruct sk_buff *tcp_make_synack(const struct sock *sk, struct dst_entry *dst,\t\t\t\tstruct request_sock *req,\t\t\t\tstruct tcp_fastopen_cookie *foc,\t\t\t\tenum tcp_synack_type synack_type,\t\t\t\tstruct sk_buff *syn_skb)&#123;\tstruct inet_request_sock *ireq = inet_rsk(req);\tconst struct tcp_sock *tp = tcp_sk(sk);\tstruct tcp_md5sig_key *md5 = NULL;\tstruct tcp_out_options opts;\tstruct sk_buff *skb;\tint tcp_header_size;\tstruct tcphdr *th;\tint mss;\tu64 now;\t//申请一个skb\tskb = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);\tif (unlikely(!skb)) &#123;\t\tdst_release(dst);\t\treturn NULL;\t&#125;\t/* Reserve space for headers. */\t//留出头部空间\tskb_reserve(skb, MAX_TCP_HEADER);\tswitch (synack_type) &#123;\tcase TCP_SYNACK_NORMAL:\t\t//普通的三次握手，将skb和sk关联\t\tskb_set_owner_w(skb, req_to_sk(req));\t\tbreak;\tcase TCP_SYNACK_COOKIE:\t\t/* Under synflood, we do not attach skb to a socket,\t\t * to avoid false sharing.\t\t */\t\tbreak;\tcase TCP_SYNACK_FASTOPEN:\t\t/* sk is a const pointer, because we want to express multiple\t\t * cpu might call us concurrently.\t\t * sk-&gt;sk_wmem_alloc in an atomic, we can promote to rw.\t\t */\t\tskb_set_owner_w(skb, (struct sock *)sk);\t\tbreak;\t&#125;\t//将skb与dst关联\tskb_dst_set(skb, dst);\t//这里计算了本端的mss 大概率是1460\tmss = tcp_mss_clamp(tp, dst_metric_advmss(dst));\t//memset tcpopt\tmemset(&amp;opts, 0, sizeof(opts));\t//获取当前时间戳\tnow = tcp_clock_ns();#ifdef CONFIG_SYN_COOKIES\t//如果是syncookies类型的ack同时启用时间戳选项,感觉永远不会走到和这个逻辑吧，syncookie会有时间戳选项吗？\tif (unlikely(synack_type == TCP_SYNACK_COOKIE &amp;&amp; ireq-&gt;tstamp_ok))\t\tskb_set_delivery_time(skb, cookie_init_timestamp(req, now),\t\t\t\t      true);\telse#endif\t&#123;\t\tskb_set_delivery_time(skb, now, true);\t\tif (!tcp_rsk(req)-&gt;snt_synack) /* Timestamp first SYNACK */ //是否是第一次发送synack\t\t\ttcp_rsk(req)-&gt;snt_synack = tcp_skb_timestamp_us(skb); //设置时间戳\t&#125;#ifdef CONFIG_TCP_MD5SIG\trcu_read_lock();\tmd5 = tcp_rsk(req)-&gt;af_specific-&gt;req_md5_lookup(sk, req_to_sk(req));#endif\t//这里又设置了一次txhash？\tskb_set_hash(skb, READ_ONCE(tcp_rsk(req)-&gt;txhash), PKT_HASH_TYPE_L4);\t/* bpf program will be interested in the tcp_flags */\tTCP_SKB_CB(skb)-&gt;tcp_flags = TCPHDR_SYN | TCPHDR_ACK;\t//构建和配置 TCP SYN-ACK 报文中的所有选项字段，包括 MSS、窗口缩放、时间戳、SACK 等。\ttcp_header_size = tcp_synack_options(sk, req, mss, skb, &amp;opts, md5,\t\t\t\t\t     foc, synack_type,\t\t\t\t\t     syn_skb) + sizeof(*th);//这里加上了20字节\tskb_push(skb, tcp_header_size);//往前推tcp头部包括选项部分的字节数\tskb_reset_transport_header(skb); //指向tcp头开始\tth = (struct tcphdr *)skb-&gt;data;\tmemset(th, 0, sizeof(struct tcphdr));\tth-&gt;syn = 1;   //syn标志位\tth-&gt;ack = 1;\t//ack标志位\ttcp_ecn_make_synack(req, th);  //ecn标志位\tth-&gt;source = htons(ireq-&gt;ir_num); \tth-&gt;dest = ireq-&gt;ir_rmt_port;\tskb-&gt;mark = ireq-&gt;ir_mark;\tskb-&gt;ip_summed = CHECKSUM_PARTIAL; //软件部分计算校验和\tth-&gt;seq = htonl(tcp_rsk(req)-&gt;snt_isn); //序列号，这个值是在收到syn包的时候设置的\t/* XXX data is queued and acked as is. No buffer/window check */\tth-&gt;ack_seq = htonl(tcp_rsk(req)-&gt;rcv_nxt);\t/* RFC1323: The window in SYN &amp; SYN/ACK segments is never scaled. */\tth-&gt;window = htons(min(req-&gt;rsk_rcv_wnd, 65535U));//这里很有可能就是65535，实际抓包比这个小一点不知道为什么//构造tcp的选项\ttcp_options_write(th, NULL, &amp;opts);\t//tcp包头的长度\tth-&gt;doff = (tcp_header_size &gt;&gt; 2);\tTCP_INC_STATS(sock_net(sk), TCP_MIB_OUTSEGS);#ifdef CONFIG_TCP_MD5SIG\t/* Okay, we have all we need - do the md5 hash if needed */\tif (md5)\t\ttcp_rsk(req)-&gt;af_specific-&gt;calc_md5_hash(opts.hash_location,\t\t\t\t\t       md5, req_to_sk(req), skb);\trcu_read_unlock();#endif\t//bpf相关\tbpf_skops_write_hdr_opt((struct sock *)sk, skb, req, syn_skb,\t\t\t\tsynack_type, &amp;opts);\t//更新skb-&gt;tstamp \tskb_set_delivery_time(skb, now, true);\ttcp_add_tx_delay(skb, tp); //设置了skb的时间戳，问题是这个时间戳在哪里起作用呢？重传？\treturn skb;&#125;\n\ntcp_make_synack中，首先申请一个skb，这个skb的大小就是最长数据包的头部的大小(大概是200+字节吧，会根据cache对齐)，然后reserve头部空间，将数据包关联路由查找的结果，调用tcp_synack_options构造选项字段(注意：这里构造哪些选项，取决于收到的syn和本端系统开启了哪些选项)，之后设置数据包的标志为（syn+ack），窗口大小，序列号，确认号等字段。\n回到tcp_v4_send_synack中，tcp_make_synack申请并设置数据包的部分字段后，会计算校验和和确定tos字段，之后调用ip_build_and_send_pkt完成synack包的发送，这里可以发现发送synack包和发送正常的数据包走的不是一个接口。具体代码如下所示：\nint ip_build_and_send_pkt(struct sk_buff *skb, const struct sock *sk,\t\t\t  __be32 saddr, __be32 daddr, struct ip_options_rcu *opt,\t\t\t  u8 tos)&#123;\tconst struct inet_sock *inet = inet_sk(sk);\tstruct rtable *rt = skb_rtable(skb);\tstruct net *net = sock_net(sk);\tstruct iphdr *iph;\t/* Build the IP header. */\t//往前推ip头可能需要的的长度\tskb_push(skb, sizeof(struct iphdr) + (opt ? opt-&gt;opt.optlen : 0));\t//指向ip头的指针\tskb_reset_network_header(skb);\tiph = ip_hdr(skb);\tiph-&gt;version  = 4;\tiph-&gt;ihl      = 5;\tiph-&gt;tos      = tos;//外面确定的，就是接收syn包的tos\tiph-&gt;ttl      = ip_select_ttl(inet, &amp;rt-&gt;dst);\tiph-&gt;daddr    = (opt &amp;&amp; opt-&gt;opt.srr ? opt-&gt;opt.faddr : daddr);\tiph-&gt;saddr    = saddr;\tiph-&gt;protocol = sk-&gt;sk_protocol;\t/* Do not bother generating IPID for small packets (eg SYNACK) */\t//小于68字节（这里为啥是68呢？）或者不允许分片，ipid就设置为0\tif (skb-&gt;len &lt;= IPV4_MIN_MTU || ip_dont_fragment(sk, &amp;rt-&gt;dst)) &#123;\t\tiph-&gt;frag_off = htons(IP_DF);\t\tiph-&gt;id = 0;\t&#125; else &#123;\t\tiph-&gt;frag_off = 0;\t\t/* TCP packets here are SYNACK with fat IPv4/TCP options.\t\t * Avoid using the hashed IP ident generator.\t\t */\t\t//如果是tcp这里使用随机的id，其他协议会根据hash算一个id\t\tif (sk-&gt;sk_protocol == IPPROTO_TCP)\t\t\tiph-&gt;id = (__force __be16)get_random_u16();\t\telse\t\t\t__ip_select_ident(net, iph, 1);\t&#125;\t//将opt字段的ip选项拷贝到数据包中\tif (opt &amp;&amp; opt-&gt;opt.optlen) &#123;\t\tiph-&gt;ihl += opt-&gt;opt.optlen&gt;&gt;2;\t\tip_options_build(skb, &amp;opt-&gt;opt, daddr, rt);\t&#125;\t//设置skb的优先级\tskb-&gt;priority = READ_ONCE(sk-&gt;sk_priority);\tif (!skb-&gt;mark)\t\tskb-&gt;mark = READ_ONCE(sk-&gt;sk_mark);\t/* Send it out. */\t//调用ip层发包接口\treturn ip_local_out(net, skb-&gt;sk, skb);&#125;\n\nip_build_and_send_pkt中设置了ip所需的各个字段和选项，这里会根据数据包是否分片来确定ip的id字段，如果是不容许分片的报文，则id从0开始，否则对于tcp协议则是使用随机数生成，最后调用ip_local_out完成数据包的发送.\n这里补充一下第一次握手接收syn包后会计算本端的窗口大小的逻辑，上面发送synack的时候会告诉对端本端窗口的大小，这个大小的计算是在收到syn调用tcp_openreq_init_rwin中完成的，具体代码如下所示：\nvoid tcp_openreq_init_rwin(struct request_sock *req,\t\t\t   const struct sock *sk_listener,\t\t\t   const struct dst_entry *dst)&#123;\tstruct inet_request_sock *ireq = inet_rsk(req);\tconst struct tcp_sock *tp = tcp_sk(sk_listener);\t//这里注意 大概返回1200/4096倍的rcv_buf的大小?\tint full_space = tcp_full_space(sk_listener);\tu32 window_clamp;\t__u8 rcv_wscale;\tu32 rcv_wnd;\tint mss;\t//用户如果有就用用户的mss，否则大概率根据mtu算一个\tmss = tcp_mss_clamp(tp, dst_metric_advmss(dst));\t//这里看用户是否set吧？\twindow_clamp = READ_ONCE(tp-&gt;window_clamp);\t/* Set this up on the first call only */\t//这里大概率是0吧\treq-&gt;rsk_window_clamp = window_clamp ? : dst_metric(dst, RTAX_WINDOW);\t/* limit the window selection if the user enforce a smaller rx buffer */\t//这里大概率也不会走\tif (sk_listener-&gt;sk_userlocks &amp; SOCK_RCVBUF_LOCK &amp;&amp;\t    (req-&gt;rsk_window_clamp &gt; full_space || req-&gt;rsk_window_clamp == 0))\t\treq-&gt;rsk_window_clamp = full_space;\t//大概率也返回0\trcv_wnd = tcp_rwnd_init_bpf((struct sock *)req);\tif (rcv_wnd == 0)\t//大概率也返回0\t\trcv_wnd = dst_metric(dst, RTAX_INITRWND);\telse if (full_space &lt; rcv_wnd * mss)\t\tfull_space = rcv_wnd * mss;\t/* tcp_full_space because it is guaranteed to be the first packet */\t//根据监听sk，最大可用空间大小，mss，是否开启窗口缩放，等来计算一个窗口缩放因子，这里设置了rsk_rcv_wnd\ttcp_select_initial_window(sk_listener, full_space,\t\tmss - (ireq-&gt;tstamp_ok ? TCPOLEN_TSTAMP_ALIGNED : 0),\t\t&amp;req-&gt;rsk_rcv_wnd,\t\t&amp;req-&gt;rsk_window_clamp,\t\tireq-&gt;wscale_ok,\t\t&amp;rcv_wscale,\t\trcv_wnd);\tireq-&gt;rcv_wscale = rcv_wscale;//传入传出的窗口缩放因子，会synack的时候会用到&#125;\n\ntcp_openreq_init_rwin其实就做三件事情，计算本端接收窗口的大小，计算窗口缩放因子，计算rsk_window_clamp（叫窗口钳制值？）\n实际计算上述三个变量的过程都在tcp_select_initial_window中实现，而tcp_openreq_init_rwin的作用是做准备工作，工作的内容是计算full_space(这里就是sk_rcvbuf的三分之一左右, 三分之一的原因是因为一个数据包实际所用空间是一个page的三分之一左右？)，计算mss，以及根据bpf或者用户配置获取rcv_wnd 。\n然后将准备工作的参数传入**tcp_select_initial_window中完成实际本端接收窗口的大小，窗口缩放因子，rsk_window_clamp的计算**, 具体代码如下所示：\nvoid tcp_select_initial_window(const struct sock *sk, int __space, __u32 mss,\t\t\t       __u32 *rcv_wnd, __u32 *window_clamp,\t\t\t       int wscale_ok, __u8 *rcv_wscale,\t\t\t       __u32 init_rcv_wnd)&#123;\tunsigned int space = (__space &lt; 0 ? 0 : __space);\t/* If no clamp set the clamp to the max possible scaled window */\tif (*window_clamp == 0)\t//设置为最大1G\t\t(*window_clamp) = (U16_MAX &lt;&lt; TCP_MAX_WSCALE);\tspace = min(*window_clamp, space);\t/* Quantize space offering to a multiple of mss if possible. */\tif (space &gt; mss)\t\t//注意这里取整mss的整数倍\t\tspace = rounddown(space, mss);\t/* NOTE: offering an initial window larger than 32767\t * will break some buggy TCP stacks. If the admin tells us\t * it is likely we could be speaking with such a buggy stack\t * we will truncate our initial window offering to 32K-1\t * unless the remote has sent us a window scaling option,\t * which we interpret as a sign the remote TCP is not\t * misinterpreting the window field as a signed quantity.\t */\tif (READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_workaround_signed_windows))//默认不开启\t\t(*rcv_wnd) = min(space, MAX_TCP_WINDOW);\telse\t\t(*rcv_wnd) = min_t(u32, space, U16_MAX);//65535\t//这里大概率是0\tif (init_rcv_wnd)\t\t*rcv_wnd = min(*rcv_wnd, init_rcv_wnd * mss);\t*rcv_wscale = 0;\t//开启窗口缩放\tif (wscale_ok) &#123;\t\t/* Set window scaling on max possible window */\t\tspace = max_t(u32, space, READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_rmem[2]));//最小值\t\tspace = max_t(u32, space, READ_ONCE(sysctl_rmem_max));//系统级限制\t\tspace = min_t(u32, space, *window_clamp); //窗口钳制值限制\t\t*rcv_wscale = clamp_t(int, ilog2(space) - 15,//计算缩放银子\t\t\t\t      0, TCP_MAX_WSCALE);\t&#125;\t/* Set the clamp no higher than max representable value */\t(*window_clamp) = min_t(__u32, U16_MAX &lt;&lt; (*rcv_wscale), *window_clamp);&#125;\n\ntcp_select_initial_window中window_clamp的最终大概率为full_space，而rcv_wnd最终大概率为min(space, U16_MAX)而wscale_ok最终大概率为7。\n这里有个问题，由上面逻辑可以知道full_space是根据rcv_buf计算出来的，也就是rcv_buf的三分之一左右，为什么rcv_buf大小为128k的情况下，窗口大小是64k左右呢？？？？\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP三次握手-接收ack (四)","url":"/2025/09/24/TCP%E8%A2%AB%E5%8A%A8%E6%89%93%E5%BC%80%E6%8E%A5%E6%94%B6ack(%E5%9B%9B)/","content":"回到tcp_check_req中，计算完rtt相关字段后，则到了最后一步，叫做hashdance，也就是将新创建的sock加入全连接队列具体代码如下所示：\nstruct sock *inet_csk_complete_hashdance(struct sock *sk, struct sock *child,                     struct request_sock *req, bool own_req)&#123;    //正常情况，新创建的子sock成功插入ehash后own_req为ture    if (own_req) &#123;        //从ehash中删除(这里应该不会在删除了)，删除定时器        inet_csk_reqsk_queue_drop(req-&gt;rsk_listener, req);        //从半连接队列中移除了        reqsk_queue_removed(&amp;inet_csk(req-&gt;rsk_listener)-&gt;icsk_accept_queue, req);        //如果监听sock和req 指向的sock不是一个,基本上不会是这种情况        if (sk != req-&gt;rsk_listener) &#123;            /* another listening sk has been selected,             * migrate the req to it.             */            struct request_sock *nreq;            /* hold a refcnt for the nreq-&gt;rsk_listener             * which is assigned in inet_reqsk_clone()             */            sock_hold(sk);            //由于原始的 req关联的是旧的监听套接字，不能直接用于新的监听套接字。            //因此需要调用 inet_reqsk_clone  克隆一个新的请求块            nreq = inet_reqsk_clone(req, sk);            if (!nreq) &#123;                inet_child_forget(sk, req, child);                goto child_put;            &#125;            refcount_set(&amp;nreq-&gt;rsk_refcnt, 1);            //加入全连接队列，更新三次握手成功数量            if (inet_csk_reqsk_queue_add(sk, nreq, child)) &#123;                __NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMIGRATEREQSUCCESS);                reqsk_migrate_reset(req);                reqsk_put(req); //处理原来的req                return child; //直接返回新的sock            &#125;            __NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMIGRATEREQFAILURE);            reqsk_migrate_reset(nreq);            __reqsk_free(nreq);        &#125; else if (inet_csk_reqsk_queue_add(sk, req, child)) &#123; //正常走这个分支，加入全连接队列            return child; //返回新创建的sock，注意这里其实并没有释放req这个结构，req是在外面被释放的        &#125;    &#125;    /* Too bad, another child took ownership of the request, undo. */child_put:    bh_unlock_sock(child);    sock_put(child);    return NULL;&#125;\n\n上述代码的逻辑为**，如果 own_req 为 true（子 socket 成功ehash），就会从半连接队列移除 req；如果监听 socket 发生变化，就克隆一个新的 req 并迁移到新的监听 socket（基本不会发生这个情况吧）；否则调用inet_csk_reqsk_queue_add将新的sock加入全连接队列，并返回新的 sock**。\n加入全连接队列，更新三次握手成功数量具体代码如下所示：\n//加入全连接队列，更新三次握手成功数量struct sock *inet_csk_reqsk_queue_add(struct sock *sk,                      struct request_sock *req,                      struct sock *child)&#123;    struct request_sock_queue *queue = &amp;inet_csk(sk)-&gt;icsk_accept_queue;    spin_lock(&amp;queue-&gt;rskq_lock);    //如果应用程序关掉监听套件字了？这里直接把资源都释放了    if (unlikely(sk-&gt;sk_state != TCP_LISTEN)) &#123;        inet_child_forget(sk, req, child);        child = NULL;    &#125; else &#123;        req-&gt;sk = child; //关联req和新创建的sock        req-&gt;dl_next = NULL;        //注意这里加入了全连接队列        if (queue-&gt;rskq_accept_head == NULL)            WRITE_ONCE(queue-&gt;rskq_accept_head, req);        else            queue-&gt;rskq_accept_tail-&gt;dl_next = req;        queue-&gt;rskq_accept_tail = req;        sk_acceptq_added(sk); //sk_ack_backlog++（三次握手成功的数量）    &#125;    spin_unlock(&amp;queue-&gt;rskq_lock);    return child;&#125;static inline void sk_acceptq_added(struct sock *sk)&#123;    WRITE_ONCE(sk-&gt;sk_ack_backlog, sk-&gt;sk_ack_backlog + 1);&#125;\n\n至此tcp_check_req全部逻辑就执行完毕，回到tcp_v4_rcv中 **tcp_check_req**返回了新创建的sock **此时sk与nsk不是一个sock具体代码如下所示\n//bfp相关的过滤钩子，返回0表示被接收        if (!tcp_filter(sk, skb)) &#123;            th = (const struct tcphdr *)skb-&gt;data;            iph = ip_hdr(skb);            //根据数据包字段填充私有控制结构            tcp_v4_fill_cb(skb, iph, th);            //创建新的sock，这里可能返回监听sk，或者空，或者新的sock            nsk = tcp_check_req(sk, skb, req, false, &amp;req_stolen);        &#125; else &#123;            drop_reason = SKB_DROP_REASON_SOCKET_FILTER;        &#125;        if (!nsk) &#123;//如果返回空则表示收到的报文可能不合法，打概率默默丢弃了            reqsk_put(req);            if (req_stolen) &#123;                tcp_v4_restore_cb(skb);                sock_put(sk);                goto lookup;            &#125;            goto discard_and_relse;        &#125;        //连接跟踪相关复位        nf_reset_ct(skb);        if (nsk == sk) &#123;            reqsk_put(req);            tcp_v4_restore_cb(skb);        //正常情况下走这个流程并返回0         &#125; else if (tcp_child_process(sk, nsk, skb)) &#123;            tcp_v4_send_reset(nsk, skb);            goto discard_and_relse;        &#125; else &#123;            sock_put(sk);            return 0;        &#125;\n\n因此上述代码会调用tcp_child_process进一步进行处理**，\nint tcp_child_process(struct sock *parent, struct sock *child,              struct sk_buff *skb)    __releases(&amp;((child)-&gt;sk_lock.slock))&#123;    int ret = 0;    //三次握手被动打开接收ack走到这里后这里应该是syn_rcv的状态    int state = child-&gt;sk_state;    /* record sk_napi_id and sk_rx_queue_mapping of child. */    //这里设置了用那个队列和napi    sk_mark_napi_id_set(child, skb);    tcp_segs_in(tcp_sk(child), skb);    //三次握手收到ack后这里用户进程会操作sock吗？ 可能有其他的进程调用？    if (!sock_owned_by_user(child)) &#123;        ret = tcp_rcv_state_process(child, skb);        /* Wakeup parent, send SIGIO */        if (state == TCP_SYN_RECV &amp;&amp; child-&gt;sk_state != state)            parent-&gt;sk_data_ready(parent);    &#125; else &#123;        /* Alas, it is possible again, because we do lookup         * in main socket hash table and lock on listening         * socket does not protect us more.         */        __sk_add_backlog(child, skb);    &#125;    bh_unlock_sock(child);    sock_put(child);    return ret;&#125;\n\n上述tcp_child_process中，首先把当前sock的状态保存起来了，此时是TCP_SYN_RECV, 如果用户没有持有sock则调用tcp_rcv_state_process进一步处理，注意，这里把状态设置成 了TCP_ESTABLISHED。因此child-&gt;sk_state的状态就是不是TCP_SYN_RECV了，则会进一步调用sk_data_ready唤醒监听 socket 的等待队列，告诉用户进程有新的连接可以 accept 了\ntcp_rcv_state_process第三次握手的处理TCP_SYN_RECV状态的代码如下所示：\nint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)&#123;    ...\tcase TCP_SYN_RECV:\t\ttp-&gt;delivered++; /* SYN-ACK delivery isn&#x27;t tracked in tcp_ack */\t\tif (!tp-&gt;srtt_us)\t\t\t//这里计算rtt和平滑rtt\t\t\ttcp_synack_rtt_meas(sk, req);\t\tif (req) &#123;\t\t\t//如果不是TFO的话这里的req就是空\t\t\ttcp_rcv_synrecv_state_fastopen(sk);\t\t&#125; else &#123;\t\t\t//撤销超时重传导致的丢包怀疑标记\t\t\ttcp_try_undo_spurious_syn(sk);\t\t\ttp-&gt;retrans_stamp = 0;\t\t\ttcp_init_transfer(sk, BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB,\t\t\t\t\t  skb);\t\t\t//设置用户已经读取到的序列号，这里色湖之为rcv_nxt\t\t\tWRITE_ONCE(tp-&gt;copied_seq, tp-&gt;rcv_nxt);\t\t&#125;\t\tsmp_mb();\t\ttcp_set_state(sk, TCP_ESTABLISHED);\t\tsk-&gt;sk_state_change(sk);\t\t/* Note, that this wakeup is only for marginal crossed SYN case.\t\t * Passively open sockets are not waked up, because\t\t * sk-&gt;sk_sleep == NULL and sk-&gt;sk_socket == NULL.\t\t */\t\t//交叉syn会走这里\t\tif (sk-&gt;sk_socket)\t\t\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT);\t\t//初始化snd_una\t\ttp-&gt;snd_una = TCP_SKB_CB(skb)-&gt;ack_seq;\t\t//根据窗口和窗口缩放因子\t\ttp-&gt;snd_wnd = ntohs(th-&gt;window) &lt;&lt; tp-&gt;rx_opt.snd_wscale;\t\t//记录发送窗口更新时候的序列号\t\ttcp_init_wl(tp, TCP_SKB_CB(skb)-&gt;seq);\t\t//通告mss的大小减去时间戳选项\t\tif (tp-&gt;rx_opt.tstamp_ok)\t\t\ttp-&gt;advmss -= TCPOLEN_TSTAMP_ALIGNED;\t\t//这里通常不会走？ 拥塞算法没有实现钩子？\t\tif (!inet_csk(sk)-&gt;icsk_ca_ops-&gt;cong_control)\t\t\ttcp_update_pacing_rate(sk);\t\t/* Prevent spurious tcp_cwnd_restart() on first data packet */\t\ttp-&gt;lsndtime = tcp_jiffies32;\t\ttcp_initialize_rcv_mss(sk);\t\t//处理fastpath\t\ttcp_fast_path_on(tp);\t\tbreak;         ...    &#125;\n\n上述代码主要是针对三次握手收到最后一个ack后（此时新创建的sock是TCP_SYN_RCV状态）进行处理，核心工作其实就是设置成建立连接状态和对一系列字段进行初始化(拥塞窗口，慢启动阈值，乱序阈值，rto，接收端最大窗口大小，发送窗口大小，初始化una。。。。) 上述代码中会调用tcp_init_transfer完成一部分的初始化工作，包括拥塞窗口，接收窗口的阈值设置等，具体代码如下所示：\nvoid tcp_init_transfer(struct sock *sk, int bpf_op, struct sk_buff *skb)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\t//初始化pmtu探测的范围\ttcp_mtup_init(sk);\t//查一次路由\ticsk-&gt;icsk_af_ops-&gt;rebuild_header(sk);\t//尝试利用 历史缓存的网络性能参数（RTT、ssthresh、乱序阈值等）\ttcp_init_metrics(sk);\t/* Initialize the congestion window to start the transfer.\t * Cut cwnd down to 1 per RFC5681 if SYN or SYN-ACK has been\t * retransmitted. In light of RFC6298 more aggressive 1sec\t * initRTO, we only reset cwnd when more than 1 SYN/SYN-ACK\t * retransmission has occurred.\t */\t//如果三次握手阶段丢包，要降低cwnd所以设置为了1\tif (tp-&gt;total_retrans &gt; 1 &amp;&amp; tp-&gt;undo_marker)\t\ttcp_snd_cwnd_set(tp, 1);\telse\t//这里有用户没有显示配置的话大概率就是10个mss\t\ttcp_snd_cwnd_set(tp, tcp_init_cwnd(tp, __sk_dst_get(sk)));\t//每次调整拥塞窗口的时间戳\ttp-&gt;snd_cwnd_stamp = tcp_jiffies32;\t//bpf相关\tbpf_skops_established(sk, bpf_op, skb);\t/* Initialize congestion control unless BPF initialized it already: */\tif (!icsk-&gt;icsk_ca_initialized) //调用拥塞算法初始化的钩子\t\ttcp_init_congestion_control(sk);\t//设置接收窗口阈值\ttcp_init_buffer_space(sk);&#125;\n\ntcp_init_transfer首先初始化了pmtu的探测范围(超时重传的黑洞检测会用到，发包通路中貌似也会调用)，具体代码如下所示：\n//初始化PMTU相关的字段void tcp_mtup_init(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct net *net = sock_net(sk);\t//系统是否启用mtu探测，默认没有启用\ticsk-&gt;icsk_mtup.enabled = READ_ONCE(net-&gt;ipv4.sysctl_tcp_mtu_probing) &gt; 1;\t//计算一个mtu的最大值\ticsk-&gt;icsk_mtup.search_high = tp-&gt;rx_opt.mss_clamp + sizeof(struct tcphdr) +\t\t\t       icsk-&gt;icsk_af_ops-&gt;net_header_len;\t//计算一个mtu的最小值\ticsk-&gt;icsk_mtup.search_low = tcp_mss_to_mtu(sk, READ_ONCE(net-&gt;ipv4.sysctl_tcp_base_mss));\t//初始化探测报文的大小为 0\ticsk-&gt;icsk_mtup.probe_size = 0;\tif (icsk-&gt;icsk_mtup.enabled)\t\ticsk-&gt;icsk_mtup.probe_timestamp = tcp_jiffies32;&#125;\n\n完成pmtu相关字段的初始化后会调用  icsk-&gt;icsk_af_ops-&gt;rebuild_header再查一次路由，这个函数在超时重传中也调用过。之后会调用tcp_init_metrics根据历史缓存的网络性能参数（RTT、ssthresh、cwnd、乱序阈值等），来指导初始化拥塞窗口字段，具体代码如下\nvoid tcp_init_metrics(struct sock *sk)&#123;\tstruct dst_entry *dst = __sk_dst_get(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct net *net = sock_net(sk);\tstruct tcp_metrics_block *tm;\tu32 val, crtt = 0; /* cached RTT scaled by 8 */\t//验证路由是否有效\tsk_dst_confirm(sk);\tif (!dst)\t\tgoto reset;\trcu_read_lock();\t//取出历史 metric\ttm = tcp_get_metrics(sk, dst, true);\tif (!tm) &#123;\t\trcu_read_unlock();\t\tgoto reset;\t&#125;\tif (tcp_metric_locked(tm, TCP_METRIC_CWND))\t\ttp-&gt;snd_cwnd_clamp = tcp_metric_get(tm, TCP_METRIC_CWND); //最大值？？\tval = READ_ONCE(net-&gt;ipv4.sysctl_tcp_no_ssthresh_metrics_save) ?//默认是1\t      0 : tcp_metric_get(tm, TCP_METRIC_SSTHRESH);\tif (val) &#123;\t\ttp-&gt;snd_ssthresh = val;\t\tif (tp-&gt;snd_ssthresh &gt; tp-&gt;snd_cwnd_clamp)\t\t\ttp-&gt;snd_ssthresh = tp-&gt;snd_cwnd_clamp;\t&#125; else &#123;\t\t/* ssthresh may have been reduced unnecessarily during.\t\t * 3WHS. Restore it back to its initial default.\t\t */\t\ttp-&gt;snd_ssthresh = TCP_INFINITE_SSTHRESH;  //设置慢启动阈值为全F\t&#125;\tval = tcp_metric_get(tm, TCP_METRIC_REORDERING); //从历史连接中获取乱序阈值 默认是3\tif (val &amp;&amp; tp-&gt;reordering != val)\t\ttp-&gt;reordering = val; //更新tp管理的乱序阈值上线\tcrtt = tcp_metric_get(tm, TCP_METRIC_RTT); //从metric中获取缓存的rtt\trcu_read_unlock();reset:\t/* The initial RTT measurement from the SYN/SYN-ACK is not ideal\t * to seed the RTO for later data packets because SYN packets are\t * small. Use the per-dst cached values to seed the RTO but keep\t * the RTT estimator variables intact (e.g., srtt, mdev, rttvar).\t * Later the RTO will be updated immediately upon obtaining the first\t * data RTT sample (tcp_rtt_estimator()). Hence the cached RTT only\t * influences the first RTO but not later RTT estimation.\t *\t * But if RTT is not available from the SYN (due to retransmits or\t * syn cookies) or the cache, force a conservative 3secs timeout.\t *\t * A bit of theory. RTT is time passed after &quot;normal&quot; sized packet\t * is sent until it is ACKed. In normal circumstances sending small\t * packets force peer to delay ACKs and calculation is correct too.\t * The algorithm is adaptive and, provided we follow specs, it\t * NEVER underestimate RTT. BUT! If peer tries to make some clever\t * tricks sort of &quot;quick acks&quot; for time long enough to decrease RTT\t * to low value, and then abruptly stops to do it and starts to delay\t * ACKs, wait for troubles.\t */\t//计算rto\tif (crtt &gt; tp-&gt;srtt_us) &#123;\t\t/* Set RTO like tcp_rtt_estimator(), but from cached RTT. */\t\tcrtt /= 8 * USEC_PER_SEC / HZ;\t\tinet_csk(sk)-&gt;icsk_rto = crtt + max(2 * crtt, tcp_rto_min(sk));\t\t\t&#125; else if (tp-&gt;srtt_us == 0) &#123;\t\t/* RFC6298: 5.7 We&#x27;ve failed to get a valid RTT sample from\t\t * 3WHS. This is most likely due to retransmission,\t\t * including spurious one. Reset the RTO back to 3secs\t\t * from the more aggressive 1sec to avoid more spurious\t\t * retransmission.\t\t */\t\ttp-&gt;rttvar_us = jiffies_to_usecs(TCP_TIMEOUT_FALLBACK);\t\ttp-&gt;mdev_us = tp-&gt;mdev_max_us = tp-&gt;rttvar_us;\t\tinet_csk(sk)-&gt;icsk_rto = TCP_TIMEOUT_FALLBACK;\t&#125;&#125;\n\n上述tcp_init_metrics中首先调用tcp_get_metrics查hash表取出历史连接信息（如果有的话，没有则会创建一个）。之后设置了慢启动阈值，如果用户没有配置的话这里默认是全F,止之后从历史连接中获取获取乱序阈值和rtt注意这里可能都是零吧，如果没有历史连接数据的话。最后根据srtt计算rto。\n回到tcp_init_transfer中，获取历史连接数据之后，会进一步设置cwnd如果在三次握手阶段丢包了则初始化的拥塞窗口为1，否则为10个MSS,并记录此次调整拥塞窗口的时间。之后会调用拥塞算法初始化的钩子，最后调用tcp_init_buffer_space设置本端窗口的钳制值。\n具体代码如下所示：\nstatic void tcp_init_buffer_space(struct sock *sk)&#123;\tint tcp_app_win = READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_app_win); //默认是31\tstruct tcp_sock *tp = tcp_sk(sk);\tint maxwin;\t//用户是否setsockopt显示设置缓冲大小，默认是没有的\tif (!(sk-&gt;sk_userlocks &amp; SOCK_SNDBUF_LOCK))\t\ttcp_sndbuf_expand(sk);\ttcp_mstamp_refresh(tp);\ttp-&gt;rcvq_space.time = tp-&gt;tcp_mstamp;\ttp-&gt;rcvq_space.seq = tp-&gt;copied_seq;\t //这个在三次握手的中间也调用过，用于告诉对端本端窗口的大小，是根据rcv_buf计算出来的\tmaxwin = tcp_full_space(sk);\tif (tp-&gt;window_clamp &gt;= maxwin) &#123;\t\ttp-&gt;window_clamp = maxwin; //限制到最大缓存 这个可以理解为真实的缓存\t\t//最终大概率window_clamp = maxwin\t\tif (tcp_app_win &amp;&amp; maxwin &gt; 4 * tp-&gt;advmss)\t\t\ttp-&gt;window_clamp = max(maxwin -\t\t\t\t\t       (maxwin &gt;&gt; tcp_app_win),\t\t\t\t\t       4 * tp-&gt;advmss);\t&#125;\t/* Force reservation of one segment. */\t//接收窗口要让出一个mss\tif (tcp_app_win &amp;&amp;\t    tp-&gt;window_clamp &gt; 2 * tp-&gt;advmss &amp;&amp;\t    tp-&gt;window_clamp + tp-&gt;advmss &gt; maxwin)\t\ttp-&gt;window_clamp = max(2 * tp-&gt;advmss, maxwin - tp-&gt;advmss);\t//tp-&gt;rcv_ssthresh  这个值是发送synack的时候计算出来的(根据rcv_buf)\t///这个叫什么接收慢启动阈值？，就应该叫接受窗口阈值吧\ttp-&gt;rcv_ssthresh = min(tp-&gt;rcv_ssthresh, tp-&gt;window_clamp);\ttp-&gt;snd_cwnd_stamp = tcp_jiffies32;\ttp-&gt;rcvq_space.space = min3(tp-&gt;rcv_ssthresh, tp-&gt;rcv_wnd,   //预测未来的接收缓冲区状态？？\t\t\t\t    (u32)TCP_INIT_CWND * tp-&gt;advmss);&#125;\n\ntcp_init_buffer_space中，首先根据rcv_buf计算出full_space这个值可以理解为实际数据包的可以使用的窗口大小，根据这个大小，去设置最大的窗口的带线啊哦，并让出一个mss的空间（防止死锁？？），之后更新rcv_ssthresh(计算tcp窗口大小字段的时候会用到）。\n回到tcp_rcv_state_process中，初始化上述字段后，会根据确认号设置设置una，之后根据对端通告的窗口大小和缩放因子，计算发送窗口的大小**，最后初始化了接收的mss因为在 TCP 建立连接初期，本端还没有直接得到对端的 MSS 信息，所以只计算了一个估计值，核心思想是宁可低估，也不能高估**，这值最终会影响ack的触发，具体代码如下所示：\nvoid tcp_initialize_rcv_mss(struct sock *sk)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\tunsigned int hint = min_t(unsigned int, tp-&gt;advmss, tp-&gt;mss_cache);\thint = min(hint, tp-&gt;rcv_wnd / 2);\thint = min(hint, TCP_MSS_DEFAULT);\thint = max(hint, TCP_MIN_MSS);\tinet_csk(sk)-&gt;icsk_ack.rcv_mss = hint;&#125;\n\n最后调用tcp_fast_path_on设置了**TCP fastpath**中需要的标志位，其实就是根据当前的窗口大小，包头长度，ack标志到flag字段中，这个flag会在接下来收到数据包的时候判断是否可以直接走快速路径，设置快速路径的代码如下所示：\nstatic inline void tcp_fast_path_on(struct tcp_sock *tp)&#123;\t__tcp_fast_path_on(tp, tp-&gt;snd_wnd &gt;&gt; tp-&gt;rx_opt.snd_wscale);&#125;static inline void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)&#123;\t/* mptcp hooks are only on the slow path */\tif (sk_is_mptcp((struct sock *)tp))\t\treturn;\ttp-&gt;pred_flags = htonl((tp-&gt;tcp_header_len &lt;&lt; 26) |\t\t\t       ntohl(TCP_FLAG_ACK) |\t\t\t       snd_wnd);&#125;\n\n至此，服务端的三次握手逻辑就结束了~~~。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP三次握手-接收ack (一)","url":"/2025/09/18/TCP%E8%A2%AB%E5%8A%A8%E6%89%93%E5%BC%80%E6%8E%A5%E6%94%B6ack%EF%BC%88%E4%B8%80%EF%BC%89/","content":"还是从tcp_v4_rcv说起，当接收到客户端发送的最后一个ack 后首先调用__inet_lookup_skb查找所属的sock，对于服务端来说，第一次握手找到的是listensock，收到客户端的ack后查找到的应该是reqsock，具体代码如下所示\nint tcp_v4_rcv(struct sk_buff *skb)&#123;    ...    sk = __inet_lookup_skb(net-&gt;ipv4.tcp_death_row.hashinfo,                   skb, __tcp_hdrlen(th), th-&gt;source,                   th-&gt;dest, sdif, &amp;refcounted);    ...&#125;\n\n__inet_lookup_skb第一次握手的时候也调用过，进一步会调用__inet_lookup：\nstatic inline struct sock *__inet_lookup(struct net *net,                     struct inet_hashinfo *hashinfo,                     struct sk_buff *skb, int doff,                     const __be32 saddr, const __be16 sport,                     const __be32 daddr, const __be16 dport,                     const int dif, const int sdif,                     bool *refcounted)&#123;    u16 hnum = ntohs(dport);    struct sock *sk;    //查找ehash这里其实用的就是五元组加网口    sk = __inet_lookup_established(net, hashinfo, saddr, sport,                       daddr, hnum, dif, sdif);    *refcounted = true;    if (sk)        return sk;    *refcounted = false;    //如果上面ehash中没有找到，那就找listenhash    return __inet_lookup_listener(net, hashinfo, skb, doff, saddr,                      sport, daddr, hnum, dif, sdif);&#125;\n\n这里第三次握手是调用的__inet_lookup_established找到的是req，其实就是根据四元组计算一个hash值然后找到对应的hash桶，遍历这个桶，比较ip和port，如果匹配则进一步比较网口，如果sk指定了网口，但是不是收包的网口（如果是reqsock会绑定网口吗？），则也返回false具体代码如下所示：\nstruct sock *__inet_lookup_established(struct net *net,                  struct inet_hashinfo *hashinfo,                  const __be32 saddr, const __be16 sport,                  const __be32 daddr, const u16 hnum,                  const int dif, const int sdif)&#123;    INET_ADDR_COOKIE(acookie, saddr, daddr);//计算源ip目的ip的组合 比较key的时候会用到    //组合源port和目的port    const __portpair ports = INET_COMBINED_PORTS(sport, hnum);    struct sock *sk;    const struct hlist_nulls_node *node;    /* Optimize here for direct hit, only listening connections can     * have wildcards anyways.     */    //四元组计算一个hash值    unsigned int hash = inet_ehashfn(net, daddr, hnum, saddr, sport);    unsigned int slot = hash &amp; hashinfo-&gt;ehash_mask;    //找到具体的hash桶    struct inet_ehash_bucket *head = &amp;hashinfo-&gt;ehash[slot];begin:    sk_nulls_for_each_rcu(sk, node, &amp;head-&gt;chain) &#123;        if (sk-&gt;sk_hash != hash)            continue;        //比较ip和port还有网口是否相同        if (likely(inet_match(net, sk, acookie, ports, dif, sdif))) &#123;            if (unlikely(!refcount_inc_not_zero(&amp;sk-&gt;sk_refcnt)))//正在被释放？                goto out;            if (unlikely(!inet_match(net, sk, acookie, //再次比较看是否匹配                         ports, dif, sdif))) &#123;                sock_gen_put(sk);                goto begin;//再找一次            &#125;            goto found;        &#125;    &#125;    /*     * if the nulls value we got at the end of this lookup is     * not the expected one, we must restart lookup.     * We probably met an item that was moved to another chain.     */    if (get_nulls_value(node) != slot)        goto begin;out:    sk = NULL;found:    return sk;&#125;\n\n在tcp_v4_rcv中找到&#96;&#96;sock之后会根据sk_state（此时是TCP_NEW_SYN_RECV&#96;状态）进入对应的分支，具体代码如下所示：\nint tcp_v4_rcv(struct sk_buff *skb)&#123;    ...    //如果是正常的三次握手，服务端收到客户端的ack包就会进入这个分支    if (sk-&gt;sk_state == TCP_NEW_SYN_RECV) &#123;        //先获取管理半连接状态的结构        struct request_sock *req = inet_reqsk(sk);        bool req_stolen = false;        struct sock *nsk;        //这个sk是监听套接字        sk = req-&gt;rsk_listener;        //ipsec相关        if (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))            drop_reason = SKB_DROP_REASON_XFRM_POLICY;        else            drop_reason = tcp_inbound_md5_hash(sk, skb,                           &amp;iph-&gt;saddr, &amp;iph-&gt;daddr,                           AF_INET, dif, sdif);        if (unlikely(drop_reason)) &#123;            sk_drops_add(sk, skb);            reqsk_put(req);            goto discard_it;        &#125;        //计算校验和，如果这里硬件计算过了，软件就不算了        if (tcp_checksum_complete(skb)) &#123;            reqsk_put(req);            goto csum_error;        &#125;        //这里sk是listen的sk，会不是listen状态吗？比如close了？        if (unlikely(sk-&gt;sk_state != TCP_LISTEN)) &#123;            //如果启用了端口重用看看能不找到一个可以替代的sk，然后继续，否则goto要重新查找一下            //这里如果再找一便没找到是不是就直接rst了？？？            nsk = reuseport_migrate_sock(sk, req_to_sk(req), skb);            if (!nsk) &#123;                inet_csk_reqsk_queue_drop_and_put(sk, req);                goto lookup;            //这里是找到了可以迁移的sock            sk = nsk;            /* reuseport_migrate_sock() has already held one sk_refcnt             * before returning.             */        &#125; else &#123;            /* We own a reference on the listener, increase it again             * as we might lose it too soon.             */            sock_hold(sk);        &#125;        refcounted = true;        nsk = NULL;        //bfp相关的过滤钩子，返回0表示被接收        if (!tcp_filter(sk, skb)) &#123;            th = (const struct tcphdr *)skb-&gt;data;            iph = ip_hdr(skb);            //根据数据包字段填充私有控制结构            tcp_v4_fill_cb(skb, iph, th);            //创建新的sock，这里可能返回监听sk，肯那            nsk = tcp_check_req(sk, skb, req, false, &amp;req_stolen);        &#125; else &#123;            drop_reason = SKB_DROP_REASON_SOCKET_FILTER;        &#125;        if (!nsk) &#123;//如果返回空则表示收到的报文可能不合法，打概率默默丢弃了            reqsk_put(req);            if (req_stolen) &#123;                /* Another cpu got exclusive access to req                 * and created a full blown socket.                 * Try to feed this packet to this socket                 * instead of discarding it.                 */                tcp_v4_restore_cb(skb);                sock_put(sk);                goto lookup;            &#125;            goto discard_and_relse;        &#125;        //连接跟踪相关复位        nf_reset_ct(skb);        if (nsk == sk) &#123;            reqsk_put(req);            tcp_v4_restore_cb(skb);        //正常情况下走这个流程并返回0         &#125; else if (tcp_child_process(sk, nsk, skb)) &#123;            tcp_v4_send_reset(nsk, skb);            goto discard_and_relse;        &#125; else &#123;            sock_put(sk);            return 0;        &#125;    &#125;    ...&#125;\n\n上述代码中，收到客户端的ack报文后，首先通过reqsock拿到监听sock然后计算校验和，然后会判断监听套接字的状态是否已经不是listen状态了（比如被close了）如果不是listen状态了则调用reuseport_migrate_sock看看能不找到一个可以替代的sk，具体代码如下所示：\nstruct sock *reuseport_migrate_sock(struct sock *sk,                    struct sock *migrating_sk,                    struct sk_buff *skb)&#123;    struct sock_reuseport *reuse;    struct sock *nsk = NULL;    bool allocated = false;    struct bpf_prog *prog;    u16 socks;    u32 hash;    rcu_read_lock();    //获取端口重用组，如果为空就直接返回了    reuse = rcu_dereference(sk-&gt;sk_reuseport_cb);    if (!reuse)        goto out;    //这个端口重用组中的sock的个数    socks = READ_ONCE(reuse-&gt;num_socks);    if (unlikely(!socks))        goto failure;    /* paired with smp_wmb() in __reuseport_add_sock() */    smp_rmb();    hash = migrating_sk-&gt;sk_hash;    prog = rcu_dereference(reuse-&gt;prog);    //是否根据bpf程序选择，如果不用bpf就使用hash    if (!prog || prog-&gt;expected_attach_type != BPF_SK_REUSEPORT_SELECT_OR_MIGRATE) &#123;        //默认不启用，表示是否能将请求迁移到其他sock        if (READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_migrate_req))             goto select_by_hash;        goto failure;    &#125;    if (!skb) &#123;        skb = alloc_skb(0, GFP_ATOMIC);        if (!skb)            goto failure;        allocated = true;    &#125;    nsk = bpf_run_sk_reuseport(reuse, sk, prog, skb, migrating_sk, hash);    if (allocated)        kfree_skb(skb);select_by_hash:    if (!nsk)    //根据hash和cpu亲和性选择sock，listen的时候也调用这个        nsk = reuseport_select_sock_by_hash(reuse, hash, socks);    if (IS_ERR_OR_NULL(nsk) || unlikely(!refcount_inc_not_zero(&amp;nsk-&gt;sk_refcnt))) &#123;        nsk = NULL;        goto failure;    &#125;out:    rcu_read_unlock();    return nsk;failure:    __NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMIGRATEREQFAILURE);    goto out;&#125;\n\n接下来会调用tcp_filter(安全和bpf的钩子)判断是否放行这个数据包，如果放行则调用tcp_v4_fill_cb根据数据包的信息填充skb 的是私有字段（seq,endseq,ackseq等），之后调用tcp_check_req，注意这里是关键，这里的核心工作可以总结如下：\n\n解析 TCP 选项 &amp; PAWS 检查\n处理 SYN 重传（对端重发 SYN） → 触发重发 SYN+ACK 并重置定时器\n验证 ACK 是否有效（非 TFO）：ACK 对不上的直接交给监听 socket 回 RST\n校验序列号是否在窗口内（不在→发一个 ACK（挑战 ACK）然后丢弃）\n更新时间戳最近值 ts_recent（仅在合适条件）\n剥离越界的 SYN 位（SYN 被认为是“超窗的一个 bit”会被去掉）\n若报文带 RST 或 SYN → 走“胚胎期复位”路径（统计 + RST&#x2F;drop）\n确认 ACK 必须置位（否则静默丢弃）\n非 TFO：TCP_DEFER_ACCEPT 做“空 ACK 丢弃”（应用要求有数据才唤醒）\n创建子 socket（由半连接变成全连接）→ hashdance 完成三次握手\n监听队列溢出：根据 sysctl 行为选择丢弃或发 RST\n错误&#x2F;复位路径清理 request，并更新统计\n\n具体代码如下所示：\nstruct sock *tcp_check_req(struct sock *sk, struct sk_buff *skb,               struct request_sock *req,               bool fastopen, bool *req_stolen)&#123;    struct tcp_options_received tmp_opt;    struct sock *child;    const struct tcphdr *th = tcp_hdr(skb);    __be32 flg = tcp_flag_word(th) &amp; (TCP_FLAG_RST|TCP_FLAG_SYN|TCP_FLAG_ACK);    bool paws_reject = false;    bool own_req;    tmp_opt.saw_tstamp = 0;    if (th-&gt;doff &gt; (sizeof(struct tcphdr)&gt;&gt;2)) &#123;        //处理数据包的选项，在接收syn段的时候调用过了        tcp_parse_options(sock_net(sk), skb, &amp;tmp_opt, 0, NULL);        //有时间戳选项        if (tmp_opt.saw_tstamp) &#123;            tmp_opt.ts_recent = READ_ONCE(req-&gt;ts_recent);//注意：这里是之前收到syn包中的时间戳            if (tmp_opt.rcv_tsecr)//回显的时间戳                tmp_opt.rcv_tsecr -= tcp_rsk(req)-&gt;ts_off;//减去偏移            /* We do not store true stamp, but it is not required,             * it can be estimated (approximately)             * from another data.             */            //计算一个时间点，用当前时间，减去第二次握手可以容忍的最大时间，也就是不会有比这个时间点更早的包？？这个时间用来是否可能发生序列号回绕            tmp_opt.ts_recent_stamp = ktime_get_seconds() - reqsk_timeout(req, TCP_RTO_MAX) / HZ;            //返回false表示通过            paws_reject = tcp_paws_reject(&amp;tmp_opt, th-&gt;rst);        &#125;    &#125;    /* Check for pure retransmitted SYN. */    //处理纯syn包的重传，这里其实就是重新调用了一下send_synack    if (TCP_SKB_CB(skb)-&gt;seq == tcp_rsk(req)-&gt;rcv_isn &amp;&amp;        flg == TCP_FLAG_SYN &amp;&amp;        !paws_reject) &#123;        /*         * RFC793 draws (Incorrectly! It was fixed in RFC1122)         * this case on figure 6 and figure 8, but formal         * protocol description says NOTHING.         * To be more exact, it says that we should send ACK,         * because this segment (at least, if it has no data)         * is out of window.         *         *  CONCLUSION: RFC793 (even with RFC1122) DOES NOT         *  describe SYN-RECV state. All the description         *  is wrong, we cannot believe to it and should         *  rely only on common sense and implementation         *  experience.         *         * Enforce &quot;SYN-ACK&quot; according to figure 8, figure 6         * of RFC793, fixed by RFC1122.         *         * Note that even if there is new data in the SYN packet         * they will be thrown away too.         *         * Reset timer after retransmitting SYNACK, similar to         * the idea of fast retransmit in recovery.         */        //这里传入了last_oow_ack_time,也就是上次发送synack的时间，需要进行安全相关的检查        //如果检查通过了直接重传synack包并重新设置synack定时器        if (!tcp_oow_rate_limited(sock_net(sk), skb,                      LINUX_MIB_TCPACKSKIPPEDSYNRECV,                      &amp;tcp_rsk(req)-&gt;last_oow_ack_time) &amp;&amp;            !inet_rtx_syn_ack(sk, req)) &#123;//重传synack            unsigned long expires = jiffies;            expires += reqsk_timeout(req, TCP_RTO_MAX);            if (!fastopen)                mod_timer_pending(&amp;req-&gt;rsk_timer, expires);            else                req-&gt;rsk_timer.expires = expires;        &#125;        return NULL;//外面什么也不做    &#125;    /* Further reproduces section &quot;SEGMENT ARRIVES&quot;       for state SYN-RECEIVED of RFC793.       It is broken, however, it does not work only       when SYNs are crossed.       You would think that SYN crossing is impossible here, since       we should have a SYN_SENT socket (from connect()) on our end,       but this is not true if the crossed SYNs were sent to both       ends by a malicious third party.  We must defend against this,       and to do that we first verify the ACK (as per RFC793, page       36) and reset if it is invalid.  Is this a true full defense?       To convince ourselves, let us consider a way in which the ACK       test can still pass in this &#x27;malicious crossed SYNs&#x27; case.       Malicious sender sends identical SYNs (and thus identical sequence       numbers) to both A and B:        A: gets SYN, seq=7        B: gets SYN, seq=7       By our good fortune, both A and B select the same initial       send sequence number of seven :-)        A: sends SYN|ACK, seq=7, ack_seq=8        B: sends SYN|ACK, seq=7, ack_seq=8       So we are now A eating this SYN|ACK, ACK test passes.  So       does sequence test, SYN is truncated, and thus we consider       it a bare ACK.       If icsk-&gt;icsk_accept_queue.rskq_defer_accept, we silently drop this       bare ACK.  Otherwise, we create an established connection.  Both       ends (listening sockets) accept the new incoming connection and try       to talk to each other. 8-)       Note: This case is both harmless, and rare.  Possibility is about the       same as us discovering intelligent life on another plant tomorrow.       But generally, we should (RFC lies!) to accept ACK       from SYNACK both here and in tcp_rcv_state_process().       tcp_rcv_state_process() does not, hence, we do not too.       Note that the case is absolutely generic:       we cannot optimize anything here without       violating protocol. All the checks must be made       before attempt to create socket.     */    /* RFC793 page 36: &quot;If the connection is in any non-synchronized state ...     *                  and the incoming segment acknowledges something not yet     *                  sent (the segment carries an unacceptable ACK) ...     *                  a reset is sent.&quot;     *     * Invalid ACK: reset will be sent by listening socket.     * Note that the ACK validity check for a Fast Open socket is done     * elsewhere and is checked directly against the child socket rather     * than req because user data may have been sent out.     */    //这个好像叫做处理交叉syn，或者ack不合法这里返回了listen sock listensock会回rst    if ((flg &amp; TCP_FLAG_ACK) &amp;&amp; !fastopen &amp;&amp;        (TCP_SKB_CB(skb)-&gt;ack_seq !=         tcp_rsk(req)-&gt;snt_isn + 1))        return sk;    /* Also, it would be not so bad idea to check rcv_tsecr, which     * is essentially ACK extension and too early or too late values     * should cause reset in unsynchronized states.     */    /* RFC793: &quot;first check sequence number&quot;. */    //注意：这里如果paws不通过的话，或者序号不在接收窗口内（这里感觉和上面的判断有点冲突呢？ 上面仅保证的是ack？    // 因为走到这个分支可能不光是ack?上面分支直接就rst了， 这里可能是回一个挑战ack）比如ack正确，但数据包太大了？？？ 或者 paw返回ture？    if (paws_reject || !tcp_in_window(TCP_SKB_CB(skb)-&gt;seq, TCP_SKB_CB(skb)-&gt;end_seq,                      tcp_rsk(req)-&gt;rcv_nxt, tcp_rsk(req)-&gt;rcv_nxt + req-&gt;rsk_rcv_wnd)) &#123;        /* Out of window: send ACK and drop. */        if (!(flg &amp; TCP_FLAG_RST) &amp;&amp; //没有rst段，回一个挑战ack            !tcp_oow_rate_limited(sock_net(sk), skb,                      LINUX_MIB_TCPACKSKIPPEDSYNRECV,                      &amp;tcp_rsk(req)-&gt;last_oow_ack_time))            req-&gt;rsk_ops-&gt;send_ack(sk, skb, req);        if (paws_reject)            NET_INC_STATS(sock_net(sk), LINUX_MIB_PAWSESTABREJECTED);        return NULL;//默默丢弃    &#125;    /* In sequence, PAWS is OK. */    /* TODO: We probably should defer ts_recent change once     * we take ownership of @req.     */    //如果启用了时间戳选项，且序列号在下一个期待接收的后面就更新req的时间戳    if (tmp_opt.saw_tstamp &amp;&amp; !after(TCP_SKB_CB(skb)-&gt;seq, tcp_rsk(req)-&gt;rcv_nxt))        WRITE_ONCE(req-&gt;ts_recent, tmp_opt.rcv_tsval);    //如果收到数据包的序列号等于对端三次握手的序列号，就清掉syn标志 这是为什么呢？？？，防止后续影响处理？？    if (TCP_SKB_CB(skb)-&gt;seq == tcp_rsk(req)-&gt;rcv_isn) &#123;        /* Truncate SYN, it is out of window starting           at tcp_rsk(req)-&gt;rcv_isn + 1. */        flg &amp;= ~TCP_FLAG_SYN;    &#125;    /* RFC793: &quot;second check the RST bit&quot; and     *     &quot;fourth, check the SYN bit&quot;     */    //走到这里表示已经验证过ack的合法性，正常情况下应该只有ack，这里直接goto    if (flg &amp; (TCP_FLAG_RST|TCP_FLAG_SYN)) &#123;        TCP_INC_STATS(sock_net(sk), TCP_MIB_ATTEMPTFAILS);        goto embryonic_reset;    &#125;    /* ACK sequence verified above, just make sure ACK is     * set.  If ACK not set, just silently drop the packet.     *     * XXX (TFO) - if we ever allow &quot;data after SYN&quot;, the     * following check needs to be removed.     */    //上面判断过 ack_seq == snt_isn + 1 这里再次确定报文一定带ack，感觉这个函数里面的逻辑有点乱套啊    if (!(flg &amp; TCP_FLAG_ACK))        return NULL;    /* For Fast Open no more processing is needed (sk is the     * child socket).     */    if (fastopen)        return sk;    /* While TCP_DEFER_ACCEPT is active, drop bare ACK. */    //用户set sockopt设置rskq_defer_accept 则默默丢弃不带数据的ack    if (req-&gt;num_timeout &lt; READ_ONCE(inet_csk(sk)-&gt;icsk_accept_queue.rskq_defer_accept) &amp;&amp;        TCP_SKB_CB(skb)-&gt;end_seq == tcp_rsk(req)-&gt;rcv_isn + 1) &#123;        inet_rsk(req)-&gt;acked = 1;        __NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDEFERACCEPTDROP);        return NULL; //外层什么也不做    &#125;    /* OK, ACK is valid, create big socket and     * feed this segment to it. It will repeat all     * the tests. THIS SEGMENT MUST MOVE SOCKET TO     * ESTABLISHED STATE. If it will be dropped after     * socket is created, wait for troubles.     */    //这里返回了新创建的sock，并插入了ehash移除了原来的req    child = inet_csk(sk)-&gt;icsk_af_ops-&gt;syn_recv_sock(sk, skb, req, NULL,                             req, &amp;own_req);    if (!child)        goto listen_overflow;    //这里直接跳过    if (own_req &amp;&amp; rsk_drop_req(req)) &#123;        reqsk_queue_removed(&amp;inet_csk(req-&gt;rsk_listener)-&gt;icsk_accept_queue, req);        inet_csk_reqsk_queue_drop_and_put(req-&gt;rsk_listener, req);        return child;    &#125;    //skb的hash 给到sk的hash    sock_rps_save_rxhash(child, skb);    //注意这里更新了rtt    tcp_synack_rtt_meas(child, req);    *req_stolen = !own_req;    return inet_csk_complete_hashdance(sk, child, req, own_req);listen_overflow:    if (sk != req-&gt;rsk_listener)        __NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMIGRATEREQFAILURE);    if (!READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_abort_on_overflow)) &#123;        inet_rsk(req)-&gt;acked = 1;        return NULL;    &#125;embryonic_reset:    //如果收到的包不含rst 那就主动回一个rst    if (!(flg &amp; TCP_FLAG_RST)) &#123;        /* Received a bad SYN pkt - for TFO We try not to reset         * the local connection unless it&#x27;s really necessary to         * avoid becoming vulnerable to outside attack aiming at         * resetting legit local connections.         */        req-&gt;rsk_ops-&gt;send_reset(sk, skb);    &#125; else if (fastopen) &#123; /* received a valid RST pkt */        reqsk_fastopen_remove(sk, req, true);        tcp_reset(sk, skb);    &#125;    if (!fastopen) &#123;        //从监听 socket 的半连接队列里移除一个 request_sock        bool unlinked = inet_csk_reqsk_queue_drop(sk, req);        if (unlinked)            __NET_INC_STATS(sock_net(sk), LINUX_MIB_EMBRYONICRSTS);        *req_stolen = !unlinked; //表示不要二次释放    &#125;    return NULL;&#125;","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP三次握手-接收ack (三)","url":"/2025/09/22/TCP%E8%A2%AB%E5%8A%A8%E6%89%93%E5%BC%80%E6%8E%A5%E6%94%B6ack%EF%BC%88%E4%B8%89%EF%BC%89/","content":"回到tcp_v4_syn_recv_sock中，完成新sock的创建后，设置了sk的GSO的类型，设置了地址，所属的网口索引，ipoption，多播相关等字段。然后随机生成ipid, 并根据系统选项设置数据包的tos字段，随后调用查路由的接口，并调用sk_setup_caps设置GSO（注意，这里如果是TCP协议会默认设置上NETIF_F_GSO标志，这个标志里面其实包括了驱动注册的TSO标志位，而不管驱动是否真正注册了TSO能力，从协议栈的视角看，就是支持TSO的，而是否真正卸载分段的工作是在数据包交给网卡驱动前validate_xmit_skb判断的）上述sk_setup_caps代码如下所示：\nvoid sk_setup_caps(struct sock *sk, struct dst_entry *dst)&#123;\tu32 max_segs = 1;\t//这里拿到设备注册的features\tsk-&gt;sk_route_caps = dst-&gt;dev-&gt;features;\tif (sk_is_tcp(sk)) //tcp协议默认注册GSO\t\tsk-&gt;sk_route_caps |= NETIF_F_GSO;\tif (sk-&gt;sk_route_caps &amp; NETIF_F_GSO)\t\tsk-&gt;sk_route_caps |= NETIF_F_GSO_SOFTWARE; //这里是软件兜底，这个标志位中包含TSO标志位\tif (unlikely(sk-&gt;sk_gso_disabled)) //只有md5相关会禁用，正常不会走到这个分支\t\tsk-&gt;sk_route_caps &amp;= ~NETIF_F_GSO_MASK;\tif (sk_can_gso(sk)) &#123; //如果是TCP这里一定会走到\t\tif (dst-&gt;header_len &amp;&amp; !xfrm_dst_offload_ok(dst)) &#123; \t\t\tsk-&gt;sk_route_caps &amp;= ~NETIF_F_GSO_MASK;\t\t&#125; else &#123; //大概率走这个分支\t\t\tsk-&gt;sk_route_caps |= NETIF_F_SG | NETIF_F_HW_CSUM;\t\t\tsk-&gt;sk_gso_max_size = sk_dst_gso_max_size(sk, dst); //这里大概率返回65535左右\t\t\t/* pairs with the WRITE_ONCE() in netif_set_gso_max_segs() */\t\t\tmax_segs = max_t(u32, READ_ONCE(dst-&gt;dev-&gt;gso_max_segs), 1); \t\t&#125;\t&#125;\tsk-&gt;sk_gso_max_segs = max_segs; //65535左右\tsk_dst_set(sk, dst);//关联dst&#125;\n\n回到tcp_v4_syn_recv_sock中，注册GSO能力后，随后调用tcp_ca_openreq_child设置拥塞算法，手下你判断使用户是否显示设置了拥塞算法，如果没有设置，则使用系统默认的拥塞算法，具体代码如下所示：\nvoid tcp_ca_openreq_child(struct sock *sk, const struct dst_entry *dst)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tu32 ca_key = dst_metric(dst, RTAX_CC_ALGO);\tbool ca_got_dst = false;\t//看看用户是否配置,大概率不走这里优先从路由表 metric 获取指定的拥塞控制算法\tif (ca_key != TCP_CA_UNSPEC) &#123;\t\tconst struct tcp_congestion_ops *ca;\t\trcu_read_lock();\t\tca = tcp_ca_find_key(ca_key);\t\tif (likely(ca &amp;&amp; bpf_try_module_get(ca, ca-&gt;owner))) &#123;\t\t\ticsk-&gt;icsk_ca_dst_locked = tcp_ca_dst_locked(dst);\t\t\ticsk-&gt;icsk_ca_ops = ca;\t\t\tca_got_dst = true;\t\t&#125;\t\trcu_read_unlock();\t&#125;\t//如果用户们没有配置，则调用tcp_assign_congestion_control使用系统默认的\t/* If no valid choice made yet, assign current system default ca. */\tif (!ca_got_dst &amp;&amp;\t    (!icsk-&gt;icsk_ca_setsockopt ||\t     !bpf_try_module_get(icsk-&gt;icsk_ca_ops, icsk-&gt;icsk_ca_ops-&gt;owner)))\t\ttcp_assign_congestion_control(sk);\t//注意这里设置了拥塞状态\ttcp_set_ca_state(sk, TCP_CA_Open);&#125;\n\n回到tcp_v4_syn_recv_sock中，设置完拥塞算法后，计算tp的mss和通告的mss，这里其实就是计算了两个mss一个是mss_cache,一个是adv_mss前者是通过tcp_sync_mss计算的，会根据系统最小值和协商值以及窗口大小值进行比较后设置，后者是根据设备的mtu设置，超时重传中具体介绍过这个函数，具体代码如下所示：\n //三次握手，icmp报文，或者tcp的mtu探测  超时重传貌似都会调用它unsigned int tcp_sync_mss(struct sock *sk, u32 pmtu)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tint mss_now;\t//当前传入的pmtu小于历史 的最大值，缩小范围\tif (icsk-&gt;icsk_mtup.search_high &gt; pmtu)\t\ticsk-&gt;icsk_mtup.search_high = pmtu;\t\t//计算当前的mss\tmss_now = tcp_mtu_to_mss(sk, pmtu);\t//根据窗口大小调整再次调整mss,可能会变小\tmss_now = tcp_bound_to_half_wnd(tp, mss_now);\t/* And store cached results */\t//把传进来的pmtu保存起来\ticsk-&gt;icsk_pmtu_cookie = pmtu;\t//这个enble 在tcp超时的retry1中进行黑洞检查的时候会使能正常应该不会走到这个分支吧\tif (icsk-&gt;icsk_mtup.enabled)\t\t//如果使能的mtu 探测， 那可能会再次变小这个mss ，这个mss的值不能超过search_low\t\tmss_now = min(mss_now, tcp_mtu_to_mss(sk, icsk-&gt;icsk_mtup.search_low));\t//\ttp-&gt;mss_cache = mss_now;\treturn mss_now;&#125;static inline u16 tcp_mss_clamp(const struct tcp_sock *tp, u16 mss)&#123;\t/* We use READ_ONCE() here because socket might not be locked.\t * This happens for listeners.\t */\tu16 user_mss = READ_ONCE(tp-&gt;rx_opt.user_mss);\treturn (user_mss &amp;&amp; user_mss &lt; mss) ? user_mss : mss;&#125;\n\n随后调用tcp_initialize_rcv_mss计算的两个mss中选个最小值，给延迟ack使用, 具体代码如下所示：\nvoid tcp_initialize_rcv_mss(struct sock *sk)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\tunsigned int hint = min_t(unsigned int, tp-&gt;advmss, tp-&gt;mss_cache);\thint = min(hint, tp-&gt;rcv_wnd / 2);\thint = min(hint, TCP_MSS_DEFAULT);\thint = max(hint, TCP_MIN_MSS);\tinet_csk(sk)-&gt;icsk_ack.rcv_mss = hint;&#125;\n\n随后tcp_v4_syn_recv_sock中调用__inet_inherit_port将新创建sock加入到bhash中 因为bhash是管理端口的hash表，新创建的sock也理所应当被管理，具体代码如下所示：\nint __inet_inherit_port(const struct sock *sk, struct sock *child)&#123;\tstruct inet_hashinfo *table = tcp_or_dccp_get_hashinfo(sk);\tunsigned short port = inet_sk(child)-&gt;inet_num;\tstruct inet_bind_hashbucket *head, *head2;\tbool created_inet_bind_bucket = false;\tstruct net *net = sock_net(sk);\tbool update_fastreuse = false;\tstruct inet_bind2_bucket *tb2;\tstruct inet_bind_bucket *tb;\tint bhash, l3mdev;\tbhash = inet_bhashfn(net, port, table-&gt;bhash_size);\thead = &amp;table-&gt;bhash[bhash]; //bhash桶\thead2 = inet_bhashfn_portaddr(table, child, net, port);//bhash2的桶\tspin_lock(&amp;head-&gt;lock);\tspin_lock(&amp;head2-&gt;lock);\t//监听套接字的bhash桶和监听套接字的bhash2桶\ttb = inet_csk(sk)-&gt;icsk_bind_hash;\ttb2 = inet_csk(sk)-&gt;icsk_bind2_hash;\tif (unlikely(!tb || !tb2)) &#123;\t\tspin_unlock(&amp;head2-&gt;lock);\t\tspin_unlock(&amp;head-&gt;lock);\t\treturn -ENOENT;\t&#125;\t//几乎不会走这个分支吧\tif (tb-&gt;port != port) &#123;\t\tl3mdev = inet_sk_bound_l3mdev(sk);\t\t/* NOTE: using tproxy and redirecting skbs to a proxy\t\t * on a different listener port breaks the assumption\t\t * that the listener socket&#x27;s icsk_bind_hash is the same\t\t * as that of the child socket. We have to look up or\t\t * create a new bind bucket for the child here. */\t\tinet_bind_bucket_for_each(tb, &amp;head-&gt;chain) &#123;\t\t\tif (inet_bind_bucket_match(tb, net, port, l3mdev))\t\t\t\tbreak;\t\t&#125;\t\tif (!tb) &#123;\t\t\ttb = inet_bind_bucket_create(table-&gt;bind_bucket_cachep,\t\t\t\t\t\t     net, head, port, l3mdev);\t\t\tif (!tb) &#123;\t\t\t\tspin_unlock(&amp;head2-&gt;lock);\t\t\t\tspin_unlock(&amp;head-&gt;lock);\t\t\t\treturn -ENOMEM;\t\t\t&#125;\t\t\tcreated_inet_bind_bucket = true;\t\t&#125;\t\tupdate_fastreuse = true;\t\tgoto bhash2_find;\t//这里是端口相同ip地址不同的情况进入这个分支 比如父sock监听在0.0.0.0:8080新的sock实际使用的是 192.168.1.1:8080\t//这里注意：如果新sock和listensock地址都相同那也不进入这个分支\t&#125; else if (!inet_bind2_bucket_addr_match(tb2, child)) &#123; \t\tl3mdev = inet_sk_bound_l3mdev(sk);bhash2_find:\t\t//遍历 bhash2 的桶中查找是否有一个条目可以匹配地址和ip\t\ttb2 = inet_bind2_bucket_find(head2, net, port, l3mdev, child);\t\tif (!tb2) &#123;\t\t\t//没有的话就创建一个\t\t\ttb2 = inet_bind2_bucket_create(table-&gt;bind2_bucket_cachep,\t\t\t\t\t\t       net, head2, port,\t\t\t\t\t\t       l3mdev, child);\t\t\tif (!tb2)\t\t\t\tgoto error;\t\t&#125;\t&#125;\tif (update_fastreuse)\t\t//更新fastreuse字段\t\tinet_csk_update_fastreuse(tb, child);\t//将子sock和 tb 和tb2关联起来\tinet_bind_hash(child, tb, tb2, port);\tspin_unlock(&amp;head2-&gt;lock);\tspin_unlock(&amp;head-&gt;lock);\treturn 0;error:\tif (created_inet_bind_bucket)\t\tinet_bind_bucket_destroy(table-&gt;bind_bucket_cachep, tb);\tspin_unlock(&amp;head2-&gt;lock);\tspin_unlock(&amp;head-&gt;lock);\treturn -ENOMEM;&#125;\n\n回到tcp_v4_syn_recv_sock中，这里到最后一步了，会调用inet_ehash_nolisten将新创建的sock插入ehash中，并移除原来插入的reqsock移除具体代码如下所示：\nbool inet_ehash_nolisten(struct sock *sk, struct sock *osk, bool *found_dup_sk)&#123;\tbool ok = inet_ehash_insert(sk, osk, found_dup_sk);\tif (ok) &#123;\t\tsock_prot_inuse_add(sock_net(sk), sk-&gt;sk_prot, 1);\t&#125; else &#123;\t\tthis_cpu_inc(*sk-&gt;sk_prot-&gt;orphan_count);\t\tinet_sk_set_state(sk, TCP_CLOSE);\t\tsock_set_flag(sk, SOCK_DEAD);\t\tinet_csk_destroy_sock(sk);\t&#125;\treturn ok;&#125;//三次握手被动打开接收syn包后会把reqsock插入(后两个参数为NULL)//三次握手接收ack的后两个参数不为空bool inet_ehash_insert(struct sock *sk, struct sock *osk, bool *found_dup_sk)&#123;\tstruct inet_hashinfo *hashinfo = tcp_or_dccp_get_hashinfo(sk);\tstruct inet_ehash_bucket *head;\tstruct hlist_nulls_head *list;\tspinlock_t *lock;\tbool ret = true;\tWARN_ON_ONCE(!sk_unhashed(sk));\tsk-&gt;sk_hash = sk_ehashfn(sk);\t//ehash的桶\thead = inet_ehash_bucket(hashinfo, sk-&gt;sk_hash);\tlist = &amp;head-&gt;chain;\tlock = inet_ehash_lockp(hashinfo, sk-&gt;sk_hash);\tspin_lock(lock);\t//如果 osk 不为空（三次握手收到ACK）\tif (osk) &#123;\t\tWARN_ON_ONCE(sk-&gt;sk_hash != osk-&gt;sk_hash);\t\t//这里删除了req\t\tret = sk_nulls_del_node_init_rcu(osk);\t//三次握手收到syn\t&#125; else if (found_dup_sk) &#123;\t\t*found_dup_sk = inet_ehash_lookup_by_sk(sk, list);\t\tif (*found_dup_sk)\t\t\tret = false;\t&#125;\tif (ret)\t//插入sock\t\t__sk_nulls_add_node_rcu(sk, list);\tspin_unlock(lock);\treturn ret;&#125;\n\ntcp_v4_syn_recv_sock中，如果将新创建的sk成功插入ehash后，会直接返回新创建的sock，正常三次握手流程到这里就会返回了新创建的sock返回后，tcp_check_req中就拿到了新创建的sock，之后还会做两个主要的工作，一个是调用tcp_synack_rtt_meas计算rtt，另一个是将req从半连接队列移除，这里回到tcp_check_req中，首先看计算rtt的逻辑：\nvoid tcp_synack_rtt_meas(struct sock *sk, struct request_sock *req)&#123;\tstruct rate_sample rs;\tlong rtt_us = -1L;\t//如果没有重传过，则根据当当前时间(收到ack)和发送synack的时间计算一个rtt\tif (req &amp;&amp; !req-&gt;num_retrans &amp;&amp; tcp_rsk(req)-&gt;snt_synack)\t\trtt_us = tcp_stamp_us_delta(tcp_clock_us(), tcp_rsk(req)-&gt;snt_synack);\t//注意这里的里标志是synack\ttcp_ack_update_rtt(sk, FLAG_SYN_ACKED, rtt_us, -1L, rtt_us, &amp;rs);&#125;//三次握手收到ack创建新的sock后会调用这个第3和第6个传入的值相同，flag为synack //清理重传队列中也会调用这个函数static bool \ttcp_ack_update_rtt(struct sock *sk, const int flag,\t\t\t       long seq_rtt_us, long sack_rtt_us,\t\t\t       long ca_rtt_us, struct rate_sample *rs)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\t/* Prefer RTT measured from ACK&#x27;s timing to TS-ECR. This is because\t * broken middle-boxes or peers may corrupt TS-ECR fields. But\t * Karn&#x27;s algorithm forbids taking RTT if some retransmitted data\t * is acked (RFC6298).\t */\t//如果 seq_rtt_us无效则尝试使用sack_rtt_us（如果有 SACK 信息）\tif (seq_rtt_us &lt; 0)\t\tseq_rtt_us = sack_rtt_us;\t/* RTTM Rule: A TSecr value received in a segment is used to\t * update the averaged RTT measurement only if the segment\t * acknowledges some new data, i.e., only if it advances the\t * left edge of the send window.\t * See draft-ietf-tcplw-high-performance-00, section 3.3.\t */\t//如果基于序列号时间戳无效，则用时间戳选项计算（如果有的话），比如三次握手中是不会走这个分支的\tif (seq_rtt_us &lt; 0 &amp;&amp; tp-&gt;rx_opt.saw_tstamp &amp;&amp; tp-&gt;rx_opt.rcv_tsecr &amp;&amp;\t    flag &amp; FLAG_ACKED) &#123;\t\t//当前的时间减去回显的时间\t\tu32 delta = tcp_time_stamp(tp) - tp-&gt;rx_opt.rcv_tsecr;\t\t//单位转换转换为微妙\t\tif (likely(delta &lt; INT_MAX / (USEC_PER_SEC / TCP_TS_HZ))) &#123;\t\t\tif (!delta)\t\t\t\tdelta = 1;\t\t\tseq_rtt_us = delta * (USEC_PER_SEC / TCP_TS_HZ);\t\t\tca_rtt_us = seq_rtt_us;\t\t&#125;\t&#125;\t//记录rtt的结果\trs-&gt;rtt_us = ca_rtt_us; /* RTT of last (S)ACKed packet (or -1) */\tif (seq_rtt_us &lt; 0)\t\treturn false;\t/* ca_rtt_us &gt;= 0 is counting on the invariant that ca_rtt_us is\t * always taken together with ACK, SACK, or TS-opts. Any negative\t * values will be skipped with the seq_rtt_us &lt; 0 check above.\t */\t//\ttcp_update_rtt_min(sk, ca_rtt_us, flag);\t//更新平滑 RTT（SRTT）\ttcp_rtt_estimator(sk, seq_rtt_us);\t//设置rto usecs_to_jiffies((tp-&gt;srtt_us &gt;&gt; 3) + tp-&gt;rttvar_us);\ttcp_set_rto(sk);\t/* RFC6298: only reset backoff on valid RTT measurement. */\tinet_csk(sk)-&gt;icsk_backoff = 0;\treturn true;&#125;\n\n上述代码主要是更新了最小min rtt和平滑srtt（） \n具体逻如下，如果 seq_rtt_us 不可用（这里参数传入的seq_rtt_us的值是当前时间减去发送synack的时间），就使用 sack_rtt_us，如果 seq_rtt_us 仍然无效，但启用时间戳选项并且 ACK 了新数据，就用本地时间戳减去对端回显时间戳（TSecr）来计算 RTT。\n计算出rtt后，也就是说有了此次采样的数据，随后会调用tcp_update_rtt_min更新历史的最小的rtt具体代码如下所示：\nstatic void tcp_update_rtt_min(struct sock *sk, u32 rtt_us, const int flag)&#123;\tu32 wlen = READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_min_rtt_wlen) * HZ; //300s\tstruct tcp_sock *tp = tcp_sk(sk);\t//如果ACK可能被延迟且当前RTT大于已知最小RTT，则忽略此次测量 直接不更新了\tif ((flag &amp; FLAG_ACK_MAYBE_DELAYED) &amp;&amp; rtt_us &gt; tcp_min_rtt(tp)) &#123;\t\t/* If the remote keeps returning delayed ACKs, eventually\t\t * the min filter would pick it up and overestimate the\t\t * prop. delay when it expires. Skip suspected delayed ACKs.\t\t */\t\treturn;\t&#125;\t//更新rttmin字段，rtt_min其实是一个长度为3的数组，每个数组有两个元素，分别是时间和值\tminmax_running_min(&amp;tp-&gt;rtt_min, wlen, tcp_jiffies32,\t\t\t   rtt_us ? : jiffies_to_usecs(1));&#125;\n\ntcp_update_rtt_min中会调用minmax_running_min根据时间（是否过期了）和当前rtt的值更新min_rtt,具体代码如下所示：\nu32 minmax_running_min(struct minmax *m, u32 win, u32 t, u32 meas)&#123;\tstruct minmax_sample val = &#123; .t = t, .v = meas &#125;;\tif (unlikely(val.v &lt;= m-&gt;s[0].v) ||\t  /* found new min? */ //发现了最小值？\t    unlikely(val.t - m-&gt;s[2].t &gt; win))\t  /* nothing left in window? *///最老的是否过期？\t\treturn minmax_reset(m, t, meas);  /* forget earlier samples */ //用最新的值重置这个几个值之后直接返回\tif (unlikely(val.v &lt;= m-&gt;s[1].v))\t\tm-&gt;s[2] = m-&gt;s[1] = val; //更新第二个和第三个\telse if (unlikely(val.v &lt;= m-&gt;s[2].v))\t\tm-&gt;s[2] = val; //更新第三个\t//处理是否过期的情况，条件就是没过期，也没有比最小的rtt小\treturn minmax_subwin_update(m, win, &amp;val);&#125;\n\nminmax_running_min中如果发现当前的rtt比保存的最小的rtt（注意，这里minmax中保存了三个rtt）小或者发现过期了，则直接用当前的测量值复位minmax保存的值。\n如果发现当前的rtt比第二个或者第三个小的话，也进行更新，更新后不直接返回，而是进一步调用minmax_subwin_update处理是否有过期的情况，具体逻辑如下所示：\nstatic u32 minmax_subwin_update(struct minmax *m, u32 win,\t\t\t\tconst struct minmax_sample *val)&#123;\tu32 dt = val-&gt;t - m-&gt;s[0].t;\t//最小值已经过期\tif (unlikely(dt &gt; win)) &#123;\t\t/*\t\t * Passed entire window without a new val so make 2nd\t\t * choice the new val &amp; 3rd choice the new 2nd choice.\t\t * we may have to iterate this since our 2nd choice\t\t * may also be outside the window (we checked on entry\t\t * that the third choice was in the window).\t\t */\t\tm-&gt;s[0] = m-&gt;s[1];\t\tm-&gt;s[1] = m-&gt;s[2];\t\tm-&gt;s[2] = *val;\t\tif (unlikely(val-&gt;t - m-&gt;s[0].t &gt; win)) &#123;\t\t\tm-&gt;s[0] = m-&gt;s[1];\t\t\tm-&gt;s[1] = m-&gt;s[2];\t\t\tm-&gt;s[2] = *val;\t\t&#125;\t//第二候选与第一候选同时获得，且过了1/4窗口时间\t&#125; else if (unlikely(m-&gt;s[1].t == m-&gt;s[0].t) &amp;&amp; dt &gt; win/4) &#123;\t\t/*\t\t * We&#x27;ve passed a quarter of the window without a new val\t\t * so take a 2nd choice from the 2nd quarter of the window.\t\t */\t\tm-&gt;s[2] = m-&gt;s[1] = *val;//更新\t//过期了一半\t&#125; else if (unlikely(m-&gt;s[2].t == m-&gt;s[1].t) &amp;&amp; dt &gt; win/2) &#123;\t\t/*\t\t * We&#x27;ve passed half the window without finding a new val\t\t * so take a 3rd choice from the last half of the window\t\t */\t\tm-&gt;s[2] = *val;\t&#125;\treturn m-&gt;s[0].v;&#125;\n\nminmax_subwin_update() 用于在一个时间窗口内维护最小值，它保存三个候选值（当前最小值、次小值和第三候选），根据样本的时间戳判断是否过期：如果最小值超过窗口就依次前移；如果过去了 1&#x2F;4 或 1&#x2F;2 窗口时间还没有新值，就用最新样本更新候选，从而保证返回的 s[0].v 始终是窗口内的平滑最小。\n回到tcp_ack_update_rtt中，更新完成最小rtt后，会进一步调用tcp_rtt_estimator更新平滑rtt，具体代码如下所示：\nstatic void tcp_rtt_estimator(struct sock *sk, long mrtt_us)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tlong m = mrtt_us; /* RTT */ //当前计算的rtt\tu32 srtt = tp-&gt;srtt_us; //平滑rtt\t/*\tThe following amusing code comes from Jacobson&#x27;s\t *\tarticle in SIGCOMM &#x27;88.  Note that rtt and mdev\t *\tare scaled versions of rtt and mean deviation.\t *\tThis is designed to be as fast as possible\t *\tm stands for &quot;measurement&quot;.\t *\t *\tOn a 1990 paper the rto value is changed to:\t *\tRTO = rtt + 4 * mdev\t *\t * Funny. This algorithm seems to be very broken.\t * These formulae increase RTO, when it should be decreased, increase\t * too slowly, when it should be increased quickly, decrease too quickly\t * etc. I guess in BSD RTO takes ONE value, so that it is absolutely\t * does not matter how to _calculate_ it. Seems, it was trap\t * that VJ failed to avoid. 8)\t */\t//不是第一次测量，这里注意srtt右移3位是真正的srtt\tif (srtt != 0) &#123;\t\tm -= (srtt &gt;&gt; 3);\t/* m is now error in rtt est */\t\tsrtt += m;\t\t/* rtt = 7/8 rtt + 1/8 new *///这里计算了加权的值新值的权重是1/8\t\t//下面叫做\t\tif (m &lt; 0) &#123; //这里m经过上面计算后表示的是误差，如果小于0 表示rtt当前rtt减小了\t\t\tm = -m;\t\t/* m is now abs(error) *///取一个绝对值\t\t\tm -= (tp-&gt;mdev_us &gt;&gt; 2);   /* similar update on mdev */ //误差- mdev/4 //避免rtt减少过快？会影响rto?\t\t\t/* This is similar to one of Eifel findings.\t\t\t * Eifel blocks mdev updates when rtt decreases.\t\t\t * This solution is a bit different: we use finer gain\t\t\t * for mdev in this case (alpha*beta).\t\t\t * Like Eifel it also prevents growth of rto,\t\t\t * but also it limits too fast rto decreases,\t\t\t * happening in pure Eifel.\t\t\t */\t\t\tif (m &gt; 0)//表示偏差不是太多，这里在×8\t\t\t\tm &gt;&gt;= 3;\t\t&#125; else &#123;\t\t\tm -= (tp-&gt;mdev_us &gt;&gt; 2);   /* similar update on mdev */\t\t&#125;\t\ttp-&gt;mdev_us += m;\t\t/* mdev = 3/4 mdev + 1/4 new */ //更新rtt的偏差\t\tif (tp-&gt;mdev_us &gt; tp-&gt;mdev_max_us) &#123; //如果当前波动大于历史最大的波动，则更新\t\t\ttp-&gt;mdev_max_us = tp-&gt;mdev_us;\t\t\t////只有当mdev_max_us大于rttvar_us的时候才更新，可目的是更新的更保守？\t\t\tif (tp-&gt;mdev_max_us &gt; tp-&gt;rttvar_us) \t\t\t\ttp-&gt;rttvar_us = tp-&gt;mdev_max_us;\t\t&#125;\t\tif (after(tp-&gt;snd_una, tp-&gt;rtt_seq)) &#123;//这里其实是una和sndnxt在比较，可以理解为是否有数据包在传输吧\t\t\tif (tp-&gt;mdev_max_us &lt; tp-&gt;rttvar_us) //如果最大波动小于这个值的话保守的减少rtt\t\t\t\ttp-&gt;rttvar_us -= (tp-&gt;rttvar_us - tp-&gt;mdev_max_us) &gt;&gt; 2;\t\t\ttp-&gt;rtt_seq = tp-&gt;snd_nxt; //更新rtt_seq 为 snd_nxt\t\t\ttp-&gt;mdev_max_us = tcp_rto_min_us(sk);  //设置最大波动值为最小rto的时间\t\t\ttcp_bpf_rtt(sk);\t\t&#125;\t&#125; else &#123;\t\t//第一次测量\t\t/* no previous measure. */\t\tsrtt = m &lt;&lt; 3;\t\t/* take the measured time to be rtt *///左移三位为初始的srtt\t\ttp-&gt;mdev_us = m &lt;&lt; 1;\t/* make sure rto = 3*rtt */ //平均偏差设置为 RTT 样本的 2 倍\t\ttp-&gt;rttvar_us = max(tp-&gt;mdev_us, tcp_rto_min_us(sk)); //用于计算rttvar_us，计算rto会用到，确保大于最小的rto\t\ttp-&gt;mdev_max_us = tp-&gt;rttvar_us; //设置记录最大波动的值\t\ttp-&gt;rtt_seq = tp-&gt;snd_nxt; //更新rtt_seq\t\ttcp_bpf_rtt(sk);\t&#125;\t//更新平滑rtt\ttp-&gt;srtt_us = max(1U, srtt);&#125;\n\ntcp_rtt_estimator() 用来更新 TCP 的平滑 srtt和偏差mdev（历史的一个波动情况 mdev &#x3D; 3&#x2F;4 mdev + 1&#x2F;4 new ）、rttvar(用于计算rto)，如果已有历史 RTT，就用新的测量值和旧 srtt 做加权平均（1&#x2F;8 新值 + 7&#x2F;8 旧值），并调整偏差，避免 RTO 增减过快。如果是第一次测量，就直接用当前 RTT 初始化 srtt、mdev 和 rttvar，从代码逻辑来看**，整体的思想是当rtt减少的时候，减少偏差的速度被限制，当 RTT 变大时，可以更快地增加偏差，从而快速拉高 RTO，减少误判超时。第一次采样时，直接用样本 RTT 初始化 srtt，并设置较大的偏差。**\n计算完rtt后会更新rto具体代码如下所示\n//设置rto usecs_to_jiffies((tp-&gt;srtt_us &gt;&gt; 3) + tp-&gt;rttvar_us);tcp_set_rto(sk);\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP三次握手-接收syn段 (三)","url":"/2025/09/09/TCP%E8%A2%AB%E5%8A%A8%E6%89%93%E5%BC%80%E6%8E%A5%E6%94%B6syn%20(%E4%B8%89)/","content":"tcp_conn_request中首先读取系统参数  syncookies &#x3D; READ_ONCE(net-&gt;ipv4.sysctl_tcp_syncookies) 如果为1（默认是1）则表示当半连接队列满了(这里半连接队列的大小也是由系统参数决定)的时候开启syncookie如果为2则表示强制开启syncookie。拿到syncookie系统参数后会进一步判断当前场景下是否需要走syncookie模式。\n对应的代码如下所示：\nif ((syncookies == 2 || inet_csk_reqsk_queue_is_full(sk)) &amp;&amp; !isn) &#123;\t//是否需要cookie\twant_cookie = tcp_syn_flood_action(sk, rsk_ops-&gt;slab_name);\tif (!want_cookie)\t\tgoto drop;&#125;\n\n如果系统参数不是强制开启syncookies则进一步通过inet_csk_reqsk_queue_is_full检查，这里其实就是比较一下当前半连接数目是否超过系统配置的半连接队列长度，如果没有，则进一步检查是否是timewait状态的重用，即isn不为0。\n上述满足任何一个条件都会设置want_cookie标志。\n之后会进一步判断是否全连接队列满了(也就是listen系统调用的第二个参数，默认是4096), 满了则直接丢弃。\n然后调用inet_reqsk_alloc申请管理半连接队列的结构，很多信息都保存在这个结构中，这个就是插入ehash的结构(前提是走正常三次握手流程)\n具体代码如下所示：\n//这里的第三个参数表示是否跟listensocket相关联struct request_sock *inet_reqsk_alloc(const struct request_sock_ops *ops,\t\t\t\t      struct sock *sk_listener,\t\t\t\t      bool attach_listener)&#123;\t//这里申请了一个reqsock 注意：这个结构体里面包了一层sock外面是这里根据req拿到inet_request_sock\tstruct request_sock *req = reqsk_alloc(ops, sk_listener,\t\t\t\t\t       attach_listener);\tif (req) &#123;\t\t//这里根据req拿到inet_request_sock\t\tstruct inet_request_sock *ireq = inet_rsk(req);\t\t//初始化一些字段\t\tireq-&gt;ireq_opt = NULL;//ip选项相关#if IS_ENABLED(CONFIG_IPV6)\t\tireq-&gt;pktopts = NULL;#endif\t\tatomic64_set(&amp;ireq-&gt;ir_cookie, 0);\t\tireq-&gt;ireq_state = TCP_NEW_SYN_RECV;//注意这里设置了TCP_NEW_SYN_RECV状态！\t\twrite_pnet(&amp;ireq-&gt;ireq_net, sock_net(sk_listener));//设置网络命名空间\t\tireq-&gt;ireq_family = sk_listener-&gt;sk_family;\t\treq-&gt;timeout = TCP_TIMEOUT_INIT;//这个应该是对端如果不回ack的初始时间 1s\t&#125;\treturn req;&#125;\n\ninet_reqsk_alloc中调用reqsk_alloc完成实际的申请后会对一些字段进行初始化，注意：这里将req的状态设置为TCP_NEW_SYN_RECV（低版本内核没有的一个状态）。\nreqsk_alloc中会根据syncookie确定是否将这个req关联一个sock具体代码如下所示：\nstatic inline struct request_sock *reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener,\t    bool attach_listener)&#123;\tstruct request_sock *req;\t//从slab 缓存里申请\treq = kmem_cache_alloc(ops-&gt;slab, GFP_ATOMIC | __GFP_NOWARN);\tif (!req)\t\treturn NULL;\treq-&gt;rsk_listener = NULL;\t//是否需要关联一个listen的socket 如果开启了cookies就不关联\tif (attach_listener) &#123;\t\tif (unlikely(!refcount_inc_not_zero(&amp;sk_listener-&gt;sk_refcnt))) &#123;//给建通套接字的引用计数加1\t\t\tkmem_cache_free(ops-&gt;slab, req);\t\t\treturn NULL;\t\t&#125;\t\treq-&gt;rsk_listener = sk_listener;\t&#125;\treq-&gt;rsk_ops = ops;//注册req的ops\treq_to_sk(req)-&gt;sk_prot = sk_listener-&gt;sk_prot;//注册sock的ops\tsk_node_init(&amp;req_to_sk(req)-&gt;sk_node);\tsk_tx_queue_clear(req_to_sk(req)); //设置tx_maping队列\treq-&gt;saved_syn = NULL;\treq-&gt;timeout = 0;\treq-&gt;num_timeout = 0;\treq-&gt;num_retrans = 0;\treq-&gt;sk = NULL;\trefcount_set(&amp;req-&gt;rsk_refcnt, 0);\treturn req;&#125;\n\n回到tcp_conn_request中，当申请req结构成功后会调用tcp_parse_options来解析接收到的syn中的选项字段（可能包括MSS  窗口缩放，时间戳，sack，TFO），并把选项保存到tmp_opt中，例如对端开启了某个选项，同时本机系统也支持的话，则会设置具体选项的有效标志位。后续封包会根据标志位做不同处理。具体代码如下所示：\nvoid tcp_parse_options(const struct net *net,\t\t       const struct sk_buff *skb,\t\t       struct tcp_options_received *opt_rx, int estab,\t\t       struct tcp_fastopen_cookie *foc)&#123;\tconst unsigned char *ptr;\tconst struct tcphdr *th = tcp_hdr(skb);\t//计算选项的长度\tint length = (th-&gt;doff * 4) - sizeof(struct tcphdr);\t//指向选项的起始位置\tptr = (const unsigned char *)(th + 1);\topt_rx-&gt;saw_tstamp = 0;\topt_rx-&gt;saw_unknown = 0;\t//TLV\twhile (length &gt; 0) &#123;\t\tint opcode = *ptr++; //读取kind\t\tint opsize;\t\tswitch (opcode) &#123;\t\tcase TCPOPT_EOL:   //选项结束直接return\t\t\treturn;\t\tcase TCPOPT_NOP:\t/* Ref: RFC 793 section 3.1 */ //占位 continue\t\t\tlength--;\t\t\tcontinue;\t\tdefault:\t\t\tif (length &lt; 2)  //剩余长度不够了 直接retunr\t\t\t\treturn;\t\t\topsize = *ptr++; //这个就是length\t\t\tif (opsize &lt; 2) /* &quot;silly options&quot; */\t\t\t\treturn;\t\t\tif (opsize &gt; length)\t\t\t\treturn;\t/* don&#x27;t parse partial options */\t\t\tswitch (opcode) &#123;\t\t\tcase TCPOPT_MSS://2表示mss\t\t\t\tif (opsize == TCPOLEN_MSS &amp;&amp; th-&gt;syn &amp;&amp; !estab) &#123;//长度必须是4，且是syn包不在est\t\t\t\t\tu16 in_mss = get_unaligned_be16(ptr);//这个就是对端通告的mss大小！\t\t\t\t\tif (in_mss) &#123;\t\t\t\t\t\tif (opt_rx-&gt;user_mss &amp;&amp; //用户如果设置了mss就比较一下选择哪个\t\t\t\t\t\t    opt_rx-&gt;user_mss &lt; in_mss)\t\t\t\t\t\t\tin_mss = opt_rx-&gt;user_mss;\t\t\t\t\t\topt_rx-&gt;mss_clamp = in_mss;  //或者直接保存对端通告的\t\t\t\t\t&#125;\t\t\t\t&#125;\t\t\t\tbreak;\t\t\tcase TCPOPT_WINDOW://3窗口缩放\t\t\t\tif (opsize == TCPOLEN_WINDOW &amp;&amp; th-&gt;syn &amp;&amp;\t\t\t\t    !estab &amp;&amp; READ_ONCE(net-&gt;ipv4.sysctl_tcp_window_scaling)) &#123; //是否支持窗口缩放\t\t\t\t\t__u8 snd_wscale = *(__u8 *)ptr;//读取窗口缩放值\t\t\t\t\topt_rx-&gt;wscale_ok = 1;\t//标记对端支持窗口缩放\t\t\t\t\tif (snd_wscale &gt; TCP_MAX_WSCALE) &#123; //14 表示 1G ?\t\t\t\t\t\tnet_info_ratelimited(&quot;%s: Illegal window scaling value %d &gt; %u received\\n&quot;,\t\t\t\t\t\t\t\t     __func__,\t\t\t\t\t\t\t\t     snd_wscale,\t\t\t\t\t\t\t\t     TCP_MAX_WSCALE);\t\t\t\t\t\tsnd_wscale = TCP_MAX_WSCALE;\t\t\t\t\t&#125;\t\t\t\t\topt_rx-&gt;snd_wscale = snd_wscale;//保存窗口缩放大小\t\t\t\t&#125;\t\t\t\tbreak;\t\t\tcase TCPOPT_TIMESTAMP: //时间戳\t\t\t\tif ((opsize == TCPOLEN_TIMESTAMP) &amp;&amp;\t\t\t\t    ((estab &amp;&amp; opt_rx-&gt;tstamp_ok) ||\t\t\t\t     (!estab &amp;&amp; READ_ONCE(net-&gt;ipv4.sysctl_tcp_timestamps)))) &#123; //是否开启时间戳选项\t\t\t\t\topt_rx-&gt;saw_tstamp = 1;\t\t\t\t\topt_rx-&gt;rcv_tsval = get_unaligned_be32(ptr); //对端的时间戳\t\t\t\t\topt_rx-&gt;rcv_tsecr = get_unaligned_be32(ptr + 4); //对端回显的时间戳  两个值用来计算rtt?\t\t\t\t&#125;\t\t\t\tbreak;\t\t\tcase TCPOPT_SACK_PERM: //4 sack\t\t\t\tif (opsize == TCPOLEN_SACK_PERM &amp;&amp; th-&gt;syn &amp;&amp;\t\t\t\t    !estab &amp;&amp; READ_ONCE(net-&gt;ipv4.sysctl_tcp_sack)) &#123; //是否开启sack选项\t\t\t\t\topt_rx-&gt;sack_ok = TCP_SACK_SEEN; \t\t\t\t\ttcp_sack_reset(opt_rx);  //初始化sack相关字段\t\t\t\t&#125;\t\t\t\tbreak;\t\t\tcase TCPOPT_SACK://这个是检查携带的sack是否和法？什么情况下会走进这个case呢？建立连接？\t\t\t\tif ((opsize &gt;= (TCPOLEN_SACK_BASE + TCPOLEN_SACK_PERBLOCK)) &amp;&amp;\t\t\t\t   !((opsize - TCPOLEN_SACK_BASE) % TCPOLEN_SACK_PERBLOCK) &amp;&amp;\t\t\t\t   opt_rx-&gt;sack_ok) &#123;\t\t\t\t\tTCP_SKB_CB(skb)-&gt;sacked = (ptr - 2) - (unsigned char *)th;\t\t\t\t&#125;\t\t\t\tbreak;#ifdef CONFIG_TCP_MD5SIG\t\t\tcase TCPOPT_MD5SIG:\t\t\t\t/* The MD5 Hash has already been\t\t\t\t * checked (see tcp_v&#123;4,6&#125;_rcv()).\t\t\t\t */\t\t\t\tbreak;#endif\t\t\tcase TCPOPT_FASTOPEN: //TFO相关， 注意这里传入了foc\t\t\t\ttcp_parse_fastopen_option(\t\t\t\t\topsize - TCPOLEN_FASTOPEN_BASE,\t\t\t\t\tptr, th-&gt;syn, foc, false);\t\t\t\tbreak;\t\t\tcase TCPOPT_EXP: //实验性质的选项？。。。。\t\t\t\t/* Fast Open option shares code 254 using a\t\t\t\t * 16 bits magic number.\t\t\t\t */\t\t\t\tif (opsize &gt;= TCPOLEN_EXP_FASTOPEN_BASE &amp;&amp;\t\t\t\t    get_unaligned_be16(ptr) ==\t\t\t\t    TCPOPT_FASTOPEN_MAGIC) &#123;\t\t\t\t\ttcp_parse_fastopen_option(opsize -\t\t\t\t\t\tTCPOLEN_EXP_FASTOPEN_BASE,\t\t\t\t\t\tptr + 2, th-&gt;syn, foc, true);\t\t\t\t\tbreak;\t\t\t\t&#125;\t\t\t\tif (smc_parse_options(th, opt_rx, ptr, opsize))\t\t\t\t\tbreak;\t\t\t\topt_rx-&gt;saw_unknown = 1;\t\t\t\tbreak;\t\t\tdefault:\t\t\t\topt_rx-&gt;saw_unknown = 1;\t\t\t&#125;\t\t\tptr += opsize-2;\t\t\tlength -= opsize;\t\t&#125;\t&#125;&#125;\n\n解析完对端的syn包后，会调用tcp_openreq_init完成进一步的初始化，包括初始化接收窗口，保存syn包的序列号，保存源端口和目的端口等。。。。具体代码如下所示：\nstatic void tcp_openreq_init(struct request_sock *req,\t\t\t     const struct tcp_options_received *rx_opt,\t\t\t     struct sk_buff *skb, const struct sock *sk)&#123;\tstruct inet_request_sock *ireq = inet_rsk(req);\treq-&gt;rsk_rcv_wnd = 0;\t\t/* So that tcp_send_synack() knows! *///接收窗口初始化为0\ttcp_rsk(req)-&gt;rcv_isn = TCP_SKB_CB(skb)-&gt;seq; //客户端的初始序列号\ttcp_rsk(req)-&gt;rcv_nxt = TCP_SKB_CB(skb)-&gt;seq + 1; //期望接收的下一个序列号\ttcp_rsk(req)-&gt;snt_synack = 0;\ttcp_rsk(req)-&gt;last_oow_ack_time = 0;\t//根据opt设置是否支持一些选项\treq-&gt;mss = rx_opt-&gt;mss_clamp; //mss\treq-&gt;ts_recent = rx_opt-&gt;saw_tstamp ? rx_opt-&gt;rcv_tsval : 0;\tireq-&gt;tstamp_ok = rx_opt-&gt;tstamp_ok;\tireq-&gt;sack_ok = rx_opt-&gt;sack_ok;\tireq-&gt;snd_wscale = rx_opt-&gt;snd_wscale;\tireq-&gt;wscale_ok = rx_opt-&gt;wscale_ok;\tireq-&gt;acked = 0;\tireq-&gt;ecn_ok = 0;\tireq-&gt;ir_rmt_port = tcp_hdr(skb)-&gt;source; //客户端端口\tireq-&gt;ir_num = ntohs(tcp_hdr(skb)-&gt;dest); //服务端端口\tireq-&gt;ir_mark = inet_request_mark(sk, skb); //设置mark 是不是iptable那个mark？#if IS_ENABLED(CONFIG_SMC)\tireq-&gt;smc_ok = rx_opt-&gt;smc_ok &amp;&amp; !(tcp_sk(sk)-&gt;smc_hs_congested &amp;&amp;\t\t\ttcp_sk(sk)-&gt;smc_hs_congested(sk));#endif&#125;\n\n之后会调用req的查找路由回调(目的应该就是确定数据包肯定能路由到吧？)，对应的具体代码为tcp_v4_route_req，这里会设置req源目的地址(上面已经设置了源目的端口)具体代码如下所示：\nstatic struct dst_entry *tcp_v4_route_req(const struct sock *sk,\t\t\t\t\t  struct sk_buff *skb,\t\t\t\t\t  struct flowi *fl,\t\t\t\t\t  struct request_sock *req)&#123;\t//设置源ip地址和目的ip地址\ttcp_v4_init_req(req, sk, skb);\tif (security_inet_conn_request(sk, skb, req))\t\treturn NULL;\t//调用查路由接口返回dst\treturn inet_csk_route_req(sk, &amp;fl-&gt;u.ip4, req);&#125;static void tcp_v4_init_req(struct request_sock *req,\t\t\t    const struct sock *sk_listener,\t\t\t    struct sk_buff *skb)&#123;\tstruct inet_request_sock *ireq = inet_rsk(req);\tstruct net *net = sock_net(sk_listener);\t//注意：设置本地ip地址\tsk_rcv_saddr_set(req_to_sk(req), ip_hdr(skb)-&gt;daddr);\t//设置源ip地址\tsk_daddr_set(req_to_sk(req), ip_hdr(skb)-&gt;saddr);\t//处理ip选项\tRCU_INIT_POINTER(ireq-&gt;ireq_opt, tcp_v4_save_options(net, skb));&#125;\n\n查找路由完成后，如果是正常的三次握手流程，会判断当前半连接队列的数目是否已经超过了四分之三，如果超过，且同时新来的连接不是以前的旧连接的话则直接回丢弃。如果没有被丢弃的话，则执行关键的一步操作，调用tcp_v4_init_seq根据四元组和时间戳初始化序列号，具体代码如下所示：\nstatic u32 tcp_v4_init_seq(const struct sk_buff *skb)&#123;\t//根据四元组初始化序列号\treturn secure_tcp_seq(ip_hdr(skb)-&gt;daddr,\t\t\t      ip_hdr(skb)-&gt;saddr,\t\t\t      tcp_hdr(skb)-&gt;dest,\t\t\t      tcp_hdr(skb)-&gt;source);&#125;u32 secure_tcp_seq(__be32 saddr, __be32 daddr,\t\t   __be16 sport, __be16 dport)&#123;\tu32 hash;\t//初始化一个密钥？\tnet_secret_init();\t//计算一个hash值\thash = siphash_3u32((__force u32)saddr, (__force u32)daddr,\t\t\t    (__force u32)sport &lt;&lt; 16 | (__force u32)dport,\t\t\t    &amp;net_secret);\t//加入时间戳\treturn seq_scale(hash);&#125;\n\n回到tcp_conn_request中，之后会调用tcp_ecn_create_request进一步判断是否可以使能ecn具体代码如下所示：\nstatic void tcp_ecn_create_request(struct request_sock *req,\t\t\t\t   const struct sk_buff *skb,\t\t\t\t   const struct sock *listen_sk,\t\t\t\t   const struct dst_entry *dst)&#123;\tconst struct tcphdr *th = tcp_hdr(skb);\tconst struct net *net = sock_net(listen_sk);\tbool th_ecn = th-&gt;ece &amp;&amp; th-&gt;cwr;\tbool ect, ecn_ok;\tu32 ecn_ok_dst;\t//检查syn包中是否设置了ecn标志位\tif (!th_ecn)\t\treturn;\tect = !INET_ECN_is_not_ect(TCP_SKB_CB(skb)-&gt;ip_dsfield);//ip层必须要支持ecn\tecn_ok_dst = dst_feature(dst, DST_FEATURE_ECN_MASK);\tecn_ok = READ_ONCE(net-&gt;ipv4.sysctl_tcp_ecn) || ecn_ok_dst; //系统是否使能了ecn\t// 数据包没有ecn能力 但是系统启用了 || 拥塞算法要求启用 || 网络路径要求启用 || bpf要求启用\tif (((!ect || th-&gt;res1) &amp;&amp; ecn_ok) || tcp_ca_needs_ecn(listen_sk) ||\t    (ecn_ok_dst &amp; DST_FEATURE_ECN_CA) ||\t    tcp_bpf_ca_needs_ecn((struct sock *)req))\t\tinet_rsk(req)-&gt;ecn_ok = 1; //表示启用ecn&#125;\n\n如果是syncookie模式，则会调用cookie_init_sequence来初始化序列号，这里会根据四元组，接收到syn包的序列号，还有mss的索引，以及时间戳信息，计算一个序列号。\n具体代码如下所示：\nstatic inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,\t\t\t\t\t const struct sock *sk, struct sk_buff *skb,\t\t\t\t\t __u16 *mss)&#123;\t//这个sk是listensock\ttcp_synq_overflow(sk);\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_SYNCOOKIESSENT);\t//cookie_v4_init_sequence初始化序列号 \treturn ops-&gt;cookie_init_seq(skb, mss);&#125;__u32 cookie_v4_init_sequence(const struct sk_buff *skb, __u16 *mssp)&#123;\tconst struct iphdr *iph = ip_hdr(skb);\tconst struct tcphdr *th = tcp_hdr(skb);\treturn __cookie_v4_init_sequence(iph, th, mssp);&#125;u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,\t\t\t      u16 *mssp)&#123;\tint mssind;\tconst __u16 mss = *mssp;\tfor (mssind = ARRAY_SIZE(msstab) - 1; mssind ; mssind--)\t\tif (mss &gt;= msstab[mssind])\t\t\tbreak;\t//\t536,1300,1440,1460\t//标准化mss\t*mssp = msstab[mssind];\t//这里还传入四元组，发送端序号，还有mss的索引值，\treturn secure_tcp_syn_cookie(iph-&gt;saddr, iph-&gt;daddr,\t\t\t\t     th-&gt;source, th-&gt;dest, ntohl(th-&gt;seq),\t\t\t\t     mssind);&#125;static __u32 secure_tcp_syn_cookie(__be32 saddr, __be32 daddr, __be16 sport,\t\t\t\t   __be16 dport, __u32 sseq, __u32 data)&#123;\t/*\t * Compute the secure sequence number.\t * The output should be:\t *   HASH(sec1,saddr,sport,daddr,dport,sec1) + sseq + (count * 2^24)\t *      + (HASH(sec2,saddr,sport,daddr,dport,count,sec2) % 2^24).\t * Where sseq is their sequence number and count increases every\t * minute by 1.\t * As an extra hack, we add a small &quot;data&quot; value that encodes the\t * MSS into the second hash value.\t */\tu32 count = tcp_cookie_time();//一个时间戳\t//整个32位值 = [时间count] + [基础哈希] + [客户端序列号] + [验证哈希]\treturn (cookie_hash(saddr, daddr, sport, dport, 0, 0) +\t\tsseq + (count &lt;&lt; COOKIEBITS) +\t\t((cookie_hash(saddr, daddr, sport, dport, count, 1) + data)\t\t &amp; COOKIEMASK));&#125;\n\n如果是正常三次握手流程，同时客户设置保存syn包的选项，则会把收到的syn包头保存，具体代码如下所示：\nstatic void tcp_reqsk_record_syn(const struct sock *sk,\t\t\t\t struct request_sock *req,\t\t\t\t const struct sk_buff *skb)&#123;\t//用户是否同setsockopt设置保存syn的选项，如果是1,则保存传输头部， 如果是2则保存完整头部\tif (tcp_sk(sk)-&gt;save_syn) &#123;\t\tu32 len = skb_network_header_len(skb) + tcp_hdrlen(skb);\t\tstruct saved_syn *saved_syn;\t\tu32 mac_hdrlen;\t\tvoid *base;\t\tif (tcp_sk(sk)-&gt;save_syn == 2) &#123;  /* Save full header. */\t\t\tbase = skb_mac_header(skb);\t\t\tmac_hdrlen = skb_mac_header_len(skb);\t\t\tlen += mac_hdrlen; //加上mac头\t\t&#125; else &#123;\t\t\t//save_syn为1 表示base指向ip头\t\t\tbase = skb_network_header(skb);\t\t\tmac_hdrlen = 0;\t\t&#125;\t\tsaved_syn = kmalloc(struct_size(saved_syn, data, len),\t\t\t\t    GFP_ATOMIC);\t\tif (saved_syn) &#123;\t\t\tsaved_syn-&gt;mac_hdrlen = mac_hdrlen;\t\t\tsaved_syn-&gt;network_hdrlen = skb_network_header_len(skb);\t\t\tsaved_syn-&gt;tcp_hdrlen = tcp_hdrlen(skb);\t\t\tmemcpy(saved_syn-&gt;data, base, len);//根据长度拷贝数据包头\t\t\treq-&gt;saved_syn = saved_syn;\t\t&#125;\t&#125;&#125;\n\n完成上述一系列工作后，则会将申请的req结构插入到ehash中，同时更新半连接队列的统计计数，并启动syn+ack定时器，具体代码如下所示：\nvoid inet_csk_reqsk_queue_hash_add(struct sock *sk, struct request_sock *req,\t\t\t\t   unsigned long timeout)&#123;\t//挂一个定时器，并将req加入到ehash中\treqsk_queue_hash_req(req, timeout);\tinet_csk_reqsk_queue_added(sk);//这里更新了半连接状态的计数。&#125;static void reqsk_queue_hash_req(struct request_sock *req,\t\t\t\t unsigned long timeout)&#123;\t//这里启动syn + ack定时器\ttimer_setup(&amp;req-&gt;rsk_timer, reqsk_timer_handler, TIMER_PINNED);\tmod_timer(&amp;req-&gt;rsk_timer, jiffies + timeout);\t//将这个半连接结构插入到ehash中\tinet_ehash_insert(req_to_sk(req), NULL, NULL);\t/* before letting lookups find us, make sure all req fields\t * are committed to memory and refcnt initialized.\t */\tsmp_wmb();\trefcount_set(&amp;req-&gt;rsk_refcnt, 2 + 1);&#125;static inline void inet_csk_reqsk_queue_added(struct sock *sk)&#123;\treqsk_queue_added(&amp;inet_csk(sk)-&gt;icsk_accept_queue);&#125;static inline void reqsk_queue_added(struct request_sock_queue *queue)&#123;\tatomic_inc(&amp;queue-&gt;young);//syn请求计数\tatomic_inc(&amp;queue-&gt;qlen);//队列长度计数&#125;\n\n之后会调用af_ops-&gt;send_synacksend_synack进入到第二次握手的逻辑中，这里注意，如果是syncookie则发送syn+ack后立即会释放这个req。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP三次握手-接收ack (二)","url":"/2025/09/20/TCP%E8%A2%AB%E5%8A%A8%E6%89%93%E5%BC%80%E6%8E%A5%E6%94%B6ack%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"tcp_check_req中首先解析数据包的选项，如果有时间戳选项的话则会根据时间戳调用tcp_paws_reject进一步进行PAWS检查，判断是否发生序列号回绕（如果发生回绕后面可能会回复挑战ack），tcp_paws_reject代码如下所示：\nstatic inline bool tcp_paws_reject(const struct tcp_options_received *rx_opt,                   int rst)&#123;    if (tcp_paws_check(rx_opt, 0))        return false;    /* RST segments are not recommended to carry timestamp,       and, if they do, it is recommended to ignore PAWS because       &quot;their cleanup function should take precedence over timestamps.&quot;       Certainly, it is mistake. It is necessary to understand the reasons       of this constraint to relax it: if peer reboots, clock may go       out-of-sync and half-open connections will not be reset.       Actually, the problem would be not existing if all       the implementations followed draft about maintaining clock       via reboots. Linux-2.2 DOES NOT!       However, we can relax time bounds for RST segments to MSL.     */    //如果当前报文是一个 RST 报文，并且当前时间已经超过了之前保存的时间戳记录时间（ts_recent_stamp）加上 PAWS 的 MSL 时间（60秒），那么就返回 false    if (rst &amp;&amp; !time_before32(ktime_get_seconds(),                  rx_opt-&gt;ts_recent_stamp + TCP_PAWS_MSL))        return false;    return true;&#125;static inline bool tcp_paws_check(const struct tcp_options_received *rx_opt,                  int paws_win)&#123;    //这里一个是上一次发送数据包的时间戳（比如说这是在处理三次握手的ack则这个第一个值就是syn包的时间戳），一个是当前数据包的时间戳 paws_win为0      if ((s32)(rx_opt-&gt;ts_recent - rx_opt-&gt;rcv_tsval) &lt;= paws_win)        return true;    //时间超过24天也通过    if (unlikely(!time_before32(ktime_get_seconds(),                    rx_opt-&gt;ts_recent_stamp + TCP_PAWS_24DAYS)))        return true;    /*     * Some OSes send SYN and SYNACK messages with tsval=0 tsecr=0,     * then following tcp messages have valid values. Ignore 0 value,     * or else &#x27;negative&#x27; tsval might forbid us to accept their packets.     */    //上一次发包时间没有记录也通过    if (!rx_opt-&gt;ts_recent)        return true;    return false;&#125;\n\ntcp_paws_check来真正检查数据包是否发生回绕，如果当前报文上一次发送数据包的时间戳小于当前数据包的时间戳 或者超过24天，则认为序列号没有发生回绕，直接返回否true则返回false。\n之后tcp_check_req中处理纯syn包的重传，首先调用__tcp_oow_rate_limited判断是否需要限制速率(为了防止频繁ack的攻击？)，如果不需要限制则回复一个synack具体代码如下所示：\n//处理纯syn包的重传，这里其实就是重新调用了一下send_synackif (TCP_SKB_CB(skb)-&gt;seq == tcp_rsk(req)-&gt;rcv_isn &amp;&amp;    flg == TCP_FLAG_SYN &amp;&amp;    !paws_reject) &#123;    //这里传入了last_oow_ack_time,也就是上次发送synack的时间，需要进行安全相关的检查    //如果检查通过了直接重传synack包并重新设置synack定时器    if (!tcp_oow_rate_limited(sock_net(sk), skb,                  LINUX_MIB_TCPACKSKIPPEDSYNRECV,                  &amp;tcp_rsk(req)-&gt;last_oow_ack_time) &amp;&amp;        !inet_rtx_syn_ack(sk, req)) &#123;//重传synack        unsigned long expires = jiffies;        expires += reqsk_timeout(req, TCP_RTO_MAX);        if (!fastopen)            mod_timer_pending(&amp;req-&gt;rsk_timer, expires);        else            req-&gt;rsk_timer.expires = expires;    &#125;    return NULL;//外面什么也不做&#125;\n\n上述tcp_oow_rate_limited 用来限制对syn或者ack包的频繁响应，具体代码如下所示：\nbool tcp_oow_rate_limited(struct net *net, const struct sk_buff *skb,              int mib_idx, u32 *last_oow_ack_time)&#123;    /* Data packets without SYNs are not likely part of an ACK loop. */    //对于有数据包，且不是syn包直接返回false    if ((TCP_SKB_CB(skb)-&gt;seq != TCP_SKB_CB(skb)-&gt;end_seq) &amp;&amp;        !tcp_hdr(skb)-&gt;syn)        return false;    //是否需要限制的检查    return __tcp_oow_rate_limited(net, mib_idx, last_oow_ack_time);&#125;static bool __tcp_oow_rate_limited(struct net *net, int mib_idx,                   u32 *last_oow_ack_time)&#123;    /* Paired with the WRITE_ONCE() in this function. */    u32 val = READ_ONCE(*last_oow_ack_time);    if (val) &#123;        s32 elapsed = (s32)(tcp_jiffies32 - val);        if (0 &lt;= elapsed &amp;&amp;            //读取系统参数并和上面的间隔比较 这个系统参数默认是500毫秒            elapsed &lt; READ_ONCE(net-&gt;ipv4.sysctl_tcp_invalid_ratelimit)) &#123;            NET_INC_STATS(net, mib_idx);            //收到限制            return true;    /* rate-limited: don&#x27;t send yet! */        &#125;    &#125;    /* Paired with the prior READ_ONCE() and with itself,     * as we might be lockless.     */    //更新这个值    WRITE_ONCE(*last_oow_ack_time, tcp_jiffies32);    //没有限制    return false;   /* not rate-limited: go ahead, send dupack now! */&#125;\n\n之后tcp_check_req中处理交叉syn攻击的情况，也就是说如果数据包中有ack标志，但是确认号不对，这里直接返回监听sk，外面会回复rst报文，具体代码如下所示：\nif ((flg &amp; TCP_FLAG_ACK) &amp;&amp; !fastopen &amp;&amp;    (TCP_SKB_CB(skb)-&gt;ack_seq !=     tcp_rsk(req)-&gt;snt_isn + 1))    return sk;\n\n之后tcp_check_req中处理数据包不在接收窗口内的情况，此时会回复一个挑战ack，返回NULL默默丢弃数据包\nif (paws_reject || !tcp_in_window(TCP_SKB_CB(skb)-&gt;seq, TCP_SKB_CB(skb)-&gt;end_seq,                  tcp_rsk(req)-&gt;rcv_nxt, tcp_rsk(req)-&gt;rcv_nxt + req-&gt;rsk_rcv_wnd)) &#123;    /* Out of window: send ACK and drop. */    if (!(flg &amp; TCP_FLAG_RST) &amp;&amp; //没有rst段，回一个挑战ack        !tcp_oow_rate_limited(sock_net(sk), skb,                  LINUX_MIB_TCPACKSKIPPEDSYNRECV,                  &amp;tcp_rsk(req)-&gt;last_oow_ack_time))        req-&gt;rsk_ops-&gt;send_ack(sk, skb, req);    if (paws_reject)        NET_INC_STATS(sock_net(sk), LINUX_MIB_PAWSESTABREJECTED);    return NULL;//默默丢弃&#125;\n\n如果用户set sockopt设置rskq_defer_accept 则默默丢弃不带数据的ack，具体代码如下所示：\nif (req-&gt;num_timeout &lt; READ_ONCE(inet_csk(sk)-&gt;icsk_accept_queue.rskq_defer_accept) &amp;&amp;    TCP_SKB_CB(skb)-&gt;end_seq == tcp_rsk(req)-&gt;rcv_isn + 1) &#123;    inet_rsk(req)-&gt;acked = 1;    __NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDEFERACCEPTDROP);    return NULL; //外层什么也不做&#125;\n\n如果上述一系列操作都通过了的话，则会调用syn_recv_sock，这个函数是关键（这里会创建新的sock，针对全连接队列长度做处理， 设置GSO,地址， 网口，ipid，tos，mss等信息），syn_recv_sock具体代码如下所示：\n//服务端被动打开 收到ack会调用struct sock *tcp_v4_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,                  struct request_sock *req,                  struct dst_entry *dst,                  struct request_sock *req_unhash,                  bool *own_req)&#123;    struct inet_request_sock *ireq;    bool found_dup_sk = false;    struct inet_sock *newinet;    struct tcp_sock *newtp;    struct sock *newsk;#ifdef CONFIG_TCP_MD5SIG    const union tcp_md5_addr *addr;    struct tcp_md5sig_key *key;    int l3index;#endif    struct ip_options_rcu *inet_opt;    //注意三次握手完成的数量如果大于了 listen设置的第二个参数，这里直接goto    //最外层会返回NULL 等于什么也不做    if (sk_acceptq_is_full(sk))        goto exit_overflow;    //创建一个新的sock并初始化一些列字段    newsk = tcp_create_openreq_child(sk, req, skb);    if (!newsk)        goto exit_nonewsk;    //这里设置了GSO的类型    newsk-&gt;sk_gso_type = SKB_GSO_TCPV4;    //将sock关联skb的dst    inet_sk_rx_dst_set(newsk, skb);    newtp             = tcp_sk(newsk);    newinet           = inet_sk(newsk);    ireq              = inet_rsk(req);    //设置地址，网口，ipotion，多播等字段    sk_daddr_set(newsk, ireq-&gt;ir_rmt_addr);    sk_rcv_saddr_set(newsk, ireq-&gt;ir_loc_addr);    newsk-&gt;sk_bound_dev_if = ireq-&gt;ir_iif;    newinet-&gt;inet_saddr   = ireq-&gt;ir_loc_addr;    inet_opt          = rcu_dereference(ireq-&gt;ireq_opt);    RCU_INIT_POINTER(newinet-&gt;inet_opt, inet_opt);    newinet-&gt;mc_index     = inet_iif(skb);    newinet-&gt;mc_ttl       = ip_hdr(skb)-&gt;ttl;    newinet-&gt;rcv_tos      = ip_hdr(skb)-&gt;tos;    inet_csk(newsk)-&gt;icsk_ext_hdr_len = 0;    if (inet_opt)        inet_csk(newsk)-&gt;icsk_ext_hdr_len = inet_opt-&gt;opt.optlen;    //设置ipid 这里是随机生成的    atomic_set(&amp;newinet-&gt;inet_id, get_random_u16());    /* Set ToS of the new socket based upon the value of incoming SYN.     * ECT bits are set later in tcp_init_transfer().     */     //默认不开启这个系统选项，决定是否使用使用客户端syn包的tos    if (READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_reflect_tos))        newinet-&gt;tos = tcp_rsk(req)-&gt;syn_tos &amp; ~INET_ECN_MASK;    //如果是第三次握手这里一定是空的    if (!dst) &#123;        //调用查路由的接口        dst = inet_csk_route_child_sock(sk, newsk, req);        if (!dst)            goto put_and_exit;    &#125; else &#123;        /* syncookie case : see end of cookie_v4_check() */    &#125;    //设置GSO能力    sk_setup_caps(newsk, dst);    //这里设置了拥塞算法    tcp_ca_openreq_child(newsk, dst);    //如果用户没有显示配置mtu则拿到设备的mtu 后计算mss    tcp_sync_mss(newsk, dst_mtu(dst));    //从dst_metric_advmss （用户配置或者是1460）和用户显式设置的 MSS 之间取一个更小的合法值。    newtp-&gt;advmss = tcp_mss_clamp(tcp_sk(sk), dst_metric_advmss(dst));    //上述计算的两个mss中选个最小值，给延迟ack使用！    tcp_initialize_rcv_mss(newsk);#ifdef CONFIG_TCP_MD5SIG    l3index = l3mdev_master_ifindex_by_index(sock_net(sk), ireq-&gt;ir_iif);    /* Copy over the MD5 key from the original socket */    addr = (union tcp_md5_addr *)&amp;newinet-&gt;inet_daddr;    key = tcp_md5_do_lookup(sk, l3index, addr, AF_INET);    if (key) &#123;        if (tcp_md5_key_copy(newsk, addr, AF_INET, 32, l3index, key))            goto put_and_exit;        sk_gso_disable(newsk);    &#125;#endif    //将新创建的sock加入bhash中， 因为bhash是管理端口的，新创建的sock也理所应当被管理    if (__inet_inherit_port(sk, newsk) &lt; 0)        goto put_and_exit;    //将新创建的sock加入ehahs中，同时移除了原本存在的req    *own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash),                       &amp;found_dup_sk);    if (likely(*own_req)) &#123;        //syn相关的选项        tcp_move_syn(newtp, req);        ireq-&gt;ireq_opt = NULL;    &#125; else &#123;        newinet-&gt;inet_opt = NULL;        if (!req_unhash &amp;&amp; found_dup_sk) &#123;            /* This code path should only be executed in the             * syncookie case only             */            bh_unlock_sock(newsk);            sock_put(newsk);            newsk = NULL;        &#125;    &#125;    //这里返回了新创建的sock    return newsk;exit_overflow:    NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);exit_nonewsk:    dst_release(dst);exit:    tcp_listendrop(sk);    return NULL;put_and_exit:    newinet-&gt;inet_opt = NULL;    inet_csk_prepare_forced_close(newsk);    tcp_done(newsk);    goto exit;&#125;\n上述代码中首先调用sk_acceptq_is_full判断全连接队列是否满了，如果满了的话直接返回返回NULL，否则则会调用\ntcp_create_openreq_child创建一个新的sock并初始化序列号具体代码如下所示：\n //三次握手服务端接收ack会调用，这里第一个参数是监听sk吧 主要工作就是完成初始化struct sock *tcp_create_openreq_child(const struct sock *sk,\t\t\t\t      struct request_sock *req,\t\t\t\t      struct sk_buff *skb)&#123;\t//申请一个sock结构，并初始化相关字段\tstruct sock *newsk = inet_csk_clone_lock(sk, req, GFP_ATOMIC);\tconst struct inet_request_sock *ireq = inet_rsk(req);\tstruct tcp_request_sock *treq = tcp_rsk(req);\tstruct inet_connection_sock *newicsk;\tconst struct tcp_sock *oldtp;\tstruct tcp_sock *newtp;\tu32 seq;\tif (!newsk)\t\treturn NULL;\tnewicsk = inet_csk(newsk);\tnewtp = tcp_sk(newsk);\toldtp = tcp_sk(sk);\t//smc相关\tsmc_check_reset_syn_req(oldtp, req, newtp);\t/* Now setup tcp_sock */\tnewtp-&gt;pred_flags = 0;\t//设置上一次更新的rcv_nxt\tseq = treq-&gt;rcv_isn + 1;\tnewtp-&gt;rcv_wup = seq;\tWRITE_ONCE(newtp-&gt;copied_seq, seq);\t//这里设置了下一个接收的序号\tWRITE_ONCE(newtp-&gt;rcv_nxt, seq);\t//初始化接受了多少个段\tnewtp-&gt;segs_in = 1;\tseq = treq-&gt;snt_isn + 1;\t//下一个等待确认的序号\tnewtp-&gt;snd_sml = newtp-&gt;snd_una = seq;\tWRITE_ONCE(newtp-&gt;snd_nxt, seq);\tnewtp-&gt;snd_up = seq;\t//这里初始化了tsq队列！\tINIT_LIST_HEAD(&amp;newtp-&gt;tsq_node);\tINIT_LIST_HEAD(&amp;newtp-&gt;tsorted_sent_queue);\t//更新接收窗口 fastpath会用到？？\ttcp_init_wl(newtp, treq-&gt;rcv_isn);\tminmax_reset(&amp;newtp-&gt;rtt_min, tcp_jiffies32, ~0U);\t//设置最后一个数据包接收的时间\tnewicsk-&gt;icsk_ack.lrcvtime = tcp_jiffies32;\tnewtp-&gt;lsndtime = tcp_jiffies32;\tnewsk-&gt;sk_txhash = READ_ONCE(treq-&gt;txhash);\t//重传计数\tnewtp-&gt;total_retrans = req-&gt;num_retrans;\t//这里初始化了多个定时器\ttcp_init_xmit_timers(newsk);\tWRITE_ONCE(newtp-&gt;write_seq, newtp-&gt;pushed_seq = treq-&gt;snt_isn + 1);\t//保活定时器，这里会启动吗？\tif (sock_flag(newsk, SOCK_KEEPOPEN))\t\tinet_csk_reset_keepalive_timer(newsk,\t\t\t\t\t       keepalive_time_when(newtp));\tnewtp-&gt;rx_opt.tstamp_ok = ireq-&gt;tstamp_ok; //是否支持时间戳选项\tnewtp-&gt;rx_opt.sack_ok = ireq-&gt;sack_ok;\t\t//是否支持sack选项\tnewtp-&gt;window_clamp = req-&gt;rsk_window_clamp; //窗口大小，从req直接赋值tcp_select_initial_window\tnewtp-&gt;rcv_ssthresh = req-&gt;rsk_rcv_wnd;  //慢启动的初始值tcp_select_initial_window\tnewtp-&gt;rcv_wnd = req-&gt;rsk_rcv_wnd;\t\t\t//设置接收窗口的大小\tnewtp-&gt;rx_opt.wscale_ok = ireq-&gt;wscale_ok; //是否支持窗口缩放\tif (newtp-&gt;rx_opt.wscale_ok) &#123;//保存窗口缩放因子\t\tnewtp-&gt;rx_opt.snd_wscale = ireq-&gt;snd_wscale; \t\tnewtp-&gt;rx_opt.rcv_wscale = ireq-&gt;rcv_wscale;\t&#125; else &#123;\t\tnewtp-&gt;rx_opt.snd_wscale = newtp-&gt;rx_opt.rcv_wscale = 0;//不支持窗口u缩放\t\tnewtp-&gt;window_clamp = min(newtp-&gt;window_clamp, 65535U); //设置为最大65535\t&#125;\t////这里计算了发送窗口大小，是接受的窗口大小和缩放因子计算得到\tnewtp-&gt;snd_wnd = ntohs(tcp_hdr(skb)-&gt;window) &lt;&lt; newtp-&gt;rx_opt.snd_wscale;\t//最大窗口大小\tnewtp-&gt;max_window = newtp-&gt;snd_wnd;\t//如果开启了时间戳选项，更新相关字段，并修改tcp头的长度\tif (newtp-&gt;rx_opt.tstamp_ok) &#123;\t\tnewtp-&gt;rx_opt.ts_recent = READ_ONCE(req-&gt;ts_recent);\t\tnewtp-&gt;rx_opt.ts_recent_stamp = ktime_get_seconds();\t\tnewtp-&gt;tcp_header_len = sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;\t&#125; else &#123;\t\tnewtp-&gt;rx_opt.ts_recent_stamp = 0;\t\tnewtp-&gt;tcp_header_len = sizeof(struct tcphdr);\t&#125;\t//是否冲传过syn ack\tif (req-&gt;num_timeout) &#123;\t\tnewtp-&gt;undo_marker = treq-&gt;snt_isn;//记录synaack的序列号这意味着，后续任何对序列号​​大于等于​​ snt_isn的数据的确认，都可能用于判断此次 SYN-ACK 重传是否必要\t\tnewtp-&gt;retrans_stamp = div_u64(treq-&gt;snt_synack,//重传的时间戳\t\t\t\t\t       USEC_PER_SEC / TCP_TS_HZ);\t&#125;\tnewtp-&gt;tsoffset = treq-&gt;ts_off;//记录时间戳偏移#ifdef CONFIG_TCP_MD5SIG\tnewtp-&gt;md5sig_info = NULL;\t/*XXX*/#endif\tif (skb-&gt;len &gt;= TCP_MSS_DEFAULT + newtp-&gt;tcp_header_len)\t\tnewicsk-&gt;icsk_ack.last_seg_size = skb-&gt;len - newtp-&gt;tcp_header_len; //记录最后一个段的大小\tnewtp-&gt;rx_opt.mss_clamp = req-&gt;mss; //协商的mss\ttcp_ecn_openreq_child(newtp, req); //是否支持ecn\tnewtp-&gt;fastopen_req = NULL;\tRCU_INIT_POINTER(newtp-&gt;fastopen_rsk, NULL);\tnewtp-&gt;bpf_chg_cc_inprogress = 0;\ttcp_bpf_clone(sk, newsk);\t__TCP_INC_STATS(sock_net(sk), TCP_MIB_PASSIVEOPENS);\treturn newsk;&#125;\n\ntcp_create_openreq_child中首先调用inet_csk_clone_lock拿到sock，之后初始化sock中的各个字段，例如下个待接收的接收的序列号，待确认的序列号，初始化rtt，初始化定时器，设置支持的选项等。上述通告inet_csk_clone_lock拿到sock具体代码如下所示：\nstruct sock *inet_csk_clone_lock(const struct sock *sk,\t\t\t\t const struct request_sock *req,\t\t\t\t const gfp_t priority)&#123;\t//申请一个sock初始化各个字段 copy listensock部分字段到sock中\tstruct sock *newsk = sk_clone_lock(sk, priority);\tif (newsk) &#123;\t\t//拿到inet_connection_sock\t\tstruct inet_connection_sock *newicsk = inet_csk(newsk);\t\t//注意这里将新申请的sock的状态设置为了TCP_SYN_RECV，在外层的处理tcp_rcv_state_process中会设置为TCP_ESTABLISHED\t\tinet_sk_set_state(newsk, TCP_SYN_RECV);\t\t//清空bind相关的指针\t\tnewicsk-&gt;icsk_bind_hash = NULL;\t\tnewicsk-&gt;icsk_bind2_hash = NULL;\t\t//设置端口号\t\tinet_sk(newsk)-&gt;inet_dport = inet_rsk(req)-&gt;ir_rmt_port;\t\tinet_sk(newsk)-&gt;inet_num = inet_rsk(req)-&gt;ir_num;\t\tinet_sk(newsk)-&gt;inet_sport = htons(inet_rsk(req)-&gt;ir_num);\t\t/* listeners have SOCK_RCU_FREE, not the children */\t\t//新创建的sock不想要rcu机制释放\t\tsock_reset_flag(newsk, SOCK_RCU_FREE);\t\t//管理多播的链表\t\tinet_sk(newsk)-&gt;mc_list = NULL;\t\t//设置mark\t\tnewsk-&gt;sk_mark = inet_rsk(req)-&gt;ir_mark;\t\tatomic64_set(&amp;newsk-&gt;sk_cookie,\t\t\t     atomic64_read(&amp;inet_rsk(req)-&gt;ir_cookie));\t\t//初始化重传相关字段\t\tnewicsk-&gt;icsk_retransmits = 0;\t\tnewicsk-&gt;icsk_backoff\t  = 0;\t\tnewicsk-&gt;icsk_probes_out  = 0;\t\tnewicsk-&gt;icsk_probes_tstamp = 0;\t\t/* Deinitialize accept_queue to trap illegal accesses. */\t\t//子套接字没有监听队列，memset一下\t\tmemset(&amp;newicsk-&gt;icsk_accept_queue, 0, sizeof(newicsk-&gt;icsk_accept_queue));\t\tinet_clone_ulp(req, newsk, priority);\t\t//调用安全相关钩子\t\tsecurity_inet_csk_clone(newsk, req);\t&#125;\treturn newsk;&#125;\n\ninet_csk_clone_lock中调用sk_clone_lock完成实际的申请动作，sk_clone_lock中的工作就是从slab中申请一个sock结构，完成初始化队列，**拷贝监听sock的部分字段到新申请的sock**中等工作。\n之后回到inet_csk_clone_lock中，和外面类似，也是进行了一系列初始化的工作，例如设置新创建sock的端口号，*注意这里设置*sock的状态为TCP_SYN_RECV********，最最外层（int tcp_rcv_state_process中）的处理完成后会设置TCP_ESTABLISHED，可以发现这个状态存在的时间很短。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP三次握手-接收syn段 (二)","url":"/2025/09/09/TCP%E8%A2%AB%E5%8A%A8%E6%89%93%E5%BC%80%E6%8E%A5%E6%94%B6syn%20(%E4%BA%8C)/","content":"上一篇文章分析了在被动打开的场景下，tcp_v4_rcv中首先调用__inet_lookup_skb查找数据包所属的sk之后会调用tcp_v4_do_rcv完成进一步处理\nint tcp_v4_rcv(struct sk_buff *skb)&#123;\t...\t\t//查找数据包所属的sock\tsk = __inet_lookup_skb(net-&gt;ipv4.tcp_death_row.hashinfo,\t\t\t       skb, __tcp_hdrlen(th), th-&gt;source,\t\t\t       th-&gt;dest, sdif, &amp;refcounted);\t...\t//处理三次握手被动打开接收syn段\tif (sk-&gt;sk_state == TCP_LISTEN) &#123;\t\tret = tcp_v4_do_rcv(sk, skb);\t\tgoto put_and_return;\t&#125;\t...&#125;\n\ntcp_v4_do_rcv代码如下所示：\nint tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)&#123;\tenum skb_drop_reason reason;\tstruct sock *rsk;\tif (sk-&gt;sk_state == TCP_ESTABLISHED) &#123; /* Fast path */\t\tstruct dst_entry *dst;\t\tdst = rcu_dereference_protected(sk-&gt;sk_rx_dst,\t\t\t\t\t\tlockdep_sock_is_held(sk));\t\tsock_rps_save_rxhash(sk, skb);\t\tsk_mark_napi_id(sk, skb);\t\tif (dst) &#123;\t\t\tif (sk-&gt;sk_rx_dst_ifindex != skb-&gt;skb_iif ||\t\t\t    !INDIRECT_CALL_1(dst-&gt;ops-&gt;check, ipv4_dst_check,\t\t\t\t\t     dst, 0)) &#123;\t\t\t\tRCU_INIT_POINTER(sk-&gt;sk_rx_dst, NULL);\t\t\t\tdst_release(dst);\t\t\t&#125;\t\t&#125;\t\ttcp_rcv_established(sk, skb);\t\treturn 0;\t&#125;\treason = SKB_DROP_REASON_NOT_SPECIFIED;\tif (tcp_checksum_complete(skb))\t\tgoto csum_err;\t//处理第一次握手收到syn包和第三次（使用cookies的情况下）是不是都走这个分支？？？\t//好像这个分支里面的逻辑是只有cookies的情况下走的吧\tif (sk-&gt;sk_state == TCP_LISTEN) &#123;\t\t//处理cookie\t\tstruct sock *nsk = tcp_v4_cookie_check(sk, skb);\t\tif (!nsk)\t\t\tgoto discard;\t\tif (nsk != sk) &#123;\t\t\tif (tcp_child_process(sk, nsk, skb)) &#123;\t\t\t\trsk = nsk;\t\t\t\tgoto reset;\t\t\t&#125;\t\t\treturn 0;\t\t&#125;\t&#125; else\t\tsock_rps_save_rxhash(sk, skb);\t//第一次握手肯定走这里\tif (tcp_rcv_state_process(sk, skb)) &#123;\t\trsk = sk;\t\tgoto reset;\t&#125;\treturn 0;reset:\ttcp_v4_send_reset(rsk, skb);discard:\tkfree_skb_reason(skb, reason);\t/* Be careful here. If this function gets more complicated and\t * gcc suffers from register pressure on the x86, sk (in %ebx)\t * might be destroyed here. This current version compiles correctly,\t * but you have been warned.\t */\treturn 0;csum_err:\treason = SKB_DROP_REASON_TCP_CSUM;\ttrace_tcp_bad_csum(skb);\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\tgoto discard;&#125;\n\ntcp_v4_do_rcv中，如果当前sock处于listen状态，则调用tcp_v4_cookie_check进行处理，由于分析的是第一次握手，这里的返回值一定是监听的sk也就是说nsk==sk ，因此不会执行tcp_child_process逻辑，相当于整个分支什么也没做，启用了syncookie且第三次握手收到ack的时候（注意此时的sk仍处于listen状态，因为找到的是listen``sock所以会进入）才会执行当前分支里面的逻辑。\n因此tcp_v4_do_rcv中第一次握手收到syn后会执行tcp_rcv_state_process，tcp_rcv_state_process对于syn包的处理的相关逻辑实现如下：\nint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tconst struct tcphdr *th = tcp_hdr(skb);\tstruct request_sock *req;\tint queued = 0;\tbool acceptable;\tSKB_DR(reason);\tswitch (sk-&gt;sk_state) &#123;\tcase TCP_CLOSE:\t\tSKB_DR_SET(reason, TCP_CLOSE);\t\tgoto discard;\tcase TCP_LISTEN:\t\t//listen 状态收到带ack的包，直接rst\t\tif (th-&gt;ack)\t\t\treturn 1;\t\t//收到rst包，直接丢弃\t\tif (th-&gt;rst) &#123;\t\t\tSKB_DR_SET(reason, TCP_RESET);\t\t\tgoto discard;\t\t&#125;\t\tif (th-&gt;syn) &#123;\t\t\t//同时置位syn和fin丢弃\t\t\tif (th-&gt;fin) &#123;\t\t\t\tSKB_DR_SET(reason, TCP_FLAGS);\t\t\t\tgoto discard;\t\t\t&#125;\t\t\t/* It is possible that we process SYN packets from backlog,\t\t\t * so we need to make sure to disable BH and RCU right there.\t\t\t */\t\t\trcu_read_lock();\t\t\tlocal_bh_disable();\t\t\t//调用inet_connection_sock的ops进行进一步处理  这里是在tcp_init_sock中注册的\t\t\tacceptable = icsk-&gt;icsk_af_ops-&gt;conn_request(sk, skb) &gt;= 0;\t\t\tlocal_bh_enable();\t\t\trcu_read_unlock();\t\t\tif (!acceptable)//不能够接收，比如资源不足，直接回rst\t\t\t\treturn 1;\t\t\t//正常情况\t\t\tconsume_skb(skb);\t\t\treturn 0;\t\t&#125;\t\tSKB_DR_SET(reason, TCP_FLAGS);\t\tgoto discard;\t...&#125;\n\ntcp_rcv_state_process中对于syn包的处理逻辑为调用conn_request该回调是在tcp_init_sock中注册的。对应的具体回调为tcp_v4_conn_request\nint tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)&#123;\t/* Never answer to SYNs send to broadcast or multicast */\t//syn包是多播或者是广播地址，直接丢弃\tif (skb_rtable(skb)-&gt;rt_flags &amp; (RTCF_BROADCAST | RTCF_MULTICAST))\t\tgoto drop;\treturn tcp_conn_request(&amp;tcp_request_sock_ops,\t\t\t\t&amp;tcp_request_sock_ipv4_ops, sk, skb);drop:\ttcp_listendrop(sk);\treturn 0;&#125;\n\ntcp_v4_conn_request首先判断是否是发往广播或者是多播的地址的数据包，如果是则直接丢弃，否则调用tcp_conn_request进一步处理，这里注意，函数参数中传入了半连接结构的ops tcp_conn_request则是处理第一次握手的核心核心部分，主要做了如下几件事情\n 根据系统配置和半连接长度判断是否开启syncookie，申请一个管理半连接状态的结构req，针对接收到的syn包解析选项字段，保存序号和端口信息并查找路由，初始化序列号，根据用户配置决定是否保存syn包。将创建管理半连接的队列加入ehash中，并挂一个syn+ack定时器(注意：低版本内核应该不是加入到ehash中)，如果是syncookie模式，则还需要释放掉申请的半连接管理结构，上述逻辑代码如下所示：\nint tcp_conn_request(struct request_sock_ops *rsk_ops,\t\t     const struct tcp_request_sock_ops *af_ops,\t\t     struct sock *sk, struct sk_buff *skb)&#123;\tstruct tcp_fastopen_cookie foc = &#123; .len = -1 &#125;;\t__u32 isn = TCP_SKB_CB(skb)-&gt;tcp_tw_isn;\tstruct tcp_options_received tmp_opt;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct net *net = sock_net(sk);\tstruct sock *fastopen_sk = NULL;\tstruct request_sock *req;\tbool want_cookie = false;\tstruct dst_entry *dst;\tstruct flowi fl;\tu8 syncookies;\t//拿到系统参数关于syncookie的配置\tsyncookies = READ_ONCE(net-&gt;ipv4.sysctl_tcp_syncookies);\t/* TW buckets are converted to open requests without\t * limitations, they conserve resources and peer is\t * evidently real one.\t */\t//是否启用syn cookies\t//半连接队列是否已经达到上限，通过跟半连接队里数进行比较\t//这里的isn表示初始序列号 在timewait状态的时候这个isn可能不是0\tif ((syncookies == 2 || inet_csk_reqsk_queue_is_full(sk)) &amp;&amp; !isn) &#123;\t\t//是否需要cookie\t\twant_cookie = tcp_syn_flood_action(sk, rsk_ops-&gt;slab_name);\t\tif (!want_cookie)\t\t\tgoto drop;\t&#125;\t//如果全连接队列满了，这里直接丢弃了\tif (sk_acceptq_is_full(sk)) &#123;\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\t\tgoto drop;\t&#125;\t//申请一个半连接req结构： 注意如果cookie的话后面会释放\treq = inet_reqsk_alloc(rsk_ops, sk, !want_cookie);\tif (!req)\t\tgoto drop;\t//设置syncookies\treq-&gt;syncookie = want_cookie;\ttcp_rsk(req)-&gt;af_specific = af_ops;//挂上ops\ttcp_rsk(req)-&gt;ts_off = 0; //时间戳偏移清零#if IS_ENABLED(CONFIG_MPTCP)\ttcp_rsk(req)-&gt;is_mptcp = 0;#endif\t//清楚选项相关\ttcp_clear_options(&amp;tmp_opt);\ttmp_opt.mss_clamp = af_ops-&gt;mss_clamp; //这里是默认的536\ttmp_opt.user_mss  = tp-&gt;rx_opt.user_mss; //用户配置的mss？这里的tp是监听套接字吧，大概率是0\t//从收到的 TCP 报文头部里逐项解析 TCP 选项MSS、窗口扩大、时间戳、SACK、Fast Open、MD5等，然后把解析结果写进 opt_rx\ttcp_parse_options(sock_net(sk), skb, &amp;tmp_opt, 0,\t\t\t  want_cookie ? NULL : &amp;foc);\t//如果要启用syn cookie的话是不能够有时间戳和窗口缩放选项的\tif (want_cookie &amp;&amp; !tmp_opt.saw_tstamp)\t\ttcp_clear_options(&amp;tmp_opt);\tif (IS_ENABLED(CONFIG_SMC) &amp;&amp; want_cookie)\t\ttmp_opt.smc_ok = 0;\t//设置是否支持时间戳\ttmp_opt.tstamp_ok = tmp_opt.saw_tstamp;\t//保存端口，序列号信息\ttcp_openreq_init(req, &amp;tmp_opt, skb, sk);\tinet_rsk(req)-&gt;no_srccheck = inet_test_bit(TRANSPARENT, sk);\t/* Note: tcp_v6_init_req() might override ir_iif for link locals */\tinet_rsk(req)-&gt;ir_iif = inet_request_bound_dev_if(sk, skb);\t//调用req的ops查路由，然后拿到dst 这里调用的函数为tcp_v4_route_req(设置的req中sock的地址)\tdst = af_ops-&gt;route_req(sk, skb, &amp;fl, req);\tif (!dst)\t\tgoto drop_and_free;\tif (tmp_opt.tstamp_ok)\t\t//根据源ip目的ip算出一个时间戳偏移，这个的用处好像是相当于一个加密，防止攻击？\t\ttcp_rsk(req)-&gt;ts_off = af_ops-&gt;init_ts_off(net, skb);\t//没有使用使用syncookie 同时不是timewait的重用，大多数情况是这样的吧\tif (!want_cookie &amp;&amp; !isn) &#123;\t\t//注意这里获取了半连接队列的最大长度 \t\t\tint max_syn_backlog = READ_ONCE(net-&gt;ipv4.sysctl_max_syn_backlog);\t\t/* Kill the following clause, if you dislike this way. */\t\tif (!syncookies &amp;&amp; //没有启用syncookie\t\t    (max_syn_backlog - inet_csk_reqsk_queue_len(sk) &lt; \t\t     (max_syn_backlog &gt;&gt; 2)) &amp;&amp;  //队列长度超过了四分之三？\t\t    !tcp_peer_is_proven(req, dst)) &#123; //从metries中查找是否历史连接过\t\t\t/* Without syncookies last quarter of\t\t\t * backlog is filled with destinations,\t\t\t * proven to be alive.\t\t\t * It means that we continue to communicate\t\t\t * to destinations, already remembered\t\t\t * to the moment of synflood.\t\t\t */\t\t\t//这里直接拒绝连接了\t\t\tpr_drop_req(req, ntohs(tcp_hdr(skb)-&gt;source),\t\t\t\t    rsk_ops-&gt;family);\t\t\tgoto drop_and_release;\t\t&#125;\t\t//否则初始化服务端的序列号tcp_v4_init_seq\t\tisn = af_ops-&gt;init_seq(skb);\t&#125;\t//协商ecn\ttcp_ecn_create_request(req, skb, sk, dst);\t//syn cookie的话会走这个分支初始化序列号\tif (want_cookie) &#123;\t\tisn = cookie_init_sequence(af_ops, sk, skb, &amp;req-&gt;mss);//这里传入的mss是对端通告的大小\t\t//如果对端不支持时间戳，则禁用ecn\t\tif (!tmp_opt.tstamp_ok)\t\t\tinet_rsk(req)-&gt;ecn_ok = 0;\t&#125;\t//这里设置了序列号\ttcp_rsk(req)-&gt;snt_isn = isn;\ttcp_rsk(req)-&gt;txhash = net_tx_rndhash();\ttcp_rsk(req)-&gt;syn_tos = TCP_SKB_CB(skb)-&gt;ip_dsfield;\ttcp_openreq_init_rwin(req, sk, dst);\tsk_rx_queue_set(req_to_sk(req), skb);\t//如果是正常握手流程\tif (!want_cookie) &#123;\t\t//根据用户选项是否需要保存syn包\t\ttcp_reqsk_record_syn(sk, req, skb);\t\tfastopen_sk = tcp_try_fastopen(sk, skb, req, &amp;foc, dst);\t&#125;\t//TFO相关\tif (fastopen_sk) &#123;\t\taf_ops-&gt;send_synack(fastopen_sk, dst, &amp;fl, req,\t\t\t\t    &amp;foc, TCP_SYNACK_FASTOPEN, skb);\t\t/* Add the child socket directly into the accept queue */\t\tif (!inet_csk_reqsk_queue_add(sk, req, fastopen_sk)) &#123;\t\t\treqsk_fastopen_remove(fastopen_sk, req, false);\t\t\tbh_unlock_sock(fastopen_sk);\t\t\tsock_put(fastopen_sk);\t\t\tgoto drop_and_free;\t\t&#125;\t\tsk-&gt;sk_data_ready(sk);\t\tbh_unlock_sock(fastopen_sk);\t\tsock_put(fastopen_sk);\t&#125; else &#123;\t\ttcp_rsk(req)-&gt;tfo_listener = false;\t\tif (!want_cookie) &#123;\t\t\treq-&gt;timeout = tcp_timeout_init((struct sock *)req);//初始化超时重传的时间\t\t\t//将半连接结构插入到ehash中，更新半连接的数量\t\t\tinet_csk_reqsk_queue_hash_add(sk, req, req-&gt;timeout);\t\t&#125;\t\t//发送syn_ack\t\taf_ops-&gt;send_synack(sk, dst, &amp;fl, req, &amp;foc,\t\t\t\t    !want_cookie ? TCP_SYNACK_NORMAL :\t\t\t\t\t\t   TCP_SYNACK_COOKIE,\t\t\t\t    skb);\t\t//如果是syncookie 这个要释放半连接结构\t\tif (want_cookie) &#123;\t\t\treqsk_free(req);\t\t\treturn 0;\t\t&#125;\t&#125;\treqsk_put(req);\treturn 0;drop_and_release:\tdst_release(dst);drop_and_free:\t__reqsk_free(req);drop:\ttcp_listendrop(sk);\treturn 0;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP三次握手-接收syn段 (一)","url":"/2025/09/07/TCP%E8%A2%AB%E5%8A%A8%E6%89%93%E5%BC%80%E6%8E%A5%E6%94%B6syn%EF%BC%88%E4%B8%80%EF%BC%89/","content":"本节内容分析TCP三次握手被动打开，即接收syn包的的处理流程，具体来说，当协议栈IP层收到数据包后，在ip_local_deliver_finish中会根据不同的protocol类型分发给不同的处理函数，对于TCP报文，则全都会调用tcp_v4_rcv进一步处理，tcp_v4_rcv则是在inet_init中注册的。\n下面看一下tcp_v4_rcv针对syn包的处理，下面省略了其他状态的处理，只展示接收syn包相关的处理逻辑，具体代码如下：\nint tcp_v4_rcv(struct sk_buff *skb)&#123;\tstruct net *net = dev_net(skb-&gt;dev);\tenum skb_drop_reason drop_reason;\tint sdif = inet_sdif(skb);\tint dif = inet_iif(skb);\tconst struct iphdr *iph;\tconst struct tcphdr *th;\tbool refcounted;\tstruct sock *sk;\tint ret;\tdrop_reason = SKB_DROP_REASON_NOT_SPECIFIED;\t//不是发给本机的包，直接丢\tif (skb-&gt;pkt_type != PACKET_HOST)\t\tgoto discard_it;\t/* Count it even if it&#x27;s bad */\t__TCP_INC_STATS(net, TCP_MIB_INSEGS);\t//拉一下tcp头，确保线性部分至少有TCP头\tif (!pskb_may_pull(skb, sizeof(struct tcphdr)))\t\tgoto discard_it;\tth = (const struct tcphdr *)skb-&gt;data;\t//坏包\tif (unlikely(th-&gt;doff &lt; sizeof(struct tcphdr) / 4)) &#123;\t\tdrop_reason = SKB_DROP_REASON_PKT_TOO_SMALL;\t\tgoto bad_packet;\t&#125;\t//把选项部分也拉到线性部分\tif (!pskb_may_pull(skb, th-&gt;doff * 4))\t\tgoto discard_it;\t/* An explanation is required here, I think.\t * Packet length and doff are validated by header prediction,\t * provided case of th-&gt;doff==0 is eliminated.\t * So, we defer the checks. */\tif (skb_checksum_init(skb, IPPROTO_TCP, inet_compute_pseudo))\t\tgoto csum_error;\tth = (const struct tcphdr *)skb-&gt;data;\tiph = ip_hdr(skb);lookup:\t//查找数据包所属的sock\tsk = __inet_lookup_skb(net-&gt;ipv4.tcp_death_row.hashinfo,\t\t\t       skb, __tcp_hdrlen(th), th-&gt;source,\t\t\t       th-&gt;dest, sdif, &amp;refcounted);\tif (!sk)\t\tgoto no_tcp_socket;process:\t...\t//处理三次握手被动打开接收syn段\tif (sk-&gt;sk_state == TCP_LISTEN) &#123;\t\tret = tcp_v4_do_rcv(sk, skb);\t\tgoto put_and_return;\t&#125;\t...&#125;\n\n上述代码中，涉及到接收到syn包的处理，主要就两个地方，一是调用__inet_lookup_skb来查找数据包所关联的sock，由于处理syn包自然找到的是监听的sock。二是调用tcp_v4_do_rcv进行进一步处理。下面看一下__inet_lookup_skb是如何找到sock的，具体代码如下：\nstatic inline struct sock *__inet_lookup_skb(struct inet_hashinfo *hashinfo,\t\t\t\t\t     struct sk_buff *skb,\t\t\t\t\t     int doff,\t\t\t\t\t     const __be16 sport,\t\t\t\t\t     const __be16 dport,\t\t\t\t\t     const int sdif,\t\t\t\t\t     bool *refcounted)&#123;\tstruct net *net = dev_net(skb_dst(skb)-&gt;dev);\tconst struct iphdr *iph = ip_hdr(skb);\tstruct sock *sk;\t//相当于一个快速路径，先看看skb有没有已经关联了一个sk(ip_rcvfinish中会设置)\tsk = inet_steal_sock(net, skb, doff, iph-&gt;saddr, sport, iph-&gt;daddr, dport,\t\t\t     refcounted, inet_ehashfn);\tif (IS_ERR(sk))\t\treturn NULL;\tif (sk)\t\treturn sk;\t//传入源ip目的ip源port目的port 注意：这里的hash表其实隐含了协议类型\t//所以通常说是五元组找sk？？？？？\treturn __inet_lookup(net, hashinfo, skb,\t\t\t     doff, iph-&gt;saddr, sport,\t\t\t     iph-&gt;daddr, dport, inet_iif(skb), sdif,\t\t\t     refcounted);&#125;\n\n__inet_lookup_skb中首先调用inet_steal_sock来尝试从skb的结构体中直接拿到sock（相当于一个快速路径），如果找到了，就直接返回了。注意：这里的sock和skb是查路由的时候关联上的，但是第一个syn包能不能走这个快速路径不知道，inet_steal_sock具体代码如下所示：\nstruct sock *inet_steal_sock(struct net *net, struct sk_buff *skb, int doff,\t\t\t     const __be32 saddr, const __be16 sport,\t\t\t     const __be32 daddr, const __be16 dport,\t\t\t     bool *refcounted, inet_ehashfn_t *ehashfn)&#123;\tstruct sock *sk, *reuse_sk;\tbool prefetched;\t//尝试从skb中获取sk，是不是ip层查路由时候拿到的？\tsk = skb_steal_sock(skb, refcounted, &amp;prefetched);\tif (!sk)\t\treturn NULL;\t//这里大概率走这个分支吧，因为prefetched 为false？\tif (!prefetched || !sk_fullsock(sk))\t\treturn sk;\t//TCP非listen状态的处理\tif (sk-&gt;sk_protocol == IPPROTO_TCP) &#123;\t\tif (sk-&gt;sk_state != TCP_LISTEN)\t\t\treturn sk;\t//UDP处理\t&#125; else if (sk-&gt;sk_protocol == IPPROTO_UDP) &#123;\t\tif (sk-&gt;sk_state != TCP_CLOSE)\t\t\treturn sk;\t&#125; else &#123;\t\treturn sk;\t&#125;\treuse_sk = inet_lookup_reuseport(net, sk, skb, doff,\t\t\t\t\t saddr, sport, daddr, ntohs(dport),\t\t\t\t\t ehashfn);\tif (!reuse_sk)\t\treturn sk;\t/* We&#x27;ve chosen a new reuseport sock which is never refcounted. This\t * implies that sk also isn&#x27;t refcounted.\t */\tWARN_ON_ONCE(*refcounted);\treturn reuse_sk;&#125;\n\ninet_steal_sock中的主要逻辑为调用skb_steal_sock尝试从 skb 里直接“偷”出一个已经绑定的 socket。具体代码如下所示：\nstatic inline struct sock *skb_steal_sock(struct sk_buff *skb, bool *refcounted, bool *prefetched)&#123;\t\t//当前的skb是否挂了sk\tif (skb-&gt;sk) &#123;\t\tstruct sock *sk = skb-&gt;sk;\t\t*refcounted = true;\t\t//是否预取的，bpf可能会预取？，否则应该是false\t\t*prefetched = skb_sk_is_prefetched(skb);\t\tif (*prefetched)\t\t//根据sk的状态返回是否需要引用计数，只有完整的状态会返回true(非timewait和new_syn_recv)\t\t\t*refcounted = sk_is_refcounted(sk);\t\tskb-&gt;destructor = NULL; //怕里面有清掉sock的操作？\t\tskb-&gt;sk = NULL; //因为这个sock已经被偷走了？所以不能关联这个skb了？\t\treturn sk;\t&#125;\t*prefetched = false;\t*refcounted = false;\treturn NULL;&#125;\n\nskb_steal_sock做的事情很简单，其实就是根据skb获取一个sock，注意获取sock后，解除了skb和这个sock的关联。\n如果上述快速路径没有获取到sock的话，则在__inet_lookup_skb中会调用__inet_lookup继续查找sock（根据源ip，目的ip，源port，目的port）。具体代码如下所示：\nstatic inline struct sock *__inet_lookup(struct net *net,\t\t\t\t\t struct inet_hashinfo *hashinfo,\t\t\t\t\t struct sk_buff *skb, int doff,\t\t\t\t\t const __be32 saddr, const __be16 sport,\t\t\t\t\t const __be32 daddr, const __be16 dport,\t\t\t\t\t const int dif, const int sdif,\t\t\t\t\t bool *refcounted)&#123;\tu16 hnum = ntohs(dport);\tstruct sock *sk;\t//查找ehash\tsk = __inet_lookup_established(net, hashinfo, saddr, sport,\t\t\t\t       daddr, hnum, dif, sdif);\t*refcounted = true;\tif (sk)\t\treturn sk;\t*refcounted = false;\t//如果上面ehash中没有找到，那就找listenhash\treturn __inet_lookup_listener(net, hashinfo, skb, doff, saddr,\t\t\t\t      sport, daddr, hnum, dif, sdif);&#125;\n\n__inet_lookup中首先会在已建立连接的hash表中查找sock如果没有找到，则在监听hash表中继续查找，因为是三次握手处理syn包，则在已建立的hash表中一定找不到sock所以需要调用__inet_lookup_listener进一步查找，具体代码如下所示：\nstruct sock *__inet_lookup_listener(struct net *net,\t\t\t\t    struct inet_hashinfo *hashinfo,\t\t\t\t    struct sk_buff *skb, int doff,\t\t\t\t    const __be32 saddr, __be16 sport,\t\t\t\t    const __be32 daddr, const unsigned short hnum,\t\t\t\t    const int dif, const int sdif)&#123;\tstruct inet_listen_hashbucket *ilb2;\tstruct sock *result = NULL;\tunsigned int hash2;\t/* Lookup redirect from BPF */\t//bpf相关使用bpf的程序找sk?\tif (static_branch_unlikely(&amp;bpf_sk_lookup_enabled) &amp;&amp;\t    hashinfo == net-&gt;ipv4.tcp_death_row.hashinfo) &#123;\t\tresult = inet_lookup_run_sk_lookup(net, IPPROTO_TCP, skb, doff,\t\t\t\t\t\t   saddr, sport, daddr, hnum, dif,\t\t\t\t\t\t   inet_ehashfn);\t\tif (result)\t\t\tgoto done;\t&#125;\t//根据目的ip目的port计算一个hash值\thash2 = ipv4_portaddr_hash(net, daddr, hnum);\t//找到对应的hash桶\tilb2 = inet_lhash2_bucket(hashinfo, hash2);\t//注意： 这里是精确查找，区分和下面的anyaddr\tresult = inet_lhash2_lookup(net, ilb2, skb, doff,\t\t\t\t    saddr, sport, daddr, hnum,\t\t\t\t    dif, sdif);\tif (result)\t\tgoto done;\t/* Lookup lhash2 with INADDR_ANY */\t//和上面一样只不过这次找的是anyaddr\thash2 = ipv4_portaddr_hash(net, htonl(INADDR_ANY), hnum);\tilb2 = inet_lhash2_bucket(hashinfo, hash2);\t//这里anyaddr + port\tresult = inet_lhash2_lookup(net, ilb2, skb, doff,\t\t\t\t    saddr, sport, htonl(INADDR_ANY), hnum,\t\t\t\t    dif, sdif);done:\tif (IS_ERR(result))\t\treturn NULL;\treturn result;&#125;\n\n上述代码主要的目的是根据传入的参数，也就是ip和port来找到sock，上述找的过程可以分为两步，第一步是精确找到，即根据数据包的目的ip和目的port去到listen``hash表中是否有匹配的sock，如果没找到则执行第二步，根据anyaddr和port去找到一个桶，然后看这个桶中是否有匹配的sock。**查找并返回结果的代码inet_lhash2_lookup**如下所示：\n/* called with rcu_read_lock() : No refcount taken on the socket */static struct sock *inet_lhash2_lookup(struct net *net,\t\t\t\tstruct inet_listen_hashbucket *ilb2,\t\t\t\tstruct sk_buff *skb, int doff,\t\t\t\tconst __be32 saddr, __be16 sport,\t\t\t\tconst __be32 daddr, const unsigned short hnum,\t\t\t\tconst int dif, const int sdif)&#123;\tstruct sock *sk, *result = NULL;\tstruct hlist_nulls_node *node;\tint score, hiscore = 0;\t//遍历桶下挂的每个sock，为每个套接字计算得分\tsk_nulls_for_each_rcu(sk, node, &amp;ilb2-&gt;nulls_head) &#123;\t\t//为每个套接字计算得分\t\tscore = compute_score(sk, net, hnum, daddr, dif, sdif);\t\tif (score &gt; hiscore) &#123;\t\t\t//处理reuseport 的情况，如果开启了reuseport则优先返回reuseport的sock\t\t\tresult = inet_lookup_reuseport(net, sk, skb, doff,\t\t\t\t\t\t       saddr, sport, daddr, hnum, inet_ehashfn);\t\t\tif (result)\t\t\t\treturn result;\t\t\t//没开启reuseport的情况，则遍历所有的之后返回一个得分最高的\t\t\tresult = sk;\t\t\thiscore = score;\t\t&#125;\t&#125;\treturn result;&#125;\n\ninet_lhash2_lookup主要做的工作就是遍历hash桶下挂的所有sock为每一个sock根据传入的ip和port信息来计算得分，然后返回得分最高的一个（得分根据cpu亲和性，绑定的设备等进行加分），如果某个sock加入了端口重用组的话，则会进一步进行处理，从端口重用组根据hash选一个合适的sock，上述计算得分的compute_score逻辑如下所示：\nstatic inline int compute_score(struct sock *sk, struct net *net,\t\t\t\tconst unsigned short hnum, const __be32 daddr,\t\t\t\tconst int dif, const int sdif)&#123;\tint score = -1;\t//前提是网络命令空间，和端口号必须相同是\tif (net_eq(sock_net(sk), net) &amp;&amp; sk-&gt;sk_num == hnum &amp;&amp;\t\t\t!ipv6_only_sock(sk)) &#123;\t\t//如果ip地址不同这里直就返回了\t\tif (sk-&gt;sk_rcv_saddr != daddr)\t\t\treturn -1;\t\t//检查接口是否匹配比如绑定的接口和接收的接口不一致就直接返回-1\t\tif (!inet_sk_bound_dev_eq(net, sk-&gt;sk_bound_dev_if, dif, sdif))\t\t\treturn -1;\t\tscore =  sk-&gt;sk_bound_dev_if ? 2 : 1;//如果指定了特定的接口，加2分，否则1分\t\tif (sk-&gt;sk_family == PF_INET)\t\t\tscore++;\t\t//当前的cpuip和收包的cpuid在同一个加分\t\tif (READ_ONCE(sk-&gt;sk_incoming_cpu) == raw_smp_processor_id())\t\t\tscore++;\t&#125;\treturn score;&#125;\n\ncompute_score中，如果ip地址不同则直接返回了，如果绑定了网口，但是网口不一致，也直接返回不匹配。如果指定了特定的接口则加2分，如果在同一个cpu上加一分。\n计算得分后，会进一步处理reuseport的情况，具体代码如下所示：\n */struct sock *inet_lookup_reuseport(struct net *net, struct sock *sk,\t\t\t\t   struct sk_buff *skb, int doff,\t\t\t\t   __be32 saddr, __be16 sport,\t\t\t\t   __be32 daddr, unsigned short hnum,\t\t\t\t   inet_ehashfn_t *ehashfn)&#123;\tstruct sock *reuse_sk = NULL;\tu32 phash;\tif (sk-&gt;sk_reuseport) &#123;\t\t//根据源ip目的ip 源端口 目的端口计算出一个hash值\t\tphash = INDIRECT_CALL_2(ehashfn, udp_ehashfn, inet_ehashfn,\t\t\t\t\tnet, daddr, hnum, saddr, sport);\t\treuse_sk = reuseport_select_sock(sk, phash, skb, doff);\t&#125;\treturn reuse_sk;&#125;\n\n如果当前的sk没有关联的端口重用组，则不走reuseport相关的逻辑，否则的话计算一个hash值，然后调用reuseport_select_sock从端口重用组中选择一个合适的sk具体代码如下所示：\nstruct sock *reuseport_select_sock(struct sock *sk,\t\t\t\t   u32 hash,\t\t\t\t   struct sk_buff *skb,\t\t\t\t   int hdr_len)&#123;\tstruct sock_reuseport *reuse;\tstruct bpf_prog *prog;\tstruct sock *sk2 = NULL;\tu16 socks;\trcu_read_lock();\t//这个是listen时候更新的端口重用组，里面挂的都是reuseport的sock\treuse = rcu_dereference(sk-&gt;sk_reuseport_cb);\t/* if memory allocation failed or add call is not yet complete */\tif (!reuse)\t\tgoto out;\tprog = rcu_dereference(reuse-&gt;prog);\t////当前端口重用组有多少个sock\tsocks = READ_ONCE(reuse-&gt;num_socks);\tif (likely(socks)) &#123;\t\t/* paired with smp_wmb() in __reuseport_add_sock() */\t\tsmp_rmb();//内存屏障\t\tif (!prog || !skb)\t\t\tgoto select_by_hash;\t\t//bpf相关，根据bpf程序选择sk\t\tif (prog-&gt;type == BPF_PROG_TYPE_SK_REUSEPORT)\t\t\tsk2 = bpf_run_sk_reuseport(reuse, sk, prog, skb, NULL, hash);\t\telse\t\t\tsk2 = run_bpf_filter(reuse, socks, prog, skb, hdr_len);select_by_hash:\t\t/* no bpf or invalid bpf result: fall back to hash usage */\t\tif (!sk2)\t\t//根据哈希从多个重用端口的套接字中选择一个\t\t\tsk2 = reuseport_select_sock_by_hash(reuse, hash, socks);\t&#125;out:\trcu_read_unlock();\treturn sk2;&#125;\n\n注意:上述提到的端口重用组是在listen系统调用中处理的。\nreuseport_select_sock中调用reuseport_select_sock_by_hash完成进一步工作，具体代码如下所示：\nstatic struct sock *reuseport_select_sock_by_hash(struct sock_reuseport *reuse,\t\t\t\t\t\t  u32 hash, u16 num_socks)&#123;\tstruct sock *first_valid_sk = NULL;\tint i, j;\t//根据hash值映射一个索引\ti = j = reciprocal_scale(hash, num_socks);\tdo &#123;\t\tstruct sock *sk = reuse-&gt;socks[i];\t\t//如果端口重用组的sock已经是建立连接了那肯定就直接continue了\t\tif (sk-&gt;sk_state != TCP_ESTABLISHED) &#123;\t\t\t/* Paired with WRITE_ONCE() in __reuseport_(get|put)_incoming_cpu(). */\t\t\t//如果整个端口重用组没有记录cpu则就用当前的\t\t\tif (!READ_ONCE(reuse-&gt;incoming_cpu))\t\t\t\treturn sk;\t\t\t//如果当前的cpu和sk的cpu是同一个cpu则直接返回\t\t\t/* Paired with WRITE_ONCE() in reuseport_update_incoming_cpu(). */\t\t\tif (READ_ONCE(sk-&gt;sk_incoming_cpu) == raw_smp_processor_id())\t\t\t\treturn sk;\t\t\t//这个sk用来兜底返回\t\t\tif (!first_valid_sk)\t\t\t\tfirst_valid_sk = sk;\t\t&#125;\t\ti++;\t\tif (i &gt;= num_socks)\t\t\ti = 0;\t&#125; while (i != j);\treturn first_valid_sk;&#125;\n\nreuseport_select_sock_by_hash中的逻辑就是更具hash确定端口重用组中的一个索引，从这索引开始遍历端口重用组，然后可以说是根据cpu的亲和性选择一个sock返回，至此查找sock的工作就完成了。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP输出 tcp_sendmsg_locked（一）","url":"/2025/11/18/TCP%E8%BE%93%E5%87%BAtcp_sendmsg_locked%EF%BC%88%E4%B8%80%EF%BC%89/","content":"tcp_sendmsg_locked为发送tcp数据包的核心函数，主要完成数据包从用户态到内核态的拷贝，根据mss，GSO等信息封装数据包，最终将数据包放入发送队列，更新内存记账等。具体代码如下所示：\nint tcp_sendmsg_locked(struct sock *sk, struct msghdr *msg, size_t size)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct ubuf_info *uarg = NULL;\tstruct sk_buff *skb;\tstruct sockcm_cookie sockc;\tint flags, err, copied = 0;\tint mss_now = 0, size_goal, copied_syn = 0;\tint process_backlog = 0;\tint zc = 0;\tlong timeo;\tflags = msg-&gt;msg_flags;\t//零拷贝处理\tif ((flags &amp; MSG_ZEROCOPY) &amp;&amp; size) &#123;\t\tif (msg-&gt;msg_ubuf) &#123;\t\t\tuarg = msg-&gt;msg_ubuf;\t\t\tif (sk-&gt;sk_route_caps &amp; NETIF_F_SG)\t\t\t\tzc = MSG_ZEROCOPY;\t\t&#125; else if (sock_flag(sk, SOCK_ZEROCOPY)) &#123;\t\t\tskb = tcp_write_queue_tail(sk);\t\t\tuarg = msg_zerocopy_realloc(sk, size, skb_zcopy(skb));\t\t\tif (!uarg) &#123;\t\t\t\terr = -ENOBUFS;\t\t\t\tgoto out_err;\t\t\t&#125;\t\t\tif (sk-&gt;sk_route_caps &amp; NETIF_F_SG)\t\t\t\tzc = MSG_ZEROCOPY;\t\t\telse\t\t\t\tuarg_to_msgzc(uarg)-&gt;zerocopy = 0;\t\t&#125;\t&#125; else if (unlikely(msg-&gt;msg_flags &amp; MSG_SPLICE_PAGES) &amp;&amp; size) &#123;\t\tif (sk-&gt;sk_route_caps &amp; NETIF_F_SG)\t\t\tzc = MSG_SPLICE_PAGES;\t&#125;\t//TFO相关\tif (unlikely(flags &amp; MSG_FASTOPEN ||\t\t     inet_test_bit(DEFER_CONNECT, sk)) &amp;&amp;\t    !tp-&gt;repair) &#123;\t\terr = tcp_sendmsg_fastopen(sk, msg, &amp;copied_syn, size, uarg);\t\tif (err == -EINPROGRESS &amp;&amp; copied_syn &gt; 0)\t\t\tgoto out;\t\telse if (err)\t\t\tgoto out_err;\t&#125;\t//设置tcp发送超时时间\ttimeo = sock_sndtimeo(sk, flags &amp; MSG_DONTWAIT);\t//于判断当前 TCP 连接是否因为应用程序没有提供足够数据\ttcp_rate_check_app_limited(sk);  /* is sending application-limited? */\t/* Wait for a connection to finish. One exception is TCP Fast Open\t * (passive side) where data is allowed to be sent before a connection\t * is fully established.\t */\t//TFO\tif (((1 &lt;&lt; sk-&gt;sk_state) &amp; ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) &amp;&amp;\t    !tcp_passive_fastopen(sk)) &#123;\t\terr = sk_stream_wait_connect(sk, &amp;timeo);\t\tif (err != 0)\t\t\tgoto do_error;\t&#125;\t//热迁移场景\tif (unlikely(tp-&gt;repair)) &#123;\t\tif (tp-&gt;repair_queue == TCP_RECV_QUEUE) &#123;\t\t\tcopied = tcp_send_rcvq(sk, msg, size);\t\t\tgoto out_nopush;\t\t&#125;\t\terr = -EINVAL;\t\tif (tp-&gt;repair_queue == TCP_NO_QUEUE)\t\t\tgoto out_err;\t\t/* &#x27;common&#x27; sending to sendq */\t&#125;\t//处理用户的控制信息，保存到sockc中\tsockcm_init(&amp;sockc, sk);\tif (msg-&gt;msg_controllen) &#123;\t\terr = sock_cmsg_send(sk, msg, &amp;sockc);\t\tif (unlikely(err)) &#123;\t\t\terr = -EINVAL;\t\t\tgoto out_err;\t\t&#125;\t&#125;\t/* This should be in poll */\tsk_clear_bit(SOCKWQ_ASYNC_NOSPACE, sk);\t/* Ok commence sending. */\tcopied = 0;restart:\t//计算当前的mss ,这里传出的size_goal大概率是64k\tmss_now = tcp_send_mss(sk, &amp;size_goal, flags);\terr = -EPIPE;\tif (sk-&gt;sk_err || (sk-&gt;sk_shutdown &amp; SEND_SHUTDOWN))\t\tgoto do_error;\twhile (msg_data_left(msg)) &#123;\t\tssize_t copy = 0;\t\t//队列尾部取出一个数据包\t\tskb = tcp_write_queue_tail(sk);\t\tif (skb)\t\t\t//计算剩余可以拷贝的空间\t\t\tcopy = size_goal - skb-&gt;len;\t\t//没有可以copy的空间了\t\tif (copy &lt;= 0 || !tcp_skb_can_collapse_to(skb)) &#123;\t\t\tbool first_skb;new_segment://判断是否有缓冲区，如果发送的队列比发送的缓冲区大，这里就直接goto了\t\t\tif (!sk_stream_memory_free(sk))\t\t\t\tgoto wait_for_space;\t\t\t//连续创建了16个skb\t\t\tif (unlikely(process_backlog &gt;= 16)) &#123;\t\t\t\tprocess_backlog = 0;\t\t\t\tif (sk_flush_backlog(sk))\t\t\t\t\tgoto restart;\t\t\t&#125;\t\t\t//重传队列和发送队列是否都是空\t\t\tfirst_skb = tcp_rtx_and_write_queues_empty(sk);\t\t\t//申请一个skb\t\t\tskb = tcp_stream_alloc_skb(sk, sk-&gt;sk_allocation,\t\t\t\t\t\t   first_skb);\t\t\tif (!skb) //如果申请skb失败，那就等\t\t\t\tgoto wait_for_space;\t\t\t//增加创建skb的计数\t\t\tprocess_backlog++;\t\t\t//数据包入队列，更新内存相关统计\t\t\ttcp_skb_entail(sk, skb);\t\t\tcopy = size_goal; \t\t\t/* All packets are restored as if they have\t\t\t * already been sent. skb_mstamp_ns isn&#x27;t set to\t\t\t * avoid wrong rtt estimation.\t\t\t */\t\t\tif (tp-&gt;repair)\t\t\t\tTCP_SKB_CB(skb)-&gt;sacked |= TCPCB_REPAIRED;\t\t&#125;\t\t//注意这里开始是把数据包追加到skb上\t\t/* Try to append data to the end of skb. */\t\t//copy表示能容纳的最大字节数，如果大于用户数据，就拷贝用户指定的大小，否则肯定是最大大小，合理\t\tif (copy &gt; msg_data_left(msg))\t\t\tcopy = msg_data_left(msg);\t\tif (zc == 0) &#123;\t\t\tbool merge = true;\t\t\tint i = skb_shinfo(skb)-&gt;nr_frags;\t\t\t//获取一个page的缓存，这里大概率是获取当前线程的page\t\t\tstruct page_frag *pfrag = sk_page_frag(sk);\t\t\t//确保上面获取的页有足够的空间，没有的话这里会申请一个page，申请失败会goto\t\t\tif (!sk_page_frag_refill(sk, pfrag))\t\t\t\tgoto wait_for_space;\t\t\t//判断当前数据是否能追加到 skb 的最后一个 frag 上 (通常应该是可以)\t\t\tif (!skb_can_coalesce(skb, i, pfrag-&gt;page,\t\t\t\t\t      pfrag-&gt;offset)) &#123;\t\t\t\tif (i &gt;= READ_ONCE(sysctl_max_skb_frags)) &#123; //17 表示一个skb最多挂17个非线性的\t\t\t\t\ttcp_mark_push(tp, skb);\t\t\t\t\tgoto new_segment;\t\t\t\t&#125;\t\t\t\tmerge = false;//不能合并\t\t\t&#125;\t\t\t//计算可以copy的大小\t\t\tcopy = min_t(int, copy, pfrag-&gt;size - pfrag-&gt;offset);\t\t\tif (unlikely(skb_zcopy_pure(skb) || skb_zcopy_managed(skb))) &#123;\t\t\t\tif (tcp_downgrade_zcopy_pure(sk, skb))\t\t\t\t\tgoto wait_for_space;\t\t\t\tskb_zcopy_downgrade_managed(skb);\t\t\t&#125;\t\t\t//内存记账，这里面一个是全局的一个是sk自己的，这里可能会缩小copy\t\t\tcopy = tcp_wmem_schedule(sk, copy);\t\t\tif (!copy)\t\t\t\tgoto wait_for_space;\t\t\t//真正的copy\t\t\terr = skb_copy_to_page_nocache(sk, &amp;msg-&gt;msg_iter, skb,\t\t\t\t\t\t       pfrag-&gt;page,\t\t\t\t\t\t       pfrag-&gt;offset,\t\t\t\t\t\t       copy);\t\t\tif (err)\t\t\t\tgoto do_error;\t\t\t/* Update the skb. */\t\t\t//可以合并的情况\t\t\tif (merge) &#123;\t\t\t\tskb_frag_size_add(&amp;skb_shinfo(skb)-&gt;frags[i - 1], copy);\t\t\t&#125; else &#123; //申请了新的page\t\t\t\tskb_fill_page_desc(skb, i, pfrag-&gt;page,\t\t\t\t\t\t   pfrag-&gt;offset, copy);\t\t\t\tpage_ref_inc(pfrag-&gt;page);//增加引用计数\t\t\t&#125;\t\t\tpfrag-&gt;offset += copy;//更新page的offset\t\t&#125; else if (zc == MSG_ZEROCOPY)  &#123;\t\t\t/* First append to a fragless skb builds initial\t\t\t * pure zerocopy skb\t\t\t */\t\t\tif (!skb-&gt;len)\t\t\t\tskb_shinfo(skb)-&gt;flags |= SKBFL_PURE_ZEROCOPY;\t\t\tif (!skb_zcopy_pure(skb)) &#123;\t\t\t\tcopy = tcp_wmem_schedule(sk, copy);\t\t\t\tif (!copy)\t\t\t\t\tgoto wait_for_space;\t\t\t&#125;\t\t\terr = skb_zerocopy_iter_stream(sk, skb, msg, copy, uarg);\t\t\tif (err == -EMSGSIZE || err == -EEXIST) &#123;\t\t\t\ttcp_mark_push(tp, skb);\t\t\t\tgoto new_segment;\t\t\t&#125;\t\t\tif (err &lt; 0)\t\t\t\tgoto do_error;\t\t\tcopy = err;\t\t&#125; else if (zc == MSG_SPLICE_PAGES) &#123;\t\t\t/* Splice in data if we can; copy if we can&#x27;t. */\t\t\tif (tcp_downgrade_zcopy_pure(sk, skb))\t\t\t\tgoto wait_for_space;\t\t\tcopy = tcp_wmem_schedule(sk, copy);\t\t\tif (!copy)\t\t\t\tgoto wait_for_space;\t\t\terr = skb_splice_from_iter(skb, &amp;msg-&gt;msg_iter, copy,\t\t\t\t\t\t   sk-&gt;sk_allocation);\t\t\tif (err &lt; 0) &#123;\t\t\t\tif (err == -EMSGSIZE) &#123;\t\t\t\t\ttcp_mark_push(tp, skb);\t\t\t\t\tgoto new_segment;\t\t\t\t&#125;\t\t\t\tgoto do_error;\t\t\t&#125;\t\t\tcopy = err;\t\t\tif (!(flags &amp; MSG_NO_SHARED_FRAGS))\t\t\t\tskb_shinfo(skb)-&gt;flags |= SKBFL_SHARED_FRAG;\t\t\tsk_wmem_queued_add(sk, copy);\t\t\tsk_mem_charge(sk, copy);\t\t&#125;\t\t//若当前这个 skb 之前没 copy（即刚创建或第一次写）则清除 PSH 标志位\t\tif (!copied)\t\t\tTCP_SKB_CB(skb)-&gt;tcp_flags &amp;= ~TCPHDR_PSH;\t\t//发送缓冲区中最后一个字节的下一个序列号\t\tWRITE_ONCE(tp-&gt;write_seq, tp-&gt;write_seq + copy);\t\t//该 skb 已覆盖的 TCP 数据范围\t\tTCP_SKB_CB(skb)-&gt;end_seq += copy;\t\ttcp_skb_pcount_set(skb, 0); //初始认为这是 non-GSO skb\t\t//这个值最终作为 send() 的返回值\t\tcopied += copy;\t\t//判断是否已经拷贝完用户数据\t\tif (!msg_data_left(msg)) &#123;\t\t\tif (unlikely(flags &amp; MSG_EOR))\t\t\t\tTCP_SKB_CB(skb)-&gt;eor = 1;\t\t\tgoto out;//跳出主循环 out\t\t&#125;\t\t//决定是否继续累积写，还是 push（发送)\t\t//skb还没写够期望大小 \t\tif (skb-&gt;len &lt; size_goal || (flags &amp; MSG_OOB) || unlikely(tp-&gt;repair))\t\t\tcontinue;\t\t//判断现在是否应该把数据push 给 TCP 输出层\t\tif (forced_push(tp)) &#123; //如果当前写入的数据量已经超过发送窗口的一半，那就必须强制 push\t\t\ttcp_mark_push(tp, skb); //设置push标志位\t\t\t__tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);\t\t&#125; else if (skb == tcp_send_head(sk)) //当前 skb 恰好是发送队列的头部\t\t\ttcp_push_one(sk, mss_now); //，只发送一个\t\tcontinue;wait_for_space: //skb 或者page申请失败都会走这里\t\tset_bit(SOCK_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags);\t\ttcp_remove_empty_skb(sk);//如果队列尾部那个skb是空的就把它删掉\t\tif (copied) //copy 了一部分数据到某个skb ，，先把这部分内容 push 出去。\t\t\ttcp_push(sk, flags &amp; ~MSG_MORE, mss_now, //不再等待更多数据\t\t\t\t TCP_NAGLE_PUSH, size_goal); //忽略 Nagle 延迟策略，立即发送\t\t//等待\t\terr = sk_stream_wait_memory(sk, &amp;timeo);\t\tif (err != 0)\t\t\tgoto do_error;\t\t//重新计算mss\t\tmss_now = tcp_send_mss(sk, &amp;size_goal, flags);\t&#125;out:\tif (copied) &#123;\t\ttcp_tx_timestamp(sk, sockc.tsflags);\t\t//write_queue 中积累的skb按条件发送到ip层可能立即发 也可能等Nagle/pacing。\t\ttcp_push(sk, flags, mss_now, tp-&gt;nonagle, size_goal);\t&#125;out_nopush:\t/* msg-&gt;msg_ubuf is pinned by the caller so we don&#x27;t take extra refs */\tif (uarg &amp;&amp; !msg-&gt;msg_ubuf)\t\tnet_zcopy_put(uarg);\treturn copied + copied_syn;do_error:\ttcp_remove_empty_skb(sk);\tif (copied + copied_syn)\t\tgoto out;out_err:\t/* msg-&gt;msg_ubuf is pinned by the caller so we don&#x27;t take extra refs */\tif (uarg &amp;&amp; !msg-&gt;msg_ubuf)\t\tnet_zcopy_put_abort(uarg, true);\terr = sk_stream_error(sk, flags, err);\t/* make sure we wake any epoll edge trigger waiter */\tif (unlikely(tcp_rtx_and_write_queues_empty(sk) &amp;&amp; err == -EAGAIN)) &#123;\t\tsk-&gt;sk_write_space(sk);\t\ttcp_chrono_stop(sk, TCP_CHRONO_SNDBUF_LIMITED);\t&#125;\treturn err;&#125;\n\ntcp_sendmsg_locked中首先判断是否为零拷贝或者TFO模式，之后获取超时时间，并调用tcp_rate_check_app_limited判断是否处于应用层发送速率过慢状态(会影响bbr等拥塞算法的处理)，具体代码如下所：\nvoid tcp_rate_check_app_limited(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tif (/* We have less than one packet to send. */\t\t//待发送的数据不足一个mss &amp;&amp; 缓冲区没有数据 &amp;&amp; 在途数据包小于拥塞窗口\t    tp-&gt;write_seq - tp-&gt;snd_nxt &lt; tp-&gt;mss_cache &amp;&amp;\t    /* Nothing in sending host&#x27;s qdisc queues or NIC tx queue. */\t    sk_wmem_alloc_get(sk) &lt; SKB_TRUESIZE(1) &amp;&amp;\t    /* We are not limited by CWND. */\t    tcp_packets_in_flight(tp) &lt; tcp_snd_cwnd(tp) &amp;&amp;\t    /* All lost packets have been retransmitted. */\t\t//数据包都已经重传\t    tp-&gt;lost_out &lt;= tp-&gt;retrans_out)\t\t//存在已经交付的数据包，则设置为应用受限制\t\ttp-&gt;app_limited =\t\t\t(tp-&gt;delivered + tcp_packets_in_flight(tp)) ? : 1;&#125;\n\n当同时满足以下所有条件时，连接会被标记为应用受限：\n\n待发送数据不足一个MSS\n发送缓冲区中没有排队数据\n在途数据量小于拥塞窗口\n所有丢失的数据包都已重传\n\n总的来说，当TCP有能力发送更多数据但应用程序没有及时提供时，就标记为应用受限状态\n之后调用tcp_send_mss计算当前的mss和size_goal这里的size_goal可以理解为一个skb最多可以承载多少数据，理想情况下这里是64k，具体代码如下所示：\nint tcp_send_mss(struct sock *sk, int *size_goal, int flags)&#123;\tint mss_now;\t//大概率返回1460，受mtu，双方协商，pmtu等影响可能不是这个值\tmss_now = tcp_current_mss(sk);\t//这里如果支持GSO或TSO 这里应该就是64k\t*size_goal = tcp_xmit_size_goal(sk, mss_now, !(flags &amp; MSG_OOB));\treturn mss_now;&#125;\n\ntcp_send_mss中首先获取当前的mss，之后调用tcp_xmit_size_goal进一步计算size_goal具体代码如下所示：\nstatic unsigned int tcp_xmit_size_goal(struct sock *sk, u32 mss_now,\t\t\t\t       int large_allowed)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 new_size_goal, size_goal;\t//如果是带外数据直接返回mss\tif (!large_allowed)\t\treturn mss_now;\t/* Note : tcp_tso_autosize() will eventually split this later */\t//通常是64k\tnew_size_goal = tcp_bound_to_half_wnd(tp, sk-&gt;sk_gso_max_size);\t/* We try hard to avoid divides here */\t//第一次进来是0吧，会进入下面unlikey 设置gso_segs size_goal\tsize_goal = tp-&gt;gso_segs * mss_now;\tif (unlikely(new_size_goal &lt; size_goal ||\t\t     new_size_goal &gt;= size_goal + mss_now)) &#123;\t\ttp-&gt;gso_segs = min_t(u16, new_size_goal / mss_now, \t\t//64/mss\t\t\t\t     sk-&gt;sk_gso_max_segs);\t\tsize_goal = tp-&gt;gso_segs * mss_now;\t\t\t\t\t//64k?\t&#125;\treturn max(size_goal, mss_now);\t\t\t\t\t\t\t//64k&#125;\n\ntcp_xmit_size_goal首先调用tcp_bound_to_half_wnd钳制一下大小，使其确保一个GSO大包不会超过窗口的一半，这里传入的参数是set_sk_caps设置的，默认是64k。具体代码如下所示：\nstatic inline int tcp_bound_to_half_wnd(struct tcp_sock *tp, int pktsize)&#123;\tint cutoff;\t/* When peer uses tiny windows, there is no use in packetizing\t * to sub-MSS pieces for the sake of SWS or making sure there\t * are enough packets in the pipe for fast recovery.\t *\t * On the other hand, for extremely large MSS devices, handling\t * smaller than MSS windows in this way does make sense.\t */\t //tcp的窗口大于默认mss的话就将窗口设置为一半\tif (tp-&gt;max_window &gt; TCP_MSS_DEFAULT)\t\tcutoff = (tp-&gt;max_window &gt;&gt; 1);\telse\t\tcutoff = tp-&gt;max_window;\t\t\t\t\t\t\t //如果当前窗口很小？ 就是通告的窗口大小\tif (cutoff &amp;&amp; pktsize &gt; cutoff) \t\t\t\t\t\t  //如果mss超过上面的得到的阈值\t\treturn max_t(int, cutoff, 68U - tp-&gt;tcp_header_len);  \t//设置为阈值\telse\t\treturn pktsize;\t//通常应该走这个吧&#125;\n\n回到tcp_xmit_size_goal得到钳制之后的大小后，会计算gso_segs(也就是一个skb最多可以容纳多少个段）并根据段数和mss得到size_goal，这里size_goal就是一个skb可以承载最大数据包的大小。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输出"]},{"title":"VFS虚拟文件系统","url":"/2025/05/19/VFS%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","content":"VFS(虚拟文件系统)1.什么是VFSLinux 需要支持多种不同的文件系统（因为不同的文件系统有不同的特点），同时还要为用户提供一组统一的接口，因此要实现这个目的，就要将对各种不同文件系统和管理纳入到一个统一的框架中，也就是同一组系统调用，对各种不同的文件系统进行操作，这就是存在VFS的目的。\n这样，就可以对用户程序隐去各种不同文件系统的细节，为用户程序提供一个统一的、抽象的、虚拟的文件系统，这就是所谓“虚拟文件系统” - VFS（Virtual Filesystem Switch）。这个抽象层由一组标准的、抽象的文件操作构成，以系统调用的形式提供于用户程序，如read（）、write（）、lseek（）等等。这样，用户程序就可以 把所有的文件都看作一致的、抽象的“VFS文件”，通过这些系统调用对文件进行操作，而无需关心具体的文件属于什么文件系统以及具体文件系统的设计和实现,也就是说VFS是一个内核软件层，使应用程序与具体的文件系统解耦。\n举一个例子：在编写应用程序时，会经常使用到write（）系统调用，也就是向一个文件中写入数据。函数的原型为 ssize_t write(int fd, const void *buf, size_t count); 用户程序调用write（f, &amp;buf, len）的含义为向文件描述符为f的文件中，写入len个字节数据，。下图为write（）将数据写入到设备上的宏观流程。我们看到首先通过虚拟文件系统VFS，然后根据不同文件系统的write（）方法将数据写入物理设备上，宏观的调用流程如下图所示：\n\n2.VFS 整体架构虚拟文件系统作为内核中的一个抽象层，起到一个中间层的作用，对上(应用程序)提供统一接口，应用程序只需使用标准的文件操作（如 open、read、write1），无需关心底层是哪种文件系统（EXT4、NTFS、FAT等），对下为各种具体的文件系统（如 ext4、XFS 等）提供了统一的接口（其实就是实现不同文件系统的ops集合），VFS在内核中的整体架构如下所示。\n\n\n\n上述图片为VFS整体架构图，图片中各个组件作用大概如下：\n\nAPP：用户程序通过系统调用读写文件\nPage Cache：缓存文件的数据内容，例如次读取文件时从磁盘加载到页缓存，后续直接读缓存，避免磁盘I&#x2F;O。\nDirectory cache：缓存文件路径到Dentry的映射，减少频繁解析路径的开销。\nInode缓存：缓存文件的元数据（权限、大小、数据块位置等）\nBuffer Cache：缓存磁盘块的原始数据（已逐步被Page Cache取代，但在某些场景仍用于块设备操作）\n磁盘文件系统（ext2&#x2F;ext3&#x2F;ext4）：\next2：早期非日志式文件系统，简单但易崩溃损坏。\next3：增加日志功能，提升崩溃恢复能力。\next4：支持更大文件&#x2F;分区、延迟分配等高级特性。\n\n\n伪文件系统：\nproc：虚拟文件系统，动态暴露内核状态（如 &#x2F;proc&#x2F;cpuinfo）\nsysfs：提供设备&#x2F;驱动信息的统一接口（如 &#x2F;sys&#x2F;class）\n\n\n\n3.VFS关键数据结构VFS中包含着向物理文件系统转换的一系列数据结构，Linux中VFS层依靠四个主要的数据结构来述其结构信息，分别为超级块、索引结点、目录项和文件对象。这四个数据结构作用如下：\n3.1 Superblock（超级块）\n功能：超级块(块指的是存储和管理数据的基本单位)对象由各自的文件系统实现，用来存储文件系统的信息，如块大小、块数量等。这个对象对应为文件系统超级块或者文件系统控制块，它存储在磁盘特定的扇区上。不是基于磁盘的文件系统临时生成超级块，并保存在内存中，注意：所有超级块对象都以双向循环链表的形式链接在一起被管理。\n\n用途： \n\n超级块与物理文件系统一一对应。\n在挂载时初始化，帮助管理文件系统。\n\n\n\n管理超级块的结构体如下所示：\nstruct super_block &#123;    struct list_head    s_list;               // 指向链表的指针    dev_t               s_dev;                // 设备标识符    unsigned long       s_blocksize;          // 以字节为单位的块大小    loff_t              s_maxbytes;           // 文件大小上限    struct file_system_type    *s_type;       // 文件系统类型    const struct super_operations    *s_op;   // SuperBlock 操作函数，write_inode、put_inode 等    const struct dquot_operations    *dq_op;  // 磁盘限额函数    struct dentry        *s_root;             // 根目录&#125;\n\n3.2 Inode(索引节点)\n功能：每个文件都有一个唯一的inode，存储了文件的元数据，如文件大小、权限、访问时间等。它是文件系统中文件的抽象表示，不包含文件名。\n特点：inode存储在磁盘中（伪文件系统除外），在需要的时候会被加载到内存中，具体情况如下：inode 是文件系统的元数据结构，直接存储在磁盘上，用于长期保存文件的元信息（如权限、大小、块位置等）例如在Ext4文件系统中，inode集中存放在磁盘的固定区域。当访问某个文件时，会根据具体的磁盘上的inode(也就是磁盘中的inode, 比如ext4_inode_info)，来填充VFS的创建的inode(用私有指针指一下)。\n\nVFS管理的inode结构如下所示：\nstruct inode &#123;    umode_t                 i_mode;          // 文件权限及类型    kuid_t                  i_uid;           // user id    kgid_t                  i_gid;           // group id    const struct inode_operations    *i_op;  // inode 操作函数，如 create，mkdir，lookup，rename 等    struct super_block      *i_sb;           // 所属的 SuperBlock    loff_t                  i_size;          // 文件大小    struct timespec         i_atime;         // 文件最后访问时间    struct timespec         i_mtime;         // 文件最后修改时间    struct timespec         i_ctime;         // 文件元数据最后修改时间（包括文件名称）    const struct file_operations    *i_fop;  // 文件操作函数，open、write 等    void                    *i_private;      // 文件系统的私有数据&#125;\n\n3.3 Dentry(目录项)Dentry的核心作用是在内存中建立文件名（路径）与 inode 之间的高效映射。每个 Dentry 代表路径中一个特定部分。对于“&#x2F;bin&#x2F;ls”、“&#x2F;”、“bin”和“ls”都是目录项对象。前面是两个目录，最后一个是普通文件。在路径中， 包括普通文件在内，每一个部分都是目录项对象。目录项是描述文件的逻辑属性，只存在于内存中，举个例子，当调用open()函数打开一个文件时，内核会第一时间根据文件路径到 DEntry Cache 里面寻找相应的 DEntry，找到了就直接构造一个file对象并返回。如果该文件不在缓存中，那么 VFS 会根据找到的最近目录一级一级地向下加载，直到找到相应的文件。期间 VFS 会缓存所有被加载生成的dentry。注意:一个 INode 可能被多个 DEntry 所关联，即相当于为某一文件创建了多个文件路径.\nVFS管理的Dentry结构如下所示：\nstruct dentry &#123;    struct dentry *d_parent;     // 父目录    struct qstr d_name;          // 文件名称    struct inode *d_inode;       // 关联的 inode    struct list_head d_child;    // 父目录中的子目录和文件    struct list_head d_subdirs;  // 当前目录中的子目录和文件&#125;\n\n3.4 file 文件对象虚拟文件系统最后一个主要对象是文件对象，文件对象表示进程已打开的文件，每个进程都持有一个fd[]数组，数组里面存放的是指向file结构体的指针，同一进程的不同fd可以指向同一个file对象，file是内核中的数据结构，表示一个被进程打开的文件，和进程相关联。当应用程序调用open()函数的时候，VFS 就会创建相应的file对象。注意： file会通过Dentry找到inode，file的ops集合（read，write等）其实就是inode的i_fops; 这样感觉就实现了进程和文件系统之间的解耦。\nfile结构如下所示：\nstruct file &#123;    struct path                   f_path;    struct inode                  *f_inode;    const struct file_operations  *f_op;    unsigned int                  f_flags;    fmode_t                       f_mode;    loff_t                        f_pos;    struct fown_struct            f_owner;&#125;\n\n下图为上述四个关键数据结构的关系图：\n\n4.挂载4.1 什么叫挂载挂载（Mounting） 是将存储设备（如硬盘、U盘）或文件系统（如Ext4、NTFS）关联到Linux目录树中某个目录（称为挂载点）的过程。挂载后，访问该目录实际指向目标设备或文件系统的内容，而原目录下的文件会被临时隐藏。例如，将U盘挂载到&#x2F;mnt&#x2F;usb后，访问此目录即访问U盘数据，卸载后恢复原目录内容。内核通过虚拟文件系统（VFS）管理挂载表，动态路由路径解析，实现对多文件系统的统一访问。简言之，挂载是让外部存储“接入”目录树的机制，用户通过目录操作文件，无需关心物理设备细节。\n挂载是在用户态发起mount命令，该命令执行的时候需要指定文件系统的类型（例如Ext2）和文件系统数据的位置（也就是dev）。通过这些关键信息，VFS就可以完成Ext2文件系统的初始化，并将其关联到当前已经存在的文件系统当中，也就是建立起下面所示的文件系统树。\n\n如上图所示，该系统根文件系统是Ext4文件系统，而在其&#x2F;mnt目录下面又分别挂载了Ext4文件系统和XFS文件系统。最后形成了一个由多个文件系统组成的文件系统树。\n4.2 挂载点 挂载点（Mount Point）是 Linux系统中用于将外部存储设备或文件系统接入到目录树的一个空目录。通过挂载操作，该目录会成为访问目标文件系统的入口，原有内容会被临时隐藏，转而显示被挂载设备或文件系统的内容。\n一个挂载点用一个vfsmount来表示，属于VFS层的一部分，在用户执行mount系统调用的时候会被创建，它记录了文件系统实例与目录树的关联关系，是挂载机制的核心实现，作用如下：\n\n关联挂载点与超级块：记录被挂载的超级块，其实就是知道被挂载的是哪个文件系统\n支持路径解析：当用户访问路径时，VFS 通过 vfsmount 确定目标文件系统的位置。例如，当访问&#x2F;mnt&#x2F;data&#x2F;file.txt时VFS 发现 &#x2F;mnt&#x2F;data 是挂载点（进而可以拿到超级块的信息）进而调用目标文件系统的方法继续查找要操作的文件。\n\nvfsmount结构如下所示：\nstruct vfsmount &#123;\tstruct dentry *mnt_root;\t//挂载的目录\tstruct super_block *mnt_sb;\t//指向超级块\tint mnt_flags;\tstruct mnt_idmap *mnt_idmap;&#125; __randomize_layout;\n","categories":["文件系统学习"],"tags":["VFS"]},{"title":"TCP尾部丢包探测定时器","url":"/2025/08/16/TLP%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"尾部丢包探测(TLP)定时器因为 TCP 的 快速重传需要重复 ACK 才能触发，而尾部丢包时往往没有新数据推动 ACK；如果依赖 RTO 定时器又太慢，动辄几百毫秒甚至秒级，造成应用延迟。于是引入 TLP 定时器，在 RTO 之前提前触发一次探测报文，用来快速确认是否真的丢包，从而避免长时间等待 RTO，提高尾部丢包情况下的恢复速度。\nTLP使用的定时器与超时重传用的是同一个定时器，具体定定时器到期的主要逻辑为，首先判断发送队列是否为空，如果不为空或者空间不够发送，则尝试从发送队列拿出一个个数据包直接发送，若发送队列为空，则从重传队列中拿到一个数据包发送\n//主动发生一个数据包void tcp_send_loss_probe(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb;\tint pcount;\tint mss = tcp_current_mss(sk);\t/* At most one outstanding TLP */\tif (tp-&gt;tlp_high_seq) //tcpack中会把这个设置为0\t\tgoto rearm_timer;\ttp-&gt;tlp_retrans = 0;  //标记是否触发了TLP\tskb = tcp_send_head(sk); //从发送队列拿到一个数据包。其实就是看一下数据包是否为空\t//如果数据包不为空，且当前空间有一个mss的大小，\tif (skb &amp;&amp; tcp_snd_wnd_test(tp, skb, mss)) &#123;\t\tpcount = tp-&gt;packets_out;\t\ttcp_write_xmit(sk, mss, TCP_NAGLE_OFF, 2, GFP_ATOMIC);\t\t//相当于在判断是否发送成功\t\tif (tp-&gt;packets_out &gt; pcount) \t\t\tgoto probe_sent;\t\tgoto rearm_timer;\t&#125;\t//如果发送队列为空或者可用空间不足，就从重传队列中拿到一个数据包\tskb = skb_rb_last(&amp;sk-&gt;tcp_rtx_queue);\tif (unlikely(!skb)) &#123;\t\tWARN_ONCE(tp-&gt;packets_out,\t\t\t  &quot;invalid inflight: %u state %u cwnd %u mss %d\\n&quot;,\t\t\t  tp-&gt;packets_out, sk-&gt;sk_state, tcp_snd_cwnd(tp), mss);\t\tinet_csk(sk)-&gt;icsk_pending = 0;\t\treturn;\t&#125;\t//通过fclone判断是否已经在主机中了\tif (skb_still_in_host_queue(sk, skb))\t\tgoto rearm_timer;\t//计算当前数据包有几个段\tpcount = tcp_skb_pcount(skb);\tif (WARN_ON(!pcount))\t\tgoto rearm_timer;\t//超过一个段的情况需要分段\tif ((pcount &gt; 1) &amp;&amp; (skb-&gt;len &gt; (pcount - 1) * mss)) &#123;\t\tif (unlikely(tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb,\t\t\t\t\t  (pcount - 1) * mss, mss,\t\t\t\t\t  GFP_ATOMIC)))\t\t\tgoto rearm_timer;\t\tskb = skb_rb_next(skb);\t&#125;\tif (WARN_ON(!skb || !tcp_skb_pcount(skb)))\t\tgoto rearm_timer;\t//重传一个数据包\tif (__tcp_retransmit_skb(sk, skb, 1))\t\tgoto rearm_timer;\ttp-&gt;tlp_retrans = 1;probe_sent:\t/* Record snd_nxt for loss detection. */\t//这里其实相当与标记了tlp重传\ttp-&gt;tlp_high_seq = tp-&gt;snd_nxt;\t//更新统计信息\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPLOSSPROBES);\t/* Reset s.t. tcp_rearm_rto will restart timer from now */\tinet_csk(sk)-&gt;icsk_pending = 0;rearm_timer:\ttcp_rearm_rto(sk);&#125;\n\n如果从重传队列中取出一个数据包发送，在经过一系列处理后会调用__tcp_retransmit_skb 注意重传定时器也使用这个函数，这里面主要对数据包进行了裁剪，查看路由是否有效，计算mss，计算数据包的的断数，尝试合并重传数据包，然后调用tcp_transmit_skb这里不在粘贴这部分代码了。\n尾部丢包探测(TLP)定时器的激活TLP定时器的激活由tcp_schedule_loss_probe完成，具体代码如下：\nbool tcp_schedule_loss_probe(struct sock *sk, bool advancing_rto)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 timeout, timeout_us, rto_delta_us;\tint early_retrans;\t/* Don&#x27;t do any loss probe on a Fast Open connection before 3WHS\t * finishes.\t */\t//fastopen 不启动tlp\tif (rcu_access_pointer(tp-&gt;fastopen_rsk))\t\treturn false;\t//默认是3\tearly_retrans = READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_early_retrans);\t/* Schedule a loss probe in 2*RTT for SACK capable connections\t * not in loss recovery, that are either limited by cwnd or application.\t */\tif ((early_retrans != 3 &amp;&amp; early_retrans != 4) ||\t    !tp-&gt;packets_out || !tcp_is_sack(tp) ||  //是否支持sack，如果不支持sack也不能启动tlp，为什么呢？\t    (icsk-&gt;icsk_ca_state != TCP_CA_Open &amp;&amp;\t     icsk-&gt;icsk_ca_state != TCP_CA_CWR))\t\treturn false;\t/* Probe timeout is 2*rtt. Add minimum RTO to account\t * for delayed ack when there&#x27;s one outstanding packet. If no RTT\t * sample is available then probe after TCP_TIMEOUT_INIT.\t */\t//先拿到rtt\tif (tp-&gt;srtt_us) &#123;\t\ttimeout_us = tp-&gt;srtt_us &gt;&gt; 2;//先算一个pto 这里除了4,\t\tif (tp-&gt;packets_out == 1)\t//只有一个未确认数据包的情况\t\t\ttimeout_us += tcp_rto_min_us(sk);  //加一个最小rto？ 200ms\t\telse //有多个未确认数据包的情况\t\t\ttimeout_us += TCP_TIMEOUT_MIN_US; //2ms\t\ttimeout = usecs_to_jiffies(timeout_us);\t&#125; else &#123;\t\ttimeout = TCP_TIMEOUT_INIT; //如果没有rtt就是1s\t&#125;\t/* If the RTO formula yields an earlier time, then use that time. */\t//直接用当前rto的原始值，还是rto的剩余时间，由传入的参数决定\trto_delta_us = advancing_rto ?\t\t\tjiffies_to_usecs(inet_csk(sk)-&gt;icsk_rto) :\t\t\ttcp_rto_delta_us(sk);  /* How far in future is RTO? */\tif (rto_delta_us &gt; 0)\t//取最小值\t\ttimeout = min_t(u32, timeout, usecs_to_jiffies(rto_delta_us));\ttcp_reset_xmit_timer(sk, ICSK_TIME_LOSS_PROBE, timeout, TCP_RTO_MAX);\treturn true;&#125;\n\ntcp_schedule_loss_probe 首先判断是否可以调度定时器，如果拥塞状态机的状态不是open或拥塞状态， 或者系统参数没有开启TLP或者没有未确认的数据包，就直接返回了，不需要调度丢包探测定时器。否则的话就先获取rtt，然后根据rtt计算一个超时时间，启动定时器。\n接下来看上述调度tcp_schedule_loss_probe的地方，有两个地方，一个是发送数据包的路径上，另一个是tcp_ack的处理中，具体代码如下所示：\nstatic bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,\t\t\t   int push_one, gfp_t gfp)&#123;...\t\t//激活tlp定时器，如果等于2表示已经激活过了\t\tif (push_one != 2)\t\t\ttcp_schedule_loss_probe(sk, false);...&#125;\n\n/* This routine deals with incoming acks, but not outgoing ones. */static int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)&#123;...\t/* If needed, reset TLP/RTO timer when RACK doesn&#x27;t set. */\t//在清理重传队列的时候可能会设置上这个标志位，比如有新的数据包被确认的时候，或者检测到乱续或者丢包 肯定需要重新设置这个定时器了\tif (flag &amp; FLAG_SET_XMIT_TIMER)\t\ttcp_set_xmit_timer(sk);...&#125;static void tcp_set_xmit_timer(struct sock *sk)&#123;\tif (!tcp_schedule_loss_probe(sk, true))\t\ttcp_rearm_rto(sk);&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP定时器","TCP"]},{"title":"inet_sock 与 inet_connnection_sock","url":"/2025/07/14/inet_sock%E7%BB%93%E6%9E%84/","content":"inet_sock 是 Linux 内核中专门用于表示 IPv4 协议套接字的结构体，它继承自通用的 sock 结构，并扩展了诸如本地和远程 IP 地址、端口号、IP 头部字段（如 TTL、TOS）、MSS 以及 IP 选项等关键字段。在 TCP 和 UDP 协议的实现中，inet_sock 提供了协议栈处理 IPv4 网络连接所需的关键信息，是 IPv4 套接字在内核中的核心表示。\nstruct inet_sock &#123;\t/* sk and pinet6 has to be the first two members of inet_sock */\tstruct sock\t\tsk;  //#if IS_ENABLED(CONFIG_IPV6)\tstruct ipv6_pinfo\t*pinet6;#endif\t/* Socket demultiplex comparisons on incoming packets. */#define inet_daddr\t\tsk.__sk_common.skc_daddr //比如tcp/udp connect 或会设置（如果udp不connect好像不会设置）// bind中设置 //inet-&gt;inet_rcv_saddr = inet-&gt;inet_saddr = addr-&gt;sin_addr.s_addr;#define inet_rcv_saddr\t\tsk.__sk_common.skc_rcv_saddr #define inet_dport\t\tsk.__sk_common.skc_dport //目的port，比如建立连接的时候会设置#define inet_num\t\tsk.__sk_common.skc_num   //本地端口，比如bind的时候会设置\tunsigned long\t\tinet_flags;\t//源ip地址 tcp bind中会设置 ！！！如果没有调用bind 那就会在connect中查找路由设置\t__be32\t\t\tinet_saddr;    \t__s16\t\t\tuc_ttl;  //单播的ttl，系统默认是64,可以通过setsockopt设置\t__be16\t\t\tinet_sport;  //bind中设置，或者，autobind中设置\tstruct ip_options_rcu __rcu\t*inet_opt; //ip选项\tatomic_t\t\tinet_id; //ip id   //发送的时候根据4元组加hash生成\t__u8\t\t\ttos;   //dscp+ecn  可以通过setsockopt设置\t__u8\t\t\tmin_ttl;  //setsockopt设置。tcp收包的时候可以根据ttldrop掉\t__u8\t\t\tmc_ttl;\t//多播ttl\t__u8\t\t\tpmtudisc;  //根据系统配置决定是否开启mtu探测\t__u8\t\t\trcv_tos;  //接收的tos，用户可以通过setsockopt获取\t__u8\t\t\tconvert_csum;\tint\t\t\tuc_index;  \t//输出网络设备的索引，通过setsockopt设置\tint\t\t\tmc_index;\t//多播，同上\t__be32\t\t\tmc_addr; //发送多播的源ip地址，注意，这指的是单播地址\tstruct &#123;\t\t__u16 lo;\t\t__u16 hi;\t&#125;\t\t\tlocal_port_range; //本地port的范围\tstruct ip_mc_socklist __rcu\t*mc_list; //一个链表，管理的是当前sock加入的所有多播地址信息，以及模式\tstruct inet_cork_full\tcork;  //udp发包用到这个字段。存的是ip层待添字段的信息&#125;;\n\ninet_connection_sock 是 Linux 内核中用于表示基于连接的 IPv4 套接字（如 TCP）的结构体，它在 inet_sock 的基础上进一步扩展，添加了处理连接状态所需的字段，例如定时器、接收窗口控制、连接超时管理、状态回调函数等。该结构是 TCP 等面向连接协议在内核中的核心控制块，负责管理连接建立、维护和释放的各个阶段。\nstruct inet_connection_sock &#123;\t/* inet_sock has to be the first member! */\tstruct inet_sock\t  icsk_inet; \t\t//inet_sock\tstruct request_sock_queue icsk_accept_queue;  //服务端使用，管理全链接队列和半连接队列，在listen的时候会初始化\tstruct inet_bind_bucket\t  *icsk_bind_hash;\t\t//管理端口号\ttcp connectin的时候会用到\tstruct inet_bind2_bucket  *icsk_bind2_hash;\t\t//管理端口号的ip地址 \tunsigned long\t\t  icsk_timeout;\t\t\t\t//tcp定时器使用保存下次执行定时器任务的时间，比如计算下次重传的时间，退避计算 \tstruct timer_list\t  icsk_retransmit_timer;\t//重传定时器 \tstruct timer_list\t  icsk_delack_timer;\t\t//延迟ack定时器\t__u32\t\t\t  icsk_rto;\t\t\t\t\t\t//下一次重传的时间\t__u32                     icsk_rto_min;\t\t\t//rto的最小值 防止因为rtt过小，导致不必要的重传\t__u32                     icsk_delack_max;\t\t//发送延迟ack的最大时间\t__u32\t\t\t  icsk_pmtu_cookie;\t\t\t\t//缓存路径mtu 计算mss时候要用到，connect的时候会初始化，icsk_sync_mss会设置这个值\tconst struct tcp_congestion_ops *icsk_ca_ops;\t//拥塞控制算法ops集合\t////tcp层和ip层直接的接口的ops集合\tconst struct inet_connection_sock_af_ops *icsk_af_ops;  \tconst struct tcp_ulp_ops  *icsk_ulp_ops;\t\t//扩展的接口？？？\tvoid __rcu\t\t  *icsk_ulp_data;\tvoid (*icsk_clean_acked)(struct sock *sk, u32 acked_seq);\t\t//TLS相关\tunsigned int\t\t  (*icsk_sync_mss)(struct sock *sk, u32 pmtu); //icsk_sync_mss\t__u8\t\t\t  icsk_ca_state:5,\t\t\t  //拥塞状态机的几种状态\t\t\t\t  icsk_ca_initialized:1,\t\t//是否初始化了拥塞算法\t\t\t\t  icsk_ca_setsockopt:1,\t\t\t//用户是否通过setsockopt设置过拥塞算法\t\t\t\t  icsk_ca_dst_locked:1;\t\t\t//bfp相关，是否因为路由改变加锁？？\t__u8\t\t\t  icsk_retransmits;\t\t\t//重传次数\t__u8\t\t\t  icsk_pending;\t\t\t\t//标志位，是否有某种定时器事件待处理\t__u8\t\t\t  icsk_backoff;\t\t\t\t\t//重传次数，计算rto的时候会用到\t__u8\t\t\t  icsk_syn_retries;\t\t\t\t//重传syn包的最大次数\t__u8\t\t\t  icsk_probes_out;\t\t\t//零窗口探测的次数\t__u16\t\t\t  icsk_ext_hdr_len;\t\t\t//ip选项的长度\tstruct &#123;\t\t__u8\t\t  pending;\t /* ACK is pending\t\t\t   *///是否有带发送的ack\t\t__u8\t\t  quick;\t /* Scheduled number of quick acks\t   *///快速ack\t\t__u8\t\t  pingpong;\t /* The session is interactive\t\t   *///是否是pingpang模式，也就是交互模式\t\t__u8\t\t  retry;\t /* Number of attempts\t\t\t   */\t//延迟ack重传次数？\t\t__u32\t\t  ato;\t\t /* Predicted tick of soft clock\t   */\t//延迟发送ack的时间\t\tunsigned long\t  timeout;\t /* Currently scheduled timeout\t\t   *///超时时间，到期后发送ack\t\t__u32\t\t  lrcvtime;\t /* timestamp of last received data packet */\t//最后一个数据包收上来的时间\t\t__u16\t\t  last_seg_size; /* Size of last incoming segment\t   *///最后一个段的大小\t\t__u16\t\t  rcv_mss;\t /* MSS used for delayed ACK decisions\t   *///接收报文的mss\t&#125; icsk_ack;     //管理延迟ack的结构，\tstruct &#123;\t\t/* Range of MTUs to search */\t\tint\t\t  search_high;\t\t\t\t\t\t//MTU探测的范围,发送的时候会使用二分查找确定一个值\t\tint\t\t  search_low;\t\t/* Information on the current probe. */\t\tu32\t\t  probe_size:31,\t\t\t\t\t\t//上面high和low计算出来的一个值探测的size\t\t/* Is the MTUP feature enabled for this connection? */\t\t\t\t  enabled:1;\t\t\t\t\t\t\t\t\t//是否使能路径mtu探测\t\tu32\t\t  probe_timestamp;\t\t\t\t\t\t//探测的时间戳\t&#125; icsk_mtup;\tu32\t\t\t  icsk_probes_tstamp;\tu32\t\t\t  icsk_user_timeout;\tu64\t\t\t  icsk_ca_priv[104 / sizeof(u64)];#define ICSK_CA_PRIV_SIZE\t  sizeof_field(struct inet_connection_sock, icsk_ca_priv)&#125;;\n\n","categories":["网络协议栈源码学习"],"tags":["sock"]},{"title":"IPSec","url":"/2025/05/18/ipsec/","content":"IPSec1.IPSec 简介起源随着Internet的发展，越来越多的企业直接通过Internet进行互联，但由于IP协议未考虑安全性，而且Internet上有大量的不可靠用户和网络设备，所以用户业务数据要穿越这些未知网络，根本无法保证数据的安全性，数据易被伪造、篡改或窃取。因此，迫切需要一种兼容IP协议的通用的网络安全方案。为了解决上述问题，IPSec（Internet Protocol Security）应运而生。IPSec是对IP的安全性补充，其工作在IP层，为IP网络通信提供透明的安全服务。\n定义IPSec是IETF（Internet Engineering Task Force）制定的一组开放的网络安全协议。它并不是一个单独的协议，而是一系列为IP网络提供安全性的协议和服务的集合，包括认证头AH（Authentication Header）和封装安全载荷ESP（Encapsulating SecurityPayload）两个安全协议、密钥交换和用于验证及加密的一些算法等。通过这些协议，在两个设备之间建立一条IPSec隧道。数据通过IPSec隧道进行转发，实现保护数据的安全性。\n受益IPSec通过加密与验证等方式，从以下几个方面保障了用户业务数据在Internet中的安全传输：\n\n数据来源验证：接收方验证发送方身份是否合法。\n数据加密：发送方对数据进行加密，以密文的形式在Internet上传送，接收方对接收的加密数据进行解密后处理或直接转发。\n数据完整性：接收方对接收的数据进行验证，以判定报文是否被篡改。\n抗重放：接收方拒绝旧的或重复的数据包，防止恶意用户通过重复发送捕获到的数据包所进行的攻击。\n\n2.IPSec原理描述2.1IPSec 协议框架2.1.1安全联盟安全联盟SA（Security Association）是通信对等体间对某些要素的协定，它描述了对等体间如何利用安全服务（例如加密）进行安全的通信。这些要素包括对等体间使用何种安全协议、要保护的数据流特征、对等体间传输的数据的封装模式、协议采用的加密和验证算法，以及用于数据安全转换、传输的密钥和SA的生存周期等。IPSec安全传输数据的前提是在IPSec对等体（即运行IPSec协议的两个端点）之间成功建立安全联盟。IPSec安全联盟简称IPSec SA，由一个三元组来唯一标识，这个三元组包括安全参数索引SPI（Security Parameter Index）、目的IP地址和使用的安全协议号（AH或ESP）。其中，SPI是为唯一标识SA而生成的一个32位比特的数值，它被封装在AH和ESP头中。IPSec SA是单向的逻辑连接，通常成对建立（Inbound和Outbound）。因此两个IPSec对等体之间的双向通信，最少需要建立一对IPSec SA形成一个安全互通的IPSec隧道，分别对两个方向的数据流进行安全保护。另外，IPSec SA的个数还与安全协议相关。如果只使用AH或ESP来保护两个对等体之间的流量，则对等体之间就有两个SA，每个方向上一个。如果对等体同时使用了AH和ESP，那么对等体之间就需要四个SA，每个方向上两个，分别对应AH和ESP。建立IPSec SA有两种方式：手工方式和IKE方式。二者的主要差异如表所示。\n\n\n\n对比项\n手工方式建立IPSec SA\nIKE方式自动建立IPSec SA\n\n\n\n加密&#x2F;验证密钥管理\n手工配置、手动刷新，易出错\n通过DH算法动态生成并自动刷新\n\n\n密钥管理成本\n高（需人工维护所有节点密钥）\n低（自动协商和轮换）\n\n\nSPI（安全参数索引）\n手工配置\n随机生成\n\n\n生存周期\n无限制，SA永久存在（除非手动删除）\n由生存周期参数控制，SA自动过期和重建\n\n\n安全性\n低（静态密钥易被破解，无前向保密）\n高（动态密钥、支持PFS、抗重放攻击）\n\n\n适用场景\n小型网络、临时测试环境\n中小型至大型网络、生产环境\n\n\n2.1.2安全协议IPSec使用认证头AH（Authentication Header）和封装安全载荷ESP EncapsulatingSecurity Payload）两种IP传输层协议来提供认证或加密等安全服务。\n\nAH协议：AH仅支持认证功能，不支持加密功能。AH在每一个数据包的标准IP报头后面添加一个AH报文头。AH对数据包和认证密钥进行Hash计算，接收方收到带有计算结果的数据包后，执行同样的Hash计算并与原计算结果比较，传输过程中对数据的任何更改将使计算结果无效，这样就提供了数据来源认证和数据完整性校验。AH协议的完整性验证范围为整个IP报文。\n\nESP协议：ESP支持认证和加密功能。ESP在每一个数据包的标准IP报头后面添加一个ESP报文头，并在数据包后面追加一个ESP尾（ESP Trailer和ESP Auth data）。与AH不同的是，ESP将数据中的有效载荷进行加密后再封装到数据包中，以保证数据的机密性，但ESP没有对IP头的内容进行保护，除非IP头被封装在ESP内部（采用隧道模式）。\n\n\nAH协议与ESP协议的比较如下所示：\n\n\n\n安全特性\nAH (认证头)\nESP (封装安全载荷)\n\n\n\n协议号\n51\n50\n\n\n数据完整性校验\n支持（验证整个IP报文）\n支持（传输模式：不验证IP头；隧道模式：验证整个IP报文）\n\n\n数据源验证\n支持\n支持\n\n\n数据加密\n不支持\n支持\n\n\n防报文重放攻击\n支持\n支持\n\n\nNAT-T (NAT穿越)\n不支持\n支持\n\n\nAH报文头结构\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| 下一个头部 (8 bits) | 载荷长度 (8 bits) |  保留 (16 bits)         |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                   安全参数索引 (SPI, 32 bits)                   |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                   序列号 (Sequence Number, 32 bits)            |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|                                                               ||                认证数据 (可变长度，32 bits的整数倍)               ||                                                               |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n AH 报文头字段含义\n\n\n\n字段名\n长度\n含义\n\n\n\n下一头部\n8 bits\n标识 AH 报文头后面的负载类型： - 传输模式：被保护的上层协议（TCP&#x2F;UDP）或 ESP 协议编号 - 隧道模式：IP 协议或 ESP 协议编号（当 AH 与 ESP 同时使用时，下一头部为 ESP 报文头）\n\n\n负载长度\n8 bits\n以 32 比特为单位的 AH 报文头长度减 2（缺省值：4）\n\n\n保留字段\n16 bits\n保留将来使用，缺省为 0\n\n\nSPI\n32 bits\nIPSec 安全参数索引，唯一标识安全联盟（SA）\n\n\n序列号\n32 bits\n从 1 开始的单向递增计数器，防止重放攻击\n\n\n认证数据\n变长字段（32 比特整数倍，通常 96 bits）\n包含完整性校验值（ICV），用于接收方校验数据完整性。认证算法：- ✅ 推荐：SHA2、SM3- ⚠️ 不安全：MD5、SHA1（存在安全隐患）\n\n\nESP 报文结构\n+-----------------------------------------------+ &lt;-- ESP头部| 安全参数索引（SPI）                              |+-----------------------------------------------+| 序列号                                         |+-----------------------------------------------+ &lt;-- 加密部分开始|                                               || 负载数据（Payload，变长）                        ||                                               |+-----------------------------------------------+| 填充字段（0～255字节 Padding）                   |+-----------------------------------------------+| 填充长度（1B） | 下一头部（1B）                   |+-----------------------------------------------+ &lt;-- ESP尾部（加密部分结束）|                                               || 认证数据（ICV，完整性校验值，变长                  ||                                               |+-----------------------------------------------+ &lt;-- ESP认证部分\n\n ESP 报文头字段含义\n\n\n\n字段名\n长度\n含义\n\n\n\nSPI\n32 bits\nIPSec 安全参数索引，唯一标识安全联盟（SA）\n\n\n序列号\n32 bits\n从 1 开始的单向递增计数器，防止重放攻击\n\n\n负载数据\n变长\n原始 IP 报文中的可变长度数据内容（保护内容类型由下一头部字段标识）\n\n\n填充字段\n0-255 字节\n用于补齐加密算法要求的块长度\n\n\n填充长度\n8 bits\n表示填充字段的字节数（0 表示无填充）\n\n\n下一头部\n8 bits\n标识下一个负载类型：- 传输模式：上层协议编号（如 TCP&#x3D;6&#x2F;UDP&#x3D;17）- 隧道模式：IP 协议（IPv4&#x3D;4&#x2F;IPv6&#x3D;41）\n\n\n认证数据\n变长\n完整性校验值（ICV），需 32 位对齐\n\n\n2.1.3封装模式封装模式是指将AH或ESP相关的字段插入到原始IP报文中，以实现对报文的认证和加密，封装模式有传输模式和隧道模式两种。\n传输模式\n在传输模式中，AH头或ESP头被插入到IP头与传输层协议头之间，保护TCP&#x2F;UDP&#x2F;ICMP负载。由于传输模式未添加额外的IP头，所以原始报文中的IP地址在加密后报文的IP头中可见。以TCP报文为例，原始报文经过传输模式封装后，报文格式如下所示。\n\n隧道模式\n在隧道模式下，AH头或ESP头被插到原始IP头之前，另外生成一个新的报文头放到AH头或ESP头之前，保护IP头和负载。以TCP报文为例，原始报文经隧道模式封装后的报文结构如下图所示。\n\n隧道模式下，与AH协议相比，ESP协议的完整性验证范围不包括新IP头，无法保证新IP头的安全。\n传输模式和隧道模式比较\n传输模式和隧道模式的区别在于：\n\n从安全性来讲，隧道模式优于传输模式。它可以完全地对原始IP数据包进行验证和加密。隧道模式下可以隐藏内部IP地址，协议类型和端口。\n从性能来讲，隧道模式因为有一个额外的IP头，所以它将比传输模式占用更多带宽。\n从场景来讲，传输模式主要应用于两台主机或一台主机和一台VPN网关之间通信；隧道模式主要应用于两台VPN网关之间或一台主机与一台VPN网关之间的通信。当安全协议同时采用AH和ESP时，AH和ESP协议必须采用相同的封装模式。\n\n2.1.4 加密和验证IPSec提供了两种安全机制：加密和验证。加密机制保证数据的机密性，防止数据在传输过程中被窃听；验证机制能保证数据真实可靠，防止数据在传输过程中被仿冒和篡改。\n加密\nIPSec采用对称加密算法对数据进行加密和解密。如下图所示，数据发送方和接收方使用相同的密钥进行加密、解密。\n用于加密和解密的对称密钥可以手工配置，也可以通过IKE协议自动协商生成。常用的对称加密算法包括：数据加密标准DES（Data Encryption Standard）、3DES（Triple Data Encryption Standard）、先进加密标准AES（Advanced EncryptionStandard）国密算法（SM1和SM4）。其中，DES和3DES算法安全性低，存在安全风险，不推荐使用。\n验证\nIPSec的加密功能，无法验证解密后的信息是否是原始发送的信息或完整。IPSec采用HMAC（Keyed-Hash Message Authentication Code）功能，比较完整性校验值ICV进行数据包完整性和真实性验证。通常情况下，加密和验证通常配合使用。如图所示，在IPSec发送方，加密后的报文通过验证算法和对称密钥生成完整性校验值ICV，IP报文和完整性校验值ICV同时发给对端；在IPSec接收方，使用相同的验证算法和对称密钥对加密报文进行处理，同样得到完整性校验值ICV，然后比较完整性校验值ICV进行数据完整性和真实性验证，验证不通过的报文直接丢弃，验证通过的报文再进行解密。\n\n同加密一样，用于验证的对称密钥也可以手工配置，或者通过IKE协议自动协商生成。常用的验证算法包括：消息摘要MD5（Message Digest 5）、安全散列算法SHA1（Secure Hash Algorithm 1）、SHA2、国密算法SM3（Senior Middle 3）。其中，MD5、SHA1算法安全性低，存在安全风险，不推荐使用。\n2.1.5  密钥交换使用对称密钥进行加密、验证时，如何安全地共享密钥是一个很重要的问题。有两种方法解决这个问题：\n\n带外共享密钥在发送、接收设备上手工配置静态的加密、验证密钥。双方通过带外共享的方式（例如通过电话或邮件方式）保证密钥一致性。这种方式的缺点是安全性低，可扩展性差，在点到多点组网中配置密钥的工作量成倍增加。另外，为提升网络安全性需要周期性修改密钥，这种方式下也很难实施。\n使用一个安全的密钥分发协议通过IKE协议自动协商密钥。IKE采用DH算法在不安全的网络上安全地分发密钥。这种方式配置简单，可扩展性好，特别是在大型动态的网络环境下此优点更加突出。同时，通信双方通过交换密钥交换材料来计算共享的密钥，即使第三方截获了双方用于计算密钥的所有交换数据，也无法计算出真正的密钥，这样极大地提高了安全性。\n\nIKE 协议因特网密钥交换IKE（Internet Key Exchange）协议建立在Internet安全联盟和密钥管理协议ISAKMP定义的框架上，是基于UDP（User Datagram Protocol）的应用层协议。它为IPSec提供了自动协商密钥、建立IPSec安全联盟的服务，能够简化IPSec的配置和维护工作。IKE与IPSec的关系如图所示，对等体之间建立一个IKE SA完成身份验证和密钥信息交换后，在IKE SA的保护下，根据配置的AH&#x2F;ESP安全协议等参数协商出一对IPSecSA。此后，对等体间的数据将在IPSec隧道中加密传输。IKE SA是一个双向的逻辑连接，两个对等体间只建立一个IKE SA。\n\nIKE安全机制\nIKE具有一套自保护机制，可以在网络上安全地认证身份、分发密钥、建立IPSec SA：\n\n身份认证身份认证确认通信双方的身份（对等体的IP地址或名称），包括预共享密钥PSK（pre-shared key）认证、数字证书RSA（rsa-signature）认证和数字信封认证。在预共享密钥认证中，通信双方采用共享的密钥对报文进行Hash计算，判断双方的计算结果是否相同。如果相同，则认证通过；否则认证失败。当有1个对等体对应多个对等体时，需要为每个对等体配置预共享的密钥。该方法在小型网络中容易建立，但安全性较低。在数字证书认证中，通信双方使用CA证书进行数字证书合法性验证，双方各有自己的公钥（网络上传输）和私钥（自己持有）。发送方对原始报文进行Hash计算，并用自己的私钥对报文计算结果进行加密，生成数字签名。接收方使用发送方的公钥对数字签名进行解密，并对报文进行Hash计算，判断计算结果与解密后的结果是否相同。如果相同，则认证通过；否则认证失败。使用数字证书安全性高，但需要CA来颁发数字证书，适合在大型网络中使用。在数字信封认证中，发送方首先随机产生一个对称密钥，使用接收方的公钥对此对称密钥进行加密（被公钥加密的对称密钥称为数字信封），发送方用对称密钥加密报文，同时用自己的私钥生成数字签名。接收方用自己的私钥解密数字信封得到对称密钥，再用对称密钥解密报文，同时根据发送方的公钥对数字签名进行解密，验证发送方的数字签名是否正确。如果正确，则认证通过；否则认证失败。数字信封认证用于设备需要符合国家密码管理局要求时使用，此认证方法只能在IKEv1的主模式协商过程中支持。IKE支持的认证算法有：MD5、SHA1、SHA2-256、SHA2-384、SHA2-512、SM3。\n\n身份保护身份数据在密钥产生之后加密传送，实现了对身份数据的保护。IKE支持的加密算法有：DES、3DES、AES-128、AES-192、AES-256、SM1和SM4。\n\nDHDH是一种公共密钥交换方法，它用于产生密钥材料，并通过ISAKMP消息在发送和接收设备之间进行密钥材料交换。然后，两端设备各自计算出完全相同的对称密钥。该对称密钥用于计算加密和验证的密钥。在任何时候，通信双方都不交换真正的密钥。DH密钥交换是IKE的精髓所在。\n\nPFS完善的前向安全性PFS（Perfect Forward Secrecy）通过执行一次额外的DH交换，确保即使IKE SA中使用的密钥被泄露，IPSec SA中使用的密钥也不会受到损害。\n\n\n2.2IPSec 基本原理IPSec通过在IPSec对等体间建立双向安全联盟形成一个安全互通的IPSec隧道，并通过定义IPSec保护的数据流将要保护的数据引入该IPSec隧道，然后对流经IPSec隧道的数据通过安全协议进行加密和验证，进而实现在Internet上安全传输指定的数据。IPSec安全联盟可以手工建立，也可以通过IKEv1或IKEv2协议自动协商建立。本文重点介绍如何定义IPSec保护的数据流、IKE自动协商建立安全联盟的过程。\n2.2.1 定义 IPSec 保护的数据流IPSec是基于定义的感兴趣流触发对特定数据的保护，至于什么样的数据是需要IPSec保护的，可以通过以下两种方式定义。其中IPSec感兴趣流即需要IPSec保护的数据流。\n\nACL方式手工方式和IKE自动协商方式建立的IPSec隧道是由ACL来指定要保护的数据流范围，筛选出需要进入IPSec隧道的报文，ACL规则允许（permit）的报文将被保护，未匹配任何permit规则的报文将不被保护。这种方式可以利用ACL的丰富配置功能，根据IP地址、端口、协议类型等对报文进行过滤进而灵活制定IPSec的保护方法。\n路由方式通过IPSec虚拟隧道接口建立IPSec隧道，将所有路由到IPSec虚拟隧道接口的报文都进行IPSec保护，根据该路由的目的地址确定哪些数据流需要IPSec保护。其中IPSec虚拟隧道接口是一种三层逻辑接口。路由方式具有以下优点：\n通过路由将需要IPSec保护的数据流引到虚拟隧道接口，不需使用ACL定义待\n加&#x2F;解密的流量特征，简化了IPSec配置的复杂性。\n支持动态路由协议。\n通过GRE over IPSec支持对组播流量的保护。\n\n\n\n2.2.2 IKEv1 协商安全联盟的过程IKEv1 协商阶段1\nIKEv1协商阶段1的目的是建立IKE SA。IKE SA建立后对等体间的所有ISAKMP（一个框架 IKE是一种实现）消息都将通过加密和验证，这条安全通道可以保证IKEv1第二阶段的协商能够安全进行。IKEv1协商阶段1支持两种协商模式：主模式（Main Mode）和野蛮模式（AggressiveMode）。主模式包含三次双向交换，用到了六条ISAKMP信息，协商过程如下图所示。这三次交换分别是：\n\n消息①和②用于提议交换发起方发送一个或多个IKE安全提议，响应方查找最先匹配的IKE安全提议，并将这个IKE安全提议回应给发起方。匹配的原则为协商双方具有相同的加密算法、认证算法、认证方法和DH组标识。\n消息③和④用于密钥信息交换\n双方交换DH(一种密钥交换算法，不暴露私钥的情况下，计算出一个共享密钥)公共值和nonce(一个随机值)值，用于IKE SA的认证和加密密钥在这个阶段产生。消息⑤和⑥用于身份和认证信息交换（双方使用生成的密钥发送信息），双方进行身份认证和对整个主模式交换内容的认证。\n\n野蛮模式只用到三条信息，前两条消息①和②用于协商IKE安全提议，交换DH公共值、必需的辅助信息以及身份信息并且消息②中还包括响应方发送身份信息供发起方认证，消息③用于响应方认证发起方。IKEv1协商阶段1的协商过程如下图所示。\n\n与主模式相比，野蛮模式减少了交换信息的数目，提高了协商的速度，但是没有对身份信息进行加密保护。\nIKEv1 协商阶段 2\nIKEv1协商阶段2的目的就是建立用来安全传输数据的IPSec SA，并为数据传输衍生出密钥。这一阶段采用快速模式（Quick Mode）。该模式使用IKEv1协商阶段1中生成的密钥对ISAKMP消息的完整性和身份进行验证，并对ISAKMP消息进行加密，故保证了交换的安全性。IKEv1协商阶段2的协商过程如下图所示。\n\nIKEv1协商阶段2通过三条ISAKMP消息完成双方IPSec SA的建立：\n\n协商发起方发送本端的安全参数和身份认证信息。安全参数包括被保护的数据流和IPSec安全提议等需要协商的参数。身份认证信息包括第一阶段计算出的密钥和第二阶段产生的密钥材料等，可以再次认证对等体。\n协商响应方发送确认的安全参数和身份认证信息并生成新的密钥。IPSec SA数据传输需要的加密、验证密钥由第一阶段产生的密钥、SPI、协议等参数衍生得出，以保证每个IPSec SA都有自己独一无二的密钥。如果启用PFS，则需要再次应用DH算法计算出一个共享密钥，然后参与上述计算，因此在参数协商时要为PFS协商DH密钥组。\n发送方发送确认信息，确认与响应方可以通信，协商结束。\n\n2.2.3 IKEv2 协商安全联盟的过程采用IKEv2协商安全联盟比IKEv1协商过程要简化的多。要建立一对IPSec SA，IKEv1需要经历两个阶段：“主模式＋快速模式”或者“野蛮模式＋快速模式”，前者至少需要交换9条消息，后者也至少需要6条消息。而IKEv2正常情况使用2次交换共4条消息就可以完成一对IPSec SA的建立，如果要求建立的IPSec SA大于一对时，每一对IPSec SA只需额外增加1次创建子SA交换，也就是2条消息就可以完成。IKEv2定义了三种交换：初始交换（Initial Exchanges）、创建子SA交换（Create_Child_SA Exchange）以及通知交换（Informational Exchange）。\n初始交换\n正常情况下，IKEv2通过初始交换就可以完成第一对IPSec SA的协商建立。IKEv2初始交换对应IKEv1的第一阶段，初始交换包含两次交换四条消息，如下图所示。\n消息①和②属于第一次交换（称为IKE_SA_INIT交换），以明文方式完成IKE SA的参数协商，包括协商加密和验证算法，交换临时随机数和DH交换。IKE_SA_INIT交换后生成一个共享密钥材料，通过这个共享密钥材料可以衍生出IPSec SA的所有密钥。消息③和④属于第二次交换（称为IKE_AUTH交换），以加密方式完成身份认证、对前两条信息的认证和IPSec SA的参数协商。IKEv2支持RSA签名认证、预共享密钥认证以及扩展认证方法EAP（Extensible Authentication Protocol）。发起者通过在消息3中省去认证载荷来表明需要使用EAP认证。\n创建子 SA 交换\n当一个IKE SA需要创建多对IPSec SA时，需要使用创建子SA交换来协商多于一对的IPSec SA。另外，创建子SA交换还可以用于IKE SA的重协商。创建子SA交换包含一个交换两条消息，对应IKEv1协商阶段2，交换的发起者可以是初始交换的协商发起方，也可以是初始交换的协商响应方。创建子SA交换必须在初始交换完成后进行，交换消息由初始交换协商的密钥进行保护。类似于IKEv1，如果启用PFS，创建子SA交换需要额外进行一次DH交换，生成新的密钥材料。生成密钥材料后，子SA的所有密钥都从这个密钥材料衍生出来。\n通知交换\n运行IKE协商的两端有时会传递一些控制信息，例如错误信息或者通告信息，这些信息在IKEv2中是通过通知交换完成的，如下图所示。通知交换必须在IKE SA保护下进行，也就是说通知交换只能发生在初始交换之后。控制信息可能是IKE SA的，那么通知交换必须由该IKE SA来保护进行；也可能是某子SA的，那么该通知交换必须由生成该子SA的IKE SA来保护进行。\n\n","categories":["网络协议学习"],"tags":["IPSec"]},{"title":"skb操作函数(一)","url":"/2025/06/15/skb%E7%9A%84%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0/","content":"skb预留和对齐数据的预留和对齐主要skb_reserve(),skb_put(),skb_push(),skb_pull这个几个函数来完成\nskb_reserve()skb_reserve() 是用于预留头部空间（headroom）的函数，它的作用是在 sk_buff 结构中提前空出一部分头部空间或者为了对齐，以便后续协议栈可以在这个位置添加协议头（如 MAC&#x2F;IP&#x2F;TCP 头）。\n注意skb_reserve()只能用于空的SKB，通常在分配SKB之后就会调用这个函数，注意此时的data和tail指针指向最初的位置。当skb在协议栈中向下传递的时候，data指针是向上移动的，然后复制需要封装的协议头部，最后更新skb的len字段，具体如下图：\n\nskb_reserve() 函数实现如下所示：\nstatic inline void skb_reserve(struct sk_buff *skb, int len)&#123;\tskb-&gt;data += len;\tskb-&gt;tail += len;&#125;\n\nskb_push()将数据指针 skb-&gt;data 向前移动（减少），为报文头腾出空间，并返回新位置的指针\n它通常用于：\n\n在已有 payload（有效负载）前添加协议头（如 IP、TCP 头）；\n构造完整报文时，按从上层到下层的顺序组装头部。\n\n函数原型如下：\nvoid *skb_push(struct sk_buff *skb, unsigned int len)&#123;\tskb-&gt;data -= len;\tskb-&gt;len  += len;\tif (unlikely(skb-&gt;data &lt; skb-&gt;head))\t\tskb_under_panic(skb, len, __builtin_return_address(0));\treturn skb-&gt;data;&#125;\n\n举个发送tcp数据包的例子，具体步骤如下：\n\n当发送TCP数据时，会根据一些条件，比如MSS申请一个skb\nTCP需要在申请的缓冲区头部预留足够的空间，用来填充各层首部，由于不知道各层首部的长度，比如是否存在ip选项等，会预留一个最大长度\n把TCP的payload复制到数据缓存区\n负载构建完成后封装TCP的首部\n交付给IP层，封装IP首部\n交付给链路层，封装链路层首部\n\n上述封装各层头之前，都需要用skb_push移动data指针来开辟一段空间，上述具体流程如下图所示：\n\nskb_put()在 sk_buff 缓冲区末尾“添加数据空间”，即将 skb-&gt;tail 向后移动，并增加 skb-&gt;len，为你写入数据腾出空间。\nskb-&gt;tail 表示当前写入数据的末尾；\nskb_put() 把 tail 向后移动 len 字节，表示“打算添加 len 字节数据”；\n返回原始 tail 地址，你可以在这个地址处填入数据（如 payload）；\n函数原型如下：\nvoid *skb_put(struct sk_buff *skb, unsigned int len)&#123;\tvoid *tmp = skb_tail_pointer(skb);\tSKB_LINEAR_ASSERT(skb);\tskb-&gt;tail += len;\tskb-&gt;len  += len;\tif (unlikely(skb-&gt;tail &gt; skb-&gt;end))\t\tskb_over_panic(skb, len, __builtin_return_address(0));\treturn tmp;&#125;\n\n调用skb_put()前后skb结构变化如下所示：\n\nskb_pull()将 skb-&gt;data 指针向后移动（跳过前面的数据），并减少 skb-&gt;len，通常用于跳过协议头或处理完某一层协议之后。\n函数原型如下：\nvoid *skb_pull(struct sk_buff *skb, unsigned int len)&#123;\treturn skb_pull_inline(skb, len);&#125;static inline void *skb_pull_inline(struct sk_buff *skb, unsigned int len)&#123;\treturn unlikely(len &gt; skb-&gt;len) ? NULL : __skb_pull(skb, len);&#125;static inline void *__skb_pull(struct sk_buff *skb, unsigned int len)&#123;\tskb-&gt;len -= len;\tBUG_ON(skb-&gt;len &lt; skb-&gt;data_len);\treturn skb-&gt;data += len;&#125;\n\n假设我们收到一个 IP 报文，skb-&gt;data 指向 IP 头，现在我们要把 IP 头跳过去，交给 TCP 层：\nstruct iphdr *iph = ip_hdr(skb);  // 当前 data 指向 IP 头skb_pull(skb, iph-&gt;ihl * 4);      // 跳过 IP 头struct tcphdr *tcph = (struct tcphdr *)skb-&gt;data;  // 现在 data 指向 TCP 头\n\n条用skb_pull前后skb结构变化如下所示：\n\n链表管理函数skb_queue_head_init()初始化 skb 队列（链表头）\nstatic inline void __skb_queue_head_init(struct sk_buff_head *list)&#123;\tlist-&gt;prev = list-&gt;next = (struct sk_buff *)list;\tlist-&gt;qlen = 0;&#125;static inline void skb_queue_head_init(struct sk_buff_head *list)&#123;\tspin_lock_init(&amp;list-&gt;lock);\t__skb_queue_head_init(list);&#125;\n\nskb_queue_head()将一个 sk_buff（网络数据包）添加到指定 sk_buff_head 队列的头部，并加锁保护\nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk)&#123;\tunsigned long flags;\tspin_lock_irqsave(&amp;list-&gt;lock, flags);\t__skb_queue_head(list, newsk);\tspin_unlock_irqrestore(&amp;list-&gt;lock, flags);&#125;static inline void __skb_queue_head(struct sk_buff_head *list,\t\t\t\t    struct sk_buff *newsk)&#123;\t__skb_queue_after(list, (struct sk_buff *)list, newsk);&#125;static inline void __skb_queue_after(struct sk_buff_head *list,\t\t\t\t     struct sk_buff *prev,\t\t\t\t     struct sk_buff *newsk)&#123;\t__skb_insert(newsk, prev, prev-&gt;next, list);&#125;\t\t\t\tstruct sk_buff *prev, struct sk_buff *next,\t\t\t\tstruct sk_buff_head *list)&#123;\t/* See skb_queue_empty_lockless() and skb_peek_tail()\t * for the opposite READ_ONCE()\t */\tWRITE_ONCE(newsk-&gt;next, next);\tWRITE_ONCE(newsk-&gt;prev, prev);\tWRITE_ONCE(next-&gt;prev, newsk);\tWRITE_ONCE(prev-&gt;next, newsk);\tlist-&gt;qlen++;&#125;\n\nskb_dequeue从一个 skb 队列（struct sk_buff_head）头部取出并移除一个 sk_buff（网络数据包）。\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list)&#123;\tunsigned long flags;\tstruct sk_buff *result;\tspin_lock_irqsave(&amp;list-&gt;lock, flags);\tresult = __skb_dequeue(list);\tspin_unlock_irqrestore(&amp;list-&gt;lock, flags);\treturn result;&#125;static inline struct sk_buff *__skb_dequeue(struct sk_buff_head *list)&#123;\tstruct sk_buff *skb = skb_peek(list);\tif (skb)\t\t__skb_unlink(skb, list);\treturn skb;&#125;static inline void __skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)&#123;\tstruct sk_buff *next, *prev;\tWRITE_ONCE(list-&gt;qlen, list-&gt;qlen - 1);\tnext\t   = skb-&gt;next;\tprev\t   = skb-&gt;prev;\tskb-&gt;next  = skb-&gt;prev = NULL;\tWRITE_ONCE(next-&gt;prev, prev);\tWRITE_ONCE(prev-&gt;next, next);&#125;\n\nskb_queue_purge释放并清空该 skb 队列中的所有 skb，其实就是出队+kfree skb\nvoid skb_queue_purge(struct sk_buff_head *list)&#123;\tstruct sk_buff *skb;\twhile ((skb = skb_dequeue(list)) != NULL)\t\tkfree_skb(skb);&#125;\n\nskb_queue_walk一个宏，遍历skb链表中每一个元素\n#define skb_queue_walk(queue, skb) \\\t\tfor (skb = (queue)-&gt;next;\t\t\t\t\t\\\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\t\t     skb = skb-&gt;next)\n\n","categories":["网络协议栈源码学习"],"tags":["skb"]},{"title":"moudle_init()原理学习","url":"/2025/05/28/module_init%E5%8E%9F%E7%90%86%E5%AD%A6%E4%B9%A0/","content":"moudle_init()原理学习1.静态加载与动态加载内核模块代码的例子#include &lt;linux/module.h&gt;#include &lt;linux/init.h&gt; static int hello_init(void)&#123;    printk(KERN_ALERT &quot;Hello World\\n&quot;);    return 0;&#125; static void hello_exit(void)&#123;    printk(KERN_ALERT &quot;Bye Bye World\\n&quot;);&#125; module_init(hello_init);module_exit(hello_exit);\n上述编写的内核模块有两种运行方式，一是静态编译链接进内核，在系统启动过程中进行初始化；一是编译成可动态加载的module，通过insmod动态加载重定位到内核。这两种方式可以在Makefile中通过obj-y或obj-m选项进行选择。\n动态加载：\n\n可根据系统需要运行动态加载模块，以扩充内核功能，不需要时将其卸载，以释放内存空间；\n当需要修改内核功能时，只需编译相应模块，而不必重新编译整个内核。\n\n静态加载但是有些模块必须要编译到内核，随内核一起运行，从不卸载，如 vfs等\n2.动态加载与静态加载实现原理同样的内核代码通过对moudle_init这个宏的不同展开来实现动态加载与卸载下面看一下moudle_init这个宏的实现：\n#ifndef MODULE#define module_init(x)\t__initcall(x);#define module_exit(x)\t__exitcall(x);#else /* MODULE */.../* Each module must use one module_init(). */#define module_init(initfn)\t\t\t\t\t\\\tstatic inline initcall_t __maybe_unused __inittest(void)\t\t\\\t&#123; return initfn; &#125;\t\t\t\t\t\\\tint init_module(void) __copy(initfn)\t\t\t\\\t\t__attribute__((alias(#initfn)));\t\t\\\t___ADDRESSABLE(init_module, __initdata);/* This is only required if you want to be unloadable. */#define module_exit(exitfn)\t\t\t\t\t\\\tstatic inline exitcall_t __maybe_unused __exittest(void)\t\t\\\t&#123; return exitfn; &#125;\t\t\t\t\t\\\tvoid cleanup_module(void) __copy(exitfn)\t\t\\\t\t__attribute__((alias(#exitfn)));\t\t\\\t___ADDRESSABLE(cleanup_module, __exitdata);#endif\n\n从上面代码可以发现moudle_init宏的不同展开方式取决于MODULE是否被定义，也就是通过Makefile控制的，上面部分用于将模块静态编译连接进内核，下面部分用于编译可动态加载的模块。\n2.1静态加载静态加载moudle_init宏经过一系列宏展开后如下所示：\n// 静态断言（确保函数类型匹配）static_assert(__same_type(initcall_t, &amp;hello_init));// 汇编部分：将函数地址存入特定节asm volatile (    &quot;.section    \\&quot;.initcall6.init\\&quot;, \\&quot;a\\&quot;        \\n&quot;  // 定义节    &quot;__initcall_hello__0_10_hello_init6:          \\n&quot;  // 标签    &quot;.long       hello_init - .                   \\n&quot;  //函数地址与标签地址的偏移量    &quot;.previous                                   \\n&quot;    // 恢复默认节);\n\n上述这段汇编代码的作用就是在 .initcall6.init 节中定义一个条目（entry），条目内容就是 hello_init 函数的相对偏移量。具体逻辑如下：\n切换到名为 .initcall6.init 的 ELF 段（Section）。.initcall6.init 是内核用于存储初始化函数指针的特殊段，数字 6 表示优先级（数字越小优先级越高）。内核启动时，会按优先级顺序遍历这些段。__initcall_hello__0_10_hello_init6 ：定义一个标签，用于标识当前初始化函数的位置。.long       hello_init - .存储 hello_init 函数地址相对于当前标签的偏移量。\n上述存放于 .initcall6.init 段中的__initcall_hello__0_10_hello_init6 是在start_kernel(系统调用的第一个c语言程序)中调用。 具体调用流程如下所示：\nstart_kernel|--&gt; rest_init    |    --&gt; kernel_thread        |        --&gt; kernel_init            |            --&gt; kernel_init_freeable                |                --&gt; do_basic_setup                    |                    --&gt; do_initcalls                        |                        --&gt; do_initcall_level(level)                            |                            --&gt; do_one_initcall(initcall_t fn)\n\n其中do_initcalls可以理解为处理module_init的一个入口函数具体定义如下：\nstatic void __init do_initcalls(void)&#123;\tint level;//初始化级别，0-7,优先级由高到低\tsize_t len = saved_command_line_len + 1;\tchar *command_line;\tcommand_line = kzalloc(len, GFP_KERNEL);\tif (!command_line)\t\tpanic(&quot;%s: Failed to allocate %zu bytes\\n&quot;, __func__, len);\t//遍历不同的优先级\tfor (level = 0; level &lt; ARRAY_SIZE(initcall_levels) - 1; level++) &#123;\t\t/* Parser modifies command_line, restore it each time */\t\tstrcpy(command_line, saved_command_line);\t\t//执行单个级别的初始化函数\t\tdo_initcall_level(level, command_line);\t&#125;\tkfree(command_line);&#125;\ndo_initcalls 中调用do_initcall_level来​​执行某个特定级别的所有初始化函数，具体函数如下所示：\nstatic void __init do_initcall_level(int level, char *command_line)&#123;\tinitcall_entry_t *fn;\tparse_args(initcall_level_names[level],\t\t   command_line, __start___param,\t\t   __stop___param - __start___param,\t\t   level, level,\t\t   NULL, ignore_unknown_bootoption);\ttrace_initcall_level(initcall_level_names[level]);\tfor (fn = initcall_levels[level]; fn &lt; initcall_levels[level+1]; fn++)\t\tdo_one_initcall(initcall_from_entry(fn));&#125;\ndo_one_initcall中根据一个具体的level依次调用do_one_initcall执行某一个level中的所有初始化函数\n上面的initcall_levels[]是一个指针数组定义如下所示：\nstatic initcall_t *initcall_levels[] __initdata = &#123;    __initcall0_start,    __initcall1_start,    __initcall2_start,    __initcall3_start,    __initcall4_start,    __initcall5_start,    __initcall6_start,    __initcall7_start,    __initcall_end,&#125;\n上述指针数组中每个元素定义如下所示：\nextern initcall_entry_t __initcall_start[];extern initcall_entry_t __initcall0_start[];extern initcall_entry_t __initcall1_start[];extern initcall_entry_t __initcall2_start[];extern initcall_entry_t __initcall3_start[];extern initcall_entry_t __initcall4_start[];extern initcall_entry_t __initcall5_start[];extern initcall_entry_t __initcall6_start[];extern initcall_entry_t __initcall7_start[];extern initcall_entry_t __initcall_end[];\n这里 __initcallX_start 符号不是传统意义上的变量。它们的值是在内核链接时由链接器自动计算的 可以理解也就是module_init注册的函数的偏移量就存在上述的__initcall6_start[]数组中\n这里举个例子：1.驱动通过 module_init 注册\nstatic int __init usb_init(void) &#123; /* ... */ &#125;module_init(usb_init);  // 默认对应级别6\n2.译器会将其转换为：\n.section &quot;.initcall6.init&quot;, &quot;a&quot;  ; 放入级别6的段__initcall_usb_init:    .long usb_init - .  ; 存储偏移量（非直接地址）\n\n3.链接脚本（vmlinux.lds）将所有 .initcall6.init 段合并？？？\n.initcall6.init : &#123;    __initcall6_start = .;    *(.initcall6.init)  ; 包含所有级别6的驱动初始化条目    __initcall7_start = .;&#125;\n4.内核通过 initcall_levels 访问\n可以看到上面的do_initcall_level中参数为initcall_from_entry 这个函数实现了将存储在内核初始化表中的​​相对偏移量​​转换为​​实际的函数地址​，​具体定义如下：\nstatic inline initcall_t initcall_from_entry(initcall_entry_t *entry)&#123;\treturn offset_to_ptr(entry);&#125;\n\n最终的do_one_initcall完成了针对某一个level中的一个entry的初始化：\nint __init_or_module do_one_initcall(initcall_t fn)&#123;\tint count = preempt_count();\tchar msgbuf[64];\tint ret;\tif (initcall_blacklisted(fn))\t\treturn -EPERM;\tdo_trace_initcall_start(fn);    //这里执行具体的函数调用！！！\tret = fn();\tdo_trace_initcall_finish(fn, ret);\tmsgbuf[0] = 0;\tif (preempt_count() != count) &#123;\t\tsprintf(msgbuf, &quot;preemption imbalance &quot;);\t\tpreempt_count_set(count);\t&#125;\tif (irqs_disabled()) &#123;\t\tstrlcat(msgbuf, &quot;disabled interrupts &quot;, sizeof(msgbuf));\t\tlocal_irq_enable();\t&#125;\tWARN(msgbuf[0], &quot;initcall %pS returned with %s\\n&quot;, fn, msgbuf);\tadd_latent_entropy();\treturn ret;&#125;\n\n\n2.2动态加载动态加载通常通过insmod加载内核模块，原理就是执行了init_module这个系统调用。如果是动态加载，moudle_init宏展开如下所示：\nstatic inline initcall_t __maybe_unused __inittest(void) &#123;     return hello_init; &#125;int init_module(void) __attribute__((alias(&quot;hello_init&quot;)));___ADDRESSABLE(init_module, __initdata);\n\n上述__inittest 函数确保 hello_init 符合 initcall_t 类型（返回 int 且无参数）\n上述的alias 属性是 gcc 的特有属性，将定义 init_module 为函数 initfn 的别名。所以 module_init(hello_init) 的作用就是定义一个变量名 init_module，其地址和 hello_init 是一样的\n上述例子编译可动态加载模块过程中，会自动产生 HelloWorld.mod.c 文件(最终会合并所有.o链接生成一个.ko)，内容如下：\n#include &lt;linux/module.h&gt;#include &lt;linux/vermagic.h&gt;#include &lt;linux/compiler.h&gt; MODULE_INFO(vermagic, VERMAGIC_STRING); struct module __this_module__attribute__((section(&quot;.gnu.linkonce.this_module&quot;))) = &#123;    .name = KBUILD_MODNAME,    .init = init_module,#ifdef CONFIG_MODULE_UNLOAD    .exit = cleanup_module,#endif    .arch = MODULE_ARCH_INIT,&#125;; static const char __module_depends[]__used__attribute__((section(&quot;.modinfo&quot;))) =&quot;depends=&quot;;\n\n由上述代码可知，定义了一个类型为 module 的全局变量 __this_module，成员 init 为 init_module（即 hello_init），且该变量链接到 .gnu.linkonce.this_module 段中\n最终，通过执行insmod这个可执行程序会通过系统调用加载内核模块，具体流程如下：\nSYSCALL_DEFINE3(init_module, ...)|--&gt;load_module    |    --&gt; do_init_module(mod)        |        --&gt; do_one_initcall(mod-&gt;init);\n\n可以看到动态加载的方式最终也是调用到了do_one_initcall只不过传入的参数实际上就是hello_init\n","categories":["其他"],"tags":["linux内核"]},{"title":"skb操作函数(二)","url":"/2025/06/17/skb%E7%9A%84%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E4%BA%8C/","content":"删除skb尾部数据skb_trim()skb_trim() 是 Linux 内核中一个用于截断数据包长度的函数，主要用于收缩 skb 的数据长度（通常是从尾部截断）\nvoid skb_trim(struct sk_buff *skb, unsigned int len);\n\nvoid skb_trim(struct sk_buff *skb, unsigned int len)&#123;\tif (skb-&gt;len &gt; len)\t\t__skb_trim(skb, len);&#125;static inline void __skb_trim(struct sk_buff *skb, unsigned int len)&#123;\t__skb_set_length(skb, len);&#125;static inline void __skb_set_length(struct sk_buff *skb, unsigned int len)&#123;\tif (WARN_ON(skb_is_nonlinear(skb)))\t\treturn;\tskb-&gt;len = len;\tskb_set_tail_pointer(skb, len);&#125;static inline void skb_set_tail_pointer(struct sk_buff *skb, const int offset)&#123;\t//注意这里是先将tail指向data，然后在偏移offset\tskb_reset_tail_pointer(skb);\tskb-&gt;tail += offset;&#125;\n\nskb_trim操作前后如下图所示：\n\npskb_trim()pskb_trim() 是 Linux 内核中用于“安全地裁剪 skb 长度”的函数，类似于 skb_trim()，但它支持非线性（分页）skb，是一个更强大、更通用的版本。\nstatic inline int pskb_trim(struct sk_buff *skb, unsigned int len)&#123;\treturn (len &lt; skb-&gt;len) ? __pskb_trim(skb, len) : 0;&#125;static inline int __pskb_trim(struct sk_buff *skb, unsigned int len)&#123;\t//这里判断了是否存在了非线性部分的数据\tif (skb-&gt;data_len)\t\treturn ___pskb_trim(skb, len);\t__skb_trim(skb, len);\treturn 0;&#125;/* Trims skb to length len. It can change skb pointers. */int ___pskb_trim(struct sk_buff *skb, unsigned int len)&#123;\tstruct sk_buff **fragp;\tstruct sk_buff *frag;\tint offset = skb_headlen(skb); //线性的长度\tint nfrags = skb_shinfo(skb)-&gt;nr_frags; //有几个页\tint i;\tint err;\t//如果有别人在使用这个skb 调用pskb_expand_head 复制skb\tif (skb_cloned(skb) &amp;&amp;\t    unlikely((err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC))))\t\treturn err;\ti = 0;\t//线性长度大于要阶段的部分，那就直接drop page就可以了\tif (offset &gt;= len)\t\tgoto drop_pages;\tfor (; i &lt; nfrags; i++) &#123;\t\tint end = offset + skb_frag_size(&amp;skb_shinfo(skb)-&gt;frags[i]);\t\t//没到na&#x27;gepage就continue\t\tif (end &lt; len) &#123;\t\t\toffset = end;\t\t\tcontinue;\t\t&#125;\t\tskb_frag_size_set(&amp;skb_shinfo(skb)-&gt;frags[i++], len - offset);drop_pages:\t\tskb_shinfo(skb)-&gt;nr_frags = i;\t\tfor (; i &lt; nfrags; i++)\t\t\tskb_frag_unref(skb, i);\t\t//如果有fraglist非线性部分，释放fraglist\t\tif (skb_has_frag_list(skb))\t\t\tskb_drop_fraglist(skb);\t\tgoto done;\t&#125;\t//处理没有page只有fraglist的情况\tfor (fragp = &amp;skb_shinfo(skb)-&gt;frag_list; (frag = *fragp);\t     fragp = &amp;frag-&gt;next) &#123;\t\tint end = offset + frag-&gt;len;\t\tif (skb_shared(frag)) &#123;\t\t\tstruct sk_buff *nfrag;\t\t\tnfrag = skb_clone(frag, GFP_ATOMIC);\t\t\tif (unlikely(!nfrag))\t\t\t\treturn -ENOMEM;\t\t\tnfrag-&gt;next = frag-&gt;next;\t\t\tconsume_skb(frag);\t\t\tfrag = nfrag;\t\t\t*fragp = frag;\t\t&#125;\t\tif (end &lt; len) &#123;\t\t\toffset = end;\t\t\tcontinue;\t\t&#125;\t\tif (end &gt; len &amp;&amp;\t\t    unlikely((err = pskb_trim(frag, len - offset))))\t\t\treturn err;\t\tif (frag-&gt;next)\t\t\tskb_drop_list(&amp;frag-&gt;next);\t\tbreak;\t&#125;done://设置skb的len相关的字段\tif (len &gt; skb_headlen(skb)) &#123;\t\tskb-&gt;data_len -= skb-&gt;len - len;\t\tskb-&gt;len       = len;\t&#125; else &#123;\t\tskb-&gt;len       = len;\t\tskb-&gt;data_len  = 0;\t\tskb_set_tail_pointer(skb, len);\t&#125;\tif (!skb-&gt;sk || skb-&gt;destructor == sock_edemux)\t\tskb_condense(skb);\treturn 0;&#125;\n\npskb_trim操作前后如下所示：\n\n\n分割skbskb_split把一个大的 skb 分裂成两个 skb：保留前半段在原 skb 中，后半段移到新的 skb 中返回。这里具体可以分为两种情况，第一种，被拆分的数据包的线性部分就够用了。第二种，被拆分的数据包线性部分不够用，那就需要额外处理非线性部分\n注意:tcp分段中会用到这个函数，问题，为什么没考虑fraglist呢？跟如何调用这个函数有关？？\n**skb_split**() 函数原型如下：\nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len)&#123;\tint pos = skb_headlen(skb);\t//拷贝原本数据包的tx_flag\tskb_shinfo(skb1)-&gt;tx_flags |= skb_shinfo(skb)-&gt;tx_flags &amp;\t\t\t\t      SKBTX_SHARED_FRAG;\tskb_zerocopy_clone(skb1, skb, 0);//tcp zc 或者xdp 会有相应的处理\tif (len &lt; pos)\t/* Split line is inside header. *///\t//第一种情况， len的长度小于非线性部分的长度\t\tskb_split_inside_header(skb, skb1, len, pos);\telse\t\t/* Second chunk has no header, nothing to copy. */\t//第二种情况，需要处理非线性部分\t\tskb_split_no_header(skb, skb1, len, pos);&#125;\n\n上面的skb和skb1说明：\n\n原始大包在 skb 里；\n“拆出后半部分”放到新的 skb —— 就是 skb1；\n\nskb_split_inside_header 逻辑就是，把原本skb中除了要拆分出去的数据，都交友skb1去管理\nstatic inline void skb_split_inside_header(struct sk_buff *skb,\t\t\t\t\t   struct sk_buff* skb1,\t\t\t\t\t   const u32 len, const int pos)&#123;\tint i;\t//从原始 skb 的线性区中，从偏移len开始的位置，拷贝 pos - len 字节的数据到 新 skb1 的线性区尾部。\tskb_copy_from_linear_data_offset(skb, len, skb_put(skb1, pos - len),\t\t\t\t\t pos - len);\t/* And move data appendix as is. */\tfor (i = 0; i &lt; skb_shinfo(skb)-&gt;nr_frags; i++)\t\tskb_shinfo(skb1)-&gt;frags[i] = skb_shinfo(skb)-&gt;frags[i];\t//这里其实就是把原本skb的非线性部分，被新的skb1给引用了\tskb_shinfo(skb1)-&gt;nr_frags = skb_shinfo(skb)-&gt;nr_frags;\tskb_shinfo(skb)-&gt;nr_frags  = 0;\tskb1-&gt;data_len\t\t   = skb-&gt;data_len;\tskb1-&gt;len\t\t   += skb1-&gt;data_len;\tskb-&gt;data_len\t\t   = 0;\tskb-&gt;len\t\t   = len;\tskb_set_tail_pointer(skb, len);&#125;\n\n第二种情况skb_split_no_header() 用于将一个 skb 数据包在线性数据区 不需要拷贝 header 的情况下 进行拆分（split）的。此时拆分点 len 已经在非线性区域，所以只处理 frags。\nstatic inline void skb_split_no_header(struct sk_buff *skb,\t\t\t\t       struct sk_buff* skb1,\t\t\t\t       const u32 len, int pos)&#123;\tint i, k = 0;\tconst int nfrags = skb_shinfo(skb)-&gt;nr_frags;\tskb_shinfo(skb)-&gt;nr_frags = 0;\t//新的skb1的长度只有非线性部分了\tskb1-&gt;len\t\t  = skb1-&gt;data_len = skb-&gt;len - len;\t//原始的skb只保存len长度的数据\tskb-&gt;len\t\t  = len;\t//这里原始skb的非线性部分就是要保留的总长度减去一个头部 长度\tskb-&gt;data_len\t\t  = len - pos;\tfor (i = 0; i &lt; nfrags; i++) &#123;\t\t//这里的size是每一个page的长度\t\tint size = skb_frag_size(&amp;skb_shinfo(skb)-&gt;frags[i]);\t\t//非线性部分+size还没到截断的长度话\t\tif (pos + size &gt; len) &#123;\t\t\t//这里skb引用了skb\t\t\tskb_shinfo(skb1)-&gt;frags[k] = skb_shinfo(skb)-&gt;frags[i];\t\t\tif (pos &lt; len) &#123;\t\t\t\t/* Split frag.\t\t\t\t * We have two variants in this case:\t\t\t\t * 1. Move all the frag to the second\t\t\t\t *    part, if it is possible. F.e.\t\t\t\t *    this approach is mandatory for TUX,\t\t\t\t *    where splitting is expensive.\t\t\t\t * 2. Split is accurately. We make this.\t\t\t\t */\t\t\t\tskb_frag_ref(skb, i);\t\t\t\tskb_frag_off_add(&amp;skb_shinfo(skb1)-&gt;frags[0], len - pos);\t\t\t\tskb_frag_size_sub(&amp;skb_shinfo(skb1)-&gt;frags[0], len - pos);\t\t\t\tskb_frag_size_set(&amp;skb_shinfo(skb)-&gt;frags[i], len - pos);\t\t\t\tskb_shinfo(skb)-&gt;nr_frags++;\t\t\t&#125;\t\t\tk++;\t\t&#125; else\t\t\tskb_shinfo(skb)-&gt;nr_frags++;\t\t//这里更新了pos\t\tpos += size;\t&#125;\t//设置skb1 nr_frag数量\tskb_shinfo(skb1)-&gt;nr_frags = k;&#125;\n\n\n拆分后：\n\n\n拆分后：\n\n其他函数pskb_may_pull核心作用是确保 skb 的线性区域中至少有 len 字节的数据。如果不够，它会尝试从非线性区（页片 frags 或 frag_list）中 拉取（pull）数据进线性区域。\nstatic inline bool pskb_may_pull(struct sk_buff *skb, unsigned int len)&#123;\tif (likely(len &lt;= skb_headlen(skb)))\t\treturn true;\tif (unlikely(len &gt; skb-&gt;len))\t\treturn false;\treturn __pskb_pull_tail(skb, len - skb_headlen(skb)) != NULL;&#125;\n\n\n\nskb_queue_empty判断skb的队列是否为空\nstatic inline int skb_queue_empty(const struct sk_buff_head *list)&#123;\treturn list-&gt;next == (const struct sk_buff *) list;&#125;\n\nskb_get增加引用计数\nstatic inline struct sk_buff *skb_get(struct sk_buff *skb)&#123;\trefcount_inc(&amp;skb-&gt;users);\treturn skb;&#125;\n\nskb_shared判断是否有多个引用\nstatic inline int skb_shared(const struct sk_buff *skb)&#123;\treturn refcount_read(&amp;skb-&gt;users) != 1;&#125;\n\nskb_shared_check()如果skb被引用，则clone此skb并返回得到的skb\nstatic inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)&#123;\tmight_sleep_if(gfpflags_allow_blocking(pri));\tif (skb_shared(skb)) &#123;\t\tstruct sk_buff *nskb = skb_clone(skb, pri);\t\t//克隆后会对原来的skb引用计数-\t\tif (likely(nskb))\t\t\tconsume_skb(skb);\t\telse\t\t\tkfree_skb(skb);\t\tskb = nskb;\t&#125;\treturn skb;&#125;\n\nskb_unshare与skb_shared_check类似区别是一个是clone一个是copy\nstatic inline struct sk_buff *skb_unshare(struct sk_buff *skb,\t\t\t\t\t  gfp_t pri)&#123;\tmight_sleep_if(gfpflags_allow_blocking(pri));\tif (skb_cloned(skb)) &#123;\t\tstruct sk_buff *nskb = skb_copy(skb, pri);\t\t/* Free our shared copy */\t\tif (likely(nskb))\t\t\tconsume_skb(skb);\t\telse\t\t\tkfree_skb(skb);\t\tskb = nskb;\t&#125;\treturn skb;&#125;\n\nskb_orphan取消skb与sock结构的关联\nstatic inline void skb_orphan(struct sk_buff *skb)&#123;\tif (skb-&gt;destructor) &#123;\t\tskb-&gt;destructor(skb);\t\tskb-&gt;destructor = NULL;\t\tskb-&gt;sk\t\t= NULL;\t&#125; else &#123;\t\tBUG_ON(skb-&gt;sk);\t&#125;&#125;\n\nskb_cow(copy on wirte)确保skb有指定的头部空间，如果没有指定的头部空间，就重新分配。\nstatic inline int __skb_cow(struct sk_buff *skb, unsigned int headroom,\t\t\t    int cloned)&#123;\tint delta = 0;\t//不够的空间保存到delta中\tif (headroom &gt; skb_headroom(skb))\t\tdelta = headroom - skb_headroom(skb);\t//如果不够，或者skb是被clone过的\tif (delta || cloned)        //这里的第二个参数就是头部要扩充的空间，\t\treturn pskb_expand_head(skb, ALIGN(delta, NET_SKB_PAD), 0,\t\t\t\t\tGFP_ATOMIC);\treturn 0;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["skb"]},{"title":"skb_copy && skb_clone","url":"/2025/06/19/skb%E7%AE%A1%E7%90%86%E5%87%BD%E6%95%B0%E4%B8%89/","content":"skb_clone()skb_clone() 创建一个新的 sk_buff 结构体，共享原始 skb 的数据 buffer，但结构体本身是独立的。\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask)&#123;\t//这里先拿到flones，后面可能快速clone\tstruct sk_buff_fclones *fclones = container_of(skb,\t\t\t\t\t\t       struct sk_buff_fclones,\t\t\t\t\t\t       skb1);\tstruct sk_buff *n;\tif (skb_orphan_frags(skb, gfp_mask))\t\treturn NULL;\t//这里判断skb alloc的时候是否有skb2，如果有有直接用skb2\tif (skb-&gt;fclone == SKB_FCLONE_ORIG &amp;&amp;\t    refcount_read(&amp;fclones-&gt;fclone_ref) == 1) &#123;\t\tn = &amp;fclones-&gt;skb2;\t\trefcount_set(&amp;fclones-&gt;fclone_ref, 2);\t&#125; else &#123;\t\tif (skb_pfmemalloc(skb))\t\t\tgfp_mask |= __GFP_MEMALLOC;\t\t//这里分配一个skbi二狗提\t\tn = kmem_cache_alloc(skbuff_head_cache, gfp_mask);\t\tif (!n)\t\t\treturn NULL;\t\tn-&gt;fclone = SKB_FCLONE_UNAVAILABLE;\t&#125;\t//设置skb的字段\treturn __skb_clone(n, skb);&#125;\n\n上述函数返回时调用__skb_clone 设置各个字段\nstatic struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)&#123;#define C(x) n-&gt;x = skb-&gt;x\t//断开链表\tn-&gt;next = n-&gt;prev = NULL;\tn-&gt;sk = NULL;\t//这里设置了原来的大部分元数据\t__copy_skb_header(n, skb);\tC(len);//总长度\tC(data_len);//非线性部分长度\tC(mac_len);//mac头长度\tn-&gt;hdr_len = skb-&gt;nohdr ? skb_headroom(skb) : skb-&gt;hdr_len;//如果是nohdr 就保存头部空间\tn-&gt;cloned = 1; //标识是clone出来的\tn-&gt;nohdr = 0;\tn-&gt;peeked = 0; //不是peek\tC(pfmemalloc);\tn-&gt;destructor = NULL;\t//直接指针指一下\tC(tail);\tC(end);\tC(head);\tC(head_frag);\tC(data);\tC(truesize);\trefcount_set(&amp;n-&gt;users, 1);\tatomic_inc(&amp;(skb_shinfo(skb)-&gt;dataref));//增加引用计数\tskb-&gt;cloned = 1;\treturn n;#undef C&#125;\n\n上述代码如下图所示：\n\npskb_copy复制 skb 的 线性部分（即 skb-&gt;data 到 skb-&gt;tail 的部分）， 对于非线性部分（如 frags[] 和 frag_list），只是保留引用，不复制数据。\nstatic inline struct sk_buff *pskb_copy(struct sk_buff *skb,\t\t\t\t\tgfp_t gfp_mask)&#123;\treturn __pskb_copy(skb, skb_headroom(skb), gfp_mask);&#125;static inline struct sk_buff *__pskb_copy(struct sk_buff *skb, int headroom,\t\t\t\t\t  gfp_t gfp_mask)&#123;\treturn __pskb_copy_fclone(skb, headroom, gfp_mask, false);&#125;\n\n上述__pskb_copy_fclone具体实现如下，主要就是申请一个skb，并复制线性部分的空间，非线性部分就是增加原来的的引用，之后复制原始skb的部分字段。\nstruct sk_buff *__pskb_copy_fclone(struct sk_buff *skb, int headroom,\t\t\t\t   gfp_t gfp_mask, bool fclone)&#123;\t//线性部分加原来数据包头部未使用的空间\tunsigned int size = skb_headlen(skb) + headroom;\t//是否需要fclone，如果需要，alloc的时候就申请两个skb\tint flags = skb_alloc_rx_flag(skb) | (fclone ? SKB_ALLOC_FCLONE : 0);\t//alloc一个skb\tstruct sk_buff *n = __alloc_skb(size, gfp_mask, flags, NUMA_NO_NODE);```cif (!n)\tgoto out;/* Set the data pointer *///设置data和tail指针skb_reserve(n, headroom);/* Set the tail pointer and length *///设置tail指针的位置skb_put(n, skb_headlen(skb));/* Copy the bytes *///memcpy一个线性部分skb_copy_from_linear_data(skb, n-&gt;data, n-&gt;len);n-&gt;truesize += skb-&gt;data_len;n-&gt;data_len  = skb-&gt;data_len;n-&gt;len\t     = skb-&gt;len;//处理非线性部分，就是加个引用if (skb_shinfo(skb)-&gt;nr_frags) &#123;\tint i;\t//zc相关大概率不会走\tif (skb_orphan_frags(skb, gfp_mask) ||\t    skb_zerocopy_clone(n, skb, gfp_mask)) &#123;\t\tkfree_skb(n);\t\tn = NULL;\t\tgoto out;\t&#125;\tfor (i = 0; i &lt; skb_shinfo(skb)-&gt;nr_frags; i++) &#123;\t\tskb_shinfo(n)-&gt;frags[i] = skb_shinfo(skb)-&gt;frags[i];\t\tskb_frag_ref(skb, i);\t&#125;\tskb_shinfo(n)-&gt;nr_frags = i;&#125;//如果哟fraglistif (skb_has_frag_list(skb)) &#123;\tskb_shinfo(n)-&gt;frag_list = skb_shinfo(skb)-&gt;frag_list;\t//加引用计数\tskb_clone_fraglist(n);&#125;//复制元数据skb_copy_header(n, skb);```out:\treturn n;&#125;\n\n拷贝后的skb如下图所示：\n\nskb_copyskb_copy() 是 Linux 内核中用于复制一个 sk_buff（socket buffer）的函数，作用是 深拷贝整个 skb 的内容 —— 包括线性部分（head 数据）和非线性部分（paged frags），注意这里拷贝后的skb就完全是一个线性的skb了，也就是没有非线性部分了。\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t gfp_mask)&#123;\t//原来数据包的头部剩余空间\tint headerlen = skb_headroom(skb);\t//线性长度加非线性长度\tunsigned int size = skb_end_offset(skb) + skb-&gt;data_len;\t//直接申请了线性长度加非线性长度的大小，注意：这里直接线性部分变成了非线性部分\tstruct sk_buff *n = __alloc_skb(size, gfp_mask,\t\t\t\t\tskb_alloc_rx_flag(skb), NUMA_NO_NODE);\tif (!n)\t\treturn NULL;\t/* Set the data pointer */\t//设置data指针\tskb_reserve(n, headerlen);\t/* Set the tail pointer and length */\t//设置tail指针\tskb_put(n, skb-&gt;len);\t//这里把非线性部分的数据也给拷贝了，偏\tBUG_ON(skb_copy_bits(skb, -headerlen, n-&gt;head, headerlen + skb-&gt;len));\t//拷贝skb的字段\tskb_copy_header(n, skb);\treturn n;&#125;\n\n上述代码在申请skb时直接申请了线性长度加非线性长度的大小，然后调用skb_copy_bits完成真正的copy\n//offset为从哪开始拷贝，to是拷贝到那里，len是拷贝多长int skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len)&#123;\tint start = skb_headlen(skb);//线性部分长度\tstruct sk_buff *frag_iter;\tint i, copy;\tif (offset &gt; (int)skb-&gt;len - len)\t\tgoto fault;\t/* Copy header. */\t//复制线性部分\tif ((copy = start - offset) &gt; 0) &#123;\t\tif (copy &gt; len)\t\t\tcopy = len;\t\t//memcpy一个头部长度\t\tskb_copy_from_linear_data_offset(skb, offset, to, copy);\t\tif ((len -= copy) == 0)\t\t\treturn 0;\t\toffset += copy;\t\tto     += copy;\t&#125;\tfor (i = 0; i &lt; skb_shinfo(skb)-&gt;nr_frags; i++) &#123;\t\tint end;\t\tskb_frag_t *f = &amp;skb_shinfo(skb)-&gt;frags[i];//每一个page的管理结构\t\tWARN_ON(start &gt; offset + len);\t\t//线性部分的长度加上frag的长度，表示当前结束的位置(数据包当前page+线性部分)\t\tend = start + skb_frag_size(f);\t\t//是否有要高倍的数据\t\tif ((copy = end - offset) &gt; 0) &#123;\t\t\tu32 p_off, p_len, copied;\t\t\tstruct page *p;\t\t\tu8 *vaddr;\t\t\tif (copy &gt; len)\t\t\t\tcopy = len;\t\t\t//拷贝每一个页的数据\t\t\tskb_frag_foreach_page(f,\t\t\t\t\t      skb_frag_off(f) + offset - start,\t\t\t\t\t      copy, p, p_off, p_len, copied) &#123;\t\t\t\tvaddr = kmap_atomic(p);//映射页到虚拟地址\t\t\t\t//memcpy\t\t\t\tmemcpy(to + copied, vaddr + p_off, p_len);\t\t\t\tkunmap_atomic(vaddr);\t\t\t&#125;\t\t\t//如果非线性部分都拷贝完了，也就是没有fraglist就直接return了\t\t\tif ((len -= copy) == 0)\t\t\t\treturn 0;\t\t\toffset += copy;//更新offset和同也就是更新从哪开始复制，复制到哪去\t\t\tto     += copy;\t\t&#125;\t\tstart = end;//变成下一个页对应数据包长度的起始地址。\t&#125;\t//有fraglist的情况\tskb_walk_frags(skb, frag_iter) &#123;\t\tint end;\t\tWARN_ON(start &gt; offset + len);\t\tend = start + frag_iter-&gt;len;\t\tif ((copy = end - offset) &gt; 0) &#123;\t\t\tif (copy &gt; len)\t\t\t\tcopy = len;\t\t\tif (skb_copy_bits(frag_iter, offset - start, to, copy))\t\t\t\tgoto fault;\t\t\tif ((len -= copy) == 0)\t\t\t\treturn 0;\t\t\toffset += copy;\t\t\tto     += copy;\t\t&#125;\t\tstart = end;\t&#125;\tif (!len)\t\treturn 0;fault:\treturn -EFAULT;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["skb"]},{"title":"struct skb && alloc_skb","url":"/2025/06/11/sockbuff%E7%BB%93%E6%9E%84/","content":"skb结构体及各个字段如下所示：\nstruct sk_buff &#123;\tunion &#123;\t\t//联合体第一部分\t\tstruct &#123;\t\t\t/* These two members must be first. */\t\t//头尾指针拉成链，例如tcp的接收发送队列\t\t\tstruct sk_buff\t\t*next;  \t\t\tstruct sk_buff\t\t*prev;\t\t\t\t\tunion &#123;\t\t\t\tstruct net_device\t*dev; //skb所属的netdev由驱动设置，比如指向bond设备\t\t\t\t/* Some protocols might use this space to store information,\t\t\t\t * while device pointer would be NULL.\t\t\t\t * UDP receive path is one user.\t\t\t\t */\t\t\t\tunsigned long\t\tdev_scratch; //udp会复用这部分空间，比如存校验和信息\t\t\t&#125;;\t\t&#125;;\t\tstruct rb_node\t\trbnode; //红黑数节点，tcp的重传队列，或者ip重组，或者tc会用到\t\tstruct list_head\tlist; //网卡收包没送到协议栈前会用到\t&#125;;\tunion &#123;\t\tstruct sock\t\t*sk; //所属的sock\t\tint\t\t\tip_defrag_offset; //保存分片报文的offset\t&#125;;\tunion &#123;\t\tktime_t\t\ttstamp; //接收发送时候会设置时间戳，例如dev-ximt 或者netif_receive_skb\t\tu64\t\tskb_mstamp_ns;/ //tcp的tx_delay会把值赋值到这个字段上pacing相关，tcp_repair也会用到 \t&#125;;\t/*\t * This is the control buffer. It is free to use for every\t * layer. Please put your private variables there. If you\t * want to keep them across layers you have to do a skb_clone()\t * first. This is owned by whoever has the skb queued ATM.\t */\tchar\t\t\tcb[48] __aligned(8); //不同协议层的私有字段，48B\tunion &#123;\t\tstruct &#123;\t\t\tunsigned long\t_skb_refdst; //查找路由后会 设置dst\t\t\tvoid\t\t(*destructor)(struct sk_buff *skb); //skb的析构函数\t\t&#125;;\t\tstruct list_head\ttcp_tsorted_anchor;  //repair模式使用会链在tpsock的一个队列中，清重传队列会移除\t&#125;;#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\tunsigned long\t\t _nfct;#endif\tunsigned int\t\tlen,   //数据包线性部分加非线性部分的总长度\t\t\t\tdata_len; //非线性部分的长度\t__u16\t\t\tmac_len, //mac头长度\t\t\t\thdr_len;  //头部长度，好像只有clone时候会用到\t/* Following fields are _not_ copied in __copy_skb_header()\t * Note that queue_mapping is here mostly to fill a hole.\t */\t__u16\t\t\tqueue_mapping; //对应的硬件队列，通常是hash算出来的/* if you move cloned around you also must adapt those constants */#ifdef __BIG_ENDIAN_BITFIELD#define CLONED_MASK\t(1 &lt;&lt; 7)#else#define CLONED_MASK\t1#endif#define CLONED_OFFSET()\t\toffsetof(struct sk_buff, __cloned_offset)\t/* private: */\t__u8\t\t\t__cloned_offset[0]; //实际没有，类似一个占位符\t/* public: */\t__u8\t\t\tcloned:1,   //是否被克隆过\t\t\t\t\tnohdr:1,\t//是否有头部空间，noheadroom为1，表示不能给头部添加长度了，tcp会置1\t\t\t\t\tfclone:2, \t//标识使用的那种克隆\t\t\t\tpeeked:1,   //udp的peek\t\t\t\thead_frag:1,  //网卡驱动申请page的时候就会设置这个字段，好像就是标识是从page上分配的\t\t\t\tpfmemalloc:1; //申请存放数据报道page如果pfmemalloc 池分配的页  就 为1 ，\t\t\t\t\t\t\t\t//表示存在内存压力，有这个标志后协议栈会有特殊的处理#ifdef CONFIG_SKB_EXTENSIONS\t__u8\t\t\tactive_extensions;#endif\t/* fields enclosed in headers_start/headers_end are copied\t * using a single memcpy() in __copy_skb_header()\t */\t/* private: */\t__u32\t\t\theaders_start[0];//占位符\t/* public: *//* if you move pkt_type around you also must adapt those constants */#ifdef __BIG_ENDIAN_BITFIELD#define PKT_TYPE_MAX\t(7 &lt;&lt; 5)#else#define PKT_TYPE_MAX\t7#endif#define PKT_TYPE_OFFSET()\toffsetof(struct sk_buff, __pkt_type_offset)\t/* private: */\t__u8\t\t\t__pkt_type_offset[0]; //占位符\t/* public: */\t__u8\t\t\tpkt_type:3;  //驱动设置的，本地广播还是other\t__u8\t\t\tignore_df:1;  //为1表示允许分片，用于隧道和桥接等处理\t__u8\t\t\tnf_trace:1; //netfilter相关，置1后netfllter可以trace\t__u8\t\t\tip_summed:2; //标识是否硬件卸载校验和\t__u8\t\t\tooo_okay:1; //多队列场景下，如果tcp队列中没有数据，可以将这位置为1，表示可以乱序发送\t\t\t\t\t\t\t\t//发送后，在pick_tx的时候就可能会换一个硬件队列，实现负载均衡\t__u8\t\t\tl4_hash:1;  //skb的hash字段是否是4元组的hash值\t__u8\t\t\tsw_hash:1; //skb的hash是否是硬件算的，会通过描述符带上来\t__u8\t\t\twifi_acked_valid:1;\t__u8\t\t\twifi_acked:1;\t__u8\t\t\tno_fcs:1;   //驱动使用，通知硬件是否需要计算校验和\t/* Indicates the inner headers are valid in the skbuff. */\t__u8\t\t\tencapsulation:1;  //标识是否是隧道报文，接收方向由驱动设置\t__u8\t\t\tencap_hdr_csum:1;  //隧道报文外层头部 1为需要计算校验和\t__u8\t\t\tcsum_valid:1;  //校验和是否有效#ifdef __BIG_ENDIAN_BITFIELD#define PKT_VLAN_PRESENT_BIT\t7#else#define PKT_VLAN_PRESENT_BIT\t0#endif#define PKT_VLAN_PRESENT_OFFSET()\toffsetof(struct sk_buff, __pkt_vlan_present_offset)\t/* private: */\t__u8\t\t\t__pkt_vlan_present_offset[0];\t/* public: */\t__u8\t\t\tvlan_present:1;  //是否存在vlan\t__u8\t\t\tcsum_complete_sw:1; //checksum是否是软件算的\t__u8\t\t\tcsum_level:2;\t__u8\t\t\tcsum_not_inet:1;  //非标准的校验和，比如sctp\t__u8\t\t\tdst_pending_confirm:1;  //标识路由是否是pending状态，tcptransmit 时候可能会设置#ifdef CONFIG_IPV6_NDISC_NODETYPE\t__u8\t\t\tndisc_nodetype:2;#endif\t__u8\t\t\tipvs_property:1;   //四层负载均衡相关\t__u8\t\t\tinner_protocol_type:1;  //标识隧道包报文的内层是以太网还是ip头\t__u8\t\t\tremcsum_offload:1; //远程校验和卸载，用于隧道报文处理#ifdef CONFIG_NET_SWITCHDEV\t__u8\t\t\toffload_fwd_mark:1;\t__u8\t\t\toffload_l3_fwd_mark:1;#endif#ifdef CONFIG_NET_CLS_ACT\t__u8\t\t\ttc_skip_classify:1;\t__u8\t\t\ttc_at_ingress:1;#endif#ifdef CONFIG_NET_REDIRECT\t__u8\t\t\tredirected:1;\t__u8\t\t\tfrom_ingress:1;#endif#ifdef CONFIG_TLS_DEVICE\t__u8\t\t\tdecrypted:1;#endif#ifdef CONFIG_NET_SCHED\t__u16\t\t\ttc_index;\t/* traffic control index */#endif\tunion &#123;\t\t__wsum\t\tcsum;   //各层的checksum\t\tstruct &#123;\t\t\t__u16\tcsum_start;  //发送端计算校验和开始的地方\t\t\t__u16\tcsum_offset; //发送端计算校验和结束的地方\t\t&#125;;\t&#125;;\t__u32\t\t\tpriority; //用户配置，用于选择网卡硬件队列\tint\t\t\tskb_iif;  //netfreciverskb中设置，输入网口的索引\t__u32\t\t\thash; //数据包hash值硬件可以通过描述符带上来\t__be16\t\t\tvlan_proto; //vlan的type  //网卡支持vlan卸载的话就会把vlan字段赋值\t__u16\t\t\tvlan_tci; //16位 vlan id和优先级#if defined(CONFIG_NET_RX_BUSY_POLL) || defined(CONFIG_XPS)  //绕过软中断 直接轮询收包\tunion &#123;\t\tunsigned int\tnapi_id;\t\tunsigned int\tsender_cpu;\t&#125;;#endif#ifdef CONFIG_NETWORK_SECMARK\t__u32\t\tsecmark;#endif\tunion &#123;\t\t__u32\t\tmark;        //用户配置的？ 很多地方用到 iptable tc ovs\t\t__u32\t\treserved_tailroom;  //申请数据包的时候end - tail -size 也就是保留的空间\t&#125;;\tunion &#123;\t\t__be16\t\tinner_protocol; //隧道报文内内存协议类型\t\t__u8\t\tinner_ipproto;  //隧道报文内层i \t&#125;;\t__u16\t\t\tinner_transport_header;  //隧道报文4层头的偏移，用来直接找4层头\t__u16\t\t\tinner_network_header; //隧道报文3层头偏移，找3层投\t__u16\t\t\tinner_mac_header;//找二层头\t__be16\t\t\tprotocol;\t__u16\t\t\ttransport_header;\t__u16\t\t\tnetwork_header;\t__u16\t\t\tmac_header; //同上\t/* private: */\t__u32\t\t\theaders_end[0];\t/* public: */\t/* These elements must be at the end, see alloc_skb() for details.  */\tsk_buff_data_t\t\ttail;//有效数据尾部\tsk_buff_data_t\t\tend;  //缓冲区结尾\tunsigned char\t\t*head, //缓冲区头\t\t\t\t*data;  //数据头\tunsigned int\t\ttruesize; //skb结构体大小加缓冲区大小\trefcount_t\t\tusers; //引用技术#ifdef CONFIG_SKB_EXTENSIONS\t/* only useable after checking -&gt;active_extensions != 0 */\tstruct skb_ext\t\t*extensions;#endif&#125;;\n\nskb的申请通常调用的是封装了__alloc_skb的函数，比如网卡驱动使用netdev_alloc_skb，napi_alloc_skb， 或者build_skb()+alloc_pages()数据包 tcp_sendmsg_locked中使用alloc_skb_fclone 申请数据包，其实最终都调用 到__alloc_skb只不过传递的参数不同\n这里直接介绍__alloc_skb的实现\n函数原型：\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask, int flags, int node)\n\n\n\n\n参数\n含义\n\n\n\nsize\n希望为 skb 分配的数据区大小\n\n\ngfp_mask\n内存分配标志，例如 GFP_KERNEL 或 GFP_ATOMIC\n\n\nflags\nskb 分配标志，比如是否是 RX 或是否要 FCLONE\n\n\nnode\nNUMA 节点编号，在哪个节点上分配内存\n\n\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,\t\t\t    int flags, int node)&#123;\tstruct kmem_cache *cache;\tstruct skb_shared_info *shinfo;\tstruct sk_buff *skb;\tu8 *data;\tbool pfmemalloc;\t//这个根据flag的类型，选择不同的cache来分配skb，tcp发送就设置了这个标志，\t//flcone是直接申请两个skb因为tcp发送路径经常需要clone\tcache = (flags &amp; SKB_ALLOC_FCLONE)\t\t? skbuff_fclone_cache : skbuff_head_cache;\t//是否是低内存情况下分配的skb\tif (sk_memalloc_socks() &amp;&amp; (flags &amp; SKB_ALLOC_RX))\t\tgfp_mask |= __GFP_MEMALLOC;\t/* Get the HEAD */\t//这里真正申请了一个skb       结构  \tskb = kmem_cache_alloc_node(cache, gfp_mask &amp; ~__GFP_DMA, node);\tif (!skb)\t\tgoto out;\t//预取\tprefetchw(skb);\t/* We do our best to align skb_shared_info on a separate cache\t * line. It usually works because kmalloc(X &gt; SMP_CACHE_BYTES) gives\t * aligned memory blocks, unless SLUB/SLAB debug is enabled.\t * Both skb-&gt;head and skb_shared_info are cache line aligned.\t */\t //对齐\tsize = SKB_DATA_ALIGN(size);\t//注意这里的size加上了管理非线性部分的结构体\tsize += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\t// 这里用kmalloc分配了数据包缓冲区\tdata = kmalloc_reserve(size, gfp_mask, node, &amp;pfmemalloc);\tif (!data)\t\tgoto nodata;\t/* kmalloc(size) might give us more room than requested.\t * Put skb_shared_info exactly at the end of allocated zone,\t * to allow max possible filling before reallocation.\t */\t //这里减去了管理非线性部分的大小\tsize = SKB_WITH_OVERHEAD(ksize(data));\t//预取\tprefetchw(data + size);\t/*\t * Only clear those fields we need to clear, not those that we will\t * actually initialise below. Hence, don&#x27;t put any more fields after\t * the tail pointer in struct sk_buff!\t */\t //这里memst了\tmemset(skb, 0, offsetof(struct sk_buff, tail));\t/* Account for allocated memory : skb + skb-&gt;head */\t//这里是skb结构体大小，加管理非线性部分结构体大小加申请数据缓冲区大小的三者之和\tskb-&gt;truesize = SKB_TRUESIZE(size);\tskb-&gt;pfmemalloc = pfmemalloc; //是否是内存压力的情况下分配的内存\trefcount_set(&amp;skb-&gt;users, 1); //skb引用计数\tskb-&gt;head = data; \tskb-&gt;data = data;\tskb_reset_tail_pointer(skb);\tskb-&gt;end = skb-&gt;tail + size; //end始终指向缓冲区结束\tskb-&gt;mac_header = (typeof(skb-&gt;mac_header))~0U;\tskb-&gt;transport_header = (typeof(skb-&gt;transport_header))~0U;\t/* make sure we initialize shinfo sequentially */\tshinfo = skb_shinfo(skb);\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\t//共享部分引用计数++\tatomic_set(&amp;shinfo-&gt;dataref, 1);\tif (flags &amp; SKB_ALLOC_FCLONE) &#123;\t\tstruct sk_buff_fclones *fclones;\t\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\t\tskb-&gt;fclone = SKB_FCLONE_ORIG; //标记是fastclone中的原始标记\t\trefcount_set(&amp;fclones-&gt;fclone_ref, 1);\t\tfclones-&gt;skb2.fclone = SKB_FCLONE_CLONE; //设置skb2的fclone标记\t&#125;out:\treturn skb;nodata:\tkmem_cache_free(cache, skb);\tskb = NULL;\tgoto out;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["skb"]},{"title":"套接字层socket、sock、文件系统之间的关系","url":"/2025/05/21/socket_sock_file_%E6%A6%82%E5%BF%B5/","content":"1.概念内核套接字层（socket layer）是 Linux 网络协议栈中承上启下的一层，负责将用户空间的 socket API（如 socket(), bind(), send(), recv() 等）与内核中的协议栈对接，其核心作用在于实现应用层与传输层协议之间的解耦，为应用程序提供一种统一且抽象的网络通信方式，套接字机制最初由 BSD UNIX 引入，现已广泛应用于各类网络编程环境中。\n套接字层（sockets）在整个网络协议栈中的位置如下图所示：\n\n2.关键数据结构套接字层使用的关键数据结构以及作用如下：\n2.1struct socket作用：是用户空间 socket 文件描述符在内核中的抽象\n核心字段如下所示：\nstruct socket &#123;    socket_state          state;     // 套接字状态    short                 type;      // SOCK_STREAM、SOCK_DGRAM 等    struct sock          *sk;        // 指向内核协议栈的 sock 结构    const struct proto_ops *ops;     // 指向协议操作函数表，如 inet_stream_ops    ...&#125;;\n\n\n\n2.2 struct sock作用：表示一个连接或一个套接字的协议控制块（protocol control block），协议相关逻辑都在这里实现\n核心字段（以 TCP 为例）：\nstruct sock &#123;    struct socket        *sk_socket;    // 回指到 struct socket    struct proto         *sk_prot;      // 协议操作（如 tcp_prot）    struct sk_buff_head   sk_receive_queue; // 接收队列    struct sk_buff_head   sk_write_queue;   // 发送队列    int                   sk_state;     // TCP 状态，如 ESTABLISHED 等    ...&#125;;\n\n2.3 、struct proto_ops作用：socket 操作函数表，对应 socket() 返回的文件描述符上的各种操作，如 send(), recv(), bind()\n核心字段：\nstruct proto_ops &#123;    int (*release)(struct socket *);    int (*bind)(struct socket *, struct sockaddr *, int);    int (*connect)(struct socket *, struct sockaddr *, int, int);    int (*sendmsg)(struct socket *, struct msghdr *, size_t);    int (*recvmsg)(struct socket *, struct msghdr *, size_t, int);    ...&#125;;\n\n2.4  struct proto作用：proto 是 面向传输层抽象设计的接口，把具体协议（TCP、UDP）与上层逻辑解耦，让上层只调用函数指针，而不用管协议细节\nstruct proto &#123;    struct sock *(*alloc)(struct net *, struct socket *, int, gfp_t);    void (*close)(struct sock *sk, long timeout);    int  (*connect)(struct sock *sk, struct sockaddr *uaddr, int addr_len);    int  (*sendmsg)(struct sock *sk, struct msghdr *msg, size_t len);    int  (*recvmsg)(struct sock *sk, struct msghdr *msg, size_t len, int noblock, int flags, int *addr_len);    ...&#125;;\n\n2.5struct file虽不专属于 socket 层，但与 socket 强相关。\n每个 socket 在内核中表现为一个文件，用户空间调用 socket() 后返回的文件描述符 fd 会指向一个 struct file，其 private_data 就是 &#96;struct socket\n2.6 整体关系图下图展示了上述结构体的关系图，其中task_struct对应一个进程，其files指向file_struct结构，该结构的主要功能是管理fd_arry 里面的每个fd对应一个打开的文件，其中的private指针指向的是I&#x2F;O对象的专有数据，对于socket层而言，就是socket结构，socket中的ops可以理解为用户态系统调用的实现。而sock的prot则是根据协议类型，进一步更为具体的实现。\n\n","categories":["网络协议栈源码学习"],"tags":["socket"]},{"title":"socket文件系统","url":"/2025/05/26/socket%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","content":"1.套接字与文件系统每一种文件都有各自的文件类型，例如设备文件包括字符设备文件和块设备文件等，而与套接字关联的文件类型为套接字文件。\n1.1套接字文件系统的注册为了能够让套接字与文件描述符相关联，并支持特殊套接字曾的节点分配和释放，系统中增加了sockfs文件系统类型sock_fs_type,具体定义如下：\nstatic struct file_system_type sock_fs_type = &#123;\t.name =\t\t&quot;sockfs&quot;, //文件系统名字\t//挂载的时候会被调用，在kern_mount()被调用\t.init_fs_context = sockfs_init_fs_context,//初始化文件系统的回调函数，\t.kill_sb =\tkill_anon_super,//卸载文件系统&#125;;\nsock_fs_type类型的文件系统注册发生在sock_init()在start_kernel中经过一些列调用会被调到.\n//start_kernel会调用到它static int __init sock_init(void)&#123;    ...\t//注册socket类型的文件系统\terr = register_filesystem(&amp;sock_fs_type);\tif (err)\t\tgoto out;    //挂载这个文件系统，返回一个超级块\tsock_mnt = kern_mount(&amp;sock_fs_type);\tif (IS_ERR(sock_mnt)) &#123;\t\terr = PTR_ERR(sock_mnt);\t\tgoto out_mount;\t&#125;    ....&#125;\n\n在上述代码中首先调用register_filesystem(&amp;sock_fs_type);来注册socket文件系统，主要工作判断是否有相同名字文件系统并插入到链表中。然后调用kern_mount完成挂载操作并返回一个超级块，在创建socket的时候创建inode就是用的这个超级块的alloc_node回调函数。kern_mount函数定义如下\nstruct vfsmount *kern_mount(struct file_system_type *type)&#123;\tstruct vfsmount *mnt;\t//实际的挂载操作\tmnt = vfs_kern_mount(type, SB_KERNMOUNT, type-&gt;name, NULL);\tif (!IS_ERR(mnt)) &#123;\t\t/*\t\t * it is a longterm mount, don&#x27;t release mnt until\t\t * we unmount before file sys is unregistered\t\t*/\t\treal_mount(mnt)-&gt;mnt_ns = MNT_NS_INTERNAL;\t&#125;\treturn mnt;&#125;\n\n上述代码调用vfs_kern_mount中调用了vfs_kern_mount完成实际的挂载，返回的为一个挂载点，其中SB_KERNMOUNT表示表示该挂载属于内核内部命名空间。\n在vfs_kern_mount中的工作就是创建一个fs_context结构体，把具体文件系统的回调函数与ctx-&gt;ops相关联，，这里的ctx是fs的一个私有结构。然后通过fc_mount中的get_tree把sb与ctx-&gt;ops关联了起来\nstruct vfsmount *vfs_kern_mount(struct file_system_type *type,\t\t\t\tint flags, const char *name,\t\t\t\tvoid *data)&#123;\tstruct fs_context *fc;\t//mnt为返回的挂载点\tstruct vfsmount *mnt;\tint ret = 0;\tif (!type)\t\treturn ERR_PTR(-EINVAL);\t\t//分配一个fs结构，并初始化，这里把具体文件系统的回调函数与ctx-&gt;ops相关联\t//fs为ctx的一个私有结构\tfc = fs_context_for_mount(type, flags);\tif (IS_ERR(fc))\t\treturn ERR_CAST(fc);\tif (name)\t\tret = vfs_parse_fs_string(fc, &quot;source&quot;,\t\t\t\t\t  name, strlen(name));\tif (!ret)\t\tret = parse_monolithic_mount_data(fc, data);\tif (!ret)\t//分配并初始化超级块和挂载点\t//注意这里面的get_tree把超级块与ctx-&gt;ops关联了起来\t\tmnt = fc_mount(fc);\telse\t\tmnt = ERR_PTR(ret);\tput_fs_context(fc);\treturn mnt;&#125;\nfs_context_for_mount是一个包裹函数，最终实际调用的是alloc_fs_context 核心逻辑就是创建一个fs结构体并初始化然后调用文件系统注册的init_fs_context函数，将fs的ctx与文件系统的ops关联上。 函数原型如下：\nstatic struct fs_context *alloc_fs_context(struct file_system_type *fs_type,\t\t\t\t      struct dentry *reference,\t\t\t\t      unsigned int sb_flags,\t\t\t\t      unsigned int sb_flags_mask,\t\t\t\t      enum fs_context_purpose purpose)&#123;\tint (*init_fs_context)(struct fs_context *);\tstruct fs_context *fc;\tint ret = -ENOMEM;\t//分配一个fc\tfc = kzalloc(sizeof(struct fs_context), GFP_KERNEL_ACCOUNT);\tif (!fc)\t\treturn ERR_PTR(-ENOMEM);\tfc-&gt;purpose\t= purpose;\tfc-&gt;sb_flags\t= sb_flags;\tfc-&gt;sb_flags_mask = sb_flags_mask;\tfc-&gt;fs_type\t= get_filesystem(fs_type);\tfc-&gt;cred\t= get_current_cred();\tfc-&gt;net_ns\t= get_net(current-&gt;nsproxy-&gt;net_ns);\tfc-&gt;log.prefix\t= fs_type-&gt;name;\tmutex_init(&amp;fc-&gt;uapi_mutex);\tswitch (purpose) &#123;\tcase FS_CONTEXT_FOR_MOUNT:\t\tfc-&gt;user_ns = get_user_ns(fc-&gt;cred-&gt;user_ns);\t\tbreak;\tcase FS_CONTEXT_FOR_SUBMOUNT:\t\tfc-&gt;user_ns = get_user_ns(reference-&gt;d_sb-&gt;s_user_ns);\t\tbreak;\tcase FS_CONTEXT_FOR_RECONFIGURE:\t\tatomic_inc(&amp;reference-&gt;d_sb-&gt;s_active);\t\tfc-&gt;user_ns = get_user_ns(reference-&gt;d_sb-&gt;s_user_ns);\t\tfc-&gt;root = dget(reference);\t\tbreak;\t&#125;\t/* TODO: Make all filesystems support this unconditionally */\tinit_fs_context = fc-&gt;fs_type-&gt;init_fs_context;\tif (!init_fs_context)\t\tinit_fs_context = legacy_init_fs_context;\t//调用文件系统的init函数，将fs与文件系统的ops关联上。\tret = init_fs_context(fc);\tif (ret &lt; 0)\t\tgoto err_fc;\tfc-&gt;need_free = true;\treturn fc;err_fc:\tput_fs_context(fc);\treturn ERR_PTR(ret);&#125;\nret = init_fs_context(fc); 为上述文件系统注册的回调函数\nstatic int sockfs_init_fs_context(struct fs_context *fc)&#123;\tstruct pseudo_fs_context *ctx = init_pseudo(fc, SOCKFS_MAGIC);\tif (!ctx)\t\treturn -ENOMEM;\tctx-&gt;ops = &amp;sockfs_ops;//alloc_inode ,free_inode\tctx-&gt;dops = &amp;sockfs_dentry_operations;\tctx-&gt;xattr = sockfs_xattr_handlers;\treturn 0;&#125;\n\n上述init_pseudo()中主要zu了两件事，申请了ctx结构作为fs的私有结构，然后注册了get_tree回调函数。这个get_tree ，回调里面的逻辑就是创建一个超级块，然后将ctx的ops赋值给超级块的ops，在fc_mount中会调用这个回调函数，函数定义如下所示：\nstatic const struct fs_context_operations pseudo_fs_context_ops = &#123;\t.free\t\t= pseudo_fs_free,\t.get_tree\t= pseudo_fs_get_tree,//创建了一个超级块最终会调用err = fill_super(sb, fc);填充超级块&#125;;struct pseudo_fs_context *init_pseudo(struct fs_context *fc,\t\t\t\t\tunsigned long magic)&#123;\tstruct pseudo_fs_context *ctx;\tctx = kzalloc(sizeof(struct pseudo_fs_context), GFP_KERNEL);\tif (likely(ctx)) &#123;\t\tctx-&gt;magic = magic;\t\tfc-&gt;fs_private = ctx;//设置私有结构\t\tfc-&gt;ops = &amp;pseudo_fs_context_ops; //设置ops集合，如上述代码所示\t\tfc-&gt;sb_flags |= SB_NOUSER;\t\tfc-&gt;global = true;\t&#125;\treturn ctx;&#125;\n\n上述fs_context_for_mount函数在vfs_kern_mount中被调用后会调用fc_mount来申请一个超级块，并初始化相关字段\nstruct vfsmount *fc_mount(struct fs_context *fc)&#123;\t//创建并初始化超级块\tint err = vfs_get_tree(fc);\tif (!err) &#123;\t\tup_write(&amp;fc-&gt;root-&gt;d_sb-&gt;s_umount);\t\t//创建挂载点\t\treturn vfs_create_mount(fc);\t&#125;\treturn ERR_PTR(err);&#125;\nvfs_get_tree中调用fc-&gt;ops-&gt;get_tree(fc)创建并初始化了超级块，代码如下所示：\nint vfs_get_tree(struct fs_context *fc)&#123;\tstruct super_block *sb;\tint error;\tif (fc-&gt;root)\t\treturn -EBUSY;\t//这里创建了超级块\terror = fc-&gt;ops-&gt;get_tree(fc);\tif (error &lt; 0)\t\treturn error;\tif (!fc-&gt;root) &#123;\t\tpr_err(&quot;Filesystem %s get_tree() didn&#x27;t set fc-&gt;root\\n&quot;,\t\t       fc-&gt;fs_type-&gt;name);\t\t/* We don&#x27;t know what the locking state of the superblock is -\t\t * if there is a superblock.\t\t */\t\tBUG();\t&#125;\t//将申请的sb赋值给sb\tsb = fc-&gt;root-&gt;d_sb;\tWARN_ON(!sb-&gt;s_bdi);\tsuper_wake(sb, SB_BORN);\terror = security_sb_set_mnt_opts(sb, fc-&gt;security, 0, NULL);\tif (unlikely(error)) &#123;\t\tfc_drop_locked(fc);\t\treturn error;\t&#125;\tWARN((sb-&gt;s_maxbytes &lt; 0), &quot;%s set sb-&gt;s_maxbytes to &quot;\t\t&quot;negative value (%lld)\\n&quot;, fc-&gt;fs_type-&gt;name, sb-&gt;s_maxbytes);\treturn 0;&#125;","categories":["网络协议栈源码学习"],"tags":["VFS","socket"]},{"title":"socket与文件描述符的映射","url":"/2025/05/27/socket%E4%B8%8E%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%9A%84%E6%98%A0%E5%B0%84/","content":"socket与文件描述符的映射1.socket与fd的映射应用层是通过文件描述符来找到内核的socket的一系列结构，因此在调用socket系统调用创建socket的过程中，会将一个fd与一个套接字相关联，对应的函数为__sys_socket函数中的sock_map_fd在sock_map_fd中，主要做了如下几个事情，获取一个空闲的文件描述符，创建一个file实例，将fd与file实例绑定，然后将这个file实例加入到进程打开的文件指针数组中，然后再将套接字与file相关连。这样fd，file，进程，socket四者之间就紧密的联系在了一起。代码如下：\nstatic int sock_map_fd(struct socket *sock, int flags)&#123;\tstruct file *newfile;\t//从当前进程获取一个未使用的文件描述符fd\tint fd = get_unused_fd_flags(flags);\tif (unlikely(fd &lt; 0)) &#123;\t\tsock_release(sock);\t\treturn fd;\t&#125;\t//创建一个文件对象\tnewfile = sock_alloc_file(sock, flags, NULL);\t//关联fd和文件对象\tif (!IS_ERR(newfile)) &#123;\t\tfd_install(fd, newfile);\t\treturn fd;\t&#125;\t//有错误，将fd标记为未使用\tput_unused_fd(fd);\treturn PTR_ERR(newfile);&#125;\n\n上述代码调用get_unused_fd_flags是一个包裹函数最终调用return alloc_fd(0, nofile, flags);获取一个未使用的文件描述符。\nstatic int alloc_fd(unsigned start, unsigned end, unsigned flags)&#123;\t//获取当前进程的文件描述副列表\tstruct files_struct *files = current-&gt;files;\tunsigned int fd;\tint error;\t//fd表，用来指向files_struct的fdtable\tstruct fdtable *fdt;\tspin_lock(&amp;files-&gt;file_lock);repeat:\t//这里指了一下\tfdt = files_fdtable(files);\tfd = start;\t//跳过已分配的fd\tif (fd &lt; files-&gt;next_fd)\t\tfd = files-&gt;next_fd;\tif (fd &lt; fdt-&gt;max_fds)\t//返回一个没有被使用的fd\t\tfd = find_next_fd(fdt, fd);\t/*\t * N.B. For clone tasks sharing a files structure, this test\t * will limit the total number of files that can be opened.\t */\terror = -EMFILE;\tif (fd &gt;= end)\t\tgoto out;\t//这里面会判断当前的fd是否大于max_fd,如果大于可能需要对​​文件描述符表的扩容\terror = expand_files(files, fd);\tif (error &lt; 0)\t\tgoto out;\t/*\t * If we needed to expand the fs array we\t * might have blocked - try again.\t */\t//可能会阻塞在试一次\tif (error)\t\tgoto repeat;\t//这里修改一下下一个要分配的fd，加速一下\tif (start &lt;= files-&gt;next_fd)\t\tfiles-&gt;next_fd = fd + 1;\t//设置已经使用的标志\t__set_open_fd(fd, fdt);\tif (flags &amp; O_CLOEXEC)\t\t__set_close_on_exec(fd, fdt);\telse\t\t__clear_close_on_exec(fd, fdt);\terror = fd;#if 1\t/* Sanity check */\tif (rcu_access_pointer(fdt-&gt;fd[fd]) != NULL) &#123;\t\tprintk(KERN_WARNING &quot;alloc_fd: slot %d not NULL!\\n&quot;, fd);\t\trcu_assign_pointer(fdt-&gt;fd[fd], NULL);\t&#125;#endifout:\tspin_unlock(&amp;files-&gt;file_lock);\treturn error;&#125;\n\n上述代码参数中的start为0,end为系统配置一个进程最多持有的描述数量，默认通常是1024，最大是多少？？如果文件描述副的fd大于了end则会报错打开了太多文件描述符。申请到fd之后在sock_map_fd中会调用sock_alloc_file创建文件对象，具体代码如下：\nstruct file *sock_alloc_file(struct socket *sock, int flags, const char *dname)&#123;\tstruct file *file;\tif (!dname)\t\tdname = sock-&gt;sk ? sock-&gt;sk-&gt;sk_prot_creator-&gt;name : &quot;&quot;;\t//alloc一个file绑定了socket_file_ops 回调函数集合 sock_mnt为一个挂在点\tfile = alloc_file_pseudo(SOCK_INODE(sock), sock_mnt, dname,\t\t\t\tO_RDWR | (flags &amp; O_NONBLOCK),\t\t\t\t&amp;socket_file_ops);\tif (IS_ERR(file)) &#123;\t\tsock_release(sock);\t\treturn file;\t&#125;\tfile-&gt;f_mode |= FMODE_NOWAIT;\tsock-&gt;file = file;\t//注意：这里关联了socket和file\tfile-&gt;private_data = sock;\t//标记为流式文件，不支持lseek(随机访问)？\tstream_open(SOCK_INODE(sock), file);\treturn file;&#125;\n\n上述sock_alloc_file中调用alloc_file_pseudo创建了一个file结构和dentry结构，并把sock对应的inode和denry相关联，然后在alloc_file_pseudo中将file的私有指针指向socket。这一系列逻辑完成了file，socket，inode，dentry，之间的关联和绑定了文件的ops，之后就可以通过file可以找到socket，通过dentry可以快速找到inode通过inode也可以找到socket。alloc_file_pseudo实现如下:\nstruct file *alloc_file_pseudo(struct inode *inode, struct vfsmount *mnt,\t\t\t\tconst char *name, int flags,\t\t\t\tconst struct file_operations *fops)&#123;\tstatic const struct dentry_operations anon_ops = &#123;\t\t.d_dname = simple_dname\t&#125;;\tstruct qstr this = QSTR_INIT(name, strlen(name));\tstruct path path;\tstruct file *file;\t//创建一个dentry\tpath.dentry = d_alloc_pseudo(mnt-&gt;mnt_sb, &amp;this);\tif (!path.dentry)\t\treturn ERR_PTR(-ENOMEM);\tif (!mnt-&gt;mnt_sb-&gt;s_d_op)\t\td_set_d_op(path.dentry, &amp;anon_ops);\t//给挂载点加一个引用计数\tpath.mnt = mntget(mnt);\t//关联denry和socket的inode 之后可以通过这个dentry快速找到inode\td_instantiate(path.dentry, inode);\t//申请一个file，同时挂上ops (也就是read，write)\tfile = alloc_file(&amp;path, flags, fops);\tif (IS_ERR(file)) &#123;\t\tihold(inode);\t\tpath_put(&amp;path);\t&#125;\treturn file;&#125;\n\n当申请了fd和file结构之后，会调用fd_install 完成文件描述fd与file的关联，此后用户就可以通过这个fd找到file，通过file找到socket，等等一系列信息。\nvoid fd_install(unsigned int fd, struct file *file)&#123;\tstruct files_struct *files = current-&gt;files;\tstruct fdtable *fdt;\trcu_read_lock_sched();\tif (unlikely(files-&gt;resize_in_progress)) &#123;\t\trcu_read_unlock_sched();\t\tspin_lock(&amp;files-&gt;file_lock);\t\tfdt = files_fdtable(files);\t\tBUG_ON(fdt-&gt;fd[fd] != NULL);\t\t//将fdt表的fd元素指向外面申请的file，这个file的私有结构就是socket！\t\trcu_assign_pointer(fdt-&gt;fd[fd], file);\t\tspin_unlock(&amp;files-&gt;file_lock);\t\treturn;\t&#125;\t/* coupled with smp_wmb() in expand_fdtable() */\tsmp_rmb();\tfdt = rcu_dereference_sched(files-&gt;fdt);\tBUG_ON(fdt-&gt;fd[fd] != NULL);\trcu_assign_pointer(fdt-&gt;fd[fd], file);\trcu_read_unlock_sched();&#125;\n\n2.根据文件描述符获取套接字当用户创建socket返回fd之后，执行bind，listen，send，recv都要传入上述提到的fd，内核会根据用户传入的fd找到对应的文件file，进而通过私有指针找到socket，对应的函数接口为sockfd_lookup_light\n//返回值为socketstatic struct socket *sockfd_lookup_light(int fd, int *err, int *fput_needed)&#123;\t\t//根据fd从进程管理的fdtable中找到对应的file，这里struct fd结构体中的一个字段为file\tstruct fd f = fdget(fd);\tstruct socket *sock;\t*err = -EBADF;\tif (f.file) &#123;\t\t//从file的私有指针中拿到socket\t\tsock = sock_from_file(f.file);\t\tif (likely(sock)) &#123;\t\t\t*fput_needed = f.flags &amp; FDPUT_FPUT;\t\t\treturn sock;\t\t&#125;\t\t*err = -ENOTSOCK;\t\tfdput(f);\t&#125;\treturn NULL;&#125;\n\n上述fdget最终调用到__fget_light，具体代码如下：\nstatic unsigned long __fget_light(unsigned int fd, fmode_t mask)&#123;\t//通过current宏获取文件描述符表\tstruct files_struct *files = current-&gt;files;\tstruct file *file;\t/*\t * If another thread is concurrently calling close_fd() followed\t * by put_files_struct(), we must not observe the old table\t * entry combined with the new refcount - otherwise we could\t * return a file that is concurrently being freed.\t *\t * atomic_read_acquire() pairs with atomic_dec_and_test() in\t * put_files_struct().\t */\t//判断当前进程的文件描述符表是否被其他进程共享，如果没有被共享，直接无锁查找\tif (atomic_read_acquire(&amp;files-&gt;count) == 1) &#123;\t\t//逻辑很简单，直接根据fd取对应的files\t\tfile = files_lookup_fd_raw(files, fd);\t\tif (!file || unlikely(file-&gt;f_mode &amp; mask))\t\t\treturn 0;\t\treturn (unsigned long)file;\t&#125; else &#123;\t\t//加rcu锁查找，多线程可能会走这里吧？？？\t\tfile = __fget(fd, mask);\t\tif (!file)\t\t\treturn 0;\t\treturn FDPUT_FPUT | (unsigned long)file;\t&#125;&#125;\n\n上述查找file的过程可以分为快速路径和慢速路径，当文件描述符表存在被多个线程访问时，需要加锁访问，否则直接走快速路径无锁拿到file(atomic_read_acquire(&amp;files-&gt;count))\n","categories":["网络协议栈源码学习"],"tags":["socket"]},{"title":"socket系统调用","url":"/2025/05/13/socket%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8/","content":"简介socket 是网络编程中最基本的系统调用之一，用于创建一个网络通信的“端点”（即套接字，socket）。\n一、函数原型int socket(int domain, int type, int protocol);\n\n\n\n\n参数\n说明\n\n\n\ndomain\n协议族，比如：AF_INET 表示 IPv4\n\n\ntype\n套接字类型，比如：SOCK_STREAM 表示 TCP\n\n\nprotocol\n指定使用的协议，通常填 0 让系统自动选择\n\n\n二、函数调用流程用户态程序调用 socket()        ↓glibc 中封装的socket接口        ↓内部通过 syscall 指令        ↓进入内核，执行 sys_socket()\n三、代码分析1. glibc层代码分析代码位置：glibc-2.40\\sysdeps\\unix\\sysv\\linux\\socket.c\nint __socket (int fd, int type, int domain)&#123;#ifdef __ASSUME_SOCKET_SYSCALL //是否支持单独的系统调用号，肯定支持  return INLINE_SYSCALL_CALL (socket, fd, type, domain);#else\t//32位的处理器会走这个分支  return SOCKETCALL (socket, fd, type, domain);#endif&#125;libc_hidden_def (__socket)//暴露给外部程序的符号为socket() 其实是__socket()的别名weak_alias (__socket, socket)\n\n用户应用程序调用socket()实际调用的的是上述glibc中的__socket，在__socket中 INLINE_SYSCALL_CALL宏经过一系列展开后变成宏__INLINE_SYSCALL4 ，这个宏会进一步再展开，如下所示：\n#define __INLINE_SYSCALL4(name, a1, a2, a3, a4) \\  INLINE_SYSCALL (name, 4, a1, a2, a3, a4)\n\n上述的INLINE_SYSCALL 展开，结果如下所示：\n#define INLINE_SYSCALL(name, nr, args...)\t\t\t\t\\  (&#123;\t//sc_ret为系统调用的返回值    long int sc_ret = INTERNAL_SYSCALL (name, nr, args);\t//对返回值好像要简单检查一下    __glibc_unlikely (INTERNAL_SYSCALL_ERROR_P (sc_ret))\t\t\\    ? SYSCALL_ERROR_LABEL (INTERNAL_SYSCALL_ERRNO (sc_ret))\t\t\\    : sc_ret;\t\t\t\t\t\t\t\t\\  &#125;)\n\nINTERNAL_SYSCALL 进一步在展开，会变成了internal_syscall3\n这里举个例子，例如，未展开前为INTERNAL_SYSCALL(socket, 3, AF_INET, SOCK_STREAM, 0) 展开后会变成\ninternal_syscall3(__NR_socket, AF_INET, SOCK_STREAM, 0)，如果在x86架构中宏__NR_socket为41\n在glibc-2.40\\sysdeps\\unix\\sysv\\linux\\x86_64\\64\\arch-syscall.h 中有定义\n#define __NR_socket 41\n\n总之，上述宏经过一系列展开后变成了如下函数，其中number为系统调用号，arg1, arg2, arg3 为传入的参数\n#define internal_syscall3(number, arg1, arg2, arg3)\t\t\t\\(&#123;\t\t\t\t\t\t\t\t\t\\    unsigned long int resultvar;\t\t\t\t\t\\    TYPEFY (arg3, __arg3) = ARGIFY (arg3);\t\t\t \t\\    TYPEFY (arg2, __arg2) = ARGIFY (arg2);\t\t\t \t\\    TYPEFY (arg1, __arg1) = ARGIFY (arg1);\t\t\t \t\\    register TYPEFY (arg3, _a3) asm (&quot;rdx&quot;) = __arg3;\t\t\t\\    register TYPEFY (arg2, _a2) asm (&quot;rsi&quot;) = __arg2;\t\t\t\\    register TYPEFY (arg1, _a1) asm (&quot;rdi&quot;) = __arg1;\t\t\t\\    asm volatile (\t\t\t\t\t\t\t\\    &quot;syscall\\n\\t&quot;\t\t\t\t\t\t\t\\    : &quot;=a&quot; (resultvar)\t\t\t\t\t\t\t\\    : &quot;0&quot; (number), &quot;r&quot; (_a1), &quot;r&quot; (_a2), &quot;r&quot; (_a3)\t\t\t\\    : &quot;memory&quot;, REGISTERS_CLOBBERED_BY_SYSCALL);\t\t\t\\    (long int) resultvar;\t\t\t\t\t\t\\&#125;)\n\n上述代码的作用是将系统调用号和 3 个参数分别放入规定的寄存器（RAX, RDI, RSI, RDX），然后执行 syscall 指令，并将返回值保存到 resultvar：\n“memory”, REGISTERS_CLOBBERED_BY_SYSCALL 的意思是告诉编译器不要优化这段代码\n下面附上x86-64 Linux 的 syscall 调用约定传参的寄存器\n\n\n\n参数\n寄存器\n\n\n\nsyscall 编号\nRAX\n\n\n参数1\nRDI\n\n\n参数2\nRSI\n\n\n参数3\nRDX\n\n\n参数4\nR10\n\n\n参数5\nR8\n\n\n参数6\nR9\n\n\n2.内核代码分析执行上述syscall指令后CPU 会根据 MSR 寄存器（Model Specific Registers）跳转到系统调用函数，例如，x86架构则会进入到系统调用的统一入口函数entry_SYSCALL_64\nentry_SYSCALL_64函数的注册（也就把地址写入MSR寄存器）在syscall_init中完成代码如下：\nvoid syscall_init(void)&#123;\twrmsr(MSR_STAR, 0, (__USER32_CS &lt;&lt; 16) | __KERNEL_CS);\t//把entry_SYSCALL_64 的地址写入MSR_LSTAR寄存器\twrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64); \t...&#125;\n\n上述syscall_init 函数在start_kernel（）中被调用\n接下里看一下entry_SYSCALL_64的逻辑，\nSYM_CODE_START(entry_SYSCALL_64)\tUNWIND_HINT_EMPTY\t//这个是切换GS寄存器，这个寄存器是访问per-cpu变量的基址task_struct结构就依赖这个寄存器间接获得\tswapgs\t/* tss.sp2 is scratch space. */\t//将当前栈指针保存\tmovq\t%rsp, PER_CPU_VAR(cpu_tss_rw + TSS_sp2)\t//切换页表\tSWITCH_TO_KERNEL_CR3 scratch_reg=%rsp\t//切换当前栈指针到当前 CPU 的内核栈顶指针\tmovq\tPER_CPU_VAR(cpu_current_top_of_stack), %rspSYM_INNER_LABEL(entry_SYSCALL_64_safe_stack, SYM_L_GLOBAL)\t/* Construct struct pt_regs on stack */\t//把用户态的一些信息入栈（貌似就是pt_regs）\tpushq\t$__USER_DS\t\t\t\t/* pt_regs-&gt;ss */\tpushq\tPER_CPU_VAR(cpu_tss_rw + TSS_sp2)\t/* pt_regs-&gt;sp */\tpushq\t%r11\t\t\t\t\t/* pt_regs-&gt;flags */\tpushq\t$__USER_CS\t\t\t\t/* pt_regs-&gt;cs */\tpushq\t%rcx\t\t\t\t\t/* pt_regs-&gt;ip */SYM_INNER_LABEL(entry_SYSCALL_64_after_hwframe, SYM_L_GLOBAL)\t//这里存了系统调用号\tpushq\t%rax\t\t\t\t\t/* pt_regs-&gt;orig_ax */\t//其他寄存器的信息继续保存到栈中\tPUSH_AND_CLEAR_REGS rax=$-ENOSYS\t/* IRQs are off. */\t//把系统调用号放入 rdi → 作为函数的第 1 个参数\tmovq\t%rax, %rdi\t//把 pt_regs 地址放入 rsi → 作为第 2 个参数\tmovq\t%rsp, %rsi\t//调用do_syscall_64\tcall\tdo_syscall_64\t\t/* returns with IRQs disabled */\t...\tSYM_CODE_END(entry_SYSCALL_64)\n\n在用户执行 syscall 指令后，CPU 跳转到entry_SYSCALL_64，完成从用户态到内核态的切换、栈构造、参数准备，并调用 C 函数 do_syscall_64() 来处理系统调用。\n接下来看一下上述汇编代码中调用do_syscall_64 代码位于arch\\x86\\entry\\common.c中\n#ifdef CONFIG_X86_64__visible noinstr void do_syscall_64(unsigned long nr, struct pt_regs *regs)&#123;\t//这里开启了中断（进入系统调用前要先关中断在哪里没找到，好像sycall后会自动关）追踪，安全相关\tnr = syscall_enter_from_user_mode(regs, nr);\t//禁止插桩，\tinstrumentation_begin();\t//检查系统调用号是否在合法范围\tif (likely(nr &lt; NR_syscalls)) &#123;\t\tnr = array_index_nospec(nr, NR_syscalls);\t\t//nr存的是系统调用好，从系统调用表中找到对应的处理函数，然后将\t\t//返回值存入到regs-&gt;ax寄存器\t\tregs-&gt;ax = sys_call_table[nr](regs);        //32位#ifdef CONFIG_X86_X32_ABI\t&#125; else if (likely((nr &amp; __X32_SYSCALL_BIT) &amp;&amp;\t\t\t  (nr &amp; ~__X32_SYSCALL_BIT) &lt; X32_NR_syscalls)) &#123;\t\tnr = array_index_nospec(nr &amp; ~__X32_SYSCALL_BIT,\t\t\t\t\tX32_NR_syscalls);\t\tregs-&gt;ax = x32_sys_call_table[nr](regs);#endif\t&#125;\tinstrumentation_end();\tsyscall_exit_to_user_mode(regs);&#125;#endif\n\n上述代码中最关键的就是regs-&gt;ax = sys_call_table[nr](regs)找到对应的系统调用函数，并传入pt_regs，pt_regs保存了参数和用户的一些信息。\nsys_call_table[]就是一个函数指针，每个元素指向具体的函数实现，定义如下所示:\nasmlinkage const sys_call_ptr_t sys_call_table[__NR_syscall_max+1] = &#123;\t/*\t * Smells like a compiler bug -- it doesn&#x27;t work\t * when the &amp; below is removed.\t */\t[0 ... __NR_syscall_max] = &amp;__x64_sys_ni_syscall,//下面这个头文件貌似是编译生成的，include进来后就替换了上面的默认值#include &lt;asm/syscalls_64.h&gt;&#125;;\n\n上述 &lt;asm/syscalls_64.h&gt;是在编译过程中生成的，具体的流程如下：\nsyscall_64.tbl   ↓（作为输入）syscalltbl.sh 脚本   ↓（生成）syscalls_64.h   ↓（#include）用于填充 sys_call_table[]\n上述流程在\\arch\\x86\\entry\\syscalls中有体现。下面展示了部分syscall_64.tbl中的内容\n0\tcommon\tread\t\t\tsys_read1\tcommon\twrite\t\t\tsys_write2\tcommon\topen\t\t\tsys_open3\tcommon\tclose\t\t\tsys_close·········41\tcommon\tsocket\t\t\tsys_socket·········440\tcommon\tprocess_madvise\t\tsys_process_madvise\n\n系统调用号 41 对应的函数指针在 sys_call_table[]被syscalltbl.sh 处理后，最终指向 __x64_sys_socket 这个函数，而这个函数和SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol) 宏展开后是一个函数！\n","categories":["网络协议栈源码学习"],"tags":["socket"]},{"title":"sock的创建与初始化","url":"/2025/05/24/sock%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E5%88%9D%E5%A7%8B%E5%8C%96/","content":"以IPv4协议族为例，当用户态执行socket系统调用后，会调用到inet_create(),在inet_create()中会创建与socket关联的sock结构体，具体代码如下：\nstatic int inet_create(struct net *net, struct socket *sock, int protocol,\t\t       int kern)&#123;    .....\t//注意： 这里申请一个sock结构，这个sock结构可以理解为传输层协议和socket之间的一个中间层\t//对上提供socket层的结构，\t//对下与具体的协议相关\t//kern 标识这个套接字是否是内核创建的\tsk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot, kern);\tif (!sk)\t\tgoto out;    .....\t//这里初始化了上面申请的sock结构体的各个字段\tsock_init_data(sock, sk);    .....&#125;\n\nsk_alloc()上述sk_alloc()其实就是使用slab分配其分配了prot-&gt;size大小的的内存，也就是说分配了一个比sock结构体size还要大的内存，举个例子，如果是TCP协议，则分配的大小为sizeof(tcp_sock),也就是说tcp_sock内嵌了sock结构体，类似继承的关系，关系如下图所示：\n\n接下来具体看一下sk_alloc()的实现：\nstruct sock *sk_alloc(struct net *net, int family, gfp_t priority,\t\t      struct proto *prot, int kern)&#123;\tstruct sock *sk;\t//调用slab 分配一个sk结构体，注意这个结构体的大小取决与prot参数的size字段\t//__GFP_ZERO表示为内存申请后需要清零的标志位\tsk = sk_prot_alloc(prot, priority | __GFP_ZERO, family);\tif (sk) &#123;\t\t//设置协议族\t\tsk-&gt;sk_family = family;\t\t/*\t\t * See comment in struct sock definition to understand\t\t * why we need sk_prot_creator -acme\t\t */\t\t//这里很关键，将具体协议的prot关联到了sock上\t\tsk-&gt;sk_prot = sk-&gt;sk_prot_creator = prot;\t\t//记录是否是内核创建的\t\tsk-&gt;sk_kern_sock = kern;\t\tsock_lock_init(sk);\t\t//如果是用户进程创建的，就增加网络命名空间的引用计数\t\tsk-&gt;sk_net_refcnt = kern ? 0 : 1;\t\t//更新当前core上所有活跃套接字的引用计数。\t\tif (likely(sk-&gt;sk_net_refcnt)) &#123;\t\t\tget_net_track(net, &amp;sk-&gt;ns_tracker, priority);\t\t\tsock_inuse_add(net, 1);\t\t&#125; else &#123;\t\t\t__netns_tracker_alloc(net, &amp;sk-&gt;ns_tracker,\t\t\t\t\t      false, priority);\t\t&#125;\t\t//将sock与网络命名空间关联\t\tsock_net_set(sk, net);\t\t//发送缓冲区引用计数加1\t\trefcount_set(&amp;sk-&gt;sk_wmem_alloc, 1);\t\tmem_cgroup_sk_alloc(sk);\t\tcgroup_sk_alloc(&amp;sk-&gt;sk_cgrp_data);\t\tsock_update_classid(&amp;sk-&gt;sk_cgrp_data);\t\tsock_update_netprioidx(&amp;sk-&gt;sk_cgrp_data);\t\tsk_tx_queue_clear(sk);\t&#125;\treturn sk;&#125;\n\n上述代码中调用sk_prot_alloc() 根据不同的prot(也就是不同的协议)申请sock，同时设置了__GFP_ZERO 标志，表示需要将申请的内存memset，sk_prot_alloc() 的具体实现如下：\nstatic struct sock *sk_prot_alloc(struct proto *prot, gfp_t priority,\t\tint family)&#123;\tstruct sock *sk;\tstruct kmem_cache *slab;\t//从slab分配器中拿一个结构体，这个slabchace是inet_init中初始化的\tslab = prot-&gt;slab;\tif (slab != NULL) &#123;\t\tsk = kmem_cache_alloc(slab, priority &amp; ~__GFP_ZERO);\t\tif (!sk)\t\t\treturn sk;\t\t//是否需要memset，可以看到这里的大小是objsize\t\tif (want_init_on_alloc(priority))\t\t\tsk_prot_clear_nulls(sk, prot-&gt;obj_size);\t&#125; else\t//如果没使用slab就用kmalloc，kamlloc不也是slab吗？\t\tsk = kmalloc(prot-&gt;obj_size, priority);\tif (sk != NULL) &#123;\t\t//安全相关\t\tif (security_sk_alloc(sk, family, priority))\t\t\tgoto out_free;\t\t//增加引用计数\t\tif (!try_module_get(prot-&gt;owner))\t\t\tgoto out_free_sec;\t&#125;\treturn sk;out_free_sec:\tsecurity_sk_free(sk);out_free:\tif (slab != NULL)\t\tkmem_cache_free(slab, sk);\telse\t\tkfree(sk);\treturn NULL;&#125;\n\n从上述代码可知是通过slab = prot-&gt;slab;获取了一个slab对象，这个slab管理的结构体大小是inet_init()中调用proto_register()创建slab时候确定的，具体函数如下所示：\nint proto_register(struct proto *prot, int alloc_slab)&#123;\t......\tif (alloc_slab) &#123;\t\tprot-&gt;slab = kmem_cache_create_usercopy(prot-&gt;name,\t\t\t\t\tprot-&gt;obj_size, 0,\t\t\t\t\tSLAB_HWCACHE_ALIGN | SLAB_ACCOUNT |\t\t\t\t\tprot-&gt;slab_flags,\t\t\t\t\tprot-&gt;useroffset, prot-&gt;usersize,\t\t\t\t\tNULL);\t&#125;\t.......&#125;\n\n可以看到在上面创建slab缓存时，指定的size大小为prot-&gt;obj_size，例如tcp协议的obj_size则为sizeof(tcp_sock)结构体的大小。注意到上述创建slab缓存使用的接口是kmem_cache_create_usercopy，这个usercopy的意义是允许指定对象拷贝到用户空间的内存区域。prot-&gt;useroffset指的是对象中允许拷贝到用户空间的数据区域的偏移。prot-&gt;usersize指的是允许拷贝到用户空间的数据区域的大小。（tcp等协议这个字段好像都是空）\nsock_init_data()上述创建了sock结构体后紧接着就会调用sock_init_data完成初始化，sock_init_data中先从socket结构体中关联的inode中获取uid然后调用sock_init_data_uid()完成初始化，上述两个函数的代码如下：\nvoid sock_init_data(struct socket *sock, struct sock *sk)&#123;\t//注意：这里的uid是在创建socket和inode的时候设置的\t//i_uid用于表示与该 inode 关联的文件或对象的所有者用户ID\tkuid_t uid = sock ?\t\tSOCK_INODE(sock)-&gt;i_uid :\t\tmake_kuid(sock_net(sk)-&gt;user_ns, 0);\tsock_init_data_uid(sock, sk, uid);&#125;\n\nvoid sock_init_data_uid(struct socket *sock, struct sock *sk, kuid_t uid)&#123;\tsk_init_common(sk);\tsk-&gt;sk_send_head\t=\tNULL;\ttimer_setup(&amp;sk-&gt;sk_timer, NULL, 0);\tsk-&gt;sk_allocation\t=\tGFP_KERNEL;\t//设置接收和发送默认缓冲区大小\tsk-&gt;sk_rcvbuf\t\t=\tREAD_ONCE(sysctl_rmem_default);\tsk-&gt;sk_sndbuf\t\t=\tREAD_ONCE(sysctl_wmem_default);\t//即使是udp也设置状态为TCP_CLOSE\tsk-&gt;sk_state\t\t=\tTCP_CLOSE;\t//好像跟内存分配相关\tsk-&gt;sk_use_task_frag\t=\ttrue;\t//这里关联了sock与socket\tsk_set_socket(sk, sock);\tsock_set_flag(sk, SOCK_ZAPPED);\tif (sock) &#123;\t\t//设置用户配置的type类型给sock\t\tsk-&gt;sk_type\t=\tsock-&gt;type;\t\t//初始化套接字的等待队列\t\tRCU_INIT_POINTER(sk-&gt;sk_wq, &amp;sock-&gt;wq);\t\tsock-&gt;sk\t=\tsk;\t&#125; else &#123;\t\tRCU_INIT_POINTER(sk-&gt;sk_wq, NULL);\t&#125;\t//设置sock的uid\tsk-&gt;sk_uid\t=\tuid;\t\t//锁相关，没看太懂\trwlock_init(&amp;sk-&gt;sk_callback_lock);\tif (sk-&gt;sk_kern_sock)\t\tlockdep_set_class_and_name(\t\t\t&amp;sk-&gt;sk_callback_lock,\t\t\taf_kern_callback_keys + sk-&gt;sk_family,\t\t\taf_family_kern_clock_key_strings[sk-&gt;sk_family]);\telse\t\tlockdep_set_class_and_name(\t\t\t&amp;sk-&gt;sk_callback_lock,\t\t\taf_callback_keys + sk-&gt;sk_family,\t\t\taf_family_clock_key_strings[sk-&gt;sk_family]);\tsk-&gt;sk_state_change\t=\tsock_def_wakeup; //唤醒睡眠的进程，比如tcp状态发生改变的时候调用\tsk-&gt;sk_data_ready\t=\tsock_def_readable; //软中断收到数据包，唤醒睡眠的进程\tsk-&gt;sk_write_space\t=\tsock_def_write_space;//有写的空间，唤醒,好像几乎不会被调用\tsk-&gt;sk_error_report\t=\tsock_def_error_report;\tsk-&gt;sk_destruct\t\t=\tsock_def_destruct; //销毁套接字的回调\tsk-&gt;sk_frag.page\t=\tNULL;\tsk-&gt;sk_frag.offset\t=\t0;\tsk-&gt;sk_peek_off\t\t=\t-1;  //peek的偏移量\tsk-&gt;sk_peer_pid \t=\tNULL; //对端的进程id，同一个主机上才有吧？\tsk-&gt;sk_peer_cred\t=\tNULL; //也是对端的信息\tspin_lock_init(&amp;sk-&gt;sk_peer_lock);\tsk-&gt;sk_write_pending\t=\t0;  //写缓存区没有空间了\tsk-&gt;sk_rcvlowat\t\t=\t1;  //唤醒相关的水位线？1表示一个字节也唤醒\tsk-&gt;sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT; //设置接收的超时时间 全F\tsk-&gt;sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT; ////设置发送的超时时间 全F\tsk-&gt;sk_stamp = SK_DEFAULT_STAMP;#if BITS_PER_LONG==32\tseqlock_init(&amp;sk-&gt;sk_stamp_seq);#endif\tatomic_set(&amp;sk-&gt;sk_zckey, 0);#ifdef CONFIG_NET_RX_BUSY_POLL\tsk-&gt;sk_napi_id\t\t=\t0;\tsk-&gt;sk_ll_usec\t\t=\tREAD_ONCE(sysctl_net_busy_read);#endif\tsk-&gt;sk_max_pacing_rate = ~0UL;  //发送速率相关，tcp拥塞控制的时候会用到bbr算法会用到\tsk-&gt;sk_pacing_rate = ~0UL;\tWRITE_ONCE(sk-&gt;sk_pacing_shift, 10);\tsk-&gt;sk_incoming_cpu = -1;   //记录属于哪个cpu\tsk_rx_queue_clear(sk);\t/*\t * Before updating sk_refcnt, we must commit prior changes to memory\t * (Documentation/RCU/rculist_nulls.rst for details)\t */\tsmp_wmb();\trefcount_set(&amp;sk-&gt;sk_refcnt, 1);\tatomic_set(&amp;sk-&gt;sk_drops, 0);&#125;EXPORT_SYMBOL(sock_init_data_uid);\n\n上述代码主要做了如下几个个事情，调用sk_init_common 初始化sock的接受队列和发送队列，这个错误队列好像是ip层收到icmp的错误报文，会放到这个错误队列中。\nstatic void sk_init_common(struct sock *sk)&#123;\t//初始化接收，发送和错误队列。\tskb_queue_head_init(&amp;sk-&gt;sk_receive_queue);\tskb_queue_head_init(&amp;sk-&gt;sk_write_queue);\tskb_queue_head_init(&amp;sk-&gt;sk_error_queue);\trwlock_init(&amp;sk-&gt;sk_callback_lock);\t//锁相关没太懂\tlockdep_set_class_and_name(&amp;sk-&gt;sk_receive_queue.lock,\t\t\taf_rlock_keys + sk-&gt;sk_family,\t\t\taf_family_rlock_key_strings[sk-&gt;sk_family]);\tlockdep_set_class_and_name(&amp;sk-&gt;sk_write_queue.lock,\t\t\taf_wlock_keys + sk-&gt;sk_family,\t\t\taf_family_wlock_key_strings[sk-&gt;sk_family]);\tlockdep_set_class_and_name(&amp;sk-&gt;sk_error_queue.lock,\t\t\taf_elock_keys + sk-&gt;sk_family,\t\t\taf_family_elock_key_strings[sk-&gt;sk_family]);\tlockdep_set_class_and_name(&amp;sk-&gt;sk_callback_lock,\t\t\taf_callback_keys + sk-&gt;sk_family,\t\t\taf_family_clock_key_strings[sk-&gt;sk_family]);&#125;\n\n然后在sock_init_data_uid()中初始化了接收缓冲区和发送缓冲区的大小，将socket的sock字段指向当前的sock结构，然后注册唤醒进程睡眠的相关函数，例如收包的唤醒函数，有空间可写的回调函数，tcp状态发生改变的回调函数。\n","categories":["网络协议栈源码学习"],"tags":["sock"]},{"title":"struct sock","url":"/2025/06/22/sock%E7%BB%93%E6%9E%84%E4%BD%93/","content":"sock 结构体在 Linux 网络协议栈中的作用非常核心，它本质上是用于管理传输层连接状态和控制信息的内核数据结构\nsock 结构体的主要功能：1.  管理传输层连接状态\n记录 socket 状态，如 TCP 的 ESTABLISHED、LISTEN、SYN_SENT 等；\n维护连接生命周期。\n\n2. 数据收发管理\n接收队列：sk_receive_queue\n发送队列：sk_write_queue\nBacklog 队列：sk_backlog\n\n3. 缓冲区和流控\n接收缓冲区大小（sk_rcvbuf）\n发送缓冲区大小（sk_sndbuf）\n控制数据收发速率，避免内存耗尽。\n\n4. 定时器与超时管理\nsk_timer：重传、超时的定时器；\n如 TCP 的重传定时器（RTO）、保活定时器（keepalive）。\n\n5. 协议特定操作**\n每个协议（TCP&#x2F;UDP）定义自己的操作集，绑定到 sock 的 sk_prot 字段；\n例如 TCP 操作集合为 tcp_prot，UDP 为 udp_prot；\n提供 sendmsg()、recvmsg()、close() 等接口。\n\n6. 连接路由与缓存\n加速报文发送时的路由查询。\n\n7. 等待队列管理\nsk_wq：实现 select()、poll()、epoll() 等机制；\n用户空间程序等待 socket 可读&#x2F;可写事件时使用。\n\n具体字段如下所示：\nstruct sock &#123;\t/*\t * Now struct inet_timewait_sock also uses sock_common, so please just\t * don&#x27;t add nothing before this first member (__sk_common) --acme\t */\tstruct sock_common\t__sk_common;#define sk_node\t\t\t__sk_common.skc_node    //管理hash表的节点，比如udp绑定端口后会把它加入udp的hash表1#define sk_nulls_node\t\t__sk_common.skc_nulls_node //管理tcp hash表的节点#define sk_refcnt\t\t__sk_common.skc_refcnt\t\t\t\t\t//引用技术#define sk_tx_queue_mapping\t__sk_common.skc_tx_queue_mapping //选择网卡队列的时候会用到#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING#define sk_rx_queue_mapping\t__sk_common.skc_rx_queue_mapping  //选择网卡队列的时候会用到#endif#define sk_dontcopy_begin\t__sk_common.skc_dontcopy_begin  //不可以复制的起始地址#define sk_dontcopy_end\t\t__sk_common.skc_dontcopy_end \t//不可以复制的结束地址，比如tcp建联的时候会用到sock_copy#define sk_hash\t\t\t__sk_common.skc_hash  //五元组hash值，比如说插入ehash时候会用到#define sk_portpair\t\t__sk_common.skc_portpair//下面两个字段的联合体#define sk_num\t\t\t__sk_common.skc_num    //本地port，比如说绑定端口的时候会设置#define sk_dport\t\t__sk_common.skc_dport  //目的port，比如建立连接的时候会设置#define sk_addrpair\t\t__sk_common.skc_addrpair //用不到这个字段，用于联合体优化布局#define sk_daddr\t\t__sk_common.skc_daddr  //目的ip地址，比如connect的时候会设置。#define sk_rcv_saddr\t\t__sk_common.skc_rcv_saddr //本地ip地址，比如connect的时候会设置。#define sk_family\t\t__sk_common.skc_family   //协议族比如afinet#define sk_state\t\t__sk_common.skc_state \t//tcp三次握手四次挥手用到的状态#define sk_reuse\t\t__sk_common.skc_reuse  //timewait相关#define sk_reuseport\t\t__sk_common.skc_reuseport \t//重用ip端口#define sk_ipv6only\t\t__sk_common.skc_ipv6only#define sk_net_refcnt\t\t__sk_common.skc_net_refcnt  //sock正在被使用的引用计数#define sk_bound_dev_if\t\t__sk_common.skc_bound_dev_if //输出网络设备的索引，比如用户指定了输出设备#define sk_bind_node\t\t__sk_common.skc_bind_node //tcp收包bhash的节点？#define sk_prot\t\t\t__sk_common.skc_prot \t//不同协议的ops集合#define sk_net\t\t\t__sk_common.skc_net  //对应的网络命名空间#define sk_v6_daddr\t\t__sk_common.skc_v6_daddr //ipv6#define sk_v6_rcv_saddr\t__sk_common.skc_v6_rcv_saddr //ipv6#define sk_cookie\t\t__sk_common.skc_cookie#define sk_incoming_cpu\t\t__sk_common.skc_incoming_cpu //收包cpu索引#define sk_flags\t\t__sk_common.skc_flags\t//一些标志#define sk_rxhash\t\t__sk_common.skc_rxhash //接收端的hash值\t/* early demux fields */\tstruct dst_entry __rcu\t*sk_rx_dst; //接收方向查找路由获取的dst\tint\t\t\tsk_rx_dst_ifindex;\t//收包网络设备索引\tu32\t\t\tsk_rx_dst_cookie;\t//ipv6相关\t\tsocket_lock_t\t\tsk_lock;\tatomic_t\t\tsk_drops;\t\t//缓冲区溢出的丢包统计\tint\t\t\tsk_rcvlowat;\t\t//​套接字接收数据最小阈值​\tstruct sk_buff_head\tsk_error_queue;\t\t//错误队列，收到icmp报文错误报文的时候就会放到这个队列\tstruct sk_buff_head\tsk_receive_queue;  //软中断放包的接收队列\t/*\t * The backlog queue is special, it is always used with\t * the per-socket spinlock held and requires low latency\t * access. Therefore we special case it&#x27;s implementation.\t * Note : rmem_alloc is in this structure to fill a hole\t * on 64bit arches, not because its logically part of\t * backlog.\t */\tstruct &#123;\t\tatomic_t\trmem_alloc;\t\tint\t\tlen;\t\tstruct sk_buff\t*head;\t\tstruct sk_buff\t*tail;\t&#125; sk_backlog; //后备队列，tcprecv会用到////skb的turesize总和，如果这个值大于缓冲区大小，就会丢弃，收包的时候会把skb的truesize累加上#define sk_rmem_alloc sk_backlog.rmem_alloc\tint\t\t\tsk_forward_alloc; // 缓冲区大小的一个标志，在申请skb的时候会增加，放入发送队列后会减少\tu32\t\t\tsk_reserved_mem;   //setsockopt配置，保留缓冲区的大小 //tcp存在内存压力的时候会用到 #ifdef CONFIG_NET_RX_BUSY_POLL\tunsigned int\t\tsk_ll_usec;\t/* ===== mostly read cache line ===== */\tunsigned int\t\tsk_napi_id;#endif\tint\t\t\tsk_rcvbuf;       //接收缓冲区大小，skb的turesize大于sk_rcvbuf就直接丢弃，sock初始化的时候读取系统配置设置的\tint\t\t\tsk_disconnects;   //记录断开连接的次数\tstruct sk_filter __rcu\t*sk_filter;\tunion &#123;\t\tstruct socket_wq __rcu\t*sk_wq;   //等待队列，多种调度需要\t\t/* private: */\t\tstruct socket_wq\t*sk_wq_raw;\t\t/* public: */\t&#125;;#ifdef CONFIG_XFRM\tstruct xfrm_policy __rcu *sk_policy[2];#endif\tstruct dst_entry __rcu\t*sk_dst_cache;  //sock关联的路由结构，在tcp建立连接或者发包的时候会设置\tatomic_t\t\tsk_omem_alloc;\tint\t\t\tsk_sndbuf;     //发送缓冲区大小，和rcvbuf一样，当在整个协议栈中使用内存大于这个值的时候，就回返回nobuf\t\t/* ===== cache line for TX ===== */\t// //发送队列中缓存数据包的大小，入队的时候会加turesize, 这里可以理解为没有交付到第三层的内存使用\t//就是发送队列使用的内存总量，比如果说TCP清重传队列的时候就会移除了\tint\t\t\tsk_wmem_queued;   \t//这个与上面的队列类似，但是是记录方向三个部分的内存，分别为传输层，tc，网卡队列中存的包总量\t//是在发送方向释放数据包的时候减少的，也就是网卡驱动释放数据包的时候减少的\trefcount_t\t\tsk_wmem_alloc;\t////tcp小发送队列的的标志，设置标志位后当协议站中发送缓冲区的数据大于这个队列的数据时不会立即发送会\t//用taskelt来调度发送。\tunsigned long\t\tsk_tsq_flags; \tunion &#123;\t\tstruct sk_buff\t*sk_send_head;\t\tstruct rb_root\ttcp_rtx_queue; //二叉树，管理tcp的重传队列\t&#125;;\tstruct sk_buff_head\tsk_write_queue; //发送队列\t__s32\t\t\tsk_peek_off;       //peek的偏移\tint\t\t\tsk_write_pending;      //记录当前套接字写操作的阻塞的数量\t__u32\t\t\tsk_dst_pending_confirm;   //标识路由是否需要验证，比如查看邻居表项是否有效，收到tcpack可能会置位\tu32\t\t\tsk_pacing_status; /* see enum sk_pacing */   //是否是tcppacing 状态，比如是否启用tcp_pacing\tlong\t\t\tsk_sndtimeo;   //发送方向缓冲区满的时候，允许等待的时间，如果哦时间到了就会返回nobuf\tstruct timer_list\tsk_timer;   //sock的定时器，例如tcp保活用到这个定时器\t__u32\t\t\tsk_priority;     //sock的优先级，可以通过setsockopt设置，会影响tc的调度\t__u32\t\t\tsk_mark;     //会影响路由或者ovs的差表逻辑，通过setsockopt设置，只能通过setsockopt设置吗\tunsigned long\t\tsk_pacing_rate; /* bytes per second */ //tcppacing速率\tunsigned long\t\tsk_max_pacing_rate;   //tcppacing的最大速率\t//alloc page后会用这个结构管理page 例如在ip_append中会alloc page,与skb的非线性部分密切相关\tstruct page_frag\tsk_frag;\t\t\t\tnetdev_features_t\tsk_route_caps;  //这里是设备支持的能力\tint\t\t\tsk_gso_type;\t\t\t//GSO的类型\tunsigned int\t\tsk_gso_max_size; //GSO分段的最大长度\tgfp_t\t\t\tsk_allocation;   //分配\t//   //发送方向的hash值作用是赋值到skb的txhash字段中，比如在xps的时候会用到，这个hash值好像是随机值\t__u32\t\t\tsk_txhash; \t\t/*\t * Because of non atomicity rules, all\t * changes are protected by socket lock.\t */\tu8\t\t\tsk_gso_disabled : 1,  //是否使能gso\t\t\t\tsk_kern_sock : 1, \t\t//是否是内核创建的socket\t\t\t\tsk_no_check_tx : 1,    //setsockopt设置，是否不需要计算校验和\t\t\t\tsk_no_check_rx : 1,\t\t//rx方向\t\t\t\tsk_userlocks : 4;      //几个标志位，设置后防止修改某些参数，比如缓冲区大小\tu8\t\t\tsk_pacing_shift;\t\t//tsq相关\tu16\t\t\tsk_type;\t\t\t\t//用户创建socket时候设置的type比如DGRAM\tu16\t\t\tsk_protocol;\t\t\t//用户创建socket时候设置的协议类型 比如UDP\tu16\t\t\tsk_gso_max_segs;\t\t//分段最大数量\tunsigned long\t        sk_lingertime;  //用于tcp的关闭中，不为0的时候会等待配置的时间等数据包发送完后关闭\tstruct proto\t\t*sk_prot_creator;   //具体协议的ops集合\trwlock_t\t\tsk_callback_lock;\tint\t\t\tsk_err,  \t\t\t\t// 保存错误类型\t\t\t\tsk_err_soft;\tu32\t\t\tsk_ack_backlog;  \t\t//TCP中三次握手完成，但是没有被accept的数量\tu32\t\t\tsk_max_ack_backlog;\t\t//这个是listen系统调用配置的\tkuid_t\t\t\tsk_uid;\t\t\t\t//用户的id创建socket的时候赋值的\tu8\t\t\tsk_txrehash;\t\t\t//setsockopt设置，是否需要重新计算发送的txhash值从而影响队列的选择#ifdef CONFIG_NET_RX_BUSY_POLL\tu8\t\t\tsk_prefer_busy_poll;\tu16\t\t\tsk_busy_poll_budget;#endif\tspinlock_t\t\tsk_peer_lock;   //setsockopt设置peer相关的时候用到的锁\tint\t\t\tsk_bind_phc;\tstruct pid\t\t*sk_peer_pid;   //unix用的？ 进程间通信\tconst struct cred\t*sk_peer_cred;  //安全相关\tlong\t\t\tsk_rcvtimeo;  //接收的超时时间\tktime_t\t\t\tsk_stamp;\t//时间戳，用户调用setsockopt的时候会设置，把skb的时间戳保存到这里#if BITS_PER_LONG==32\tseqlock_t\t\tsk_stamp_seq;#endif\tatomic_t\t\tsk_tskey;\t\t\t//当通过set_sockopt设置时间戳信息的时候会把这个字段赋值tcp的序列号\tatomic_t\t\tsk_zckey;\t\t//零拷贝相关，几乎用不到吧\tu32\t\t\tsk_tsflags;\t\t\t//时间戳的标志位，通过setsockopt设置，会根据标志把时间戳信息放到msg中给用户\tu8\t\t\tsk_shutdown;\tu8\t\t\tsk_clockid;\tu8\t\t\tsk_txtime_deadline_mode : 1, //setsockopt设置的没发现哪里使用\t\t\t\tsk_txtime_report_errors : 1, ////setsockopt设置的没发现哪里使用\t\t\t\tsk_txtime_unused : 6;\t\t//未使用的\tbool\t\t\tsk_use_task_frag;\t\t//表示是否从当前进程获取一个page的一部分，initsock的时候设置为true，ip_append_data中会用到\t\tstruct socket\t\t*sk_socket;  //关联的socket\tvoid\t\t\t*sk_user_data;#ifdef CONFIG_SECURITY\tvoid\t\t\t*sk_security;#endif\tstruct sock_cgroup_data\tsk_cgrp_data;  //cgroup相关\tstruct mem_cgroup\t*sk_memcg;\t\t\t//  //cgroup相关\tvoid\t\t\t(*sk_state_change)(struct sock *sk); //唤醒睡眠的进程，比如tcp状态发生改变的时候调用\tvoid\t\t\t(*sk_data_ready)(struct sock *sk); //软中断收到数据包，唤醒睡眠的进程\tvoid\t\t\t(*sk_write_space)(struct sock *sk);//有写的空间，唤醒,好像几乎不会被调用\tvoid\t\t\t(*sk_error_report)(struct sock *sk);\tint\t\t\t(*sk_backlog_rcv)(struct sock *sk,   //当sock被用户访问的时候，会把报放到后备队列\t\t\t\t\t\t  struct sk_buff *skb);#ifdef CONFIG_SOCK_VALIDATE_XMIT\tstruct sk_buff*\t\t(*sk_validate_xmit_skb)(struct sock *sk,\t\t\t\t\t\t\tstruct net_device *dev,\t\t\t\t\t\t\tstruct sk_buff *skb);#endif\tvoid                    (*sk_destruct)(struct sock *sk); //销毁套接字的回调\tstruct sock_reuseport __rcu\t*sk_reuseport_cb;#ifdef CONFIG_BPF_SYSCALL\tstruct bpf_local_storage __rcu\t*sk_bpf_storage;#endif\tstruct rcu_head\t\tsk_rcu;\t\t//rcu锁，好像没用到？？\tnetns_tracker\t\tns_tracker;   //网络命名空间，好像跟trace有关\tstruct hlist_node\tsk_bind2_node;  //tcp使用，比如连接的时候会把这个node与bhash和bhash2关联起来&#125;;\n\n","categories":["网络协议栈源码学习"],"tags":["sock"]},{"title":"TCP输出tcp_sendmsg_locked（三）","url":"/2025/11/23/tcp_sendmsg_locked(%E4%B8%89)/","content":"tcp_sendmsg_locked申请数据包成功并调用tcp_skb_entail将数据包入队后，接下来的任务就是将用户数据包copy到skb非线性部分中。\n首先调用sk_page_frag从当前进程获取一个page，之后调用sk_page_frag_refill查看是否还有足够的空间可以使用，上述过程具体代码如下所示：\nstatic inline struct page_frag *sk_page_frag(struct sock *sk)&#123;\tif (sk-&gt;sk_use_task_frag)\t\treturn &amp;current-&gt;task_frag;\treturn &amp;sk-&gt;sk_frag;&#125;bool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag)&#123;\t\t//是否有32字节可以写的空间\tif (likely(skb_page_frag_refill(32U, pfrag, sk-&gt;sk_allocation)))\t\treturn true;\t//分配失败，进入内存压力\tsk_enter_memory_pressure(sk);\t//减小发送缓冲区\tsk_stream_moderate_sndbuf(sk);\treturn false;&#125;\n\nskb_page_frag_refill中会进一步判断拿到的页是否有32字节可写的空间，如果空间不够则会申请一个新的页，具体代码如下所示：\nbool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)&#123;\tif (pfrag-&gt;page) &#123;\t\tif (page_ref_count(pfrag-&gt;page) == 1) &#123; \t   //只有自己在用\t\t\tpfrag-&gt;offset = 0; \t\t\t\t\t\t //offset设置为0 \t\t\treturn true;\t\t&#125;\t\tif (pfrag-&gt;offset + sz &lt;= pfrag-&gt;size) \t\t   //别人在用，看空间是否够，如果够也直接返回了\t\t\treturn true;\t\tput_page(pfrag-&gt;page);  \t\t\t\t\t  //不够，直接put掉\t&#125;\t//准备分配新页\tpfrag-&gt;offset = 0; \t//这里通常不会走把\tif (SKB_FRAG_PAGE_ORDER &amp;&amp;\t    !static_branch_unlikely(&amp;net_high_order_alloc_disable_key)) &#123;\t\t/* Avoid direct reclaim but allow kswapd to wake */\t\tpfrag-&gt;page = alloc_pages((gfp &amp; ~__GFP_DIRECT_RECLAIM) |\t\t\t\t\t  __GFP_COMP | __GFP_NOWARN |\t\t\t\t\t  __GFP_NORETRY,\t\t\t\t\t  SKB_FRAG_PAGE_ORDER);\t\tif (likely(pfrag-&gt;page)) &#123;\t\t\tpfrag-&gt;size = PAGE_SIZE &lt;&lt; SKB_FRAG_PAGE_ORDER;\t\t\treturn true;\t\t&#125;\t&#125;\t//分配一个page\tpfrag-&gt;page = alloc_page(gfp);\tif (likely(pfrag-&gt;page)) &#123;\t\tpfrag-&gt;size = PAGE_SIZE;\t\treturn true;\t&#125;\treturn false;&#125;\n\n上述获取到page后，进一步调用skb_can_coalesce判断当前获取到的页是否已经是skb管理的页，如果不是，且当前skb已将有17个非线性部分的页了，则直接重新申请skb同时设置push标志位，并将标记为不能合并（后续用于指导设置页的元数据)\n判断是否可以合并的逻辑如下所示：\nstatic inline bool skb_can_coalesce(struct sk_buff *skb, int i,\t\t\t\t    const struct page *page, int off)&#123;\tif (skb_zcopy(skb)) //零拷贝 skb 不能合并\t\treturn false;\tif (i) &#123;\t\tconst skb_frag_t *frag = &amp;skb_shinfo(skb)-&gt;frags[i - 1]; //skb的最后一个页？\t\t//当前写入数据使用的 page 与最后的 frag 是同一物理页\t\treturn page == skb_frag_page(frag) &amp;&amp;   //判断是否是同一个页\t\t       off == skb_frag_off(frag) + skb_frag_size(frag);\t&#125;\treturn false;&#125;\n\n上述工作完成后，会计算当前获取的页还有头多少可以copy的空间，并调用tcp_wmem_schedule更新内存记账，具体代码如下所示：\nint tcp_wmem_schedule(struct sock *sk, int copy)&#123;\tint left;\t//判断是否达到全局的内存压力，如果还有足够发送空间，则直接允许这次请求\tif (likely(sk_wmem_schedule(sk, copy)))\t\treturn copy;\t/* We could be in trouble if we have nothing queued.\t * Use whatever is left in sk-&gt;sk_forward_alloc and tcp_wmem[0]\t * to guarantee some progress.\t */\t//正常内存预算不够了 TCP 进入资源紧张状态\tleft = sock_net(sk)-&gt;ipv4.sysctl_tcp_wmem[0] - sk-&gt;sk_wmem_queued;\tif (left &gt; 0) //最低保障区间还有空间，强制额外分配内存，这里可以看到每个sk最少可以用4k，没使用到4k的时候即使资源紧张也会强制调度\t\tsk_forced_mem_schedule(sk, min(left, copy));\treturn min(copy, sk-&gt;sk_forward_alloc); //回真正允许写入的数据大小&#125;\n\n可以看到，如果tcp_wmem_schedule中会处理没有足够的空间的情况，通过进一步判断当前这个sk内存使用是否到了4k，如果没使用到4k即使资源紧张的情况下也会强制调度。\n之后调用skb_copy_to_page_nocache将用户态数据拷贝到内核，并根据是否可以来更新页的相关管理字段。\n如果是第一次创建数据包，则还会清除这个数据包的psh标志位，并更新发送缓冲区的序列号和skb的end_seq,接下来判断用户数据是否已经全部拷贝完，如果完成了则直跳出循环，否则判断当前skb是否还能继续填充数据，如果可以，则继续循环拷贝数据，直到数据包拷贝完成或者当前数据包已经填充size_goal大小的数据包才会结束。\n这里分为两种情况，一是如果当前数据包已经填充满了(达到了size_goal),则判断是否需要forced_push（如果当前写入的数据量已经超过发送窗口的一半，那就必须强制 push。如果需要则标记push标志，并调用__tcp_push_pending_frames发送队列中的数据包。\n而是如果当前数据包是发送队列的头部，则只发送这一个数数据包（这个概率相对较小吧）。\n如果是循环内部由于数据包已经全部拷贝完成跳出循环，则会调用tcp_push来发送数据包，具体代码如下所示：\nvoid tcp_push(struct sock *sk, int flags, int mss_now,\t      int nonagle, int size_goal)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb;\t//从队列尾部获取数据包\tskb = tcp_write_queue_tail(sk);\tif (!skb)\t\treturn;\t//如果没有设置msgmore同时达到了窗口大小的一半，则设置push标志位\tif (!(flags &amp; MSG_MORE) || forced_push(tp))\t\ttcp_mark_push(tp, skb);\t//如果设置了oob 设置紧急指针\ttcp_mark_urg(tp, flags);\tif (tcp_should_autocork(sk, skb, size_goal)) &#123;\t\t/* avoid atomic op if TSQ_THROTTLED bit is already set */\t\tif (!test_bit(TSQ_THROTTLED, &amp;sk-&gt;sk_tsq_flags)) &#123;\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPAUTOCORKING);\t\t\tset_bit(TSQ_THROTTLED, &amp;sk-&gt;sk_tsq_flags);\t\t&#125;\t\t/* It is possible TX completion already happened\t\t * before we set TSQ_THROTTLED.\t\t */\t\t//有数据还在协议栈，延迟发送\t\tif (refcount_read(&amp;sk-&gt;sk_wmem_alloc) &gt; skb-&gt;truesize)\t\t\treturn;\t&#125;\t//是否设置msgmore\tif (flags &amp; MSG_MORE)\t\tnonagle = TCP_NAGLE_CORK;\t__tcp_push_pending_frames(sk, mss_now, nonagle);&#125;\n\ntcp_push应该是是tcp_sendmsg_locked中被调用次数最多的函数，首先拿到队尾 skb，如果没有设置msgmore 或 达到了窗口大小的一半，就给数据包打上 PUSH 标记，\n接着 TCP 会判断是否需要autocork(当前数据包小于GSO大小，系统启用了cork(默认开启)，有数据包在途中，有数据待发送)，如果满足上述条件则会设置TSQ标志位(数据包的析构函数会调用软中断发送数据包），如果有数据还在协议栈，延迟发送。否则调用\n__tcp_push_pending_frames发送数据包。\n总结：tcp_sendmsg_locked中存在三个函数完成进一步向下发包的工作，分别是__tcp_push_pending_frames，tcp_push_one和tcp_push。三者主要区别和调用的场景如下所示：\n\n\n\n函数\n所在层级\n主要作用\n在 tcp_sendmsg_locked() 里的场景\n\n\n\ntcp_push()\n最高层\n做 MSG_MORE &#x2F; Nagle &#x2F; TSQ &#x2F; autocork 的决策，然后统一 push\n一次 send()&#x2F;write() 结束，或者内存不足但已拷贝一部分数据时，整体考虑是否发送\n\n\ntcp_push_one()\n中间层\n只发送 tcp_send_head() 这一个 skb，启动流水线但不 flush 整个队列\n在循环中，当前 skb 已写满 size_goal、还没到 forced_push 阈值，但又是队列头，小心地发一个包\n\n\n__tcp_push_pending_frames()\n最底层\n不再做策略判断，直接根据 cwnd&#x2F;窗口，把 pending 的 skb 尽量发给 IP 层\n在循环中判断到 forced_push(tp)，说明窗口已经占得比较满了，需要强制把队列里的数据尽量冲出去\n\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输出"]},{"title":"TCP建立连接-listen","url":"/2025/09/01/tcp%20listen/","content":"TCP listenlisten 是服务端套接字进入监听状态的系统调用，它把一个通过 socket 和 bind 创建好的套接字标记为“被动套接字”，用来接受客户端的连接请求。调用时需要指定 backlog 参数，用来缓存尚未被应用层 accept() 取走的连接请求的上限。\n用户态程序通过glibc调用listen系统调用后，最终会调用到内核的__sys_listen由它完成套接字与地址的绑定逻辑，具体代码如下所示：\n//这里的backlog表示已经完成三次握手但是没有被accept的最大数量int __sys_listen(int fd, int backlog)&#123;\tstruct socket *sock;\tint err, fput_needed;\tint somaxconn;\t//根据fd找到对应的socket结构\tsock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed);\tif (sock) &#123;\t\tsomaxconn = READ_ONCE(sock_net(sock-&gt;sk)-&gt;core.sysctl_somaxconn);\t\tif ((unsigned int)backlog &gt; somaxconn)\t\t\tbacklog = somaxconn;\t\t//安全相关的钩子\t\terr = security_socket_listen(sock, backlog);\t\tif (!err)\t\t//调用套接字层的listen回调\t\t\terr = READ_ONCE(sock-&gt;ops)-&gt;listen(sock, backlog);\t\t//释放引用计数\t\tfput_light(sock-&gt;file, fput_needed);\t&#125;\treturn err;&#125;\n\n上述代码通过文件描述符获取socket结构体，之后从内核参数 sysctl_somaxconn 里读出系统允许的最大 backlog 值，如果用户传的 backlog 比它大，就缩小到 somaxconn ，然后调用了套接字层的listen，对应的具体实现如下所示:\nint inet_listen(struct socket *sock, int backlog)&#123;\tstruct sock *sk = sock-&gt;sk;\tint err = -EINVAL;\tlock_sock(sk);\t//检查当前socket状态和到接子的类型\tif (sock-&gt;state != SS_UNCONNECTED || sock-&gt;type != SOCK_STREAM)\t\tgoto out;\terr = __inet_listen_sk(sk, backlog);out:\trelease_sock(sk);\treturn err;&#125;\n\n inet_listen 首先获取套接字对应的 struct sock随后检查当前 socket 是否处于未连接状态且类型为 SOCK_STREAM，只有符合条件的 TCP 流式套接字才能进入监听；若条件不满足直接返回错误，否则调用内部函数 __inet_listen_sk 去完成具体的监听初始化工作。具体代码如下所示:\nint __inet_listen_sk(struct sock *sk, int backlog)&#123;\tunsigned char old_state = sk-&gt;sk_state;\tint err, tcp_fastopen;\t//必须是close或者listen状态\tif (!((1 &lt;&lt; old_state) &amp; (TCPF_CLOSE | TCPF_LISTEN)))\t\treturn -EINVAL;\t//这里设置了全连接队列的大小\t\tWRITE_ONCE(sk-&gt;sk_max_ack_backlog, backlog);\t/* Really, if the socket is already in listen state\t * we can only allow the backlog to be adjusted.\t */\t//close状态就进入这分支\tif (old_state != TCP_LISTEN) &#123;\t\t/* Enable TFO w/o requiring TCP_FASTOPEN socket option.\t\t * Note that only TCP sockets (SOCK_STREAM) will reach here.\t\t * Also fastopen backlog may already been set via the option\t\t * because the socket was in TCP_LISTEN state previously but\t\t * was shutdown() rather than close().\t\t */\t\t//初始化TFO相关，系统参数默认是1 表示客户端开启\t\ttcp_fastopen = READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_fastopen);\t\t//默认应该不会进入这个分支\t\tif ((tcp_fastopen &amp; TFO_SERVER_WO_SOCKOPT1) &amp;&amp;\t\t    (tcp_fastopen &amp; TFO_SERVER_ENABLE) &amp;&amp;\t\t    !inet_csk(sk)-&gt;icsk_accept_queue.fastopenq.max_qlen) &#123;\t\t\tfastopen_queue_tune(sk, backlog);\t\t\ttcp_fastopen_init_key_once(sock_net(sk));\t\t&#125;\t\terr = inet_csk_listen_start(sk);\t\tif (err)\t\t\treturn err;\t\t//bpf钩子\t\ttcp_call_bpf(sk, BPF_SOCK_OPS_TCP_LISTEN_CB, 0, NULL);\t&#125;\treturn 0;&#125;\n\n这段代码是内核里将 TCP 套接字切换到监听状态的核心路径：首先校验当前状态必须是 CLOSE 或 LISTEN，然后把传入的 backlog 写入 sk-&gt;sk_max_ack_backlog 作为全连接队列上限，随后调用 inet_csk_listen_start(sk) 真正把 socket 放入监听哈希表。具体代码如下所示：\n//tcplisten调进来osk为空int __inet_hash(struct sock *sk, struct sock *osk)&#123;\tstruct inet_hashinfo *hashinfo = tcp_or_dccp_get_hashinfo(sk);\tstruct inet_listen_hashbucket *ilb2;\tint err = 0;\t//处理非监听状态的套接子，直接加入ehash中\tif (sk-&gt;sk_state != TCP_LISTEN) &#123;\t\tlocal_bh_disable();\t\tinet_ehash_nolisten(sk, osk, NULL);\t\tlocal_bh_enable();\t\treturn 0;\t&#125;\tWARN_ON(!sk_unhashed(sk));\t//这里根据ip和端口找到listenhash桶（注意这里桶下面直接挂的是socket）\tilb2 = inet_lhash2_bucket_sk(hashinfo, sk);\tspin_lock(&amp;ilb2-&gt;lock);\t//套接字是否启用了reuseport\tif (sk-&gt;sk_reuseport) &#123;\t\t//将当前的套接字加入端口重用组\t\terr = inet_reuseport_add_sock(sk, ilb2);\t\tif (err)\t\t\tgoto unlock;\t&#125;\t//将套接字插入listenhash表\tif (IS_ENABLED(CONFIG_IPV6) &amp;&amp; sk-&gt;sk_reuseport &amp;&amp;\t\tsk-&gt;sk_family == AF_INET6)\t\t__sk_nulls_add_node_tail_rcu(sk, &amp;ilb2-&gt;nulls_head);\telse\t\t__sk_nulls_add_node_rcu(sk, &amp;ilb2-&gt;nulls_head);\tsock_set_flag(sk, SOCK_RCU_FREE);\tsock_prot_inuse_add(sock_net(sk), sk-&gt;sk_prot, 1);unlock:\tspin_unlock(&amp;ilb2-&gt;lock);\treturn err;&#125;\n\n上述代码的核心逻辑就是把当前套接字加listen hash表中(如果不是listen状态就直接加入ehash)，如果当前启用了reuseport选项的话，还会调用inet_reuseport_add_sock将当前套接字加入一个端口重用组（收包的时候做负载均衡貌似会用到），加入端口重用组的代码如下所示：\nstatic int inet_reuseport_add_sock(struct sock *sk,\t\t\t\t   struct inet_listen_hashbucket *ilb)&#123;\tstruct inet_bind_bucket *tb = inet_csk(sk)-&gt;icsk_bind_hash;\tconst struct hlist_nulls_node *node;\tstruct sock *sk2;\tkuid_t uid = sock_i_uid(sk);\t//便利hash桶下面挂的所有socket，看是否有与当前套接字相匹配的reuseport组加入，但是必须同时满足下面一系列条件\tsk_nulls_for_each_rcu(sk2, node, &amp;ilb-&gt;nulls_head) &#123;\t\tif (sk2 != sk &amp;&amp; //非自身\t\t    sk2-&gt;sk_family == sk-&gt;sk_family &amp;&amp;//协议族一致\t\t    ipv6_only_sock(sk2) == ipv6_only_sock(sk) &amp;&amp;\t\t    sk2-&gt;sk_bound_dev_if == sk-&gt;sk_bound_dev_if &amp;&amp; //同一个网口\t\t    inet_csk(sk2)-&gt;icsk_bind_hash == tb &amp;&amp; //绑定的端口相同\t\t    sk2-&gt;sk_reuseport &amp;&amp; uid_eq(uid, sock_i_uid(sk2)) &amp;&amp; //启用了reuseport\t\t    inet_rcv_saddr_equal(sk, sk2, false))   //地址完全相同\t\t\treturn reuseport_add_sock(sk, sk2, \t//这里是将当前的sk加入到sk2中\t\t\t\t\t\t  inet_rcv_saddr_any(sk));\t&#125;\t//没有直接可以加入的组，自己创建一个\treturn reuseport_alloc(sk, inet_rcv_saddr_any(sk));&#125;\n\n当启用了 SO_REUSEPORT 时，会尝试把它加入到已有的端口复用组中。函数会遍历 listen hash 桶里已经存在的 socket，逐个检查它们是否和当前 socket 在协议族、绑定网口、端口号、地址、reuseport 标志以及所属用户 ID 上都一致；如果找到匹配的，就调用 reuseport_add_sock 把当前 socket 加入那个组；如果没有找到合适的组，则调用 reuseport_alloc 新建一个复用组，把自己作为第一个成员放进去。加入其他sock端口组的代码reuseport_add_sock逻辑如下所示：\nint reuseport_add_sock(struct sock *sk, struct sock *sk2, bool bind_inany)&#123;\tstruct sock_reuseport *old_reuse, *reuse;\t//这里判断sk2是否有对应的reuseprot的组，如果没有，这里会新建一个\tif (!rcu_access_pointer(sk2-&gt;sk_reuseport_cb)) &#123;\t\t//创建一个管理reuseport 的组结构，并进行初始化\t\tint err = reuseport_alloc(sk2, bind_inany);\t\tif (err)\t\t\treturn err;\t&#125;\tspin_lock_bh(&amp;reuseport_lock);\treuse = rcu_dereference_protected(sk2-&gt;sk_reuseport_cb,\t\t\t\t\t  lockdep_is_held(&amp;reuseport_lock));\t//这里的old_reuse大概率为空吧\told_reuse = rcu_dereference_protected(sk-&gt;sk_reuseport_cb,\t\t\t\t\t      lockdep_is_held(&amp;reuseport_lock));\t//如果当前的套接字已经有所属的组了，并且该组中还有已经关闭的套接字，则将当前套接字加入到新组中然后直接返回了，通常不会走这个分支吧\tif (old_reuse &amp;&amp; old_reuse-&gt;num_closed_socks) &#123;\t\t/* sk was shutdown()ed before */\t\tint err = reuseport_resurrect(sk, old_reuse, reuse, reuse-&gt;bind_inany);\t\tspin_unlock_bh(&amp;reuseport_lock);\t\treturn err;\t&#125;\t//旧的里面还有其他套接字，不能在往下走了，不能让一个套接字属于多个组\tif (old_reuse &amp;&amp; old_reuse-&gt;num_socks != 1) &#123;\t\tspin_unlock_bh(&amp;reuseport_lock);\t\treturn -EBUSY;\t&#125;\t//如果满了，则需要扩容\tif (reuse-&gt;num_socks + reuse-&gt;num_closed_socks == reuse-&gt;max_socks) &#123;\t\treuse = reuseport_grow(reuse);\t\tif (!reuse) &#123;\t\t\tspin_unlock_bh(&amp;reuseport_lock);\t\t\treturn -ENOMEM;\t\t&#125;\t&#125;\t//将新套接字加入到组中\t__reuseport_add_sock(sk, reuse);\trcu_assign_pointer(sk-&gt;sk_reuseport_cb, reuse);\tspin_unlock_bh(&amp;reuseport_lock);\tif (old_reuse)\t\tcall_rcu(&amp;old_reuse-&gt;rcu, reuseport_free_rcu);\treturn 0;&#125;static void __reuseport_add_sock(struct sock *sk,\t\t\t\t struct sock_reuseport *reuse)&#123;\treuse-&gt;socks[reuse-&gt;num_socks] = sk;\t/* paired with smp_rmb() in reuseport_(select|migrate)_sock() */\tsmp_wmb();\treuse-&gt;num_socks++;\treuseport_get_incoming_cpu(sk, reuse);&#125;\n\n上述代码的逻辑为 如果 sk2 还没有组，就先新建一个(问题是会有这种情况吗？)；然后检查是否存在“复活旧组”的情况，或者防止一个 socket 同时属于多个组；如果当前组容量满了就扩容；最后调用__reuseport_add_sock把新 socket 加进组里，并把它的 sk_reuseport_cb 指针指向这个组。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP建连"]},{"title":"TCP输出 tcp_sendmsg_locked（二）","url":"/2025/11/20/tcp_sendmsg_locked(%E4%BA%8C)/","content":"tcp_sendmsg_locked中计算得到size_goal后，会进入while大循环完成skb申请，数据拷贝，更新内存记账等关键操作\n循环内部首先从队列尾部取出一个数据包并计算是否还有剩余的空间可以拷贝用户的数据，这里注意，有用户调用send发送数据包后，协议栈可能不会立马发送数据（比如nagle），因此队列尾部是可以拿到数据包的。\n首先判断队尾数据包（如果有）是否还有待拷贝的空间，没有则会尝试申请数据包，申请数据包前会首先判断是否有缓冲区，如果协议栈中当前sk发送队列使用总量大于了发送tcp发送缓冲区的大小直接就返回了。\n接下来判断重传队列和发送队列是否为空，如果为空，则标记为第一个数据包，后面申请内存的时候即使在内存压力之下也能申请成功。\n调用tcp_stream_alloc_skb申请数据包，并处理内存记账，具体代码如下所示：\nstruct sk_buff *tcp_stream_alloc_skb(struct sock *sk, gfp_t gfp,\t\t\t\t     bool force_schedule)&#123;\tstruct sk_buff *skb;\t//fclone带出来两个skb\tskb = alloc_skb_fclone(MAX_TCP_HEADER, gfp);\tif (likely(skb)) &#123;\t\tbool mem_scheduled;\t\t//计算truesize\t\tskb-&gt;truesize = SKB_TRUESIZE(skb_end_offset(skb));\t\tif (force_schedule) &#123; //第一个数据包走强制调度\t\t\tmem_scheduled = true;\t\t\t//叫强制分配内存\t\t\tsk_forced_mem_schedule(sk, skb-&gt;truesize);\t\t&#125; else &#123;\t\t\t//不是强制调度的情况，更新内存压力信息，并判断是否到达内存压力，返回0 表示申请失败了\t\t\tmem_scheduled = sk_wmem_schedule(sk, skb-&gt;truesize);\t\t&#125;\t\tif (likely(mem_scheduled)) &#123;\t\t\tskb_reserve(skb, MAX_TCP_HEADER);//这里应该直接拉到了最后面\t\t\tskb-&gt;ip_summed = CHECKSUM_PARTIAL;\t\t//设置校验和\t\t\tINIT_LIST_HEAD(&amp;skb-&gt;tcp_tsorted_anchor);\t\t\treturn skb;\t\t&#125;\t\t__kfree_skb(skb);\t&#125; else &#123;\t\tsk-&gt;sk_prot-&gt;enter_memory_pressure(sk); //如果skb都申请失败了，直接设置内存压力\t\tsk_stream_moderate_sndbuf(sk); //调小发送缓冲区\t&#125;\treturn NULL;&#125;\n\ntcp_stream_alloc_skb中调用alloc_skb_fclone申请两个skb结构体，为后续clone数据包做准备\n注意这里申请数据包的线性部分的长度为MAX_TCP_HEADER，也就是说对于TCP来说，数据包的负载并没有在skb的线性缓冲区部分，是后续用非线性部分管理的。  \n如果申请成功，则进一步计算数据包的true_size,并使用true_size更新内存记账信息，例如一个数据数据包这里会调用sk_forced_mem_schedule强制分配内存（即使在内存压力下也能申请成功）对应具体代码如下所示：\nvoid sk_forced_mem_schedule(struct sock *sk, int size)&#123;\tint delta, amt;\t//这里应该是计算超出的字节数，如果没有超出就直接返回了\tdelta = size - sk-&gt;sk_forward_alloc;\tif (delta &lt;= 0)\t\treturn;\t//计算需要几个页\tamt = sk_mem_pages(delta);\t//增加预分配内存额度\tsk_forward_alloc_add(sk, amt &lt;&lt; PAGE_SHIFT);\tsk_memory_allocated_add(sk, amt); //传入页数，里面会批量更新内存压力情况\tif (mem_cgroup_sockets_enabled &amp;&amp; sk-&gt;sk_memcg)\t\tmem_cgroup_charge_skmem(sk-&gt;sk_memcg, amt,\t\t\t\t\tgfp_memcg_charge() | __GFP_NOFAIL);&#125;\n\nsk_forced_mem_schedule首先根据当前数据包的使用的内存与前向分配的内存计算超出的内存，如果没有超出则直接返回。否则将超出的部分换算成对应的页数，并根据超出的页数更新前向分配的使用量。并调用sk_memory_allocated_add批量更新页面使用的总数。\n具体代码如下所示：\nstatic inline voidsk_memory_allocated_add(struct sock *sk, int amt)&#123;\tint local_reserve;\tpreempt_disable();\tlocal_reserve = __this_cpu_add_return(*sk-&gt;sk_prot-&gt;per_cpu_fw_alloc, amt);//++\tif (local_reserve &gt;= SK_MEMORY_PCPU_RESERVE) &#123; //批量更新 256 更新一次\t\t__this_cpu_sub(*sk-&gt;sk_prot-&gt;per_cpu_fw_alloc, local_reserve);\t\tatomic_long_add(local_reserve, sk-&gt;sk_prot-&gt;memory_allocated);\t&#125;\tpreempt_enable();&#125;\n\n如果不是强制分配内存(即不是第一个数据包)，则调用sk_wmem_schedule更新内存压力，具体代码如下所示：\nstatic inline bool sk_wmem_schedule(struct sock *sk, int size)&#123;\tint delta;\t\t//对于tcp这里一定返回true\tif (!sk_has_account(sk))\t\treturn true;\t//计算超出的页数\tdelta = size - sk-&gt;sk_forward_alloc;\treturn delta &lt;= 0 || __sk_mem_schedule(sk, delta, SK_MEM_SEND);&#125;\n\nsk_wmem_schedule计首先算超出的字节数，如果存在超出的部分，则会进一步调用__sk_mem_schedule更新内存使用量，具体代码如下所示：\nint __sk_mem_schedule(struct sock *sk, int size, int kind)&#123;\t\tint ret, amt = sk_mem_pages(size);\t//前向内存分配\tsk_forward_alloc_add(sk, amt &lt;&lt; PAGE_SHIFT);\t//跟内存压力比较返回1表示成功\tret = __sk_mem_raise_allocated(sk, size, amt, kind);\tif (!ret)//0 的情况下表示失败，回滚\t\tsk_forward_alloc_add(sk, -(amt &lt;&lt; PAGE_SHIFT));\treturn ret;&#125;\n\n__sk_mem_schedule将传入的字节数转换成页数，更新前向分配的内存统计，并调用__sk_mem_raise_allocated判断是否处于内存压力，如果是内存压力状态下，直接返回失败__sk_mem_raise_allocated具体代码如下所示：\nint __sk_mem_raise_allocated(struct sock *sk, int size, int amt, int kind)&#123;\tbool memcg_charge = mem_cgroup_sockets_enabled &amp;&amp; sk-&gt;sk_memcg;\tstruct proto *prot = sk-&gt;sk_prot;\tbool charged = true;\tlong allocated;\t//批量更新内存页数\tsk_memory_allocated_add(sk, amt);\t//获取内存页数\tallocated = sk_memory_allocated(sk);\t//cgroup相关\tif (memcg_charge &amp;&amp;\t    !(charged = mem_cgroup_charge_skmem(sk-&gt;sk_memcg, amt,\t\t\t\t\t\tgfp_memcg_charge())))\t\tgoto suppress_allocation;\t/* Under limit. */\t//小于sysctl 管理内存的的地一个阈值，这里直接返回\tif (allocated &lt;= sk_prot_mem_limits(sk, 0)) &#123;\t\tsk_leave_memory_pressure(sk); //移除内存压力状态\t\treturn 1;\t&#125;\t/* Under pressure. */\t//大于全局的第2个参数\tif (allocated &gt; sk_prot_mem_limits(sk, 1))\t//进入内存压力\t\tsk_enter_memory_pressure(sk);\t/* Over hard limit. */\t//超出最大上限了\tif (allocated &gt; sk_prot_mem_limits(sk, 2))\t\tgoto suppress_allocation;\t/* guarantee minimum buffer size under pressure */\t//在有压力的情况下，确保每个 socket 都至少有最小的发送/接收能力\tif (kind == SK_MEM_RECV) &#123;\t\t//当前套接字使用的内存是否小于4k\t\tif (atomic_read(&amp;sk-&gt;sk_rmem_alloc) &lt; sk_get_rmem0(sk, prot))\t\t\treturn 1;\t&#125; else &#123; /* SK_MEM_SEND */\t\t//同样是获取4k,但是比较内存一个是队列的 一个是主机的\t\tint wmem0 = sk_get_wmem0(sk, prot);\t\tif (sk-&gt;sk_type == SOCK_STREAM) &#123;\t\t\tif (sk-&gt;sk_wmem_queued &lt; wmem0)\t\t\t\treturn 1;\t\t&#125; else if (refcount_read(&amp;sk-&gt;sk_wmem_alloc) &lt; wmem0) &#123;\t\t\t\treturn 1;\t\t&#125;\t&#125;\t//是否有内存压力\tif (sk_has_memory_pressure(sk)) &#123;\t\tu64 alloc;\t\t//是否处于内存压力之下\t\tif (!sk_under_memory_pressure(sk))\t\t\treturn 1;\t\t//当前套接字的数量\t\talloc = sk_sockets_allocated_read_positive(sk);\t\t//全局的硬限制还大于套接字使用的内存，允许分配\t\tif (sk_prot_mem_limits(sk, 2) &gt; alloc *\t\t    sk_mem_pages(sk-&gt;sk_wmem_queued +\t\t\t\t atomic_read(&amp;sk-&gt;sk_rmem_alloc) +\t\t\t\t sk-&gt;sk_forward_alloc))\t\t\treturn 1;\t&#125;suppress_allocation:\t//抑制分配 &amp; TCP 特例 发送方向\tif (kind == SK_MEM_SEND &amp;&amp; sk-&gt;sk_type == SOCK_STREAM) &#123;\t\tsk_stream_moderate_sndbuf(sk); //调小发送缓冲区\t\t/* Fail only if socket is _under_ its sndbuf.\t\t * In this case we cannot block, so that we have to fail.\t\t */\t\t//当前队列马上要超出阈值了\t\tif (sk-&gt;sk_wmem_queued + size &gt;= sk-&gt;sk_sndbuf) &#123;\t\t\t/* Force charge with __GFP_NOFAIL */\t\t\tif (memcg_charge &amp;&amp; !charged) &#123;\t\t\t\tmem_cgroup_charge_skmem(sk-&gt;sk_memcg, amt,\t\t\t\t\tgfp_memcg_charge() | __GFP_NOFAIL);\t\t\t&#125;\t\t\treturn 1;\t\t&#125;\t&#125;\tif (kind == SK_MEM_SEND || (kind == SK_MEM_RECV &amp;&amp; charged))\t\ttrace_sock_exceed_buf_limit(sk, prot, allocated, kind);\t//回滚\tsk_memory_allocated_sub(sk, amt);\tif (memcg_charge &amp;&amp; charged)\t\tmem_cgroup_uncharge_skmem(sk-&gt;sk_memcg, amt);\treturn 0;&#125;\n\n__sk_mem_raise_allocated首先将上面转换得到的页数更新到总页数中，之后获取当前使用的内存页数。\n接下来判断当前系统使用的页面数量是否超过全局内存页数（sysctl_tcp_mem）的第一个阈值，如果小于这里直接返回。\n如果大于第一个阈值，则继续跟第二个阈值进行比较，并设置进入内存压力状态。\n如果超出了第三个阈值，则直接goto suppress_allocation缩小发送缓冲区，因为已经内存压力了所以缩小缓冲。 \n如果在第二个阈值和第三个阈值之间，则根据当前套接字使用的内存量是否小于套接字内存的第一个阈值**(这里要区分全局的内存阈值和每个套接字的内存阈值)**如果小于则允许分配，否则进一步判断全局的硬限制还大于套接字使用的内存，如果大于，则允许分配\n如果上述条件都不满足，表示分配失败并回滚之前前向分配的增加的值。\n回到tcp_stream_alloc_skb中，如果申请成功并更新相关的内存记账后，设置校验和标志位，并reserve数据包，如果申请失败则直接进入内存压力状态并调小数据包缓冲区。\n回到tcp_sendmsg_locked申请数据包成功后，调用tcp_skb_entail将数据包放到队列尾部，更新sk在发送方向方向协议栈使用的总内存量,具体代码如下所示：\nvoid tcp_skb_entail(struct sock *sk, struct sk_buff *skb)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\t//开始序列号等于结束序列号\ttcb-&gt;seq     = tcb-&gt;end_seq = tp-&gt;write_seq;\t//写队列 skb 默认会带 ACK 标志\ttcb-&gt;tcp_flags = TCPHDR_ACK;\t//表示该 skb 的 head 现在只用于 TCP\t__skb_header_release(skb);\t//skb 挂到 TCP 写队列尾部\ttcp_add_write_queue_tail(sk, skb);\t//更新sk_wmem_queued\tsk_wmem_queued_add(sk, skb-&gt;truesize);\t//入队后减去sk_forward_alloc\tsk_mem_charge(sk, skb-&gt;truesize);\tif (tp-&gt;nonagle &amp; TCP_NAGLE_PUSH)\t\ttp-&gt;nonagle &amp;= ~TCP_NAGLE_PUSH;\ttcp_slow_start_after_idle_check(sk);&#125;\n\ntcp_skb_entail中首先初始化序列号，之后设置数据包的ack标志（因为是发送数据通路，因此携带ack标志），之后数据包入队，并更新sk-&gt;sk_wmem_queued(当前sk中发送方向数据包占用协议栈的总字节数)，由于此时数据包已经入队，所以调用sk_mem_charge减去之前前向分配的数值用量。并调用tcp_slow_start_after_idle_check判断是否在在超过rto时间内没有发送数据包了，如果是则会复位拥塞控制相关信息，具体代码如下所示：\nstatic inline void tcp_slow_start_after_idle_check(struct sock *sk)&#123;\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)-&gt;icsk_ca_ops;\tstruct tcp_sock *tp = tcp_sk(sk);\ts32 delta;\t//默认关闭，|| 正在发送， || 有自己的cong_ctrol bbr就有\tif (!READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_slow_start_after_idle) ||\t    tp-&gt;packets_out || ca_ops-&gt;cong_control)\t\treturn;\tdelta = tcp_jiffies32 - tp-&gt;lsndtime; //计算idle的时间\tif (delta &gt; inet_csk(sk)-&gt;icsk_rto)\t  //超过一个rto了\t\ttcp_cwnd_restart(sk, delta);&#125;void tcp_cwnd_restart(struct sock *sk, s32 delta)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 restart_cwnd = tcp_init_cwnd(tp, __sk_dst_get(sk));        //10mss\tu32 cwnd = tcp_snd_cwnd(tp);\t\t\t\t\t\t\t\t//当前的拥塞窗口\ttcp_ca_event(sk, CA_EVENT_CWND_RESTART);\t\t\t\t     //通知具体的拥塞算法，一般都没有吧 \ttp-&gt;snd_ssthresh = tcp_current_ssthresh(sk);\t\t\t\t //当前的慢启动阈值\trestart_cwnd = min(restart_cwnd, cwnd);\t\t\t\t\t\t//restart_cwnd 不要超过当前 cwnd\twhile ((delta -= inet_csk(sk)-&gt;icsk_rto) &gt; 0 &amp;&amp; cwnd &gt; restart_cwnd) //按 RTO 间隔对 cwnd 逐步减半\t\tcwnd &gt;&gt;= 1;\ttcp_snd_cwnd_set(tp, max(cwnd, restart_cwnd));\t\t\t    //更新cwnd\ttp-&gt;snd_cwnd_stamp = tcp_jiffies32;\t\t\t\t\t       //记录更新的时间戳\ttp-&gt;snd_cwnd_used = 0;&#125;\n\ntcp_cwnd_restart重新设置了当前慢启动阈值(这里好像没有改)，并重新设置拥塞窗口的大小(最大不超过10个mss)\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输出"]},{"title":"tcp_sock","url":"/2025/07/15/tcp_sock/","content":"struct tcp_sock &#123;\t/* inet_connection_sock has to be the first member of tcp_sock */\tstruct inet_connection_sock\tinet_conn;\t\t\t\t//inet_connection_sock\tu16\ttcp_header_len;\t/* Bytes of tcp header to send\t*/\t//tcp头部长度\tu16\tgso_segs;\t/* Max number of segs per GSO packet\t*///分段数量/* *\tHeader prediction flags *\t0x5?10 &lt;&lt; 16 + snd_wnd in net byte order */\t\t__be32\tpred_flags;\t\t\t\t\t\t\t\t\t//一个标志位根据首部长和ack还有窗口大小计算，用于预测是否能走快速路径收包/* *\tRFC793 variables by their proper names. This means you can *\tread the code and the spec side by side (and laugh ...) *\tSee RFC793 and RFC1122. The RFC writes these in capitals. */\tu64\tbytes_received;\t/* RFC4898 tcpEStatsAppHCThruOctetsReceived  //实际接收的字节数，也就是确认的字节数\t\t\t\t * sum(delta(rcv_nxt)), or how many bytes\t\t\t\t * were acked.\t\t\t\t */\tu32\tsegs_in;\t/* RFC4898 tcpEStatsPerfSegsIn             //接收了多少个段\t\t\t\t * total number of segments in.\t\t\t\t */\tu32\tdata_segs_in;\t/* RFC4898 tcpEStatsPerfDataSegsIn\t\t//与上面类似，表示接收了多少个有payload的段\t\t\t\t * total number of data segments in.\t\t\t\t */ \tu32\trcv_nxt;\t/* What we want to receive next \t*/       //下一个想要接收报文的序号\tu32\tcopied_seq;\t/* Head of yet unread data\t\t*/\t\t\t\t//用户进程已经读取到的位置\tu32\trcv_wup;\t/* rcv_nxt on last window update sent\t*/\t\t//上一次更新的rcv_nxt，延迟ack可能会用到 \tu32\tsnd_nxt;\t/* Next sequence we send\t\t*/\t\t\t\t//下一个待发送的序列号\tu32\tsegs_out;\t/* RFC4898 tcpEStatsPerfSegsOut\t\t\t\t\t//发出去的段数\t\t\t\t * The total number of segments sent.\t\t\t\t */\tu32\tdata_segs_out;\t/* RFC4898 tcpEStatsPerfDataSegsOut\t\t\t//发出去的段数，包括没有payload的\t\t\t\t * total number of data segments sent.\t\t\t\t */\tu64\tbytes_sent;\t/* RFC4898 tcpEStatsPerfHCDataOctetsOut\t\t\t//发送出去的字节数，不包括头部长度\t\t\t\t * total number of data bytes sent.\t\t\t\t */\tu64\tbytes_acked;\t/* RFC4898 tcpEStatsAppHCThruOctetsAcked\t//对端已经确认的字节总数\t\t\t\t * sum(delta(snd_una)), or how many bytes\t\t\t\t * were acked.\t\t\t\t */\tu32\tdsack_dups;\t/* RFC4898 tcpEStatsStackDSACKDups\t\t\t\t//重复ack的数量，可能是\t\t\t\t * total number of DSACK blocks received\t\t\t\t */ \tu32\tsnd_una;\t/* First byte we want an ack for\t*/\t\t\t//发送出去未确认的序号 \tu32\tsnd_sml;\t/* Last byte of the most recently transmitted small packet */  //发送小于mss的数据包的最后一个字节的序列号 ，nagle算法用到\tu32\trcv_tstamp;\t/* timestamp of last received ACK (for keepalives) */  //接收数据包的时间戳，tcpack中赋值\tu32\tlsndtime;\t/* timestamp of last sent data packet (for restart window) */\t\t//记录发包时间用于计算rtt\tu32\tlast_oow_ack_time;  /* timestamp of last out-of-window ACK */\t\t//收到了乱续的ack ，可以根据这个时间触发重传\t//延迟或合并 ACK 时，会将当前的 rcv_nxt（接收窗口的下一个期望序列号）暂存到 compressed_ack_rcv_nxt\tu32\tcompressed_ack_rcv_nxt;\t\t\t\t\t\t\t\t\t\t\t\tu32\ttsoffset;\t/* timestamp offset */ //tcp三次握手时间确定这个值，确保相对tcp的时间是单调递增的，计算时间戳的时候会用到\t//将 TCP Socket 挂载到全局的 tsq_tasklet 任务队列中，实现 异步批量释放发送队列内存\tstruct list_head tsq_node; /* anchor in tsq_tasklet.head list */ //把当前sock放入软中断中等待调度\tstruct list_head tsorted_sent_queue; /* time-sorted sent but un-SACKed skbs *///时间排序的已发送但未确认队列，发送的时候会将skb挂到这里\tu32\tsnd_wl1;\t/* Sequence for window update\t\t*/  //发送窗口更新时候的序列号\tu32\tsnd_wnd;\t/* The window we expect to receive\t*/\t\t//发送窗口大小\tu32\tmax_window;\t/* Maximal window ever seen from peer\t*/  //最大接收窗口值,从tcp的窗口字段找到\tu32\tmss_cache;\t/* Cached effective mss, not including SACKS */\tu32\twindow_clamp;\t/* Maximal window to advertise\t\t*/\t//最大缓冲区大小\tu32\trcv_ssthresh;\t/* Current window clamp\t\t\t*/   //慢启动阈值\tu8\tscaling_ratio;\t/* see tcp_win_from_space() */\t\t\t//窗口缩放因子\t/* Information of the most recently (s)acked skb */\tstruct tcp_rack &#123;\t\tu64 mstamp; /* (Re)sent time of the skb */ //记录数据包​​最近一次发送或重传的时间戳\t\tu32 rtt_us;  /* Associated RTT */ //通过 ACK 报文的确认时间与数据包发送时间的差值计算得到\t\tu32 end_seq; /* Ending TCP sequence of the skb */\t//​​最近被确认的数据包的结束序列号\t\tu32 last_delivered; /* tp-&gt;delivered at last reo_wnd adj */  //之前传输已被接受的数量\t\tu8 reo_wnd_steps;   /* Allowed reordering window */  //调整乱续窗口的因子，在收到dsack的时候会更新#define TCP_RACK_RECOVERY_THRESH 16\t\tu8 reo_wnd_persist:5, /* No. of recovery since last adj */ //误判丢包，增加这个值，真是丢包就减少这个值，目的是减少乱续容忍但是对丢包敏感\t\t   dsack_seen:1, /* Whether DSACK seen after last adj */\t\t\t//发送端收到重复选择ack的时候设置\t\t   advanced:1;\t /* mstamp advanced since last lost marking */  //表示更新了rack的字段？可能处罚重传？\t&#125; rack;\tu16\tadvmss;\t\t/* Advertised MSS\t\t\t*/\tu8\tcompressed_ack;\t//收到乱续包的时候会增加  不立即发送ack\tu8\tdup_ack_counter:2, \t//重复ack的数量\t\ttlp_retrans:1,\t/* TLP is a retransmission */ //Tail Loss Probe重传\t\tunused:5;\tu32\tchrono_start;\t/* Start time in jiffies of a TCP chrono */\t//测量 TCP 连接在不同状态下的耗时，例如接收窗口不足的时候\tu32\tchrono_stat[3];\t\t//当前的类型\tu8\tchrono_type:2,\t/* current chronograph type */\t//标记为应用层限速，应该是限制cwnd的增长，initsock的时候就会设这为1\t\trate_app_limited:1,  /* rate_&#123;delivered,interval_us&#125; limited? */\t//TFO 标志通过setsockopt设置，允许发syn包的时候携带数据\t\tfastopen_connect:1, /* FASTOPEN_CONNECT sockopt */\t//在没有cookie的情况下也允许syn包携带数据\t\tfastopen_no_cookie:1, /* Allow send/recv SYN+data without a cookie */\t\t/*接收端收到乱序数据包（如序列号25-30），暂存于out_of_order_queue队列，并通过SACK块（25-31）告知发送端。当接收缓冲区不足（sk_rmem_alloc超过sk_rcvbuf阈值），内核清理乱序队列（调用tcp_prune_ofo_queue），丢弃已SACK确认的数据。发送端后续收到ACK时，发现之前SACK确认的数据未被接收端最终确认（ACK未覆盖SACK范围），判定为SACK reneging，设置is_sack_reneg:1*/\t\tis_sack_reneg:1,    /* in recovery from loss with SACK reneg? */\t\tfastopen_client_fail:2; /* reason why fastopen failed */\t\t//用了3bit 禁用  cork(等待) 和push(立即发送)\tu8\tnonagle     : 4,/* Disable Nagle algorithm?             */\t//线性重传机制，而不是指数退避，setsockopt设置\t\tthin_lto    : 1,/* Use linear timeouts for thin streams */\t//用户获取msg的时候可以获取队列中剩余的字节数\t\trecvmsg_inq : 1,/* Indicate # of bytes in queue upon recvmsg */\t\t//热迁移场景发送数据包的时候会用到\t\trepair      : 1,\t\t//tcp_enter_loss中设置该标志位，用于判断虚假丢包，如果是虚假丢包，就快速撤销丢包状态\t\tfrto        : 1;/* F-RTO (RFC5682) activated in CA_Loss */\tu8\trepair_queue; //setsockopt设置，热迁移场景用到\tu8\tsave_syn:2,\t/* Save headers of SYN packet */ //setsocktopt设置，服务端收到syn包后，决定是否提取包头\t\tsyn_data:1,\t/* SYN includes data */ //syn包是否携带数据，fastopen相关\t\tsyn_fastopen:1,\t/* SYN includes Fast Open option */ //用户设置，是否启用fastopen\t\tsyn_fastopen_exp:1,/* SYN includes Fast Open exp. option */\t\tsyn_fastopen_ch:1, /* Active TFO re-enabling probe */\t\tsyn_data_acked:1,/* data in SYN is acked by SYN-ACK */ //标记syn包携带数据后，是否被对端确认\t\tis_cwnd_limited:1;/* forward progress limited by snd_cwnd? *///由于拥塞窗口无法发包\tu32\ttlp_high_seq;\t/* snd_nxt at the time of TLP */ //丢包探测报文的序号\tu32\ttcp_tx_delay;\t/* delay (in usec) added to TX packets *///setscokopt设置，transmit的时候会add_tx_dleay\tu64\ttcp_wstamp_ns;\t/* departure time for next sent data packet */ //pacing 会用到，每次发送一个数据包后会更新这个值\tu64\ttcp_clock_cache; /* cache last tcp_clock_ns() (see tcp_mstamp_refresh()) *///当前时间缓存从寄存器读出来的时间戳tcp_clock_ns，避免反复获取，就缓存起来了/* RTT measurement */ \tu64\ttcp_mstamp;\t/* most recent packet received/sent */ //收发包的时间戳也是tcp_mstamp_refresh设置的\t//计算rtt相关，清除重传队列的时候会用到下面一系列变量\tu32\tsrtt_us;\t/* smoothed round trip time &lt;&lt; 3 in usecs */ //平滑后的rtt ，会右移3位\tu32\tmdev_us;\t/* medium deviation\t\t\t*/\t//衡量rtt的波动程度\tu32\tmdev_max_us;\t/* maximal mdev for the last rtt period\t*/ //最近 1 个 RTT 周期内最大偏差\tu32\trttvar_us;\t/* smoothed mdev_max\t\t\t*/   //平滑的 RTT 偏差估值，用于最终计算 RTO\tu32\trtt_seq;\t/* sequence number to update rttvar\t*/ //计算rtt对应数据包的序列号\tstruct  minmax rtt_min;  //连接历史中的最小 RTT 值 tcp_ack中调用\tu32\tpackets_out;\t/* Packets which are &quot;in flight&quot;\t*/ //发送的时候会设置这个值， 表示发出去还没有收到ack的数量\tu32\tretrans_out;\t/* Retransmitted packets out\t\t*/ //重传的时候会加这个值，ack的处理中会减\tu32\tmax_packets_out;  /* max packets_out in last window */  //发送的时候会更新这个值，拥塞算法会用到\tu32\tcwnd_usage_seq;  /* right edge of cwnd usage tracking flight */ //同上tcp_cwnd_validate会用到\tu16\turg_data;\t/* Saved octet of OOB data and control flags */ //收到带外数据的时候会设置这个字段？\tu8\tecn_flags;\t/* ECN status bits.\t\t\t*/      //  发送syn包的时候开启ecn会设置这个字段，或者收到网络中的ecn包\tu8\tkeepalive_probes; /* num of allowed keep alive probes\t*/  //set设置，保活探测超过这个设置就直接error\tu32\treordering;\t/* Packet reordering metric.\t\t*/     //乱序容忍度，newreno会用到，其他地方还会用到吗？\tu32\treord_seen;\t/* number of data packet reordering events */   //乱序包的数量，rack会用到\tu32\tsnd_up;\t\t/* Urgent pointer\t\t*/    //发送方会设置，oob数据的序号/* *      Options received (usually on last packet, some only on SYN packets). */\tstruct tcp_options_received rx_opt; //存储接收到的tcp选项，时间戳，scak，mss/* *\tSlow start and congestion control (see also Nagle, and Karn &amp; Partridge) */ \tu32\tsnd_ssthresh;\t/* Slow start size threshold\t\t*/ //snd_ssthresh慢启动阈值 \tu32\tsnd_cwnd;\t/* Sending congestion window\t\t*/  //发送放能发多少个mss\tu32\tsnd_cwnd_cnt;\t/* Linear increase counter\t\t*/\t//tcpack中处理拥塞的时候会根据ack设置并使用该值如bic\tu32\tsnd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */  //拥塞窗口的最大值\tu32\tsnd_cwnd_used;\t\t\t\t\t\t\t\t\t\t// 等于packetout 为发送出去还没有确认的包数\tu32\tsnd_cwnd_stamp;\t\t\t\t\t\t\t\t\t\t// 每次调整拥塞窗口的时间戳，在发送方调整拥塞窗口的时候会用到\tu32\tprior_cwnd;\t/* cwnd right before starting loss recovery */ //enterloss 的时候记录丢包前的值\tu32\tprr_delivered;\t/* Number of newly delivered packets to  //影响cwnd sack的数量会决定这个值\t\t\t\t * receiver in Recovery. */\tu32\tprr_out;\t/* Total number of pkts sent during Recovery. *///快恢复状态中发包数量，也包括重传的\tu32\tdelivered;\t/* Total data packets delivered incl. rexmits */\t//被确认的ack总数，包括重传确认的，拥塞控制会用到\tu32\tdelivered_ce;\t/* Like the above but only ECE marked packets */\t\t//ack中带有ece的总数\tu32\tlost;\t\t/* Total data packets lost incl. rexmits */\t\t\t//丢包总数，多个地方会设置\tu32\tapp_limited;\t/* limited until &quot;delivered&quot; reaches this val */  //是否受限与应用程序？发送的时候也会设置 bbr算法会用到，初始化时不为0 收到数据包后会修改这个值\tu64\tfirst_tx_mstamp;  /* start of window send phase */\t\t\t//发送的时候记录的时间戳  ，ack中也会用到，供拥塞算法使用？\tu64\tdelivered_mstamp; /* time we reached &quot;delivered&quot; */\t\t\t\t\t//bbr算法使用发送的时候会设置，处理ack的时候也会设置\tu32\trate_delivered;    /* saved rate sample: packets delivered */\t\t//用于计算发速率 tcp_rate_gen 中被设置\tu32\trate_interval_us;  /* saved rate sample: time elapsed */\t\t\t\t//时间间隔 同上， 这两个字段都是为了计算tcp的实施传输速率 ss命令可以获取 \tu32\trcv_wnd;\t/* Current receiver window\t\t*/\t\t\t\t\t\t//接收窗口的大小\tu32\twrite_seq;\t/* Tail(+1) of data held in tcp send buffer */\t\t\t//发送缓冲区中最后一个字节的下一个序列号\tu32\tnotsent_lowat;\t/* TCP_NOTSENT_LOWAT */\t\t\t\t\t\t\t\t//通过set设置的一个低水位线，会和未发送的字节比较，如果小于，就表示内存不足，好像发送就会阻塞？\tu32\tpushed_seq;\t/* Last pushed seq, required to talk to windows */\t\t//设置push标志位的时候的序列号\tu32\tlost_out;\t/* Lost packets\t\t\t*/\t\t\t\t\t\t\t\t//丢包的数量\tu32\tsacked_out;\t/* SACK&#x27;d packets\t\t\t*/\t\t\t\t\t\t\t//sack确认包的数量\tstruct hrtimer\tpacing_timer;\t\t\t\t\t\t\t\t\t\t\t//tsq的定时器，init tcp_sock的时候会设置，注意：软中的也调用这个注册的函数在free的时候\tstruct hrtimer\tcompressed_ack_timer;\t\t\t\t\t\t\t\t\t//延迟sack定时器。\t/* from STCP, retrans queue hinting */\t\t\t\tstruct sk_buff* lost_skb_hint;\t\t\t\t\t\t\t\t\t\t\t//定位第一个丢失的包 ，优化性能？\tstruct sk_buff *retransmit_skb_hint;\t\t\t\t\t\t\t\t\t//下一个要重传的数据包\t/* OOO segments go in this rbtree. Socket lock must be held. */\t\t\t//tcp的乱续队列\tstruct rb_root\tout_of_order_queue;\t\t\t\t\t\t\t\t\t\t\tstruct sk_buff\t*ooo_last_skb; /* cache rb_last(out_of_order_queue) */\t//乱续队列中欧给你最后一个数据包\t/* SACKs data, these 2 need to be together (see tcp_options_write) */\tstruct tcp_sack_block duplicate_sack[1]; /* D-SACK block */\t\t\t\t\t\tstruct tcp_sack_block selective_acks[4]; /* The SACKS themselves*/\t\t\t//发送端收到sack\tstruct tcp_sack_block recv_sack_cache[4];\t\t\t\t\t\t\t\t\t//接收端生成sack\tstruct sk_buff *highest_sack;   /* skb just after the highest\t\t\t\t//已经被接收段确认的最高序列号的数据包？？？\t\t\t\t\t * skb with SACKed bit set\t\t\t\t\t * (validity guaranteed only if\t\t\t\t\t * sacked_out &gt; 0)\t\t\t\t\t */\tint     lost_cnt_hint;\t\t\t\t\t\t\t\t\t\t\t\t\t//处理重传队列用到，一个数量，是丢包标记前未处理的数据包数量\tu32\tprior_ssthresh; /* ssthresh saved at recovery start\t*/\t\t\t\t//保存原来的慢启动阈值\tu32\thigh_seq;\t/* snd_nxt at onset of congestion\t*/\t\t\t\t\t//进入拥塞控制状态后下一个待发送的序列号\tu32\tretrans_stamp;\t/* Timestamp of the last retransmit,\t\t\t\t\t//最后一个重传的时间戳\t\t\t\t * also used in SYN-SENT to remember stamp of\t\t\t\t * the first SYN. */\tu32\tundo_marker;\t/* snd_una upon a new recovery episode. */\t\t\t//记录的是序号，enterloss或者回复时候保存的una\tint\tundo_retrans;\t/* number of undoable retransmissions. */\t\t\t\t//记录进入恢复阶段时的重传包数，用于判断是否真是丢包？？未确认重传包数量\tu64\tbytes_retrans;\t/* RFC4898 tcpEStatsPerfOctetsRetrans\t\t\t\t\t//重传skb字节书\t\t\t\t * Total data bytes retransmitted\t\t\t\t */\tu32\ttotal_retrans;\t/* Total retransmits for entire connection */\t\t\t\t//总计重传数和上面的一样__tcp_retransmit_skb中++\tu32\turg_seq;\t/* Seq of received urgent pointer */\t\t\t\t\t\t\t//指向紧急数据的序列号\tunsigned int\t\tkeepalive_time;\t  /* time before keep alive takes place */\t//保活时间不活跃后多久探测一次\tunsigned int\t\tkeepalive_intvl;  /* time interval between keep alive probes */ //探测失败后多久在探测一次\tint\t\t\tlinger2;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//finwait2状态的保持时间/* Sock_ops bpf program related variables */#ifdef CONFIG_BPF\tu8\tbpf_sock_ops_cb_flags;  /* Control calling BPF programs\t\t\t\t\t * values defined in uapi/linux/tcp.h\t\t\t\t\t */\tu8\tbpf_chg_cc_inprogress:1; /* In the middle of\t\t\t\t\t  * bpf_setsockopt(TCP_CONGESTION),\t\t\t\t\t  * it is to avoid the bpf_tcp_cc-&gt;init()\t\t\t\t\t  * to recur itself by calling\t\t\t\t\t  * bpf_setsockopt(TCP_CONGESTION, &quot;itself&quot;).\t\t\t\t\t  */#define BPF_SOCK_OPS_TEST_FLAG(TP, ARG) (TP-&gt;bpf_sock_ops_cb_flags &amp; ARG)#else#define BPF_SOCK_OPS_TEST_FLAG(TP, ARG) 0#endif\tu16 timeout_rehash;\t/* Timeout-triggered rehash attempts */\t\t\t\t\t//超时处理中会重设hash值，表示重设hash值的次数\tu32 rcv_ooopack; /* Received out-of-order packets, for tcpinfo */          //乱续数据包总数/* Receiver side RTT estimation */\tu32 rcv_rtt_last_tsecr;\tstruct &#123;\t\tu32\trtt_us;\t\tu32\tseq;\t\tu64\ttime;\t&#125; rcv_rtt_est;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//接收方的rtt,处理ack的时候会用到/* Receiver queue space */\tstruct &#123;\t\tu32\tspace;\t\tu32\tseq;\t\tu64\ttime;\t&#125; rcvq_space;/* TCP-specific MTU probe information. */\tstruct &#123;\t\tu32\t\t  probe_seq_start;\t\tu32\t\t  probe_seq_end;\t&#125; mtu_probe;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//mtu探测的消息\tu32     plb_rehash;     /* PLB-triggered rehash attempts */\t\t\t\t\t\t//拥塞算法会用到\tu32\tmtu_info; /* We received an ICMP_FRAG_NEEDED / ICMPV6_PKT_TOOBIG\t\t\t//没发现哪里会用到\t\t\t   * while socket was owned by user.\t\t\t   */#if IS_ENABLED(CONFIG_MPTCP)\tbool\tis_mptcp;#endif#if IS_ENABLED(CONFIG_SMC)\tbool\t(*smc_hs_congested)(const struct sock *sk);\tbool\tsyn_smc;\t/* SYN includes SMC */#endif#ifdef CONFIG_TCP_MD5SIG/* TCP AF-Specific parts; only used by MD5 Signature support so far */\tconst struct tcp_sock_af_ops\t*af_specific;/* TCP MD5 Signature Option information */\tstruct tcp_md5sig_info\t__rcu *md5sig_info;#endif/* TCP fastopen related information */\tstruct tcp_fastopen_request *fastopen_req;\t\t\t\t\t\t\t\t//管理TFO的结构，sendmsgtfo会设置里面的字段\t/* fastopen_rsk points to request_sock that resulted in this big\t * socket. Used to retransmit SYNACKs etc.\t */\tstruct request_sock __rcu *fastopen_rsk;\t\t\t\t\t\t\t\t//requset sock\tstruct saved_syn *saved_syn;\t\t\t\t\t\t\t\t\t\t\t//服务端收到syn包时候保存的syn包信息&#125;;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","sock"]},{"title":"TCP输出 tcp_write_xmit（一）","url":"/2025/11/25/tcp_write_xmit(%E4%B8%80)/","content":"tcp_write_xmit() 是 TCP 协议栈中负责真正“把待发送数据从发送队列写到网络上”的核心调度函数，它依据拥塞窗口、接收窗口、Nagle、TSO&#x2F;GSO、 pacing、MTU 探测等策略，决定是否发送、如何发送以及发送多少数据\n注意：除了进程上下文中通过push_one, __tcp_push_pending_frames, tcp_push会调用tcp_write_xmit发送报文，软中段上下文中，TLP定时器到期，TSQ机制被触发，或者tcp_data_snd_check等地方也会调用改接口尝试发送数据包。\ntcp_write_xmit具体逻辑如下所示：\nstatic bool tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,\t\t\t   int push_one, gfp_t gfp)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb;\tunsigned int tso_segs, sent_pkts;\tint cwnd_quota;\tint result;\tbool is_cwnd_limited = false, is_rwnd_limited = false;\tu32 max_segs;\tsent_pkts = 0;\t//更新两个时间戳\ttcp_mstamp_refresh(tp);\tif (!push_one) &#123;\t\t/* Do MTU probing. */\t\t//是否需要进行tcp的mtu探\t\tresult = tcp_mtu_probe(sk);\t\tif (!result) &#123;\t\t\treturn false;\t\t&#125; else if (result &gt; 0) &#123;\t\t\tsent_pkts = 1;\t\t&#125;\t&#125;\t//根据rtt拥塞算法等计算段数 \tmax_segs = tcp_tso_segs(sk, mss_now);\twhile ((skb = tcp_send_head(sk))) &#123;\t\tunsigned int limit;\t\tif (unlikely(tp-&gt;repair) &amp;&amp; tp-&gt;repair_queue == TCP_SEND_QUEUE) &#123;\t\t\t/* &quot;skb_mstamp_ns&quot; is used as a start point for the retransmit timer */\t\t\ttp-&gt;tcp_wstamp_ns = tp-&gt;tcp_clock_cache;\t\t\tskb_set_delivery_time(skb, tp-&gt;tcp_wstamp_ns, true);\t\t\tlist_move_tail(&amp;skb-&gt;tcp_tsorted_anchor, &amp;tp-&gt;tsorted_sent_queue);\t\t\ttcp_init_tso_segs(skb, mss_now);\t\t\tgoto repair; /* Skip network transmission */\t\t&#125;\t\t//根据发送速率计算是否可以立即发送这里可能会直接break\t\tif (tcp_pacing_check(sk))\t\t\tbreak;\t\t//根据mss计算每个skb有多少个段\t\ttso_segs = tcp_init_tso_segs(skb, mss_now);\t\tBUG_ON(!tso_segs);\t\t//计算拥塞窗口的大小\t\tcwnd_quota = tcp_cwnd_test(tp, skb);\t\tif (!cwnd_quota) &#123;\t\t\tif (push_one == 2) //这里表示的是TLP探测包，必须能发出去\t\t\t\t/* Force out a loss probe pkt. */\t\t\t\tcwnd_quota = 1;\t\t\telse //拥塞窗口不够用了，直接返回\t\t\t\tbreak;\t\t&#125;\t\t//是否超出了对端窗口的边界，如果超出了则设置标志位\t\tif (unlikely(!tcp_snd_wnd_test(tp, skb, mss_now))) &#123;\t\t\tis_rwnd_limited = true;\t\t\tbreak;\t\t&#125;\t\t//是否是一个小包\t\tif (tso_segs == 1) &#123;\t\t\t//判断是否因为nagle而不发送数据包，注意这里unlikely的\t\t\tif (unlikely(!tcp_nagle_test(tp, skb, mss_now,\t\t\t//如果不是最后一个数据包，就立即推送，合理 因为不能然后面的包不发吧\t\t\t\t\t\t     (tcp_skb_is_last(sk, skb) ? \t\t\t\t\t\t      nonagle : TCP_NAGLE_PUSH))))\t\t\t\tbreak;\t\t&#125; else &#123;\t\t//如果当前数据包有多个段，且不是发送一个数据包的情况下，判断是否由于tso 推迟发送\t\t\tif (!push_one &amp;&amp;\t\t\t    tcp_tso_should_defer(sk, skb, &amp;is_cwnd_limited,\t\t\t\t\t\t &amp;is_rwnd_limited, max_segs))\t\t\t\tbreak;\t\t&#125;\t\t\t\tlimit = mss_now;\t\t//多个段的情况\t\tif (tso_segs &gt; 1 &amp;&amp; !tcp_urg_mode(tp))\t\t\t//基于对端窗口和nagle算法，根据对端窗口和拥塞窗口 返回的是实际需要的长度，注意这里是拥塞状态也直接发送了\t\t\tlimit = tcp_mss_split_point(sk, skb, mss_now,\t\t\t\t\t\t    min_t(unsigned int,\t\t\t\t\t\t\t  cwnd_quota,  //拥塞窗口的大小\t\t\t\t\t\t\t  max_segs),  //最大的段数，与数据包无关\t\t\t\t\t\t    nonagle);\t\t//如果数据包的长度大于了窗口的限制，那这里要把直接封装的数据包拆分了\t\tif (skb-&gt;len &gt; limit &amp;&amp;\t\t    unlikely(tso_fragment(sk, skb, limit, mss_now, gfp))) //这里其实就是为什么负载用页去管理的原因！！！\t\t\tbreak;\t\t//因为TSQ机制直接返回\t\tif (tcp_small_queue_check(sk, skb, 0))\t\t\tbreak;\t\t/* Argh, we hit an empty skb(), presumably a thread\t\t * is sleeping in sendmsg()/sk_stream_wait_memory().\t\t * We do not want to send a pure-ack packet and have\t\t * a strange looking rtx queue with empty packet(s).\t\t */\t\t//空包\t\tif (TCP_SKB_CB(skb)-&gt;end_seq == TCP_SKB_CB(skb)-&gt;seq)\t\t\tbreak;\t\t//真正的发送数据包\t\tif (unlikely(tcp_transmit_skb(sk, skb, 1, gfp)))\t\t\tbreak;repair:\t\t/* Advance the send_head.  This one is sent out.\t\t * This call will increment packets_out.\t\t */\t\t//更新下一个待发送的序列号 ，加入重传队列\t\ttcp_event_new_data_sent(sk, skb);\t\t//计算发送小包序列号，nagle算法会设置\t\ttcp_minshall_update(tp, mss_now, skb);\t\tsent_pkts += tcp_skb_pcount(skb); //计算发出去的段数\t\tif (push_one)\t\t\tbreak;\t&#125;\t//更新各个阶段的时间\tif (is_rwnd_limited)\t\ttcp_chrono_start(sk, TCP_CHRONO_RWND_LIMITED);\telse\t\ttcp_chrono_stop(sk, TCP_CHRONO_RWND_LIMITED);\t//拥塞窗口受限\tis_cwnd_limited |= (tcp_packets_in_flight(tp) &gt;= tcp_snd_cwnd(tp));\tif (likely(sent_pkts || is_cwnd_limited))\t\ttcp_cwnd_validate(sk, is_cwnd_limited);\tif (likely(sent_pkts)) &#123;\t\tif (tcp_in_cwnd_reduction(sk))\t\t\ttp-&gt;prr_out += sent_pkts;\t\t/* Send one loss probe per tail loss episode. */\t\t//激活tlp定时器，如果等于2表示已经激活过了\t\tif (push_one != 2)\t\t\ttcp_schedule_loss_probe(sk, false);\t\treturn false;\t&#125;\treturn !tp-&gt;packets_out &amp;&amp; !tcp_write_queue_empty(sk);&#125;\n\n这里需要关注tcp_write_xmit中第三个参数和第四个参数：\n nonagle 用来控制是否绕过 Nagle 算法、是否允许立刻发送小包（例如 TCP_NAGLE_OFF 直接允许发送，TCP_NAGLE_CORK 则尽量攒包）； push_one 则控制本次调用最多发送多少个报文，当为 0 时按正常逻辑尽量多发，当为 1 时最多只发一个报文，而当为 2 时可以临时忽略拥塞窗口限制强制发出去（典型用于 TLP 探测）。\ntcp_write_xmit首先更新了时间戳字段，bbr算法或者rto计算中会用到。之后会调用tcp_mtu_probe判断是否需要进行MTU探测（只发送一个数据包的情况下肯定不探测了），注意这里是传输层面的MTU探测，区别于ICMP的MTU探测，具体代码如下所示：\nstatic int tcp_mtu_probe(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb, *nskb, *next;\tstruct net *net = sock_net(sk);\tint probe_size;\tint size_needed;\tint copy, len;\tint mss_now;\tint interval;\t/* Not currently probing/verifying,\t * not in recovery,\t * have enough cwnd, and\t * not SACKing (the variable headers throw things off)\t */\tif (likely(!icsk-&gt;icsk_mtup.enabled ||                 //MTU 探测功能已启用 \t\t   icsk-&gt;icsk_mtup.probe_size ||\t\t\t\t   //当前没有正在进行的探测\t\t   inet_csk(sk)-&gt;icsk_ca_state != TCP_CA_Open ||   //TCP 连接处于 Open 状态\t\t   tcp_snd_cwnd(tp) &lt; 11 ||\t\t\t\t\t\t   //拥塞窗口足够大\t\t   tp-&gt;rx_opt.num_sacks || tp-&gt;rx_opt.dsack))\t   //没有 SACK 数据\t\treturn -1;\t/* Use binary search for probe_size between tcp_mss_base,\t * and current mss_clamp. if (search_high - search_low)\t * smaller than a threshold, backoff from probing.\t */\t//计算当前mss\tmss_now = tcp_current_mss(sk);\t//二分查找计算探测的大小\tprobe_size = tcp_mtu_to_mss(sk, (icsk-&gt;icsk_mtup.search_high +\t\t\t\t    icsk-&gt;icsk_mtup.search_low) &gt;&gt; 1);\t//确认后面还有足够多的数据，否则可能就没法确认探测包了？\tsize_needed = probe_size + (tp-&gt;reordering + 1) * tp-&gt;mss_cache;\t//计算最大最小探测直接的差值\tinterval = icsk-&gt;icsk_mtup.search_high - icsk-&gt;icsk_mtup.search_low;\t/* When misfortune happens, we are reprobing actively,\t * and then reprobe timer has expired. We stick with current\t * probing process by not resetting search range to its orignal.\t */\t//probesize太大 或者搜索区间已经小于系统配置的探测阈值\tif (probe_size &gt; tcp_mtu_to_mss(sk, icsk-&gt;icsk_mtup.search_high) ||\t    interval &lt; READ_ONCE(net-&gt;ipv4.sysctl_tcp_probe_threshold)) &#123;\t\t/* Check whether enough time has elaplased for\t\t * another round of probing.\t\t */\t\t//根据时间判断是否需要更新探测的上限\t\ttcp_mtu_check_reprobe(sk);\t\treturn -1;\t&#125;\t/* Have enough data in the send queue to probe? */\t//计算待发送的数据量，如果不足以探测，直接返回\tif (tp-&gt;write_seq - tp-&gt;snd_nxt &lt; size_needed)\t\treturn -1;\t//对端通告的窗口太小，也直接返回\tif (tp-&gt;snd_wnd &lt; size_needed)\t\treturn -1;\t//计算是否超出了窗口的右边界\tif (after(tp-&gt;snd_nxt + size_needed, tcp_wnd_end(tp)))\t\treturn 0;\t/* Do we need to wait to drain cwnd? With none in flight, don&#x27;t stall */\t//在途数据包大于拥塞窗口\tif (tcp_packets_in_flight(tp) + 2 &gt; tcp_snd_cwnd(tp)) &#123;\t\tif (!tcp_packets_in_flight(tp)) //没有在途数据包？上面拥塞窗口很小？\t\t\treturn -1;\t\telse\t\t\treturn 0; //\t&#125;\t//检查数据包是否可以合并\tif (!tcp_can_coalesce_send_queue_head(sk, probe_size))\t\treturn -1;\t/* We&#x27;re allowed to probe.  Build it now. */\t//申请一个skb\tnskb = tcp_stream_alloc_skb(sk, GFP_ATOMIC, false);\tif (!nskb)\t\treturn -1;\t/* build the payload, and be prepared to abort if this fails. */\t//这里是将发送队列中的数据包，零拷贝的方式放到探测的数据包中\tif (tcp_clone_payload(sk, nskb, probe_size)) &#123;\t\ttcp_skb_tsorted_anchor_cleanup(nskb);\t\tconsume_skb(nskb);\t\treturn -1;\t&#125;\t//这里要重新更新内存记账\tsk_wmem_queued_add(sk, nskb-&gt;truesize);\tsk_mem_charge(sk, nskb-&gt;truesize);\t//取出队列中的地一个数据包\tskb = tcp_send_head(sk);\tskb_copy_decrypted(nskb, skb);\tmptcp_skb_ext_copy(nskb, skb);\t//设置数据包的序列号和ack标志\tTCP_SKB_CB(nskb)-&gt;seq = TCP_SKB_CB(skb)-&gt;seq;\tTCP_SKB_CB(nskb)-&gt;end_seq = TCP_SKB_CB(skb)-&gt;seq + probe_size;\tTCP_SKB_CB(nskb)-&gt;tcp_flags = TCPHDR_ACK;\t//插入队列头，并\ttcp_insert_write_queue_before(nskb, skb, sk);\ttcp_highest_sack_replace(sk, skb, nskb);\tlen = 0;\t//把被和合并的小包unlink\ttcp_for_write_queue_from_safe(skb, next, sk) &#123;\t\tcopy = min_t(int, skb-&gt;len, probe_size - len);\t\tif (skb-&gt;len &lt;= copy) &#123;\t\t\t/* We&#x27;ve eaten all the data from this skb.\t\t\t * Throw it away. */\t\t\tTCP_SKB_CB(nskb)-&gt;tcp_flags |= TCP_SKB_CB(skb)-&gt;tcp_flags;\t\t\t/* If this is the last SKB we copy and eor is set\t\t\t * we need to propagate it to the new skb.\t\t\t */\t\t\tTCP_SKB_CB(nskb)-&gt;eor = TCP_SKB_CB(skb)-&gt;eor;\t\t\ttcp_skb_collapse_tstamp(nskb, skb);\t\t\ttcp_unlink_write_queue(skb, sk); //从队列中摘下来\t\t\ttcp_wmem_free_skb(sk, skb);\t\t //释放数据包，更新内存记账 \t\t&#125; else &#123;\t\t\t///处理部分skb的情况 调用__pskb_trim_head减枝\t\t\tTCP_SKB_CB(nskb)-&gt;tcp_flags |= TCP_SKB_CB(skb)-&gt;tcp_flags &amp;\t\t\t\t\t\t   ~(TCPHDR_FIN|TCPHDR_PSH);\t\t\t__pskb_trim_head(skb, copy);\t\t\ttcp_set_skb_tso_segs(skb, mss_now); //设置保留数据包的GSO相关字段\t\t\tTCP_SKB_CB(skb)-&gt;seq += copy;\t\t&#125;\t\tlen += copy;\t\tif (len &gt;= probe_size)\t\t\tbreak;\t&#125;\t//初始化GSO字段信息\ttcp_init_tso_segs(nskb, nskb-&gt;len);\t/* We&#x27;re ready to send.  If this fails, the probe will\t * be resegmented into mss-sized pieces by tcp_write_xmit().\t */\tif (!tcp_transmit_skb(sk, nskb, 1, GFP_ATOMIC)) &#123;\t\t/* Decrement cwnd here because we are sending\t\t * effectively two packets. */\t\ttcp_snd_cwnd_set(tp, tcp_snd_cwnd(tp) - 1);//更新拥塞窗口--\t\ttcp_event_new_data_sent(sk, nskb);\t\ticsk-&gt;icsk_mtup.probe_size = tcp_mss_to_mtu(sk, nskb-&gt;len);//记录mtu探测的相关字段\t\ttp-&gt;mtu_probe.probe_seq_start = TCP_SKB_CB(nskb)-&gt;seq;\t\ttp-&gt;mtu_probe.probe_seq_end = TCP_SKB_CB(nskb)-&gt;end_seq;\t\treturn 1;\t&#125;\treturn -1;&#125;\n\ntcp_mtu_probe中首先判断是否不需要进行MTU探测，如果MTU 探测功能已启用 ，当前没有正在进行的探测，TCP 连接处于 Open 状态，拥塞窗口足够大，没有 SACK 数据的情况下不需要探测，否则进入MTU探测的逻辑。\n首先计算当前的mss，之后根据根据二分查找计算probe_size探测报文的大小，这里注意search_high和 low在tcp_mtup_init中会设置初始值，计算完成探测报文的大小后，会进一步确认后续数据是否足够数据确认的这个探测报文。\n之后进一步判断探测的报文是否合法，或者搜索区间已经小于系统配置的探测阈值，如果不合法则会调用tcp_mtu_check_reprobe处理是否需要修改探测的参数，具体代码如下所示：\nstatic inline void tcp_mtu_check_reprobe(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct net *net = sock_net(sk);\tu32 interval;\ts32 delta;\t//默认是60s\tinterval = READ_ONCE(net-&gt;ipv4.sysctl_tcp_probe_interval);\t//间隔的时间已经大于60s了\tdelta = tcp_jiffies32 - icsk-&gt;icsk_mtup.probe_timestamp;\tif (unlikely(delta &gt;= interval * HZ)) &#123;\t\tint mss = tcp_current_mss(sk); //计算mss\t\t/* Update current search range */\t\ticsk-&gt;icsk_mtup.probe_size = 0; //表示没有探测这个你在进行\t\ticsk-&gt;icsk_mtup.search_high = tp-&gt;rx_opt.mss_clamp +\t\t\tsizeof(struct tcphdr) +\t\t\ticsk-&gt;icsk_af_ops-&gt;net_header_len; //计算重新设置搜索范围的上限\t\ticsk-&gt;icsk_mtup.search_low = tcp_mss_to_mtu(sk, mss);//计算当前mss\t\t/* Update probe time stamp */\t\ticsk-&gt;icsk_mtup.probe_timestamp = tcp_jiffies32; //记录更新时间\t&#125;&#125;\n\n如果计算得到的探测报文合法，则进一步确定是否有足够数据进行探测，以及当前要发送的字节数是否小于对端通告的窗口大小，或者超出了可以发送的右边界，如果满足条件，会停止探测或者等待一会在探测。如果当前在途数据包大于发送的拥塞窗口大小，则也不进行探测。\n如果上述条件都不满足，则表示开始构造探测的报文了，首先申请一个skb，调用tcp_clone_payload将发送队列中的数据包，零拷贝的方式放到探测的数据包中，之后更新内存记账信息，然后从队列头部取出一个数据包并设置序列号和结束序列号，之后插入队列头部，由于当前探测数据包的payload来自于原本队列中，因此接下里会将原始数据包unlink掉，并进行trim。\n上述可以理解为探测的数据包就是由发送队列中的数据包拼接或者裁剪得到，处理完成后，调用tcp_init_tso_segs初始化GSO字段等信息，并调用tcp_transmit_skb完成发送之后记录探测报文的序号，在清理重传队列中会根据这个序号进行处理。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输出"]},{"title":"TCP输出 tcp_write_xmit（三）","url":"/2025/11/30/tcp_write_xmit(%E4%B8%89)/","content":"tcp_write_xmit中判断完是否会由于nagle算法和tso推迟发送后接下来会基于对端窗口和nagle算法调用tcp_mss_split_point计算一次发包的最大长度（这个值在下面会和skb的len比较，如果小于len就会拆包）具体代码如下所示：\nstatic unsigned int tcp_mss_split_point(const struct sock *sk,\t\t\t\t\tconst struct sk_buff *skb,\t\t\t\t\tunsigned int mss_now,\t\t\t\t\tunsigned int max_segs,\t\t\t\t\tint nonagle)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\tu32 partial, needed, window, max_len;\twindow = tcp_wnd_end(tp) - TCP_SKB_CB(skb)-&gt;seq; //接收窗口剩余空间\tmax_len = mss_now * max_segs;\t\t\t\t\t//这里是cwnd\t//如果小于可用窗口的大小，且不是最后一个数据包，直接返回\tif (likely(max_len &lt;= window &amp;&amp; skb != tcp_write_queue_tail(sk)))\t\treturn max_len;\t//计算实际需要长度\tneeded = min(skb-&gt;len, window);\t//最大长度检查\tif (max_len &lt;= needed)\t\treturn max_len;\t//计算最后一段是否是不完整的 MSS\tpartial = needed % mss_now;\t/* If last segment is not a full MSS, check if Nagle rules allow us\t * to include this last segment in this skb.\t * Otherwise, we&#x27;ll split the skb at last MSS boundary\t */\t//nagle算法检查\tif (tcp_nagle_check(partial != 0, tp, nonagle))\t\treturn needed - partial;\t//返回实际需要的长度\treturn needed;&#125;\n\ntcp_mss_split_point首先计算对端接收窗口的剩余空间，然后计算拥塞窗口剩余空间，如果拥塞窗口大小小于对端通告窗口大小，且不是最后一个数据包，直接返回拥塞窗口大小，表示直接发送一个大包，注意通常都走这里。\n否则需要计算真正可发送的长度（受窗口限制），实际可发送最大长度&#x3D; min(当前包长度, 对端窗口剩余) 如果这个值大于拥塞窗口的大小，那就发送拥塞窗口大小的长度，否则计算最后没有对齐的一部分的大小，并进行nagle算法的判断。如果需要 Nagle则返回 needed - partial\n上述计算完成可以发送的大小后，则需要和当前数据包的长度进行比较，如果数据包的长度超过了上述计算的长度，则需要调用tso_fragment在四层就进行分段，具体代码如下所示：\nstatic int tso_fragment(struct sock *sk, struct sk_buff *skb, unsigned int len,\t\t\tunsigned int mss_now, gfp_t gfp)&#123;\tint nlen = skb-&gt;len - len; //新包的长度（剩余部分）\tstruct sk_buff *buff;\tu8 flags;\t/* All of a TSO frame must be composed of paged data.  */\t//这里很重要，负载必须要在非线性部分\tDEBUG_NET_WARN_ON_ONCE(skb-&gt;len != skb-&gt;data_len);\t//申请一个数据包\tbuff = tcp_stream_alloc_skb(sk, gfp, true);\tif (unlikely(!buff))\t\treturn -ENOMEM;\tskb_copy_decrypted(buff, skb);\tmptcp_skb_ext_copy(buff, skb);\t//更新内存记账 \tsk_wmem_queued_add(sk, buff-&gt;truesize);\tsk_mem_charge(sk, buff-&gt;truesize);\tbuff-&gt;truesize += nlen;\tskb-&gt;truesize -= nlen;\t/* Correct the sequence numbers. */\t//设置序列号\tTCP_SKB_CB(buff)-&gt;seq = TCP_SKB_CB(skb)-&gt;seq + len;\tTCP_SKB_CB(buff)-&gt;end_seq = TCP_SKB_CB(skb)-&gt;end_seq;\tTCP_SKB_CB(skb)-&gt;end_seq = TCP_SKB_CB(buff)-&gt;seq;\t/* PSH and FIN should only be set in the second packet. */\t//设置标志位\tflags = TCP_SKB_CB(skb)-&gt;tcp_flags;\tTCP_SKB_CB(skb)-&gt;tcp_flags = flags &amp; ~(TCPHDR_FIN | TCPHDR_PSH);\tTCP_SKB_CB(buff)-&gt;tcp_flags = flags;\ttcp_skb_fragment_eor(skb, buff);\t//真正的分割数据包\tskb_split(skb, buff, len);\ttcp_fragment_tstamp(skb, buff);\t/* Fix up tso_factor for both original and new SKB.  */\t//重新计算GSO信息\ttcp_set_skb_tso_segs(skb, mss_now);\ttcp_set_skb_tso_segs(buff, mss_now);\t/* Link BUFF into the send queue. */\t__skb_header_release(buff);\t//新 SKB 插入到原 SKB 之后的发送队列中！\ttcp_insert_write_queue_after(skb, buff, sk, TCP_FRAG_IN_WRITE_QUEUE);\treturn 0;&#125;\n\ntso_fragment的逻辑中上来就先判断判断负载是否都只存在于非线性部分，这里就是数据包为什么需要用page来管理的关键，当需要拆分数据包时，仅通过操作指针就可以完成！\ntso_fragment中首先申请数据包，之后更新内存记账等相关字段，并设置当前数据包的序列号和标志位，调用skb_split完成执行 SKB 数据分页拆分，调用tcp_set_skb_tso_segs重新设置GSO字段，之后将新 skb 插入到原 skb 之后的发送队列中，更新GSO字段的代码如下所示：\nstatic void tcp_set_skb_tso_segs(struct sk_buff *skb, unsigned int mss_now)&#123;//设置tcp_gso_size 和gso_segs\tif (skb-&gt;len &lt;= mss_now) &#123;\t\t/* Avoid the costly divide in the normal\t\t * non-TSO case.\t\t */\t\t//设置段数为1\t\ttcp_skb_pcount_set(skb, 1);\t\t//设置gso size\t\tTCP_SKB_CB(skb)-&gt;tcp_gso_size = 0;\t&#125; else &#123;\t\ttcp_skb_pcount_set(skb, DIV_ROUND_UP(skb-&gt;len, mss_now)); //设置有多少个段\t\tTCP_SKB_CB(skb)-&gt;tcp_gso_size = mss_now; //设置每个段有多少个字节\t&#125;&#125;\n\n完成数据包的拆分后(如果需要拆分)，则**接下来会判断是否因TSQ机制(限制单个套接字占用协议栈过多资源)，**暂停发送，具体代码如下所示：\nstatic bool tcp_small_queue_check(struct sock *sk, const struct sk_buff *skb,\t\t\t\t  unsigned int factor)&#123;\tunsigned long limit;\t//取两者中的 较大值，作为limit\tlimit = max_t(unsigned long, \t\t      2 * skb-&gt;truesize,//当前数据包大小的两倍，2个帧\t\t      sk-&gt;sk_pacing_rate &gt;&gt; READ_ONCE(sk-&gt;sk_pacing_shift));\t\t\t //默认是10 ，1ms能发送的字节数目\t//如果没有启用流量整形，使用系统配置的默认输出限制,bbr就不会进入这个分支\tif (sk-&gt;sk_pacing_status == SK_PACING_NONE)\t\tlimit = min_t(unsigned long, limit,\t\t\t      READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_limit_output_bytes))\t//默认 16 × 64K把 limit 封顶\tlimit &lt;&lt;= factor;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t//重传因子调整 发送通路中是0\tif (static_branch_unlikely(&amp;tcp_tx_delay_enabled) &amp;&amp;   \t\t\t\t\t\t//false\t    tcp_sk(sk)-&gt;tcp_tx_delay) &#123;\t\tu64 extra_bytes = (u64)sk-&gt;sk_pacing_rate * tcp_sk(sk)-&gt;tcp_tx_delay;\t\t/* TSQ is based on skb truesize sum (sk_wmem_alloc), so we\t\t * approximate our needs assuming an ~100% skb-&gt;truesize overhead.\t\t * USEC_PER_SEC is approximated by 2^20.\t\t * do_div(extra_bytes, USEC_PER_SEC/2) is replaced by a right shift.\t\t */\t\textra_bytes &gt;&gt;= (20 - 1);\t\tlimit += extra_bytes;\t&#125;\t//当前sk 中协议栈的数据包已经超过了限制\tif (refcount_read(&amp;sk-&gt;sk_wmem_alloc) &gt; limit) &#123;\t\t/* Always send skb if rtx queue is empty or has one skb.\t\t * No need to wait for TX completion to call us back,\t\t * after softirq/tasklet schedule.\t\t * This helps when TX completions are delayed too much.\t\t */\t\t//如果重传队列为空或只有一个包，允许发送\t\tif (tcp_rtx_queue_empty_or_single_skb(sk))\t\t\treturn false;\t\t//标记该 socket 已被 TSQ 机制限制\t\tset_bit(TSQ_THROTTLED, &amp;sk-&gt;sk_tsq_flags);\t\t/* It is possible TX completion already happened\t\t * before we set TSQ_THROTTLED, so we must\t\t * test again the condition.\t\t */\t\tsmp_mb__after_atomic();\t\tif (refcount_read(&amp;sk-&gt;sk_wmem_alloc) &gt; limit)\t\t\treturn true;\t&#125;\treturn false;&#125;\n\ntcp_small_queue_check 是TSQ机制的核心，通过监控单个 socket 已经交给网卡队列&#x2F;qdisc 的内存量（sk_wmem_alloc），结合 skb 大小、pacing 速率和系统上限，动态决定是否暂时阻塞该 socket 的后续发送，在不影响重传的前提下有效缩短队列、改善 RTT、公平性。上述触发TSQ的机制是设置了TSQ_THROTTLEDSKB的析构函数中会判断这个标志位，如果set了则会调度TSQ的软中段任务，在软中断上下文中发送数据包。\n通过TSQ机制检查后会调用tcp_transmit_skb完成正真的发包，之后调用tcp_event_new_data_sent更新一系列字段，并加入重传队列，具体代码如下所示：\nstatic void tcp_event_new_data_sent(struct sock *sk, struct sk_buff *skb)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tunsigned int prior_packets = tp-&gt;packets_out;\tWRITE_ONCE(tp-&gt;snd_nxt, TCP_SKB_CB(skb)-&gt;end_seq); //更新下一个待发送的序列号\t__skb_unlink(skb, &amp;sk-&gt;sk_write_queue);       //unlink\ttcp_rbtree_insert(&amp;sk-&gt;tcp_rtx_queue, skb);   //加入重传队列\tif (tp-&gt;highest_sack == NULL)\t\ttp-&gt;highest_sack = skb; \t\t\t\t\t\t//设置highsack\ttp-&gt;packets_out += tcp_skb_pcount(skb); \t\t\t//计算段数 \tif (!prior_packets || icsk-&gt;icsk_pending == ICSK_TIME_LOSS_PROBE)\t\ttcp_rearm_rto(sk);\t\t\t\t\t\t\t\t//没有在途中的数据包\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPORIGDATASENT,\t\t      tcp_skb_pcount(skb));\t//如果有空间了，是否唤醒用户态的进程\ttcp_check_space(sk);&#125;\n\ntcp_event_new_data_sent在 TCP 把一个 skb 正式发送出去后，更新 snd_nxt、将数据包加入重传队列、增加在途计数、重新设置 RTO 定时器、统计发送量，并在可能的情况下唤醒被发送缓冲区限制的应用。\n接下进入tcp_write_xmit的收尾工作，while大循环结束后首先记录记录接收窗口限制时间段，然后判断是否拥塞窗口受限，如果是的或则调用tcp_cwnd_validate进行塞窗口验证和状态时间记录，具体代码如下所示：\nstatic void tcp_cwnd_validate(struct sock *sk, bool is_cwnd_limited)&#123;\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)-&gt;icsk_ca_ops;\tstruct tcp_sock *tp = tcp_sk(sk);\t/* Track the strongest available signal of the degree to which the cwnd\t * is fully utilized. If cwnd-limited then remember that fact for the\t * current window. If not cwnd-limited then track the maximum number of\t * outstanding packets in the current window. (If cwnd-limited then we\t * chose to not update tp-&gt;max_packets_out to avoid an extra else\t * clause with no functional impact.)\t */\tif (!before(tp-&gt;snd_una, tp-&gt;cwnd_usage_seq) ||   //判断是不是“新的一轮窗口周期”\t    is_cwnd_limited ||\t\t\t\t\t\t\t\t\t\t\t//是否是 cwnd-limited\t    (!tp-&gt;is_cwnd_limited &amp;&amp;\t\t\t\t\t\t\t\t\t//非 cwnd-limited 但看到了更大的在途包数量\t     tp-&gt;packets_out &gt; tp-&gt;max_packets_out)) &#123;\t\ttp-&gt;is_cwnd_limited = is_cwnd_limited;\t\t\t\t\t\t//设置cwnd\t\ttp-&gt;max_packets_out = tp-&gt;packets_out;\t\t\t\t\t\t//设置周期内最大在途数量\t\ttp-&gt;cwnd_usage_seq = tp-&gt;snd_nxt;\t\t\t\t\t\t\t//计算这一个周期起始序列号的位置\t&#125;\tif (tcp_is_cwnd_limited(sk)) &#123;\t\t\t\t\t\t\t\t\t//判断is_cwnd_limited 网络被喂满\t\t/* Network is feed fully. */\t\ttp-&gt;snd_cwnd_used = 0;\t\t\t\t\t\t\t\t\t\t//表示从现在开始重新记录\t\ttp-&gt;snd_cwnd_stamp = tcp_jiffies32;\t&#125; else &#123;\t\t/* Network starves. */\t\tif (tp-&gt;packets_out &gt; tp-&gt;snd_cwnd_used)\t\t\t\t\t//如果当前在途包数 packets_out 比之前记录的 snd_cwnd_used 更大，就更新\t\t\ttp-&gt;snd_cwnd_used = tp-&gt;packets_out;\t\t\t\t\t\t\tif (READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_slow_start_after_idle) &amp;&amp; //默认是0，通常不进入这个分支\t\t    (s32)(tcp_jiffies32 - tp-&gt;snd_cwnd_stamp) &gt;= inet_csk(sk)-&gt;icsk_rto &amp;&amp;  //相当于长时间用户没有给大量的数据了\t\t    !ca_ops-&gt;cong_control)\t\t\ttcp_cwnd_application_limited(sk);\t\t\t\t\t\t//cwnd太大了 用不了这么多 变小cwnd\t\t/* The following conditions together indicate the starvation\t\t * is caused by insufficient sender buffer:\t\t * 1) just sent some data (see tcp_write_xmit)\t\t * 2) not cwnd limited (this else condition)\t\t * 3) no more data to send (tcp_write_queue_empty())\t\t * 4) application is hitting buffer limit (SOCK_NOSPACE)\t\t */\t\tif (tcp_write_queue_empty(sk) &amp;&amp; sk-&gt;sk_socket &amp;&amp;   \t\t    test_bit(SOCK_NOSPACE, &amp;sk-&gt;sk_socket-&gt;flags) &amp;&amp;\t\t    (1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT))\t\t\ttcp_chrono_start(sk, TCP_CHRONO_SNDBUF_LIMITED);    //记录这个时间，表示是发送端用户发送速率不够\t&#125;&#125;\n\n最后，如果本轮发包中成功发送了数据，并且当前显示拥塞控制或者快恢复状态则记录发出去的包鼠，并启动TLP定时器.\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输出"]},{"title":"TCP输出 tcp_write_xmit（二）","url":"/2025/11/27/tcp_write_xmit(%E4%BA%8C)/","content":"tcp_write_xmit在进入while循环遍历发送数据包之前，首先计算max_segs的大小，这里需要注意，tcp_sendmsg_locked中只是尽可能的将用户数据copy放到一个skb中，而接下来的工作是真正决定是否能发送，能发送多少。\n首先调用tcp_tso_segs计算当前数据包的最大段数(max_segs根据rtt计算)，最大段数和当前的mss决定了一次while循环最多能发送多少数据，具体代码如下所示：\nstatic u32 tcp_tso_segs(struct sock *sk, unsigned int mss_now)&#123;\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)-&gt;icsk_ca_ops;\tu32 min_tso, tso_segs;\t//拥塞算法是否提供了钩子，bbr就提供了，否则使用默认的，通常为2 \tmin_tso = ca_ops-&gt;min_tso_segs ?\t\t\tca_ops-&gt;min_tso_segs(sk) :\t\t\tREAD_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_min_tso_segs);\t//根据rtt计算段数 \ttso_segs = tcp_tso_autosize(sk, mss_now, min_tso);\treturn min_t(u32, tso_segs, sk-&gt;sk_gso_max_segs);&#125;\n\ntcp_tso_segs进一步调用tcp_tso_autosize根据rtt自适应调整段数的大小，具体代码如下所示：\nstatic u32 tcp_tso_autosize(const struct sock *sk, unsigned int mss_now,\t\t\t    int min_tso_segs)&#123;\tunsigned long bytes;\tu32 r;\t//转换成每秒多少字节\tbytes = sk-&gt;sk_pacing_rate &gt;&gt; READ_ONCE(sk-&gt;sk_pacing_shift);\t//基于rtt做调整，r可以理解为延迟的严重程度吧\tr = tcp_min_rtt(tcp_sk(sk)) &gt;&gt; READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_tso_rtt_log);//默认是9 标志512是一个合理的网络延迟变化粒度\tif (r &lt; BITS_PER_TYPE(sk-&gt;sk_gso_max_size))//这是什么意思？\t\tbytes += sk-&gt;sk_gso_max_size &gt;&gt; r; //这里的意思就是延迟越大，max_size 越小\t//钳制\tbytes = min_t(unsigned long, bytes, sk-&gt;sk_gso_max_size);\t//计算段数 \treturn max_t(u32, bytes / mss_now, min_tso_segs);&#125;\n\n上述代码根据带宽估算和 RTT 动态决定 GSO 报文大小：RTT 越大网络越远，单个 GSO包越小；RTT 越小网络越近，GSO越大以减少 CPU 负载\n计算最大段数之后之后进入主要流程，循环从队列头部中获取数据包。\n首先调用tcp_pacing_check，判断是否可以立即发送数据包，tcp_pacing_check中首先判断是否需要tcp内部自己做pacing，下一个数据包要发送的时间小于等于当前的时钟（理想发包时间已经到了 ），则可以直接发送，不需要启动定时器，否则表示时间还没有到，返回true表示不立即发送，注意：这里启动了pacing定时器，上述具体代码如下所示：\nstatic bool tcp_pacing_check(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//sch_fq 开启这个就会返回false，表示不需要tcp内部自己pacing ，反之，如果为brr 会继续向下执行\tif (!tcp_needs_internal_pacing(sk))\t\treturn false;\t//下一个数据包要发送的时间小于等于当前的时钟，则可以直接发送，不需要启动定时器\tif (tp-&gt;tcp_wstamp_ns &lt;= tp-&gt;tcp_clock_cache)\t\treturn false;\t//如果定时器还没在排队，就启动一个pacing定时器 如果不能发送的情况 这里用软中断上下文去发包\tif (!hrtimer_is_queued(&amp;tp-&gt;pacing_timer)) &#123;\t\thrtimer_start(&amp;tp-&gt;pacing_timer,\t\t\t      ns_to_ktime(tp-&gt;tcp_wstamp_ns),\t\t\t      HRTIMER_MODE_ABS_PINNED_SOFT);\t\tsock_hold(sk);//这里引用计数++，防止socket被提前释放\t&#125;\treturn true;&#125;\n\n如果可以发送，则调用tcp_init_tso_segs根据当前数据包的总长度和mss计算当前数据包的段数，这里注意上面是根据rtt计算最大的段数，这里计算的是当前数据包的段数，不矛盾，具体代码如下所示：\nstatic int tcp_init_tso_segs(struct sk_buff *skb, unsigned int mss_now)&#123;\tint tso_segs = tcp_skb_pcount(skb);\t//没有计算过TSO的段数，或者mss值不同了\tif (!tso_segs || (tso_segs &gt; 1 &amp;&amp; tcp_skb_mss(skb) != mss_now)) &#123;\t\ttcp_set_skb_tso_segs(skb, mss_now);//重新计算GSO相关字段\t\ttso_segs = tcp_skb_pcount(skb);\t&#125;\treturn tso_segs;&#125;\n\n上述代码首先判断是否已经计算过了（比如之前计算过了，但是没有立即发送的情况），如果没有计算过或者mss变化则调用tcp_set_skb_tso_segs进行计算具体代码如下所示：\nstatic void tcp_set_skb_tso_segs(struct sk_buff *skb, unsigned int mss_now)&#123;//设置tcp_gso_size 和gso_segs\tif (skb-&gt;len &lt;= mss_now) &#123;\t\t/* Avoid the costly divide in the normal\t\t * non-TSO case.\t\t */\t\t//设置段数为1\t\ttcp_skb_pcount_set(skb, 1);\t\t//设置gso size\t\tTCP_SKB_CB(skb)-&gt;tcp_gso_size = 0;\t&#125; else &#123;\t\ttcp_skb_pcount_set(skb, DIV_ROUND_UP(skb-&gt;len, mss_now)); //设置有多少个段\t\tTCP_SKB_CB(skb)-&gt;tcp_gso_size = mss_now; //设置每个段有多少个字节\t&#125;&#125;\n\n计算完成当前数据包的的段数之后，调用tcp_cwnd_test计算当前拥塞窗口的大小，具体代码如下所示：\nstatic inline unsigned int tcp_cwnd_test(const struct tcp_sock *tp,\t\t\t\t\t const struct sk_buff *skb)&#123;\tu32 in_flight, cwnd, halfcwnd;\t/* Don&#x27;t be strict about the congestion window for the final FIN.  */\t//FIN包的特殊处理\tif ((TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN) &amp;&amp;\t    tcp_skb_pcount(skb) == 1)\t\treturn 1;\t//计算在途中的数据包\tin_flight = tcp_packets_in_flight(tp);\t//拿到拥塞窗口大小\tcwnd = tcp_snd_cwnd(tp);\tif (in_flight &gt;= cwnd)\t\treturn 0;\t/* For better scheduling, ensure we have at least\t * 2 GSO packets in flight.\t */\t//cwnd 的一半，因为想要保守一点吧\thalfcwnd = max(cwnd &gt;&gt; 1, 1U);\treturn min(halfcwnd, cwnd - in_flight); //取一个最小值&#125;\n\n上述代码用来判断 还能不能根据拥塞窗口规则发送数据。 如果窗口满了返回 0，如果能发则限制最多不超过 cwnd - in_flight 和 cwnd/2 之间的最小值，目的是既不能超过拥塞窗口限制也不能一次性把窗口全部塞满造成 burst\n计算可用的窗口大小后，如果窗口大小不足则直接返回，这里注意，对于TLP数据包，即使窗口不足也可以发送成功。\n接下来判断当前数据的结束序列号是否超出了当前已经发送未确认的序号加上对端通告窗口大小的序号（也就是结束序列号是否超过了传窗口的右边界），如果超过了则设置is_rwnd_limited（表示接收窗口受限），并直接退出发包，上述代码如下所示：\n/* Does at least the first segment of SKB fit into the send window? */static bool tcp_snd_wnd_test(const struct tcp_sock *tp,\t\t\t     const struct sk_buff *skb,\t\t\t     unsigned int cur_mss)&#123;\tu32 end_seq = TCP_SKB_CB(skb)-&gt;end_seq;\tif (skb-&gt;len &gt; cur_mss)\t\tend_seq = TCP_SKB_CB(skb)-&gt;seq + cur_mss;\t//是否超出了对端通告窗口的边界\treturn !after(end_seq, tcp_wnd_end(tp));&#125;\n\n接下来判断是小包的情况，调用tcp_nagle_test判断是否会因为nagle算法而导致不立即发包，注意这里unlikly,通常不会因为nagle而停止发包，具体判读逻辑如下所示：\nif (unlikely(!tcp_nagle_test(tp, skb, mss_now,\t\t\t//如果不是最后一个数据包，就立即推送，合理 因为不能然后面的包不发吧\t\t\t\t\t\t     (tcp_skb_is_last(sk, skb) ? \t\t\t\t\t\t      nonagle : TCP_NAGLE_PUSH))))static inline bool tcp_nagle_test(const struct tcp_sock *tp, const struct sk_buff *skb,\t\t\t\t  unsigned int cur_mss, int nonagle)&#123;\t/* Nagle rule does not apply to frames, which sit in the middle of the\t * write_queue (they have no chances to get new data).\t *\t * This is implemented in the callers, where they modify the &#x27;nonagle&#x27;\t * argument based upon the location of SKB in the send queue.\t */\tif (nonagle &amp; TCP_NAGLE_PUSH) //参数指定了，允许立即发送\t\treturn true; \t/* Don&#x27;t use the nagle rule for urgent data (or for the final FIN). */\t//如果有紧急数据和fin包，则也立即发送\tif (tcp_urg_mode(tp) || (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN))\t\treturn true;\t//进一步判断是否不立即发生\tif (!tcp_nagle_check(skb-&gt;len &lt; cur_mss, tp, nonagle))\t\treturn true;\treturn false;&#125;static bool tcp_nagle_check(bool partial, const struct tcp_sock *tp,\t\t\t    int nonagle)&#123;\treturn partial &amp;&amp;\t\t\t\t\t\t\t\t\t\t//一个小包\t\t((nonagle &amp; TCP_NAGLE_CORK) ||\t\t\t\t\t\t//用户cork了\t\t (!nonagle &amp;&amp; tp-&gt;packets_out &amp;&amp; tcp_minshall_check(tp))); //默认情况，有在途中的数据同时有小包等待确认&#125;\n\n上述代码可以看到**，如果当前的小包（注意小包的定义是小于mss的就算小包）不是最后一个或者是fin包和紧急数据，就立即发送**。\n否则用传入tcp_write_xmit的nagle参数做为参数继续计算，调用tcp_nagle_check判断是否需要攒包，如果这是个小包（不足一个 MSS），并且应用 CORK 了或者有在途中的数据同时有小包等待确认。则不立即发送，直接返回。\n如果不是一个小包，则会调用tcp_tso_should_defer对大包进行处理判断是否会因为TSO而直接返回，这里本质上可以理解为G&#x2F;TSO 版的 Nagle 算法,具体代码如下所示：\nstatic bool tcp_tso_should_defer(struct sock *sk, struct sk_buff *skb,\t\t\t\t bool *is_cwnd_limited,\t\t\t\t bool *is_rwnd_limited,\t\t\t\t u32 max_segs)&#123;\tconst struct inet_connection_sock *icsk = inet_csk(sk);\tu32 send_win, cong_win, limit, in_flight;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *head;\tint win_divisor;\ts64 delta;\t//如果处于拥塞恢复状态，立即发送\tif (icsk-&gt;icsk_ca_state &gt;= TCP_CA_Recovery)\t\tgoto send_now;\t/* Avoid bursty behavior by allowing defer\t * only if the last write was recent (1 ms).\t * Note that tp-&gt;tcp_wstamp_ns can be in the future if we have\t * packets waiting in a qdisc or device for EDT delivery.\t */\t//如果上次发送超过 1ms，立即发送（避免突发）\tdelta = tp-&gt;tcp_clock_cache - tp-&gt;tcp_wstamp_ns - NSEC_PER_MSEC;\tif (delta &gt; 0)\t\tgoto send_now;\t//在图数据包\tin_flight = tcp_packets_in_flight(tp);\tBUG_ON(tcp_skb_pcount(skb) &lt;= 1);\tBUG_ON(tcp_snd_cwnd(tp) &lt;= in_flight);\t//可以发送的窗口大小\tsend_win = tcp_wnd_end(tp) - TCP_SKB_CB(skb)-&gt;seq;\t/* From in_flight test above, we know that cwnd &gt; in_flight.  */\t//拥塞窗口的大小\tcong_win = (tcp_snd_cwnd(tp) - in_flight) * tp-&gt;mss_cache;\t//取上述两者最小\tlimit = min(send_win, cong_win);\t/* If a full-sized TSO skb can be sent, do it. */\t//空间够大直接发送\tif (limit &gt;= max_segs * tp-&gt;mss_cache)\t\tgoto send_now;\t/* Middle in queue won&#x27;t get any more data, full sendable already? */\t//不是最后一个包，但是大于这个包的大小，立即发送\tif ((skb != tcp_write_queue_tail(sk)) &amp;&amp; (limit &gt;= skb-&gt;len))\t\tgoto send_now;\twin_divisor = READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_tso_win_divisor);//默认是3\tif (win_divisor) &#123;\t\tu32 chunk = min(tp-&gt;snd_wnd, tcp_snd_cwnd(tp) * tp-&gt;mss_cache);//计算发送窗口和拥塞窗口的最小值\t\t/* If at least some fraction of a window is available,\t\t * just use it.\t\t */\t\tchunk /= win_divisor; //可发送窗口大小/3\t\tif (limit &gt;= chunk) //1/win_divisor 的窗口空间可用，就立即发送\t\t\tgoto send_now;\t&#125; else &#123;\t\t/* Different approach, try not to defer past a single\t\t * ACK.  Receiver should ACK every other full sized\t\t * frame, so if we have space for more than 3 frames\t\t * then send now.\t\t */\t\tif (limit &gt; tcp_max_tso_deferred_mss(tp) * tp-&gt;mss_cache)\t\t\tgoto send_now;\t&#125;\t/* TODO : use tsorted_sent_queue ? */\t//重传队列不是空，立即发送\thead = tcp_rtx_queue_head(sk);\tif (!head)\t\tgoto send_now;\t//计算重传包已发送的时间 delta\tdelta = tp-&gt;tcp_clock_cache - head-&gt;tstamp;\t/* If next ACK is likely to come too late (half srtt), do not defer */\t//表示不希望因为继续延迟导致 ACK 间隔被拉长\tif ((s64)(delta - (u64)NSEC_PER_USEC * (tp-&gt;srtt_us &gt;&gt; 4)) &lt; 0)\t\tgoto send_now;\t/* Ok, it looks like it is advisable to defer.\t * Three cases are tracked :\t * 1) We are cwnd-limited\t * 2) We are rwnd-limited\t * 3) We are application limited.\t */\t//拥塞窗口是瓶颈\tif (cong_win &lt; send_win) &#123;\t\tif (cong_win &lt;= skb-&gt;len) &#123;\t\t\t*is_cwnd_limited = true; //置位\t\t\treturn true;\t\t&#125;\t&#125; else &#123;\t//接收窗口限制\t\tif (send_win &lt;= skb-&gt;len) &#123;\t\t\t*is_rwnd_limited = true;//职位\t\t\treturn true;\t\t&#125;\t&#125;\t/* If this packet won&#x27;t get more data, do not wait. */\t//FIN包直接发送\tif ((TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN) ||\t    TCP_SKB_CB(skb)-&gt;eor)\t\tgoto send_now;\treturn true;send_now:\treturn false;&#125;\n\n上述代码可以分为几种情况：\n1.如果是拥塞状态，直接发，因为要优先保证重传等可靠性\n2.如果是举例上次已经超过1ms了则也立即发送，避免突然发送\n3.计算拥塞窗口 和 基于对端通告窗口计算可以发送空间， 并取两者中的最小值。如果这个值大于最大段数mss则立即发送。如果是最后一个包，但是大于这个包的大小，立即发送*\n4.根据 tcp_tso_win_divisor 看窗口比例，算出当前总的可用窗口min(rwnd, cwnd)再取其中 1/N，如果当前可发空间 limit 已经大于这块子窗口就认为窗口已经够大，可以立即发送。\n5.结合重传队列和 RTT, 如果这个重传队列头刚发不久（离预计 ACK 时间还早），希望因为继续 defer TSO 导致 ACK 间隔被拉长 &#x2F; RTT 样本失真，立即发送\n如果上述条件都不满足则表示：\n\n不在恢复态\n\n没超过 1ms 的“静默”\n\n窗口不大到可以随意发满一个 TSO\n\n重传和 RTT 时序还 ok\n\n也不是中间 skb \n此时认为延迟一下是可以接受的，同时记录时cwnd限制还是rwnd限制\n\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输出"]},{"title":"TCP输出 __tcp_transmit_skb(一)","url":"/2025/12/07/tcp%E5%8F%91%E5%8C%85tcp_transmit_skb/","content":"__tcp_transmit_skb为TCP发送路径上交给IP层的最后一个处理函数，主要完成如下工作：\n设置时间戳，构建头部各个字段，TCP选项，设置skb属于哪个sk，设置析构函数，更新内存记账。\n计算通告给对端的窗口大小，计算伪首部，更新统计字段，数据包的GSO字段。\n调用ip层发包函数，更新拥塞算法用到的时间戳字段等等，具体代码如下所示：\nstatic int __tcp_transmit_skb(struct sock *sk, struct sk_buff *skb,\t\t\t      int clone_it, gfp_t gfp_mask, u32 rcv_nxt)&#123;\tconst struct inet_connection_sock *icsk = inet_csk(sk);\tstruct inet_sock *inet;\tstruct tcp_sock *tp;\tstruct tcp_skb_cb *tcb;\tstruct tcp_out_options opts;\tunsigned int tcp_options_size, tcp_header_size;\tstruct sk_buff *oskb = NULL;\tstruct tcp_md5sig_key *md5;\tstruct tcphdr *th;\tu64 prior_wstamp;\tint err;\tBUG_ON(!skb || !tcp_skb_pcount(skb));\ttp = tcp_sk(sk);\t//发包前记录时间戳\tprior_wstamp = tp-&gt;tcp_wstamp_ns;\t//确保时间单调不减？\ttp-&gt;tcp_wstamp_ns = max(tp-&gt;tcp_wstamp_ns, tp-&gt;tcp_clock_cache);\t//给qisc用\tskb_set_delivery_time(skb, tp-&gt;tcp_wstamp_ns, true);\tif (clone_it) &#123; //需要重传的情况\t\toskb = skb; //原skb的指针\t\ttcp_skb_tsorted_save(oskb) &#123;          \t//临时把 skb 内部 _skb_refdst 字段清空\t\t\tif (unlikely(skb_cloned(oskb)))//克隆数据包\t\t\t\tskb = pskb_copy(oskb, gfp_mask);\t\t\telse\t\t\t\tskb = skb_clone(oskb, gfp_mask);\t\t&#125; tcp_skb_tsorted_restore(oskb);\t\t//还原\t\tif (unlikely(!skb))\t\t\treturn -ENOBUFS;\t\t/* retransmit skbs might have a non zero value in skb-&gt;dev\t\t * because skb-&gt;dev is aliased with skb-&gt;rbnode.rb_left\t\t */\t\tskb-&gt;dev = NULL;\t&#125;\tinet = inet_sk(sk);\ttcb = TCP_SKB_CB(skb);\tmemset(&amp;opts, 0, sizeof(opts));\t//syn 包构建选项走这里\tif (unlikely(tcb-&gt;tcp_flags &amp; TCPHDR_SYN)) &#123;\t\ttcp_options_size = tcp_syn_options(sk, skb, &amp;opts, &amp;md5);\t&#125; else &#123;\t//正常数据包选项走这里\t\ttcp_options_size = tcp_established_options(sk, skb, &amp;opts,\t\t\t\t\t\t\t   &amp;md5);\t\t/* Force a PSH flag on all (GSO) packets to expedite GRO flush\t\t * at receiver : This slightly improve GRO performance.\t\t * Note that we do not force the PSH flag for non GSO packets,\t\t * because they might be sent under high congestion events,\t\t * and in this case it is better to delay the delivery of 1-MSS\t\t * packets and thus the corresponding ACK packet that would\t\t * release the following packet.\t\t */\t\t//如果是GSO包则促使对端更快地把数据交给上层\t\tif (tcp_skb_pcount(skb) &gt; 1)\t\t\ttcb-&gt;tcp_flags |= TCPHDR_PSH;\t&#125;\ttcp_header_size = tcp_options_size + sizeof(struct tcphdr);\t/* We set skb-&gt;ooo_okay to one if this packet can select\t * a different TX queue than prior packets of this flow,\t * to avoid self inflicted reorders.\t * The &#x27;other&#x27; queue decision is based on current cpu number\t * if XPS is enabled, or sk-&gt;sk_txhash otherwise.\t * We can switch to another (and better) queue if:\t * 1) No packet with payload is in qdisc/device queues.\t *    Delays in TX completion can defeat the test\t *    even if packets were already sent.\t * 2) Or rtx queue is empty.\t *    This mitigates above case if ACK packets for\t *    all prior packets were already processed.\t */\t//是否容许这个 skb 选择和前面不同的 TX queue，协议栈几乎没包，或者重传队列为空\tskb-&gt;ooo_okay = sk_wmem_alloc_get(sk) &lt; SKB_TRUESIZE(1) ||\t\t\ttcp_rtx_queue_empty(sk);\t/* If we had to use memory reserve to allocate this skb,\t * this might cause drops if packet is looped back :\t * Other socket might not have SOCK_MEMALLOC.\t * Packets not looped back do not care about pfmemalloc.\t */\tskb-&gt;pfmemalloc = 0;\t//设置指针\tskb_push(skb, tcp_header_size);\tskb_reset_transport_header(skb);\t//清除之前的 skb-&gt;sk 等 owner 信息\tskb_orphan(skb);\tskb-&gt;sk = sk;\t//是否是纯ack，后者析构中会判断TSQ并设置 调度TSQ\tskb-&gt;destructor = skb_is_tcp_pure_ack(skb) ? __sock_wfree : tcp_wfree;\t//增加协议栈使用的内存\trefcount_add(skb-&gt;truesize, &amp;sk-&gt;sk_wmem_alloc);\t///标识路由是否需要验证，收到ack的地方会置位\tskb_set_dst_pending_confirm(skb, sk-&gt;sk_dst_pending_confirm);\t/* Build TCP header and checksum it. */\t//设置tcp头部字段\tth = (struct tcphdr *)skb-&gt;data;\tth-&gt;source\t\t= inet-&gt;inet_sport;\tth-&gt;dest\t\t= inet-&gt;inet_dport;\tth-&gt;seq\t\t\t= htonl(tcb-&gt;seq);\tth-&gt;ack_seq\t\t= htonl(rcv_nxt);\t*(((__be16 *)th) + 6)\t= htons(((tcp_header_size &gt;&gt; 2) &lt;&lt; 12) |\t\t\t\t\ttcb-&gt;tcp_flags);\tth-&gt;check\t\t= 0;\tth-&gt;urg_ptr\t\t= 0;\t/* The urg_mode check is necessary during a below snd_una win probe */\t//urg 几乎不会使用吧\tif (unlikely(tcp_urg_mode(tp) &amp;&amp; before(tcb-&gt;seq, tp-&gt;snd_up))) &#123;\t\tif (before(tp-&gt;snd_up, tcb-&gt;seq + 0x10000)) &#123;\t\t\tth-&gt;urg_ptr = htons(tp-&gt;snd_up - tcb-&gt;seq);\t\t\tth-&gt;urg = 1;\t\t&#125; else if (after(tcb-&gt;seq + 0xFFFF, tp-&gt;snd_nxt)) &#123;\t\t\tth-&gt;urg_ptr = htons(0xFFFF);\t\t\tth-&gt;urg = 1;\t\t&#125;\t&#125;\t//GSO的类型\tskb_shinfo(skb)-&gt;gso_type = sk-&gt;sk_gso_type;\tif (likely(!(tcb-&gt;tcp_flags &amp; TCPHDR_SYN))) &#123;\t//注意这里算的通告给对端的窗口大小\t\tth-&gt;window      = htons(tcp_select_window(sk));\t\ttcp_ecn_send(sk, skb, th, tcp_header_size);\t&#125; else &#123;\t\t/* RFC1323: The window in SYN &amp; SYN/ACK segments\t\t * is never scaled.\t\t */\t\tth-&gt;window\t= htons(min(tp-&gt;rcv_wnd, 65535U));\t&#125;\t//设置tcp选项\ttcp_options_write(th, tp, &amp;opts);#ifdef CONFIG_TCP_MD5SIG\t/* Calculate the MD5 hash, as we have all we need now */\tif (md5) &#123;\t\tsk_gso_disable(sk);\t\ttp-&gt;af_specific-&gt;calc_md5_hash(opts.hash_location,\t\t\t\t\t       md5, sk, skb);\t&#125;#endif\t/* BPF prog is the last one writing header option */\t//bpf相关\tbpf_skops_write_hdr_opt(sk, skb, NULL, NULL, 0, &amp;opts);\t//计算校验和，注意这里只计算伪首部\tINDIRECT_CALL_INET(icsk-&gt;icsk_af_ops-&gt;send_check,\t\t\t   tcp_v6_send_check, tcp_v4_send_check,\t\t\t   sk, skb);\t//处理快速ack\tif (likely(tcb-&gt;tcp_flags &amp; TCPHDR_ACK))\t\ttcp_event_ack_sent(sk, rcv_nxt);\t//存在负载\tif (skb-&gt;len != tcp_header_size) &#123;\t\ttcp_event_data_sent(tp, sk);\t\ttp-&gt;data_segs_out += tcp_skb_pcount(skb);\t\ttp-&gt;bytes_sent += skb-&gt;len - tcp_header_size;\t&#125;\tif (after(tcb-&gt;end_seq, tp-&gt;snd_nxt) || tcb-&gt;seq == tcb-&gt;end_seq)\t\tTCP_ADD_STATS(sock_net(sk), TCP_MIB_OUTSEGS,\t\t\t      tcp_skb_pcount(skb));\t//累计发出去的段数\ttp-&gt;segs_out += tcp_skb_pcount(skb);\t//选怎发送队列用到\tskb_set_hash_from_sk(skb, sk);\t/* OK, its time to fill skb_shinfo(skb)-&gt;gso_&#123;segs|size&#125; */\t//把段数和mss给到skb\tskb_shinfo(skb)-&gt;gso_segs = tcp_skb_pcount(skb);\tskb_shinfo(skb)-&gt;gso_size = tcp_skb_mss(skb);\t/* Leave earliest departure time in skb-&gt;tstamp (skb-&gt;skb_mstamp_ns) */\t/* Cleanup our debris for IP stacks */\tmemset(skb-&gt;cb, 0, max(sizeof(struct inet_skb_parm),\t\t\t       sizeof(struct inet6_skb_parm)));\t//设置一个时间戳\ttcp_add_tx_delay(skb, tp);\t//向ip层发送数据包\terr = INDIRECT_CALL_INET(icsk-&gt;icsk_af_ops-&gt;queue_xmit,\t\t\t\t inet6_csk_xmit, ip_queue_xmit,\t\t\t\t sk, skb, &amp;inet-&gt;cork.fl);\t//如果本地发包失败了，则进入显示拥塞状态\tif (unlikely(err &gt; 0)) &#123;\t\ttcp_enter_cwr(sk);\t\terr = net_xmit_eval(err);\t&#125;\tif (!err &amp;&amp; oskb) &#123;\t\t//更新pacing相关和rack相关用到的时间\t\ttcp_update_skb_after_send(sk, oskb, prior_wstamp);\t\t//更新bbr用到的字段\t\ttcp_rate_skb_sent(sk, oskb);\t&#125;\treturn err;&#125;\n\n__tcp_transmit_skb中首先将tp的时间戳字段给到skb(TC可能会用到？）如果这个数据包需要clone（重传路径需要）则clone这个数据包，之后构建数据包的选项字段，这里其实就是把tp的选项字段拷贝到tcp_out_options中，这里不做分析。\n接下来计算数据包的头部长度，并设置ooo_okay(决定是否可以重新选择TX队列)，当协议栈没有数据包，或者重传队列为空时，选择队列的时候可以重新计算使用哪个硬件队列。\n之后设置数据包传输的头的指针，并设置skb与当前sk相关联，设置skb的析构函数，这里注意，如果数据包存在负载，则析构函数中会包括调度TSQ机制的逻辑，当网卡驱动释放数据包时，如果TSQ被置位(tcp_write_xmit中)则会进行调度。\n最后设置tcp头部的各个字段，这里最关键的是调用tcp_select_window计算(不是syn包的情况才会调用，这里可以发现syn包是没有窗口缩放的)通告给对端窗口的计算逻辑，tcp_select_window具体逻辑如下所示：\nstatic u16 tcp_select_window(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct net *net = sock_net(sk);\tu32 old_win = tp-&gt;rcv_wnd;\tu32 cur_win, new_win;\t/* Make the window 0 if we failed to queue the data because we\t * are out of memory. The window is temporary, so we don&#x27;t store\t * it on the socket.\t */\t//内存紧张了，直接通告0窗口\tif (unlikely(inet_csk(sk)-&gt;icsk_ack.pending &amp; ICSK_ACK_NOMEM))\t\treturn 0;\t//剩余的窗口空间\tcur_win = tcp_receive_window(tp);\t//计算新窗口\tnew_win = __tcp_select_window(sk);\t//新窗口比当前窗口还小\tif (new_win &lt; cur_win) &#123; \t\t/* Danger Will Robinson!\t\t * Don&#x27;t update rcv_wup/rcv_wnd here or else\t\t * we will not be able to advertise a zero\t\t * window in time.  --DaveM\t\t *\t\t * Relax Will Robinson.\t\t */\t\t//是否允许 shrink 通常是false吧\t\tif (!READ_ONCE(net-&gt;ipv4.sysctl_tcp_shrink_window) || !tp-&gt;rx_opt.rcv_wscale) &#123;\t\t\t/* Never shrink the offered window */\t\t\tif (new_win == 0) //实在是太小了\t\t\t\tNET_INC_STATS(net, LINUX_MIB_TCPWANTZEROWINDOWADV);\t\t\tnew_win = ALIGN(cur_win, 1 &lt;&lt; tp-&gt;rx_opt.rcv_wscale);//保持窗口大小不变基于当前剩余窗口cur_win\t\t&#125;\t&#125;\t//通告窗口的大小\ttp-&gt;rcv_wnd = new_win;\t//记录本次更新窗口的序号\ttp-&gt;rcv_wup = tp-&gt;rcv_nxt;\t/* Make sure we do not exceed the maximum possible\t * scaled window.\t */\t//限制最大窗口\tif (!tp-&gt;rx_opt.rcv_wscale &amp;&amp;\t    READ_ONCE(net-&gt;ipv4.sysctl_tcp_workaround_signed_windows))\t\tnew_win = min(new_win, MAX_TCP_WINDOW);\telse\t\tnew_win = min(new_win, (65535U &lt;&lt; tp-&gt;rx_opt.rcv_wscale)); //启用 wscale 时\t/* RFC1323 scaling applied */\tnew_win &gt;&gt;= tp-&gt;rx_opt.rcv_wscale; //变成缩放后的值\t/* If we advertise zero window, disable fast path. */\tif (new_win == 0) &#123;\t\ttp-&gt;pred_flags = 0;\t\tif (old_win)\t\t\tNET_INC_STATS(net, LINUX_MIB_TCPTOZEROWINDOWADV);\t&#125; else if (old_win == 0) &#123;\t\tNET_INC_STATS(net, LINUX_MIB_TCPFROMZEROWINDOWADV);\t&#125;\t\treturn new_win;&#125;\n\ntcp_select_window中首先处理上一次存在入队失败的情况(内存不够)此时会直接通告0窗口，否则调用tcp_receive_window计算cur_win，表示的是当前已经通过的窗口里，还有多少没用完，其实就是上次通告的窗口减去已经收到但没被对方看见的部分\ntcp_receive_window具体代码如下所所示：\nstatic inline u32 tcp_receive_window(const struct tcp_sock *tp)&#123;\t\t//上一次更新时候的序号 当前通告给对端的  下一个期望接收的\ts32 win = tp-&gt;rcv_wup + tp-&gt;rcv_wnd - tp-&gt;rcv_nxt;\tif (win &lt; 0)\t\twin = 0;\treturn (u32) win;&#125;","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输出"]},{"title":"TCP输出 __tcp_transmit_skb(二)","url":"/2025/12/09/tcp%E5%8F%91%E5%8C%85tcp_transmit_skb%EF%BC%882%EF%BC%89/","content":"__tcp_select_window具体代码如下所示：\nu32 __tcp_select_window(struct sock *sk)&#123;    struct inet_connection_sock *icsk = inet_csk(sk);    struct tcp_sock *tp = tcp_sk(sk);    struct net *net = sock_net(sk);    /* MSS for the peer&#x27;s data.  Previous versions used mss_clamp     * here.  I don&#x27;t know if the value based on our guesses     * of peer&#x27;s MSS is better for the performance.  It&#x27;s more correct     * but may be worse for the performance because of rcv_mss     * fluctuations.  --SAW  1998/11/1     */    int mss = icsk-&gt;icsk_ack.rcv_mss;  //估计对端的mss    int free_space = tcp_space(sk); //这里是rcvbuf大小减去backlog还有接收队列之后实际剩余真实的内存量，当前可用的剩余空间    int allowed_space = tcp_full_space(sk);//这里是rcvbuf大小    int full_space, window;    if (sk_is_mptcp(sk))        mptcp_space(sk, &amp;free_space, &amp;allowed_space);    //取一个最大的 接收端硬限制的最大窗口    full_space = min_t(int, tp-&gt;window_clamp, allowed_space);    //连一个mss都装不下    if (unlikely(mss &gt; full_space)) &#123;        mss = full_space;        if (mss &lt;= 0)            return 0;    &#125;    /* Only allow window shrink if the sysctl is enabled and we have     * a non-zero scaling factor in effect.     */    //是否允许窗口缩小，默认不容许    if (READ_ONCE(net-&gt;ipv4.sysctl_tcp_shrink_window) &amp;&amp; tp-&gt;rx_opt.rcv_wscale)        goto shrink_window_allowed;    /* do not allow window to shrink */    //当 free_space 比 full_space 的一半要小，这里可以理解为小于实际窗口的一半吧    if (free_space &lt; (full_space &gt;&gt; 1)) &#123;        icsk-&gt;icsk_ack.quick = 0;  //关闭 quick ACK 避免频繁ack        if (tcp_under_memory_pressure(sk)) //是否在内存压力之下            tcp_adjust_rcv_ssthresh(sk);    //设置rcv_ssthresh        /* free_space might become our new window, make sure we don&#x27;t         * increase it due to wscale.         */        //free_space 齐到 wscale         free_space = round_down(free_space, 1 &lt;&lt; tp-&gt;rx_opt.rcv_wscale);        /* if free space is less than mss estimate, or is below 1/16th         * of the maximum allowed, try to move to zero-window, else         * tcp_clamp_window() will grow rcv buf up to tcp_rmem[2], and         * new incoming data is dropped due to memory limits.         * With large window, mss test triggers way too late in order         * to announce zero window in time before rmem limit kicks in.         */        //是否直接变成 0 窗口  当前可用空间 小于最大允许空间的 1/16 || 连一个 MSS 都没有        if (free_space &lt; (allowed_space &gt;&gt; 4) || free_space &lt; mss)            return 0;    &#125;    //大于发送慢启动阈值，设置为慢启动阈值！    if (free_space &gt; tp-&gt;rcv_ssthresh)        free_space = tp-&gt;rcv_ssthresh;    /* Don&#x27;t do rounding if we are using window scaling, since the     * scaled window will not line up with the MSS boundary anyway.     */     //开启了窗口缩放    if (tp-&gt;rx_opt.rcv_wscale) &#123;        window = free_space;        /* Advertise enough space so that it won&#x27;t get scaled away.         * Import case: prevent zero window announcement if         * 1&lt;&lt;rcv_wscale &gt; mss.         */        //对齐窗口缩放        window = ALIGN(window, (1 &lt;&lt; tp-&gt;rx_opt.rcv_wscale));    &#125; else &#123;        //没启用窗口缩放的情况        window = tp-&gt;rcv_wnd;        /* Get the largest window that is a nice multiple of mss.         * Window clamp already applied above.         * If our current window offering is within 1 mss of the         * free space we just keep it. This prevents the divide         * and multiply from happening most of the time.         * We also don&#x27;t do any window rounding when the free space         * is too small.         */         //窗口变化太大了，就重新设定一个新的 对齐到 MSS 的整倍数的窗口        if (window &lt;= free_space - mss || window &gt; free_space)            window = rounddown(free_space, mss);        else if (mss == full_space &amp;&amp;   //一次最大窗口只能装一个 MSS   可用空间超过 window + 一半的实际窗口）             free_space &gt; window + (full_space &gt;&gt; 1))            window = free_space;    &#125;    return window;shrink_window_allowed: //是否更激进的缩小拥塞窗口吧    /* new window should always be an exact multiple of scaling factor */    //向下对齐    free_space = round_down(free_space, 1 &lt;&lt; tp-&gt;rx_opt.rcv_wscale);    //如果 free_space 已经小于 full_space 的一半    if (free_space &lt; (full_space &gt;&gt; 1)) &#123;        icsk-&gt;icsk_ack.quick = 0;        //是否内存压力，修改慢启动阈值        if (tcp_under_memory_pressure(sk))            tcp_adjust_rcv_ssthresh(sk);        /* if free space is too low, return a zero window */        //表示空闲空间只剩下不足 1/16  ||  不足一个mss        if (free_space &lt; (allowed_space &gt;&gt; 4) || free_space &lt; mss ||            free_space &lt; (1 &lt;&lt; tp-&gt;rx_opt.rcv_wscale))            return 0;    &#125;    //大于接收端慢启动阈值    if (free_space &gt; tp-&gt;rcv_ssthresh) &#123;        free_space = tp-&gt;rcv_ssthresh;        /* new window should always be an exact multiple of scaling factor         *         * For this case, we ALIGN &quot;up&quot; (increase free_space) because         * we know free_space is not zero here, it has been reduced from         * the memory-based limit, and rcv_ssthresh is not a hard limit         * (unlike sk_rcvbuf).         */        //对齐        free_space = ALIGN(free_space, (1 &lt;&lt; tp-&gt;rx_opt.rcv_wscale));    &#125;    return free_space;&#125;\n\n__tcp_select_window中首先计算接收缓冲区的剩余空间free_space 然后计算接收缓冲区全部的可用大小（按比例计算）allowed_space，拿到这两个值后，计算理论允许的最大接收窗口full_space。\n如果连一个mss都放不下直接0窗口，\n之后判断是否允许窗口收缩(是否可以激进的主动缩小窗口)，系统默认是不允许。如果是不收缩的逻辑，则进一步判断剩余的空间大小是否已经小于了缓冲区大小的一半（已经吃掉了超过半个接收窗口）则关闭快速ack，如果在内存压力之下，则调用tcp_adjust_rcv_ssthresh设置接收端慢启动阈值**(和拥塞窗口的类似，但是这里是控制接收窗口大小的**)，最后把 free_space 向下对齐到 2^rcv_wscale 的整数倍，如果 当前可用空间 小于最大允许空间的 1&#x2F;16 或连一个 MSS 都没有，则也直接0窗口\n上述tcp_adjust_rcv_ssthresh 代码如下所示：\nstatic inline void tcp_adjust_rcv_ssthresh(struct sock *sk)&#123;       //计算剩余的内存（用户配置），通常是0    int unused_mem = sk_unused_reserved_mem(sk);    struct tcp_sock *tp = tcp_sk(sk);    //钳制一下，最小4个mss    tp-&gt;rcv_ssthresh = min(tp-&gt;rcv_ssthresh, 4U * tp-&gt;advmss);    if (unused_mem)        //如果由保留的内存则取一个最大值        tp-&gt;rcv_ssthresh = max_t(u32, tp-&gt;rcv_ssthresh,                     tcp_win_from_space(sk, unuserud_mem));&#125;\n\n如果剩余空间大于了接收端慢启动阈值，则设置为接收端慢启动阈值，目的就是用来限制窗口增长不要太快。\n最后根据是否启用窗口缩放决定最终的window大小，具体分为两种情况，如果启用了窗口缩放，则 window = free_space，再往上对齐到 2^wscale 的倍数。否则判断现在的窗口 （window）是否比 free_space 小太多（差超过 1 MSS），说明应该再增大一点，重新设为 free_space 向下对齐到 MSS 倍数，上述位没开启shrink系统选项的逻辑\n如果开启了允许窗口shrink（允许缩小窗口的情况）如果free_space &lt; full_space/2，这里和上述主路径类似，关闭快速ack，内存紧张时候降低慢启动阈值，如果窗口过小则直接零窗口。如果free_space &gt; rcv_ssthresh这里和主路径不同， 向上对齐到 2^wscale 的倍数。这里感觉主路径和shrink路径没什么区别。\n回到tcp_select_window中比较当前新计算的窗口比剩余的通过给对端窗口还要小，如果开启了shrink则这里会缩小窗口(rcv_wnd表示通过给对端的窗口大小)，并记录更新窗口时的序列号。最后更具是否开启窗口缩放选项重新计算窗口大小。\n回到__tcp_transmit_skb中，计算完成窗口大小后，调用tcp_options_write把optcopy到数据包头部中，然后调用tcp_v4_send_check计算伪首部，（注意这里只计算了伪首部）。\n如果当前待发送的报文包含ack标志则调用tcp_event_ack_sent处理压缩ack和延迟ack的定时器，具体代码如下所示：\nstatic inline void tcp_event_ack_sent(struct sock *sk, u32 rcv_nxt)&#123;    struct tcp_sock *tp = tcp_sk(sk);    //存在乱续数据包，取消压缩ack定时器    if (unlikely(tp-&gt;compressed_ack)) &#123;        NET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPACKCOMPRESSED,                  tp-&gt;compressed_ack);        tp-&gt;compressed_ack = 0;        if (hrtimer_try_to_cancel(&amp;tp-&gt;compressed_ack_timer) == 1)            __sock_put(sk);    &#125;    //两个值不同，ecn情况？    if (unlikely(rcv_nxt != tp-&gt;rcv_nxt))        return;  /* Special ACK sent by DCTCP to reflect ECN */    tcp_dec_quickack_mode(sk);    //因为已经发送ack了 所以清除延迟ack定时器，合理    inet_csk_clear_xmit_timer(sk, ICSK_TIME_DACK);&#125;\n\n如果发送的数据包存在负载则调用tcp_event_data_sent判断是否需要调用拥塞算法注册的钩子以及决定是否进入pingpong模式，具体代码如下所示：\nstatic void tcp_event_data_sent(struct tcp_sock *tp,                struct sock *sk)&#123;    struct inet_connection_sock *icsk = inet_csk(sk);    const u32 now = tcp_jiffies32;    //如果当前没有在途数据，会调用拥塞算法的钩子    if (tcp_packets_in_flight(tp) == 0)        tcp_ca_event(sk, CA_EVENT_TX_START);    //最后一个时间    tp-&gt;lsndtime = now;    /* If it is a reply for ato after last received     * packet, enter pingpong mode.     */    //是否进入pingpong模式 ，影响延迟ack    if ((u32)(now - icsk-&gt;icsk_ack.lrcvtime) &lt; icsk-&gt;icsk_ack.ato)        inet_csk_enter_pingpong_mode(sk);&#125;\n\n之后更新发出去总数据包的段数以及skb的GSO字段，最后调用ip_queue_xmit将数据包交给ip层\n如果发送失败（底层返回false）则会进入cwr拥塞状态\n如果发送成功则调用tcp_update_skb_after_send更新pacing相关和rack相关用到的时间，具体代码如下所示：\nstatic void tcp_update_skb_after_send(struct sock *sk, struct sk_buff *skb,                      u64 prior_wstamp)&#123;    struct tcp_sock *tp = tcp_sk(sk);    //是否启用pacing，注意，在TCP层面，默认是不启用的（如果使用了bbr拥塞算法，就启用）或者setsocktopt可以开启。    if (sk-&gt;sk_pacing_status != SK_PACING_NONE) &#123;        //这个是用户配置的单位是多少字节每秒，或者是bbr配置的        unsigned long rate = sk-&gt;sk_pacing_rate;        /* Original sch_fq does not pace first 10 MSS         * Note that tp-&gt;data_segs_out overflows after 2^32 packets,         * this is a minor annoyance.         */        //如果rate值有效同时发出去10个tcp段以上了则更具用户配置更新tcp_wstamp_ns也就是下一次待发送的时间戳        if (rate != ~0UL &amp;&amp; rate &amp;&amp; tp-&gt;data_segs_out &gt;= 10) &#123;            u64 len_ns = div64_ul((u64)skb-&gt;len * NSEC_PER_SEC, rate);            u64 credit = tp-&gt;tcp_wstamp_ns - prior_wstamp;            /* take into account OS jitter */            len_ns -= min_t(u64, len_ns / 2, credit);            //pacingcheck会用到            tp-&gt;tcp_wstamp_ns += len_ns;        &#125;    &#125;    //将数据包加入到按时间排序的队列中，rack会用到    list_move_tail(&amp;skb-&gt;tcp_tsorted_anchor, &amp;tp-&gt;tsorted_sent_queue);&#125;\n\n最后调用tcp_rate_skb_sent把当前的“发送时间 + 已送达计数等信息保存，存到 skb 里，等它被 ACK 时，用来算这段时间的带宽样本，给 BBR 等拥塞算法做带宽估计，具体代码如下所示：\nvoid tcp_rate_skb_sent(struct sock *sk, struct sk_buff *skb)&#123;    struct tcp_sock *tp = tcp_sk(sk);     /* In general we need to start delivery rate samples from the      * time we received the most recent ACK, to ensure we include      * the full time the network needs to deliver all in-flight      * packets. If there are no packets in flight yet, then we      * know that any ACKs after now indicate that the network was      * able to deliver those packets completely in the sampling      * interval between now and the next ACK.      *      * Note that we use packets_out instead of tcp_packets_in_flight(tp)      * because the latter is a guess based on RTO and loss-marking      * heuristics. We don&#x27;t want spurious RTOs or loss markings to cause      * a spuriously small time interval, causing a spuriously high      * bandwidth estimate.      */    //可以理解成第一个数据包    if (!tp-&gt;packets_out) &#123;        u64 tstamp_us = tcp_skb_timestamp_us(skb);        tp-&gt;first_tx_mstamp  = tstamp_us;        //第一个包的发送时间        tp-&gt;delivered_mstamp = tstamp_us;        //上一次送达计数对应的时间    &#125;    TCP_SKB_CB(skb)-&gt;tx.first_tx_mstamp = tp-&gt;first_tx_mstamp;    TCP_SKB_CB(skb)-&gt;tx.delivered_mstamp    = tp-&gt;delivered_mstamp;    TCP_SKB_CB(skb)-&gt;tx.delivered       = tp-&gt;delivered;    //在这个 skb 发送之前，已经有多少字节/包被成功确认    TCP_SKB_CB(skb)-&gt;tx.delivered_ce    = tp-&gt;delivered_ce; //带 CE 标记的已送达数据    TCP_SKB_CB(skb)-&gt;tx.is_app_limited  = tp-&gt;app_limited ? 1 : 0; //是否是应用层发包太慢&#125;","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输出"]},{"title":"TCP套接字的初始化","url":"/2025/07/28/tcp%E5%A5%97%E6%8E%A5%E5%AD%97%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96/","content":"TCP套接字的初始化在用户调用socket() 后，对于AF_INET协议族，会调用inet_create 而inet_create 中会调用具体协议的init函数，对于TCP来说就是tcp_v4_init_sock\nstatic int inet_create(struct net *net, struct socket *sock, int protocol,\t\t       int kern)&#123;···\t//这里是特定协议的初始化逻辑\tif (sk-&gt;sk_prot-&gt;init) &#123;\t\terr = sk-&gt;sk_prot-&gt;init(sk);\t\tif (err) &#123;\t\t\tsk_common_release(sk);\t\t\tgoto out;\t\t&#125;\t&#125;···&#125;\n\ntcp_v4_init_sock  具体实现如下：\nstatic int tcp_v4_init_sock(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\ttcp_init_sock(sk);\ticsk-&gt;icsk_af_ops = &amp;ipv4_specific;//ip层和tcp层直接的桥梁的ops#ifdef CONFIG_TCP_MD5SIG\ttcp_sk(sk)-&gt;af_specific = &amp;tcp_sock_ipv4_specific;//tcp的一些ops#endif\treturn 0;&#125;\n\ntcp_v4_init_sock  中调用tcp_init_sock 进一步完成初始化，除此之外还设置了与ip层相关的ops集合，其中包括ip层的发包函数，校验和的计算，三次握手中用到的回调函数，以及路由和MTU的设置等。具体代码如下：\nconst struct inet_connection_sock_af_ops ipv4_specific = &#123;\t.queue_xmit\t   = ip_queue_xmit, \t\t\t\t\t//交付给ip层的接口\t.send_check\t   = tcp_v4_send_check,\t\t\t\t\t//计算校验和\t.rebuild_header\t   = inet_sk_rebuild_header,\t\t//建连 或者重传 调用，查看是否有有效的路由信息\t.sk_rx_dst_set\t   = inet_sk_rx_dst_set,\t\t\t//设置dst 建连完成后会调用，其他地方好像没有了\t.conn_request\t   = tcp_v4_conn_request,\t\t\t//listen状态下，处理syn的回调\t.syn_recv_sock\t   = tcp_v4_syn_recv_sock,\t\t\t//完成建链 创建socket\t.net_header_len\t   = sizeof(struct iphdr),\t.setsockopt\t   = ip_setsockopt,\t.getsockopt\t   = ip_getsockopt,\t.addr2sockaddr\t   = inet_csk_addr2sockaddr,\t\t//好像没有地方用\t.sockaddr_len\t   = sizeof(struct sockaddr_in),\t.mtu_reduced\t   = tcp_v4_mtu_reduced,\t\t\t//icmp中会调用&#125;;\n\ntcp_init_sock 中，初始化了乱序队列和重传队列，初始化多个定时器，设置了拥塞算法相关的字段，设置了有写空间的回回调以及色设置mss的回调。\nvoid tcp_init_sock(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\ttp-&gt;out_of_order_queue = RB_ROOT;  //初始化红黑树\tsk-&gt;tcp_rtx_queue = RB_ROOT;\t\t//初始化重传队列\ttcp_init_xmit_timers(sk);\t\t\t//初始化tcp的定时器\tINIT_LIST_HEAD(&amp;tp-&gt;tsq_node);\t//初始化tsq 这个node是放到软中断中的\tINIT_LIST_HEAD(&amp;tp-&gt;tsorted_sent_queue);\t\t//按时间发送\ticsk-&gt;icsk_rto = TCP_TIMEOUT_INIT;\t//初始的重传超时时间 1\ticsk-&gt;icsk_rto_min = TCP_RTO_MIN;\t//最小的重传超时时间 200ms\ticsk-&gt;icsk_delack_max = TCP_DELACK_MAX; //延迟ack\ttp-&gt;mdev_us = jiffies_to_usecs(TCP_TIMEOUT_INIT); //rtt相关\tminmax_reset(&amp;tp-&gt;rtt_min, tcp_jiffies32, ~0U); //初始化历史链接中rtt的min\t/* So many TCP implementations out there (incorrectly) count the\t * initial SYN frame in their delayed-ACK and congestion control\t * algorithms that we must have the following bandaid to talk\t * efficiently to them.  -DaveM\t */\ttcp_snd_cwnd_set(tp, TCP_INIT_CWND);\t\t\t\t//初始化cwnd 10个mss\t/* There&#x27;s a bubble in the pipe until at least the first ACK. */\ttp-&gt;app_limited = ~0U;\t\t\t\t\t\t\t\t\t//这个字段表示应用程序未持续填充数据\ttp-&gt;rate_app_limited = 1;\t\t\t\t\t\t\t\t// bbr算法会yong到\t/* See draft-stevens-tcpca-spec-01 for discussion of the\t * initialization of these values.\t */\ttp-&gt;snd_ssthresh = TCP_INFINITE_SSTHRESH;  \t\t\t\t//慢启动阈值\ttp-&gt;snd_cwnd_clamp = ~0;\t\t\t\t\t\t\t\t//拥塞窗口最大值\ttp-&gt;mss_cache = TCP_MSS_DEFAULT;\t\t\t\t\t\t//初始哈uMSS\ttp-&gt;reordering = READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_reordering);  //容许的最大乱续程度 初始化为3\ttcp_assign_congestion_control(sk);\t\t\t//设置拥塞算法的ops\ttp-&gt;tsoffset = 0;\t\t\t\t\t\t\t//tcp时间戳选项会用到\ttp-&gt;rack.reo_wnd_steps = 1;\t\t\t\t\t//乱续窗口动态增长的步长\tsk-&gt;sk_write_space = sk_stream_write_space; \t//有写空间的回调\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\ticsk-&gt;icsk_sync_mss = tcp_sync_mss;\t\t\t//更新mss的回调\tWRITE_ONCE(sk-&gt;sk_sndbuf, READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_wmem[1]));\tWRITE_ONCE(sk-&gt;sk_rcvbuf, READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_rmem[1]));\ttcp_scaling_ratio_init(sk);\t\t\t\t\t//初始化窗口的缩放因子\tset_bit(SOCK_SUPPORT_ZC, &amp;sk-&gt;sk_socket-&gt;flags);\tsk_sockets_allocated_inc(sk);&#125;\n\n上述代码中的tcp_init_xmit_timers 设置了5个定时器，其中包括重传定时器，延迟ack定时器，保活定时器，pacing定时器，和压缩ack定时器，其中重传定时器，虽然只注册了一个定时器，但内部会根据pending位的不同处理不同的任务 其中包括rack，tlp探测，超时重传，零窗口探测。具体代码如下：\n//这里初始化了五个定时器，但是定时器的回调函数中，通过switch case 和 标志位会有不同的处理。void tcp_init_xmit_timers(struct sock *sk)&#123;\t//这里初始化了三个定时器，tcp_write_timer 中有多个处理函数，tcp_delack_timer 为延迟ack定时器的回调，tcp_keepalive_timer 为保活探测的定时器的回调\tinet_csk_init_xmit_timers(sk, &amp;tcp_write_timer, &amp;tcp_delack_timer,\t\t\t\t  &amp;tcp_keepalive_timer);\t//初始化了pacing相关的定时器，其实就是tsq的定时器\thrtimer_init(&amp;tcp_sk(sk)-&gt;pacing_timer, CLOCK_MONOTONIC,\t\t     HRTIMER_MODE_ABS_PINNED_SOFT);\ttcp_sk(sk)-&gt;pacing_timer.function = tcp_pace_kick;\t//压缩ack相关的定时器\thrtimer_init(&amp;tcp_sk(sk)-&gt;compressed_ack_timer, CLOCK_MONOTONIC,\t\t     HRTIMER_MODE_REL_PINNED_SOFT);\ttcp_sk(sk)-&gt;compressed_ack_timer.function = tcp_compressed_ack_kick;&#125;\n\ninet_csk_init_xmit_timers初始化了三个定时器，具体代码如下：\n/* * Using different timers for retransmit, delayed acks and probes * We may wish use just one timer maintaining a list of expire jiffies * to optimize. */void inet_csk_init_xmit_timers(struct sock *sk,\t\t\t       void (*retransmit_handler)(struct timer_list *t),\t\t\t       void (*delack_handler)(struct timer_list *t),\t\t\t       void (*keepalive_handler)(struct timer_list *t))&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\ttimer_setup(&amp;icsk-&gt;icsk_retransmit_timer, retransmit_handler, 0);//初始化重传定时器\t\ttimer_setup(&amp;icsk-&gt;icsk_delack_timer, delack_handler, 0); //延迟ack定时器\ttimer_setup(&amp;sk-&gt;sk_timer, keepalive_handler, 0);//保活定时器\ticsk-&gt;icsk_pending = icsk-&gt;icsk_ack.pending = 0; //初始化pending位&#125;\n\n以icsk_retransmit_timer为例，对应的定时器回调函数为tcp_write_timer\nstatic void tcp_write_timer(struct timer_list *t)&#123;\tstruct inet_connection_sock *icsk =\t\t\tfrom_timer(icsk, t, icsk_retransmit_timer);\tstruct sock *sk = &amp;icsk-&gt;icsk_inet.sk;\tbh_lock_sock(sk);\t//判断用户是否持有这个sock\tif (!sock_owned_by_user(sk)) &#123;\t\t//如果没有持有这个sock 就直接调用处理函数\t\ttcp_write_timer_handler(sk);\t&#125; else &#123;\t\t/* delegate our work to tcp_release_cb() */\t\t//用户持有sock 设置一个标志位 tcp_release_cb 中会处理这个定时器任务\t\tif (!test_and_set_bit(TCP_WRITE_TIMER_DEFERRED, &amp;sk-&gt;sk_tsq_flags))\t\t\tsock_hold(sk); \t&#125;\tbh_unlock_sock(sk);\tsock_put(sk);//减引用计数，加引用计数在reset timer中&#125;\n\n上述tcp_write_timer_handler为根据不同pending处理不同的逻辑，其中包括rack ， tlp ，重传， 和零窗口探测，也就是说一个定时器处理四种任务。\nvoid tcp_write_timer_handler(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tint event;\t//如果是close或者listern 或者没有设置pending直接返回\tif (((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_LISTEN)) ||\t    !icsk-&gt;icsk_pending)\t\treturn;\t//定时器没到期的情况，直接返回\tif (time_after(icsk-&gt;icsk_timeout, jiffies)) &#123;\t\tsk_reset_timer(sk, &amp;icsk-&gt;icsk_retransmit_timer, icsk-&gt;icsk_timeout);\t\treturn;\t&#125;\t//应该叫更新发包时间戳\ttcp_mstamp_refresh(tcp_sk(sk));\t//获取类型\tevent = icsk-&gt;icsk_pending;\tswitch (event) &#123;\tcase ICSK_TIME_REO_TIMEOUT:\t\ttcp_rack_reo_timeout(sk); //recent ack\t\tbreak;\tcase ICSK_TIME_LOSS_PROBE:\t\ttcp_send_loss_probe(sk); //tlp 探测\t\tbreak;\tcase ICSK_TIME_RETRANS:\t\ticsk-&gt;icsk_pending = 0;\t\ttcp_retransmit_timer(sk);//重传\t\tbreak;\tcase ICSK_TIME_PROBE0:\t\ticsk-&gt;icsk_pending = 0;\t\ttcp_probe_timer(sk); //零窗口探测\t\tbreak;\t&#125;&#125;\n\ntcp套接字的init函数中还设置了mss的回调函数，最总的mss大小取决于最大的历史窗口大小，和mtu探测得到大小，具体代码如下：\n //三次握手，icmp报文，或者pmtu探测 貌似都会调用它unsigned int tcp_sync_mss(struct sock *sk, u32 pmtu)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tint mss_now;\t//当前传入的pmtu小于历史 的最大值，缩小范围\tif (icsk-&gt;icsk_mtup.search_high &gt; pmtu)\t\ticsk-&gt;icsk_mtup.search_high = pmtu;\t\t//计算当前的mss\tmss_now = tcp_mtu_to_mss(sk, pmtu);\t//根据窗口大小调整再次调整mss,可能会变小\tmss_now = tcp_bound_to_half_wnd(tp, mss_now);\t/* And store cached results */\t//把传进来的pmtu保存起来\ticsk-&gt;icsk_pmtu_cookie = pmtu;\tif (icsk-&gt;icsk_mtup.enabled)\t\t//如果使能的mtu 探测， 那可能会再次变小这个mss ，这个mss的值不能超过search_low\t\tmss_now = min(mss_now, tcp_mtu_to_mss(sk, icsk-&gt;icsk_mtup.search_low));\t//\ttp-&gt;mss_cache = mss_now;\treturn mss_now;&#125;\n\n上述tcp_mtu_to_mss其实就是根据传入的mtu计算一个mss，这个mss不会超过三次握手中协商的mss值\n/* Calculate MSS not accounting any TCP options.  */static inline int __tcp_mtu_to_mss(struct sock *sk, int pmtu)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\tconst struct inet_connection_sock *icsk = inet_csk(sk);\tint mss_now;\t/* Calculate base mss without TCP options:\t   It is MMS_S - sizeof(tcphdr) of rfc1122\t */\t//跟据pmtu先算出一个基础mss 也就是 mtu减去包头的长度\tmss_now = pmtu - icsk-&gt;icsk_af_ops-&gt;net_header_len - sizeof(struct tcphdr);\t/* IPv6 adds a frag_hdr in case RTAX_FEATURE_ALLFRAG is set */\t//ipv6 的处理\tif (icsk-&gt;icsk_af_ops-&gt;net_frag_header_len) &#123;\t\tconst struct dst_entry *dst = __sk_dst_get(sk);\t\tif (dst &amp;&amp; dst_allfrag(dst))\t\t\tmss_now -= icsk-&gt;icsk_af_ops-&gt;net_frag_header_len;\t&#125;\t/* Clamp it (mss_clamp does not include tcp options) */\t//如果mss_now大于 双方协商的mss 设置为协商mss\tif (mss_now &gt; tp-&gt;rx_opt.mss_clamp)\t\tmss_now = tp-&gt;rx_opt.mss_clamp;\t/* Now subtract optional transport overhead */\t//减去选项的长度\tmss_now -= icsk-&gt;icsk_ext_hdr_len;\t/* Then reserve room for full set of TCP options and 8 bytes of data */\t//和系统的最小mss比较一下，确保最小的mss\tmss_now = max(mss_now,\t\t      READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_min_snd_mss));\treturn mss_now;&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","sock"]},{"title":"TCP重传__tcp_retransmit_skb","url":"/2025/08/05/tcp%E6%95%B0%E6%8D%AE%E5%8C%85%E9%87%8D%E4%BC%A0/","content":"__tcp_retransmit_skb当TCP的超时重传定时器到期后，会调用tcp_retransmit_skb进行重传。\nvoid tcp_retransmit_timer(struct sock *sk)&#123;\t...\tif (tcp_retransmit_skb(sk, tcp_rtx_queue_head(sk), 1) &gt; 0) &#123;\t...&#125;\n\ntcp_retransmit_skb中会调用__tcp_retransmit_skb完成真正的重传，具体代码如下所示：\nint tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tint err = __tcp_retransmit_skb(sk, skb, segs);\tif (err == 0) &#123;#if FASTRETRANS_DEBUG &gt; 0\t\tif (TCP_SKB_CB(skb)-&gt;sacked &amp; TCPCB_SACKED_RETRANS) &#123;\t\t\tnet_dbg_ratelimited(&quot;retrans_out leaked\\n&quot;);\t\t&#125;#endif\t\tTCP_SKB_CB(skb)-&gt;sacked |= TCPCB_RETRANS;\t\t//更新重传出去数量，tcp_ack中会减\t\ttp-&gt;retrans_out += tcp_skb_pcount(skb);\t&#125;\t/* Save stamp of the first (attempted) retransmit. */\t//记录首次的重传时间\tif (!tp-&gt;retrans_stamp)\t\ttp-&gt;retrans_stamp = tcp_skb_timestamp(skb);\t//初始化的是是-1 用于拥塞欻港口\tif (tp-&gt;undo_retrans &lt; 0)\t\ttp-&gt;undo_retrans = 0;\t//记录重传的段数\ttp-&gt;undo_retrans += tcp_skb_pcount(skb);\treturn err;&#125;\n\n由上述可知tcp_retransmit_skb仅仅是调用了__tcp_retransmit_skb然后后更新了tcp_sock部分字段。真正的关键的操作在__tcp_retransmit_skb中\n__tcp_retransmit_skb的关键逻辑为，首先通过fclone标志来判断当前重传的数据包是否已经在本机的队列中，因为重传的时候会设置clone标志位，而重传的数据包可以直接使用fclone出来的结构体，所以说可以通过这个标志位来判断数据包是否已经重传。\n然后根据数据包的序列号对这个数据包进行处理，如果当前数据包的序列号已经被部分确认了，那就需要裁剪这个skb了。（裁剪这个skb的时候会调用tcp管理内存的相关接口）\n然后查看路由是否有效（即dst是否存在）在超过retry1，后可能会进行黑洞检测，这时候的dst可能就被rst了\n重新计算当前的mss，然后计算当前可用发送窗口的大小和当前可以发送的len（使用segs和mss的乘机计算）。这里分为两种情况，如果skb的len大于计算的得到len，在这里就需要执行分段了，否则的话（也就是小于的情况）会尝试合并数据包。\n最后设置tp_sock的一些统计字段，调用tcp_transmit_skb完成数据包的发送，注意这里设置了clone标志位。\n具体代码如下所示：\nint __tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tunsigned int cur_mss;\tint diff, len, err;\tint avail_wnd;\t/* Inconclusive MTU probe */\tif (icsk-&gt;icsk_mtup.probe_size)\t\ticsk-&gt;icsk_mtup.probe_size = 0;\t//通过fclone标志判断这个数据包是否在本机的队列中，\t//因为重传的数据包是从红黑树管理的数据包fclone出来的(调用tcp发包函数的时会设置clone标志位)，所以说如果有这个标志位\t//表示fastclone的数据包一定在本机队列中，所以就可以return -1 \tif (skb_still_in_host_queue(sk, skb))\t\treturn -EBUSY;\t//如果重传数据包的seq 的小于已经确认的序列号 则表示部分确认了，那肯定需要裁剪这个skb了\tif (before(TCP_SKB_CB(skb)-&gt;seq, tp-&gt;snd_una)) &#123;\t\t//如果这个数据包都被确认过了 那就不应该出现在重传队列中，需要直接返回\t\tif (unlikely(before(TCP_SKB_CB(skb)-&gt;end_seq, tp-&gt;snd_una))) &#123;\t\t\tWARN_ON_ONCE(1);\t\t\treturn -EINVAL;\t\t&#125;\t\t//裁剪掉已经确认的部分，因此里面也会减少skb的truesize 以及调整内存相关的使用参数\t\tif (tcp_trim_head(sk, skb, tp-&gt;snd_una - TCP_SKB_CB(skb)-&gt;seq))\t\t\treturn -ENOMEM;\t&#125;\t//看看路由是否有效，如果无效了，会调用查路由的接口，然后将结果关联到sk上\tif (inet_csk(sk)-&gt;icsk_af_ops-&gt;rebuild_header(sk))\t\treturn -EHOSTUNREACH; /* Routing failure or similar. */\t//根据tcp的选项长度，以及mtu是否变化来计算新的mss\tcur_mss = tcp_current_mss(sk);\t//这里计算了窗口右边界和数据包seq的差值，其实就是在计算可用的窗口大小有多少\tavail_wnd = tcp_wnd_end(tp) - TCP_SKB_CB(skb)-&gt;seq;\t/* If receiver has shrunk his window, and skb is out of\t * new window, do not retransmit it. The exception is the\t * case, when window is shrunk to zero. In this case\t * our retransmit of one segment serves as a zero window probe.\t */\t//如果没有可用的窗口大小\tif (avail_wnd &lt;= 0) &#123;\t\t//不是最早未确认的包，直接返回，什么场景呢？？？？\t\tif (TCP_SKB_CB(skb)-&gt;seq != tp-&gt;snd_una)\t\t\treturn -EAGAIN;\t\t//这里应该叫做强制设置一个大小？？ 因为走到这里表示没有可用的空间了\t\t//然而这是地一个没被确认的数据包，必须要强制发送？？？\t\tavail_wnd = cur_mss;\t&#125;\t//计算一个mss和一个段的乘积，也就是重传数据包的大小，貌似重传的数据包只能是一个段\tlen = cur_mss * segs;\t//当前空间够用的情况\tif (len &gt; avail_wnd) &#123;\t\t//向下取整\t\tlen = rounddown(avail_wnd, cur_mss);\t\tif (!len)\t\t\tlen = avail_wnd;\t&#125;\t//问题：下面大概率走哪个分支呢？？？？？\t//如果这个数据包len 大于上面计算len 那就需要分段了 大概率走这个分支吧\tif (skb-&gt;len &gt; len) &#123;\t\tif (tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb, len,\t\t\t\t cur_mss, GFP_ATOMIC))\t\t\treturn -ENOMEM; /* We&#x27;ll try again later. */\t&#125; else &#123;\t\t//当前空间够的情况！\t\t//检查是否被clone？？？？\t\tif (skb_unclone_keeptruesize(skb, GFP_ATOMIC))\t\t\treturn -ENOMEM;\t\t//获取分段的数量，这里应该大概率就是1吧\t\tdiff = tcp_skb_pcount(skb);\t\t//重新计算分段数量，大概率也是1吧\t\ttcp_set_skb_tso_segs(skb, cur_mss);\t\tdiff -= tcp_skb_pcount(skb); //这里的diff是变化值\t\tif (diff)//大概率不会进入这个分支\t\t\ttcp_adjust_pcount(sk, skb, diff);\t\t//取mss和窗口的最小值\t\tavail_wnd = min_t(int, avail_wnd, cur_mss);\t\t//如果数据包的大小，小于上面的最小值，则尝试合并小包\t\tif (skb-&gt;len &lt; avail_wnd)\t\t\ttcp_retrans_try_collapse(sk, skb, avail_wnd);\t&#125;\t/* RFC3168, section 6.1.1.1. ECN fallback */\t//如果是syn包 不容许有ecn标志？？？\tif ((TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_SYN_ECN) == TCPHDR_SYN_ECN)\t\ttcp_ecn_clear_syn(sk, skb);\t/* Update global and local TCP statistics. */\t//更新tcp的统计字段\tsegs = tcp_skb_pcount(skb);\tTCP_ADD_STATS(sock_net(sk), TCP_MIB_RETRANSSEGS, segs);\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_SYN)\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNRETRANS);\ttp-&gt;total_retrans += segs;\ttp-&gt;bytes_retrans += skb-&gt;len;\t/* make sure skb-&gt;data is aligned on arches that require it\t * and check if ack-trimming &amp; collapsing extended the headroom\t * beyond what csum_start can cover.\t */\t//headroom 大于64k?? 不可能把\tif (unlikely((NET_IP_ALIGN &amp;&amp; ((unsigned long)skb-&gt;data &amp; 3)) ||\t\t     skb_headroom(skb) &gt;= 0xFFFF)) &#123;\t\tstruct sk_buff *nskb;\t\ttcp_skb_tsorted_save(skb) &#123;\t\t\tnskb = __pskb_copy(skb, MAX_TCP_HEADER, GFP_ATOMIC);\t\t\tif (nskb) &#123;\t\t\t\tnskb-&gt;dev = NULL;\t\t\t\terr = tcp_transmit_skb(sk, nskb, 0, GFP_ATOMIC);\t\t\t&#125; else &#123;\t\t\t\terr = -ENOBUFS;\t\t\t&#125;\t\t&#125; tcp_skb_tsorted_restore(skb);\t\tif (!err) &#123;\t\t\ttcp_update_skb_after_send(sk, skb, tp-&gt;tcp_wstamp_ns);\t\t\ttcp_rate_skb_sent(sk, skb);\t\t&#125;\t&#125; else &#123;\t\t//设置了clone标志位\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\t&#125;\t/* To avoid taking spuriously low RTT samples based on a timestamp\t * for a transmit that never happened, always mark EVER_RETRANS\t */\t//这里设置skb 了曾经重传过的标志\tTCP_SKB_CB(skb)-&gt;sacked |= TCPCB_EVER_RETRANS;\t//bpf钩子\tif (BPF_SOCK_OPS_TEST_FLAG(tp, BPF_SOCK_OPS_RETRANS_CB_FLAG))\t\ttcp_call_bpf_3arg(sk, BPF_SOCK_OPS_RETRANS_CB,\t\t\t\t  TCP_SKB_CB(skb)-&gt;seq, segs, err);\tif (likely(!err)) &#123;\t\t//tracepoint\t\ttrace_tcp_retransmit_skb(sk, skb);\t&#125; else if (err != -EBUSY) &#123;\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPRETRANSFAIL, segs);\t&#125;\treturn err;&#125;\n\n上述代码中如果需要处理序列号已经被部分确认的情况，则会调用tcp_trim_head完成数据包的裁剪，具体代码如下\n/* Remove acked data from a packet in the transmit queue. */int tcp_trim_head(struct sock *sk, struct sk_buff *skb, u32 len)&#123;\tu32 delta_truesize;\tif (skb_unclone_keeptruesize(skb, GFP_ATOMIC))\t\treturn -ENOMEM;\t// 和__pskb_pull_tail()几乎一样\tdelta_truesize = __pskb_trim_head(skb, len);\tTCP_SKB_CB(skb)-&gt;seq += len;\t//更新skb的turesize\tskb-&gt;truesize\t   -= delta_truesize;\t//更新sk-&gt;sk_wmem_queued（tcp层管理使用内存量的大小）\tsk_wmem_queued_add(sk, -delta_truesize);\tif (!skb_zcopy_pure(skb))\t\t//内存回收\t\tsk_mem_uncharge(sk, delta_truesize);\t/* Any change of skb-&gt;len requires recalculation of tso factor. */\tif (tcp_skb_pcount(skb) &gt; 1)\t\t//根据mss更新数据包的gso_size\t\ttcp_set_skb_tso_segs(skb, tcp_skb_mss(skb));\treturn 0;&#125;\n\n上述代码__pskb_trim_head 完成数据包裁剪后，调用sk_wmem_queued_add修改了sk-&gt;sk_wmem_queued（相当于tcp层管理内存的变量）\n然后会调用sk_mem_uncharge进行内存回收（也就是会影响tcp内存的三个阈值）。sk_mem_uncharge开始后的具体调用栈如下所示：\nstatic inline void sk_mem_uncharge(struct sock *sk, int size)&#123;\tif (!sk_has_account(sk))\t\treturn;\t//先更新（增加）sk-&gt;sk_forward_alloc的值这个值的单位是字节\tsk_forward_alloc_add(sk, size);\t//真正的页面回收，这里如果达到了了回收的标准，还会在减少sk_forward_alloc\tsk_mem_reclaim(sk);&#125;static inline void sk_forward_alloc_add(struct sock *sk, int val)&#123;\t/* Paired with lockless reads of sk-&gt;sk_forward_alloc */\tWRITE_ONCE(sk-&gt;sk_forward_alloc, sk-&gt;sk_forward_alloc + val);&#125;static inline void sk_mem_reclaim(struct sock *sk)&#123;\tint reclaimable;\tif (!sk_has_account(sk))\t\treturn;\t//如果用户没有配置保留的内存sk_unused_reserved_mem 就是0 ，reclaimable 则等于sk_forward_alloc\treclaimable = sk-&gt;sk_forward_alloc - sk_unused_reserved_mem(sk);\t//字节多少大于一个页大小的时候触发回收\tif (reclaimable &gt;= (int)PAGE_SIZE)\t//调用回收的逻辑\t\t__sk_mem_reclaim(sk, reclaimable);&#125;void __sk_mem_reclaim(struct sock *sk, int amount)&#123;\tamount &gt;&gt;= PAGE_SHIFT;\t//这里的amount的单位为页数！  这里面又减去了sk_forward_alloc 的值\tsk_forward_alloc_add(sk, -(amount &lt;&lt; PAGE_SHIFT));\t__sk_mem_reduce_allocated(sk, amount);&#125;static inline void sk_forward_alloc_add(struct sock *sk, int val)&#123;\t/* Paired with lockless reads of sk-&gt;sk_forward_alloc */\tWRITE_ONCE(sk-&gt;sk_forward_alloc, sk-&gt;sk_forward_alloc + val);&#125;void __sk_mem_reduce_allocated(struct sock *sk, int amount)&#123;\t//修改tcp_memory_allocated的值\tsk_memory_allocated_sub(sk, amount);\t//cgroup相关\tif (mem_cgroup_sockets_enabled &amp;&amp; sk-&gt;sk_memcg)\t\tmem_cgroup_uncharge_skmem(sk-&gt;sk_memcg, amount);\t//更新内存压力\tif (sk_under_global_memory_pressure(sk) &amp;&amp;\t    (sk_memory_allocated(sk) &lt; sk_prot_mem_limits(sk, 0)))\t\tsk_leave_memory_pressure(sk);&#125;\n\n上述就是裁剪数据包的全部逻辑，之后的逻辑为调用inet_csk(sk)-&gt;icsk_af_ops-&gt;rebuild_header(sk)判断路由是否有效，具体的函数如下所示(是在初始化tcp套接字的时候注册的)。\n//超时重传的时候会调用到int inet_sk_rebuild_header(struct sock *sk)&#123;\tstruct inet_sock *inet = inet_sk(sk);\tstruct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);\t__be32 daddr;\tstruct ip_options_rcu *inet_opt;\tstruct flowi4 *fl4;\tint err;\t/* Route is OK, nothing to do. */\t//如果路由条目存在，那直接返回就ok，\t//什么时候会继续走？ 重传超过retry1，黑洞检测的时候不是就不能直接返回了？\tif (rt)\t\treturn 0;\t/* Reroute. */\trcu_read_lock();\t//拿到ip选项\tinet_opt = rcu_dereference(inet-&gt;inet_opt);\t//拿到目的地址\tdaddr = inet-&gt;inet_daddr;\t//如果有ip选项，且有ip源路由选项，那就用选项的daddr，通常不会走这\tif (inet_opt &amp;&amp; inet_opt-&gt;opt.srr)\t\tdaddr = inet_opt-&gt;opt.faddr;\trcu_read_unlock();\t//拿到一个流的信息，\tfl4 = &amp;inet-&gt;cork.fl.u.ip4;\t//调用查路由的接口\trt = ip_route_output_ports(sock_net(sk), fl4, sk, daddr, inet-&gt;inet_saddr,\t\t\t\t   inet-&gt;inet_dport, inet-&gt;inet_sport,\t\t\t\t   sk-&gt;sk_protocol, RT_CONN_FLAGS(sk),\t\t\t\t   sk-&gt;sk_bound_dev_if);\tif (!IS_ERR(rt)) &#123;\t\terr = 0;\t\t//这里关联了sk和dst\t\tsk_setup_caps(sk, &amp;rt-&gt;dst);\t&#125; else &#123;\t\terr = PTR_ERR(rt);\t\t/* Routing failed... */\t\tsk-&gt;sk_route_caps = 0;\t\t/*\t\t * Other protocols have to map its equivalent state to TCP_SYN_SENT.\t\t * DCCP maps its DCCP_REQUESTING state to TCP_SYN_SENT. -acme\t\t */\t\t//走到这里肯定是上面查找路由失败了，如果准备换一个源ip地址继续查路由，前提是开启这个选项，且不是建立连接阶段\t\t//就换个ip地址继续查，如果失败了，就返回err\t\tif (!READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_ip_dynaddr) ||\t\t    sk-&gt;sk_state != TCP_SYN_SENT ||\t\t    (sk-&gt;sk_userlocks &amp; SOCK_BINDADDR_LOCK) ||\t\t    (err = inet_sk_reselect_saddr(sk)) != 0)\t\t\tWRITE_ONCE(sk-&gt;sk_err_soft, -err);\t&#125;\treturn err;&#125;\n\n上述代码其实就是做了一件事情，就是判断sk的dst是否存在，如果不存在，则调用路由查找接口，然后为sk关联新的dst\n之后__tcp_retransmit_skb会调用tcp_current_mss来计算当前的mss具体代码如下：\nunsigned int tcp_current_mss(struct sock *sk)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\tconst struct dst_entry *dst = __sk_dst_get(sk);\tu32 mss_now;\tunsigned int header_len;\tstruct tcp_out_options opts;\tstruct tcp_md5sig_key *md5;\t//这个可以理解为最新的mss，本质上是上次的在某些地方更新的mss\tmss_now = tp-&gt;mss_cache;\tif (dst) &#123;\t\t//这里返回了路由表或者设备的mtu\t\tu32 mtu = dst_mtu(dst);\t\tif (mtu != inet_csk(sk)-&gt;icsk_pmtu_cookie)//这个值是调用tcp_sync_mss 用来计算mss的参数 如果没变化，不会进入这个分支\t\t\t//重新根据这个值在计算一次mtu ， 这个传入的mtu会被保存到icsk_pmtu_cookie 中 其实也就是因为mtu变化了，所以要重新计算mss 合理\t\t\tmss_now = tcp_sync_mss(sk, mtu); \t&#125;\t//计算tcp选项的长度\theader_len = tcp_established_options(sk, NULL, &amp;opts, &amp;md5) +\t\t     sizeof(struct tcphdr);\t/* The mss_cache is sized based on tp-&gt;tcp_header_len, which assumes\t * some common options. If this is an odd packet (because we have SACK\t * blocks etc) then our calculated header_len will be different, and\t * we have to adjust mss_now correspondingly */\t//如果长度变化了，重新调整mss\tif (header_len != tp-&gt;tcp_header_len) &#123;\t\tint delta = (int) header_len - tp-&gt;tcp_header_len;\t\tmss_now -= delta;\t&#125;\treturn mss_now;&#125;\n\ntcp_current_mss中首先拿到上一次的mss，然后在获取当前的mtu，因为mtu可能会变化，所以根据新的mtu算出一个新的mss\n继续往下，如果当前数据包的len小于一个mss的大小，则会尝试调用tcp_retrans_try_collapse合并重传队列中的数据包，否则的话会调用分段的处理函数。，合并重传队列中数据包的代码如下：\nstatic void tcp_retrans_try_collapse(struct sock *sk, struct sk_buff *to,\t\t\t\t     int space)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb = to, *tmp;\tbool first = true;\t//是否开启合并，默认是开启的\tif (!READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_retrans_collapse))\t\treturn;\t//syn包不能合并，但是会有重传队列中除了syn包还有其他包的情况吗？？？\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_SYN)\t\treturn;\t//遍历重传队列\tskb_rbtree_walk_from_safe(skb, tmp) &#123;\t\t//判断下一个包能否合并\t\tif (!tcp_can_collapse(sk, skb))\t\t\tbreak;\t\t//再次进行判断，如果是zc就无法合并\t\tif (!tcp_skb_can_collapse(to, skb))\t\t\tbreak;\t\t//可用的空间大小，减去被合并的大小\t\tspace -= skb-&gt;len;\t\t//第一个包，也就是to吧 所以直接continue了\t\tif (first) &#123;\t\t\tfirst = false;\t\t\tcontinue;\t\t&#125;\t\tif (space &lt; 0)\t\t\tbreak;\t\t//数据包的seq超出了窗口i的右边界，直接返回\t\tif (after(TCP_SKB_CB(skb)-&gt;end_seq, tcp_wnd_end(tp)))\t\t\tbreak;\t\tif (!tcp_collapse_retrans(sk, to))\t\t\tbreak;\t&#125;&#125;\n\ntcp_retrans_try_collapse遍历重传队列，调用tcp_can_collapse判断重传队列中的数据包能否合并（fclone过，或者segs不为1则无法合并），然后调用tcp_collapse_retrans完成真正的合并。具体代码如下：\n/* Check if coalescing SKBs is legal. */static bool tcp_can_collapse(const struct sock *sk, const struct sk_buff *skb)&#123;\t//如果数据包有多个段，不能合并，为什么？\tif (tcp_skb_pcount(skb) &gt; 1)\t\treturn false;\t//如果被clone过这不能合并\tif (skb_cloned(skb))\t\treturn false;\t/* Some heuristics for collapsing over SACK&#x27;d could be invented */\t//是sack过的数据包 不能合并\tif (TCP_SKB_CB(skb)-&gt;sacked &amp; TCPCB_SACKED_ACKED)\t\treturn false;\treturn true;&#125;\n\n/* Collapses two adjacent SKB&#x27;s during retransmission. */static bool tcp_collapse_retrans(struct sock *sk, struct sk_buff *skb)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *next_skb = skb_rb_next(skb);\tint next_skb_size;\t//重传队列合并下一个数据包的size\tnext_skb_size = next_skb-&gt;len;\t//必须只有一个段\tBUG_ON(tcp_skb_pcount(skb) != 1 || tcp_skb_pcount(next_skb) != 1);\t//这里调用tcp_skb_shift 完成了连个数据包的合并，但是只有数据仅有非线性部分的数据才能合并成功， 会有仅有非线性部分的数据吗？\tif (next_skb_size &amp;&amp; !tcp_skb_shift(skb, next_skb, 1, next_skb_size))\t\treturn false;\t//处理highest_sack字段指向第一个skb\ttcp_highest_sack_replace(sk, next_skb, skb);\t/* Update sequence range on original skb. */\t//更新第一个skb的序列号\tTCP_SKB_CB(skb)-&gt;end_seq = TCP_SKB_CB(next_skb)-&gt;end_seq;\t/* Merge over control information. This moves PSH/FIN etc. over */\t//设置tcp标志位\tTCP_SKB_CB(skb)-&gt;tcp_flags |= TCP_SKB_CB(next_skb)-&gt;tcp_flags;\t/* All done, get rid of second SKB and account for it so\t * packet counting does not break.\t */\t//设置重传过的标志(如果有的话) 这个标志是在外面设置的\tTCP_SKB_CB(skb)-&gt;sacked |= TCP_SKB_CB(next_skb)-&gt;sacked &amp; TCPCB_EVER_RETRANS;\tTCP_SKB_CB(skb)-&gt;eor = TCP_SKB_CB(next_skb)-&gt;eor;\t/* changed transmit queue under us so clear hints */\t//将tp-&gt;lost_skb_hint 置为空，因为tcp ack中会用到这个值，这里已经处理重传队列了，所以要重新赋值\ttcp_clear_retrans_hints_partial(tp);\t//设置下一个要重传的skb的指针\tif (next_skb == tp-&gt;retransmit_skb_hint)\t\ttp-&gt;retransmit_skb_hint = skb;\t//因为合并了数据包要修改一些 *out字段，因为表示已经发送出去还没确认的数量，因为合并了 所以需要修改\ttcp_adjust_pcount(sk, next_skb, tcp_skb_pcount(next_skb));\t//处理时间戳\ttcp_skb_collapse_tstamp(skb, next_skb);\t//unlink数据包，调用调整内存的相关接口\ttcp_rtx_queue_unlink_and_free(next_skb, sk);\treturn true;&#125;\n\ntcp_collapse_retrans 中调用tcp_skb_shift完成真实的合并操作，之后设置一些其他地方会用到的字段，比如数据包的序列号，tcp的标志位，时间戳等，然后将被合并的数据包从重传队列中移除，同时调用内存释放相关接口函数\nstatic inline void tcp_rtx_queue_unlink_and_free(struct sk_buff *skb, struct sock *sk)&#123;\tlist_del(&amp;skb-&gt;tcp_tsorted_anchor);\t//从重传队列中unlink\ttcp_rtx_queue_unlink(skb, sk);\t//freeskb 修改wmem_queue\ttcp_wmem_free_skb(sk, skb);&#125;\n\nstatic inline void tcp_wmem_free_skb(struct sock *sk, struct sk_buff *skb)&#123;\tsk_wmem_queued_add(sk, -skb-&gt;truesize);\tif (!skb_zcopy_pure(skb))\t\tsk_mem_uncharge(sk, skb-&gt;truesize);\telse\t\tsk_mem_uncharge(sk, SKB_TRUESIZE(skb_end_offset(skb)));\t__kfree_skb(skb);&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP定时器","TCP"]},{"title":"x86汇编学习(三)","url":"/2025/06/03/x86%E6%B1%87%E7%BC%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/","content":"函数调用函数调用是一种代码封装机制，他通过参数和返回值实现特定的功能，隐藏实现的细节，提供清晰的接口\n实现函数调用需要提供以下机制：\n\n控制转移：跳转到被调用过程并在结束后返回原处；\n参数传递：将调用方的数据传给被调用方；\n返回值传递：将结果从被调用方传回；\n局部变量管理：分配并释放被调用过程所需的内存\n\nx86-64 架构的过程调用依赖一套特定的寄存器和内存使用规范，以减少开销和提高效率。\n栈帧栈帧就是每次函数调用时，在栈上分配的一块内存区域,里面通常会包含：\n\n\n\n内容\n作用\n\n\n\n参数区\n用来保存传入函数的参数（如果没用寄存器传）\n\n\n返回地址\n函数执行完后，跳回调用者的地方\n\n\n保存的寄存器\n保存调用者的寄存器内容，防止被破坏\n\n\n局部变量\n函数内部使用的变量（如 int x = 5;）\n\n\n临时空间\n编译器优化、对齐等需求\n\n\n对应的布局大体如下：\n↑ 高地址││ 调用者保存的返回地址      ← call 指令自动压入（跳回调用点）││ 可选：旧的基址指针 (%rbp)  ← 如果使用帧指针（frame pointer）││ 被调用者保存的寄存器      ← 被调用函数保存如 rbx, rbp, r12 等││ 对齐填充                  ← 保证16字节对齐要求││ 被调用者的局部变量        ← 编译器通过 sub rsp, XXX 分配空间││ 可选：超过6个参数的栈参数  ← 前6个参数通过寄存器传，其余在栈上传↓低地址\n\n\n\n\n\n一个简单的例子演示函数 P 调用 Q 时，两者的栈帧结构分别是什么样的\n// 被调用函数 Qint Q(int a, int b) &#123;    int sum = a + b;   // 局部变量    return sum;&#125;// 调用者函数 Pint P() &#123;    int x = 3, y = 4;    int result = Q(x, y);  // 调用 Q    return result + 1;&#125;\n\n栈帧分析\n内存高地址↑|-----------------------------------------|| main → call P 压入的返回地址            | ←  P 的返回地址|-----------------------------------------|| P 保存的寄存器（如 rbx）                 || P 的局部变量 y = 4                      || P 的局部变量 x = 3                      || P 的局部变量 result                     ||-----------------------------------------|| P → call Q 压入的返回地址               | ←  Q 的返回地址|-----------------------------------------|| Q 参数 a = 3（由 rdi 传，但可写入栈）     || Q 参数 b = 4（由 rsi 传，但可写入栈）     || Q 保存的寄存器（如 rbx）                 || Q 的局部变量 sum                        |↓内存低地址\n\n函数调用函数调用其实就是“跳转”到另一个地址，把控制权从函数 P 跳转到函数 Q，就是把程序计数器（PC）设置为 Q 函数起始地址。\n但跳转之后，返回原来位置怎么办？\n\n跳到 Q 之后，执行完 Q 后，还得回到 P 原来的地方继续执行。\n所以：调用时必须“记住”返回地址。\n\n在 x86-64 中，是怎么记住这个返回地址的？\n使用 call 指令\ncall Q\n\n做了两件事：\n\n把“下一条指令地址”压入栈（这就是“返回地址”）；\n跳转到 Q 函数的入口。\n\n这个压入栈的地址（称为 地址 A），就是 call Q 指令执行完后，应该继续执行的下一条指令地址。\n使用 ret 指令：\nret\n\n\n\n\n指令形式\n描述\n\n\n\ncall Label\n调用固定地址的函数（如 call Q）\n\n\ncall *Operand\n调用函数指针（如 call *%rax）\n\n\nret\n返回上一函数\n\n\n举个例子\nc语言代码如下:\nlong swap_add(long *xp, long *yp)&#123;    long x = *xp;  // 取出 *xp    long y = *yp;  // 取出 *yp    *xp = y;       // 交换 *xp 和 *yp 的值    *yp = x;    return x + y;  // 返回原值之和&#125;long caller()&#123;    long arg1 = 534;    long arg2 = 1057;    long sum = swap_add(&amp;arg1, &amp;arg2);    long diff = arg1 - arg2;    return sum * diff;&#125;\n\n对应的汇编代码如下：\nsubq $16, %rsp                  # 为局部变量 arg1 和 arg2 分配 16 字节栈空间movq $534, (%rsp)              # 把常量 534 存入栈顶 → arg1movq $1057, 8(%rsp)            # 把常量 1057 存入 [rsp + 8] → arg2leaq 8(%rsp), %rsi             # 加载 &amp;arg2 的地址到 %rsi → 作为第二个参数movq %rsp, %rdi                # 加载 &amp;arg1 的地址到 %rdi → 作为第一个参数call swap_add                  # 调用 swap_add 函数（结果保存在 %rax，返回地址被压栈）movq (%rsp), %rdx             # 把交换后的 arg1 取到 %rdxsubq 8(%rsp), %rdx            # 计算 diff = arg1 - arg2，结果保存在 %rdximulq %rdx, %rax              # sum * diff，结果保存在 %rax（作为最终返回值）addq $16, %rsp                # 恢复栈指针，释放之前为 arg1/arg2 分配的栈空间ret                           # 从 caller 函数返回，ret 会从栈中弹出返回地址并跳回\n\n数组C语言把数组实现得非常简单直接 —— 它其实就是一段连续内存块\n举例：\nchar A[12];char *B[8];int C[6];double *D[5];\n\n\n\n\n数组\n元素大小（字节）\n总大小（字节）\n起始地址\n第 i 个元素地址\n\n\n\nA\n1\n12\nx_A\nx_A + i\n\n\nB\n8\n64\nx_B\nx_B + 8i\n\n\nC\n4\n24\nx_C\nx_C + 4i\n\n\nD\n8\n40\nx_D\nx_D + 8i\n\n\n举例：\n在 x86-64 架构中，内存引用指令可以用来简化数组访问的实现。比如，假设 E 是一个 int 类型的数组，而我们想要计算 E[i] 的值。\n此时，E 的起始地址被存储在寄存器 %rdx 中，索引 i 存储在寄存器 %rcx 中。那么下面这条指令\nmovl (%rdx, %rcx, 4), %eax\n\n会执行地址计算：x_E + 4*i，读取该地址处的内存内容，并将这个值存入寄存器 %eax 中。\n指针的的运算C语言允许对指针做加法运算；\n表达式 p + i 并不是简单的“地址 + 数字”，而是地址 + i × L，其中 L 是该指针类型 T 的大小；\n\n比如 int *p，每加1其实是加4字节；\n比如 double *p，每加1是加8字节；\n\n举例：\n以表达式 E[i]（访问第 i 个元素）为例：\n\n假设 E 是一个 int 数组，起始地址在 %rdx，下标 i 存在 %rcx；\n那么 E[i] 就是从 rdx + 4*i 所在的内存读取；\n对应汇编是：\n\nmovl (%rdx, %rcx, 4), %eax\n\n说明：int 是 4 字节，所以用 movl（加载 4 字节）和缩放因子 4\n结构体C语言的 struct 用来定义一种聚合类型，它可以把多个不同类型的变量“组合成一个整体”。\n\n在内存中，结构体的所有成员是连续存放的；\n结构体变量其实就是一个内存块；\n结构体指针指向结构体起始地址，访问成员时靠偏移量；\n编译器会记录每个成员相对结构体首地址的偏移，用来计算成员地址。\n\n举例：\n假设有如下结构体\nstruct rec &#123;    int i;    int j;    int a[2];    int *p;&#125;;\n\n对应的各个字段的偏移如下所示：\n\n\n\n偏移量 (byte)\n字段内容\n\n\n\n0\ni\n\n\n4\nj\n\n\n8\na[0]\n\n\n12\na[1]\n\n\n16\np\n\n\n如果访问结构体字段 r-&gt;i 并将其存入 r-&gt;j，对应的汇编如下所示：\n假设 struct rec *r 的地址保存在 %rdi 中：\n# Registers: r in %rdimovl (%rdi), %eax        # 将 r-&gt;i 的值读入 %eaxmovl %eax, 4(%rdi)       # 把 r-&gt;i 的值写入 r-&gt;j（偏移 4 字节）\n\n","categories":["《深入理解计算机系统》"],"tags":["汇编语言"]},{"title":"x86汇编学习(一)","url":"/2025/05/27/x86%E6%B1%87%E7%BC%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/","content":"汇编语言GCC（C语言编译器）可以将程序编译为汇编代码的形式进行输出。汇编语言是机器指令的文本表示形式，它详细地列出了程序中每一条指令。然后GCC 会调用汇编器（assembler）和链接器（linker），根据这些汇编代码生成最终的可执行机器代码。\n本文基于x86-64架构,\t 它是现在最常见处理器的机器语言，也是驱动大型数据中心和超级计算机的最常见处理器的机器语言。这种语言的历史悠久，开始 于 Intel 公司 1978 年的第一个 16 位处理器，然后扩展为 32 位，最近又扩展到 64 位。\n1.x86的发展过程8086(1978 年， 29K 个晶体管）。它是第一代单芯片、 16 位微处理器之一。 \ni386(1985 年， 275K 个晶体管）。将体系结构扩展到 32 位。这是 Intel 系列中第一台全面支持 Unix操作系统的机器。\nPentium 4E(2004 年， 125M 个品体管）。增加了超线程(hyperthreading) , 这种技术可以在一个处理器上同时运行两个程序；还增加了 EM64T, 它是 Intel 对AMD提出的对 IA32 的 64 位扩展的实现，我们称之为 x86-64。\n2.程序编码假设一个C程序，有两个文件p1.c 和 p2.c 我们用 Unix命令行编译这些代码：\nlinux&gt; gcc -Og -o p p1.c p2.c \n\n命令 gcc 指的就是 GCC C 编译器。因为这是 Linux上默认的编译器。编译选项-Og告诉编译器使用的优化等级。\n实际上 gcc 命令调用了一整套的程序，将源代码转化成可执行代码。首先， C预处理器扩展源代码，插入所有用#include命令指定的文件，并展开所有用#define 声明指定的宏。其次，编译器产生两个源文件的汇编代码，名字分别为 p1.s 和 p2.s。接下来，汇编器会将汇编代码转化成二进制目标代码文件p1.a 和 p2.o。 目标代码是机器代码的一种形式，它包含所有指令的二进制表示，但是还没有填入全局值的地址（此时全局变量的地址应该没有确定）。最后，链接器将两个目标代码文件与实现库函数（例如printf)的代码合并，并产生最终的可执行代码文件p。\n虽然 C语言提供了一种模型，可以在内存中声明和分配各种数据类型的对象**，但是机器码只是简单地将内存看成一个很大的、按字节寻址的数组**。 C语言中的数据类型，例如数组和结构体，在机器代码中用一组连续的字节来表示。\n代码示例：\n假设我们写了一个 C 语言代码文件 mstore.c，包含如下的函数定义：\nlong mult2(long, long);void multstore(long x, long y, long *dest) &#123;    long t = mult2(x, y);    *dest = t;&#125;\n\n在命令行上使用 -S 选项，就能看到 C 语言编译器产生的汇编代码\nlinux&gt; gcc -Og -S mstore.c\n\n这会使 GCC 运行编译器，产生一个汇编文件 mstore.s，但是不做其他进一步的工作。（通常情况下，它还会继续调用汇编器产生目标代码文件）。\n汇编代码文件包含各种声明，包括下面几行：\nmultstore:    pushq   %rbx //把寄存器 %rbx 的值压栈，保存现场，防止被后面修改    movq    %rdx, %rbx //把第三个参数（dest 指针）保存到 %rbx 中，因为调用 mult2 之后 %rdx 可能会被破坏    call    mult2 //调用 mult2(x, y) 函数（参数 x 和 y 存在 %rdi 和 %rsi 中）。    movq    %rax, (%rbx) //将 mult2 的返回值（在 %rax 中）存储到 %rbx 指向的地址中，也就是 *dest = result。    popq    %rbx //恢复之前保存的 %rbx 的值，保持寄存器一致性。    ret\n\n要查看机器代码文件的内容，有一类称为反汇编器（disassembler）的程序非常有用。这些程序根据机器代码产生一种类似于汇编代码的格式。在 Linux 系统中，带 -d 命令行标志的程序 objdump（表示 “object dump”）可以充当这个角色：\nlinux&gt; objdump -d mstore.o\n\n结果如下\nDisassembly of function multstore in binary file mstore.o0000000000000000 &lt;multstore&gt;: Offset  Bytes         Equivalent assembly language----------------------------------------------------  0:     53            push   %rbx                ; 保存 %rbx 到栈中  1:     48 89 d3      mov    %rdx, %rbx          ; 把第三个参数 dest 存入 %rbx  4:     e8 00 00 00 00 callq  9 &lt;multstore+0x9&gt;  ; 调用 mult2(x, y)  9:     48 89 03      mov    %rax, (%rbx)        ; 把结果保存到 *dest 中  c:     5b            pop    %rbx                ; 恢复 %rbx  d:     c3            retq                       ; 返回\n\n\n\n然而生成实际可执行的代码需要对一组目标代码文件运行链接器，而这一组目标代码文件中必须含有一个main 函数。假设在文件 main.c 中有下面这样的函数：\n#include &lt;stdio.h&gt;void multstore(long, long, long *);int main() &#123;    long d;    multstore(2, 3, &amp;d);    printf(&quot;2 * 3 --&gt; %ld\\n&quot;, d);    return 0;&#125;long mult2(long a, long b) &#123;    long s = a * b;    return s;&#125;\n\n用如下命令成可执行文件prog:\nlinux&gt; gcc -Og -o prog main.c mstore.c\n\n我们也可以反汇编 prog 文件\nDisassembly of function sum multstore binary file prog0000000000400540 &lt;multstore&gt;: Offset     Bytes             Equivalent assembly language---------------------------------------------------------- 400540:    53                push   %rbx 400541:    48 89 d3          mov    %rdx, %rbx 400544:    e8 42 00 00 00    callq  40058b &lt;mult2&gt; 400549:    48 89 03          mov    %rax, (%rbx) 40054c:    5b                pop    %rbx 40054d:    c3                retq 40054e:    90                nop 40054f:    90                nop\n\n这段代码与 mstore.c 反汇编产生的代码几乎完全一样。其中一个主要的区别是左边列出的地址不同——链接器将这段代码的地址移动到了一段不同的地址范围中。第二个不同之处在于链接器填上了 callq 指令调用函数 mult2 需要使用的地址（反汇编代码第 4 行）。链接器的任务之一就是为函数调用匹配到可执行代码中函数的地址.（这里的地址0000000000400540应该就是虚拟内存中的地址）\n3.汇编伪指令假设我们用如下命令生成文件 mstore.s完整的汇编文件代码如下所示：\n\t.file\t&quot;010-mstore.c&quot; //告诉汇编器这个文件来源于哪个源文件（调试信息用）\t.text  \t\t\t\t\t//告诉汇编器后面的内容是程序代码\t.globl\tmultstore \t\t//使函数对其他文件可见（链接时可调用）\t.type\tmultstore, @function //\t指定符号类型是函数\t供链接器识别该符号为函数multstore:\tpushq\t%rbx\tmovq\t%rdx, %rbx\tcall\tmult2\tmovq\t%rax, (%rbx)\tpopq\t%rbx\tret\t.size\tmultstore, .-multstore //告诉链接器这个函数占用了多少字节。在 ELF 格式中，每个符号都可以带有大小信息\t.ident\t&quot;GCC: (Ubuntu 4.8.1-2ubuntu1~12.04) 4.8.1&quot; //加入一条标识信息，说明是用什么编译器编译的。\t.section\t.note.GNU-stack,&quot;&quot;,@progbits\n\n所有以 . 开头的行都是指导汇编器和链接器工作的伪指令。\n4.C 类型与汇编指令后缀对应表 Intel 用术语”字(word)” 表示 16 位数据类型。因此，称 32 位数为“双字”, 称 64 位数为“四字” 下表为C语言基本数据类型对应的 x86-64 表示：\n\n\n\nC 声明\nIntel 数据类型\n汇编代码后缀\n大小（字节）\n\n\n\nchar\n字节\nb（byte）\n1 字节\n\n\nshort\n字\nw（word）\n2 字节\n\n\nint\n双字\nl（long）\n4 字节\n\n\nlong\n四字\nq（quad）\n8 字节\n\n\nchar*\n四字（指针）\nq\n8 字节\n\n\nfloat\n单精度\ns（single）\n4 字节\n\n\ndouble\n双精度\nl（long）\n8 字节\n\n\n 汇编后缀的意义：\n在 x86-64 汇编中，许多指令都有后缀来说明操作数的数据大小：\n\nmovb：移动 1 字节（byte）\nmovw：移动 2 字节（word）\nmovl：移动 4 字节（long word）\nmovq：移动 8 字节（quad word）\n\n5.x86-64 架构下的通用寄存器通用寄存器：临时存储运算数据、函数参数、返回值、地址、计数、内存地址等各种信息，能够加快数据处理速度，减少内存读写等\n每个寄存器的用途（按调用约定）如下表所示：\n\n\n\n64位寄存器\n32位\n16位\n8位\n用途说明\n\n\n\n%rax\n%eax\n%ax\n%al\n返回值寄存器\n\n\n%rbx\n%ebx\n%bx\n%bl\n被调用者保存\n\n\n%rcx\n%ecx\n%cx\n%cl\n第4个参数\n\n\n%rdx\n%edx\n%dx\n%dl\n第3个参数\n\n\n%rsi\n%esi\n%si\n%sil\n第2个参数\n\n\n%rdi\n%edi\n%di\n%dil\n第1个参数\n\n\n%rbp\n%ebp\n%bp\n%bpl\n被调用者保存\n\n\n%rsp\n%esp\n%sp\n%spl\n栈指针\n\n\n%r8\n%r8d\n%r8w\n%r8b\n第5个参数\n\n\n%r9\n%r9d\n%r9w\n%r9b\n第6个参数\n\n\n%r10\n%r10d\n%r10w\n%r10b\n调用者保存\n\n\n%r11\n%r11d\n%r11w\n%r11b\n调用者保存\n\n\n%r12\n%r12d\n%r12w\n%r12b\n被调用者保存\n\n\n%r13\n%r13d\n%r13w\n%r13b\n被调用者保存\n\n\n%r14\n%r14d\n%r14w\n%r14b\n被调用者保存\n\n\n%r15\n%r15d\n%r15w\n%r15b\n被调用者保存\n\n\n所有这些寄存器本质上都是 64 位的，但我们可以只访问其中的低 32、16、8 位（如上表第二，第三，第四列所示）\n与专用寄存器的区别：\n\n\n\n类型\n说明\n例子\n\n\n\n通用寄存器\n用于各种灵活数据操作\n%rax, %rdi\n\n\n专用寄存器\n有固定用途\n%rip（指令指针），%cr3（控制）\n\n\n段寄存器\n用于段地址（早期保护模式）\n%cs, %ds\n\n\n标志寄存器\n保存运算结果标志\n%eflags\n\n\n6.x86 汇编寻址方式表寻址（Addressing）就是确定数据所在地址的过程，具体汇编的格式和含义如下表所示：\n\n\n\n类型\n格式\n操作数值表示\n名称（寻址方式）\n\n\n\n立即数\n$Imm\nImm\n立即数寻址（Immediate Addressing）\n\n\n寄存器\nr_a\nR[r_a]\n寄存器寻址（Register Addressing）\n\n\n存储器\nImm\nM[Imm]\n绝对寻址（Absolute Addressing）\n\n\n存储器\n(r_a)\nM[R[r_a]]\n间接寻址（Indirect Addressing）\n\n\n存储器\nImm(r_b)\nM[Imm + R[r_b]]\n基址 + 偏移量（Base + Offset）寻址\n\n\n存储器\n(r_b, r_i)\nM[R[r_b] + R[r_i]]\n变址寻址（Indexed Addressing）\n\n\n存储器\nImm(r_b, r_i)\nM[Imm + R[r_b] + R[r_i]]\n变址 + 偏移量寻址\n\n\n存储器\n(r_i, s)\nM[R[r_i] * s]\n比例变址寻址（Scaled Index）\n\n\n存储器\nImm(,r_i,s)\nM[Imm + R[r_i] * s]\n比例变址 + 偏移量寻址\n\n\n存储器\n(r_b, r_i, s)\nM[R[r_b] + R[r_i] * s]\n比例变址 + 基址寻址\n\n\n存储器\nImm(r_b, r_i, s)\nM[Imm + R[r_b] + R[r_i] * s]\n比例变址 + 基址 + 偏移量寻址\n\n\n格式指的是在汇编语言中书写一个操作数（比如常量、寄存器或内存地址）时，它的语法结构长什么样，也就是“写法”。\n操作数表示是指汇编语言中那个操作数在执行时真正的数值含义，如下表所示：\n\n\n\n概念\n意义\n举例\n\n\n\n格式\n汇编里写法\n8(%rbp)\n\n\n操作数表示\n真正要访问的值的含义\nM[8 + R[rbp]]\n\n\n举例：\n\n立即数寻址（Immediate Addressing）\n\nmov $5, %eax      # 把常量 5 移到寄存器 eax 中\n\n\n寄存器寻址（Register Addressing）\n\nmov %ebx, %eax    # 把 %ebx 里的值复制到 %eax\n\n\n间接寻址（Indirect Addressing)\n\nmov (%rbx), %eax  # 把 %rbx 指向的内存地址中的值加载到 %eax\n\n\n基址 + 偏移寻址（Base + Offset)\n\nmov 8(%rbp), %eax # 取栈帧中偏移 8 字节的变量到 %eax\n\n","categories":["《深入理解计算机系统》"],"tags":["汇编语言"]},{"title":"x86汇编学习(四)","url":"/2025/06/04/x86%E6%B1%87%E7%BC%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"数据对齐数据按对齐要求存放，可以让处理器和内存系统更高效地访问数据\n对齐的要求比如 64 位系统（x86-64），一个 double 或 long 类型的变量通常要求8字节对齐，也就是说，变量的地址必须是8的倍数。\n下表格列出了常见类型的对齐要求（K）：\n\n\n\nK\n类型\n\n\n\n1\nchar\n\n\n2\nshort\n\n\n4\nint, float\n\n\n8\nlong, double, char*\n\n\n对齐的实现（以结构体为例）编译器会自动插入填充字节（padding），确保每个成员的起始地址都满足其类型的对齐要求，整个结构体的大小也是最大对齐数的倍数\n举例\nstruct S1 &#123;    int i;    char c;    int j;&#125;;\n\n\n这里 int 要求4字节对齐，char 要求1字节对齐。\n如果没有填充，i（4字节）后面紧跟c（1字节），接着就是j。这样j的起始地址不是4的倍数，不符合对齐要求。\n编译器会在c和j之间插入3字节填充，使得j从8的倍数地址开始。\n结构体总大小也要满足最大对齐数的倍数。\n\n偏移 0    4   5     8内容 i |  c | 填充 | j\n\n注意：结构体 S1 的最大对齐值是4，所以整个结构体的对齐值就是4。\n内存越界引用和缓冲区溢出C 语言对数组的访问没有边界检查，如果写操作越界，不仅会破坏数据本身，还可能破坏栈上的局部变量和保存的状态信息（比如返回地址、寄存器值等）。\n如果程序使用了被破坏的状态，再执行比如函数返回（ret 指令），就会出错，甚至导致程序劫持（如攻击者可控制返回地址）。\n举例：\n/* Implementation of library function gets() */char *gets(char *s)&#123;    int c;    char *dest = s;    while ((c = getchar()) != &#x27;\\n&#x27; &amp;&amp; c != EOF)        *dest++ = c;    if (c == EOF &amp;&amp; dest == s)        /* No characters read */        return NULL;    *dest++ = &#x27;\\0&#x27;; /* Terminate string */    return s;&#125;/* Read input line and write it back */void echo()&#123;    char buf[8];    /* Way too small! */    gets(buf);    puts(buf);&#125;\n\n前面的代码实现了标准库函数 gets，并用它来说明该函数存在的严重缺陷。gets 会从标准输入读取一整行内容，直到遇到回车换行符或发生错误为止，然后把读到的字符串复制到参数 s 指定的位置，并在末尾添加一个 null 字符。在 echo 这个例子中，我们调用了 gets，让它从标准输入读取一行内容并输出到标准输出。\ngets 最大的问题在于，它无法判断为存放整个字符串预留的空间是否足够。在 echo 的演示代码中，缓冲区长度被故意设置得很小，只有 8 个字节。因此，只要输入的字符串长度超过 7 个字符（最后一位要留给结束符），就会发生越界写入的问题。\n对应的汇编代码如下：\nvoid echo()1  echo:2      subq $24, %rsp      # Allocate 24 bytes on stack3      movq %rsp, %rdi     # Compute buf as %rsp4      call gets           # Call gets5      movq %rsp, %rdi     # Compute buf as %rsp6      call puts           # Call puts7      addq $24, %rsp      # Deallocate stack space8      ret                 # Return\n\n该程序在栈上分配了 24 个字节。字符数组 buf 位于栈顶，可以看到，%rsp 被复制到 %rdi 作为调用 gets 和 puts 的参数。这个调用的参数和存储的返回指针之间的 16 字节是未被使用的。只要用户输入不超过 7 个字符，gets 返回的字符串（包括结尾的 null）就能够放进为 buf 分配的空间里。不过，长一些的字符串就会导致 gets 覆盖栈上存储的某些信息。随着字符串变长，下面的信息可能会被破坏：\n\n\n\n输入的字符串数量\n附加的被破坏的状态\n\n\n\n0~7\n无\n\n\n8~23\n未被使用的栈空间\n\n\n24~31\n返回地址\n\n\n32+\ncaller 中保存的状态\n\n\n只要输入的字符串长度不超过 23 个字符，一般不会导致严重的问题。但如果输入再长一些，返回地址以及更多保存的状态信息就可能被覆盖和破坏。如果返回地址被覆盖，当执行 ret 指令（第 8 行）时，程序可能会跳转到一个完全不可预料的位置，导致异常行为。这种内存越界写的问题在看 C 语言源码时是无法直接发现的，只有通过研究底层机器级别的程序，才能真正理解像 gets 这种函数在内存越界写时带来的危害。\n","categories":["《深入理解计算机系统》"],"tags":["汇编语言"]},{"title":"x86汇编学习(二)","url":"/2025/05/28/x86%E6%B1%87%E7%BC%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"汇编指令学习MOV指令 下表列出的是最简单形式的数据传送指令-MOV类。这些指令把数据从源位置复制到目的位置，不做任何变化。 MOV 类由四条指令组成： movb、 movw、 movl 和 movq。这些指令都执行同样的操作；主要区别在千它们操作的数据大小不同：分别是 1、 2 、 4 和 8 字节。\n\n\n\n指令\n效果\n描述\n\n\n\nMOV S, D\nD ← S\n传送\n\n\nmovb\n\n传送字节\n\n\nmovw\n\n传送字\n\n\nmovl\n\n传送双字\n\n\nmovq\n\n传送四字\n\n\nmovabsq I, R\nR ← I\n传送绝对的四字\n\n\n源操作数指定的值是一个立即数，存储在寄存器中或者内存中。目的操作数指定一个 位置，要么是一个寄存器或者，要么是一个内存地址。 x86-64 加了一条限制，传送指令的 两个操作数不能都指向内存位置。将一个值从一个内存位置复制到另一个内存位置需要两条指令——第一条指令将源值加载到寄存器中，第二条将该寄存器值写入目的位置。\n下面的 MOV指令示例给出了源和目的类型的五种可能的组合。第一个是源操作数，第二个是目的操作数：\n\n\n\n序号\n指令\n源类型 → 目的类型\n字节数\n\n\n\n1\nmovl $0x4050, %eax\nImmediate → Register\n4 bytes\n\n\n2\nmovw %bp, %sp\nRegister → Register\n2 bytes\n\n\n3\nmovb (%rdi, %rcx), %al\nMemory → Register\n1 byte\n\n\n4\nmovb $-17, (%rsp)\nImmediate → Memory\n1 byte\n\n\n5\nmovq %rax, -12(%rbp)\nRegister → Memory\n8 bytes\n\n\n数据扩展传送指令当你把一个小的数据（如 1 字节）存入一个大的寄存器（如 4 字节）时，需要决定怎么填补高位。 这个过程叫“扩展（Extend）”，有两种方式：\n\n\n\n扩展方式\n含义\n\n\n\n零扩展（Zero-Extend）\n高位填 0，不改变原数\n\n\n符号扩展（Sign-Extend）\n高位按符号位（最高位）填充，保持数值含义\n\n\n零扩展传送指令\n\n\n\n指令\n效果\n描述\n\n\n\nmovzbw\n把字节零扩展到字\n高位填 0，目标是16位\n\n\nmovzbl\n把字节零扩展到双字\n高位填 0，目标是32位\n\n\nmovzwl\n把字零扩展到双字\n把16位数扩展为32位\n\n\nmovzbq\n把字节零扩展到四字\n把8位扩展为64位（x86-64用）\n\n\nmovzwq\n把字零扩展到四字\n把16位扩展为64位（x86-64用）\n\n\n符号扩展传送指令\n\n\n\n指令\n效果\n描述\n\n\n\nmovsbw\n把字节符号扩展到字\n符号位复制到高位\n\n\nmovsbl\n把字节符号扩展到双字\n8 → 32 位，保持正负性\n\n\nmovswl\n把字符号扩展到双字\n16 → 32 位\n\n\nmovsbq\n把字节符号扩展到四字\n8 → 64 位\n\n\nmovswq\n把字符号扩展到四字\n16 → 64 位\n\n\nmovslq\n把双字符号扩展到四字\n32 → 64 位\n\n\ncltq\nrax ← sign_extend(eax)\n把 eax 的符号扩展复制到 rax\n\n\n示例\nC语言代码如下:\nlong exchange(long *xp, long y)&#123;    long x = *xp;    *xp = y;    return x;&#125;\n\n对应的汇编代码如下：\nexchange:    movq    (%rdi), %rax      # Get x at xp. Set as return value.    movq    %rsi, (%rdi)      # Store y at xp.    ret                       # Return.\n\n寄存器约定：\n\nxp 存放在 %rdi\ny  存放在 %rsi\n\n当过程开始执行时，过程参数 xp 和 y分别存储在寄存器%rdi 和%rsi 中。然后，指令 2 从内存中读出 x, 把它存放到寄存器%rax 中，直接实现了 C程序中的操作 x=*xp。然后用寄存器%rax从这个函数返回一个值，因而返回值就是 x。将 y写入到寄存器%rdi 中的 xp 指向的内存位置，直接实现了操作*xp=y。这个例子说明了如何用 MOV 指令从内存中读值到寄存器（第 2 行），如何从寄存器写到内存（第 3 行）。\n有两点值得注意。首先，我们看到 C语言中所谓的”指针”其实就是地址。间接引用指针就是将该指针放在一个寄存器中，然后在内存引用中使用这个寄存器。其次，像 x这样的局部变量通常是保存在寄存器中，而不是内存中。访问寄存器比访问内存要快得多。\nPUSH &amp;&amp; POP 指令pushq 把一个值压入栈中，popq 从栈中弹出一个值\n栈的基本概念：\n\n栈（stack）是一种先进后出（LIFO）的数据结构\n在 x86-64 架构中，栈是向低地址增长的\n栈的“顶”是当前栈的最新元素，指针 %rsp 指向这个栈顶\n\npushq 指令详解（压栈）把一个四字（8字节）值压入栈中\n实现步骤：\n\n先将 %rsp 减 8（因为栈向下生长）\n再把值存入新地址 [rsp]\n\n\n\n\n指令\n效果\n描述\n\n\n\npushq S\nR[%rsp] ← R[%rsp] - 8``M[R[%rsp]] ← S\n将四字压入栈\n\n\n因此，指令pushq %rbp等价于：\nsubq $8, %rspmovq %rbp, (%rsp)\n\n\n\npopq 指令详解（弹栈）从栈顶取出一个四字（8字节）值赋给寄存器\n实现步骤：\n\n从 [rsp] 读取值赋给目标寄存器\n再将 %rsp 加 8，栈顶上移\n\n\n\n\n指令\n效果\n描述\n\n\n\npopq D\nD ← M[R[%rsp]]``R[%rsp] ← R[%rsp] + 8\n将四字弹出栈\n\n\n因此，指令popq %rbp等价于：\nmovq (%rsp), %rbp     ; 把栈顶的值写入 %rbpaddq $8, %rsp         ; 栈指针回退，栈“弹出”一个值\n\n算术与逻辑指令\n\n\n指令\n效果\n描述\n\n\n\nleaq S, D\nD ← &amp;S\n加载有效地址\n\n\nINC D\nD ← D + 1\n加1\n\n\nDEC D\nD ← D - 1\n减1\n\n\nNEG D\nD ← -D\n取负\n\n\nNOT D\nD ← ~D\n取补\n\n\nADD S, D\nD ← D + S\n加法\n\n\nSUB S, D\nD ← D - S\n减法\n\n\nIMUL S, D\nD ← D * S\n乘法\n\n\nXOR S, D\nD ← D ^ S\n异或\n\n\nOR S, D\n&#96;D ← D\nS&#96;\n\n\nAND S, D\nD ← D &amp; S\n与\n\n\nSAL k, D\nD ← D &lt;&lt; k\n左移（算术）\n\n\nSHL k, D\nD ← D &lt;&lt; k\n左移（与SAL相同）\n\n\nSAR k, D\nD ← D &gt;&gt;a k\n算术右移（保留符号）\n\n\nSHR k, D\nD ← D &gt;&gt;l k\n逻辑右移（高位补0）\n\n\n上述加载有效地址（load effective address）指令 leaq 实际上是 movq 指令的变形。它的指令形式是从内存读数据到寄存器，但实际上它根本就没有引用内存。它的第一个操作数看上去是一个内存引用，但该指令并不是从指定的位置读入数据，而是将有效地址写入到目的操作数。类似于 C 语言中的 &amp;x —— 不是取变量的值，而是取变量的“地址”。\n特殊的算术操作当两个 64 位的整数相乘时，结果可能超过 64 位，因此需要 128 位 来表示结果。\nx86-64 指令集中支持这种情况的指令并不多，专门提供了一些用于处理 128 位乘积和除法 的指令。\n\n\n\n指令\n效果\n描述\n\n\n\nimulq S\n%rdx:%rax ← S × %rax（符号扩展）\n有符号乘法\n\n\nmulq S\n%rdx:%rax ← S × %rax（零扩展）\n无符号乘法\n\n\nclto\n%rdx ← 符号扩展(%rax)\n转换为八字\n\n\nidivq S\n%rax ← (%rdx:%rax) ÷ S``%rdx ← (%rdx:%rax) mod S\n有符号除法\n\n\ndivq S\n%rax ← (%rdx:%rax) ÷ S``%rdx ← (%rdx:%rax) mod S\n无符号除法\n\n\nimulq S：\n有符号乘法，将 S 与 %rax 相乘，结果保存在 %rdx:%rax 中。\n有“符号扩展”的作用，适用于带正负号的整数。\n\nmulq S：\n无符号乘法（没有正负号），同样保存在 %rdx:%rax。\n\n举例：\nmovq $5, %raxmovq $6, %rbximulq %rbx   ; S = %rbx\n\nRFLAGS寄存器除了整数寄存器之外，CPU 还维护一个单个位的条件码寄存器（flag），用来记录最近的操作是否：\n\n有溢出\n结果是否为 0\n是否是负数\n是否进位\n\n最常用的条件码有：\n\n\n\n条件码\n含义\n说明\n\n\n\nCF\nCarry Flag\n进位标志：无符号加法进位、减法借位\n\n\nZF\nZero Flag\n零标志：结果为 0\n\n\nSF\nSign Flag\n符号标志：结果为负（最高位为 1）\n\n\nOF\nOverflow Flag\n溢出标志：有符号加法或减法结果溢出（例如正数+正数得到负数）\n\n\n举例1：\nmov $0xFFFFFFFFFFFFFFFF, %raxadd $1, %rax     ; 溢出了\n\n%rax 原本是最大无符号值，加 1 会溢出，导致：\n\nCF = 1（发生进位）\nZF = 1（结果变为 0）\n\n举例2：\ncmp %rax, %rbx   ; 实际做的是: %rbx - %rax，设置条件码je equal_label   ; 如果 ZF == 1，跳转（即相等）\n\ncmp %rax, %rbx 会执行：%rbx - %rax，但不保存结果，只是设置条件码（flags）\n如果 结果为 0（即 %rax == %rbx），那么 ZF（Zero Flag）会被置为 1\n紧接着 je equal_label 会检查 ZF 是否为 1：\n\n如果是，跳转到标签 equal_label 处继续执行\n否则，顺序执行下一条指令\n\nSET指令指令是用来“读取条件码的状态”，并把它转换成布尔值（0 或 1）存入一个字节中（只能设置一个字节）。\n举例：\ncmp %rax, %rbx     ; 比较 %rbx - %rax，设置条件码setl %al           ; 如果 %rbx &lt; %rax（有符号），%al = 1，否则 = 0\n\n上述通过读取 RFLAGS 寄存器中的条件码位（如 ZF、SF、OF、CF）来判断是否成立。\n\n\n\n指令\n同义名\n效果\n设置条件\n\n\n\nsete D\nsetz\nD ← ZF\n相等 &#x2F; 零\n\n\nsetne D\nsetnz\nD ← ¬ZF\n不等 &#x2F; 非零\n\n\nsets D\n\nD ← SF\n负数\n\n\nsetns D\n\nD ← ¬SF\n非负数\n\n\nsetg D\nsetnle\nD ← (SF = OF) ∧ ¬ZF\n大于（有符号）\n\n\nsetge D\nsetnl\nD ← (SF = OF)\n大于等于（有符号）\n\n\nsetl D\nsetnge\nD ← SF ≠ OF\n小于（有符号）\n\n\nsetle D\nsetng\nD ← (SF ≠ OF) ∨ ZF\n小于等于（有符号）\n\n\nseta D\nsetnbe\nD ← ¬CF ∧ ¬ZF\n大于（无符号）\n\n\nsetae D\nsetnb\nD ← ¬CF\n大于等于（无符号）\n\n\nsetb D\nsetnae\nD ← CF\n小于（无符号）\n\n\nsetbe D\nsetna\nD ← CF ∨ ZF\n小于等于（无符号）\n\n\n跳转指令跳转指令的作用包括，实现条件判断（if &#x2F; else），实现循环（while &#x2F; for &#x2F; do while），实现函数跳转 &#x2F; 返回， 实现无条件跳转（goto）\n\n\n\n指令\n同义名\n跳转条件\n描述\n\n\n\njmp Label\n\n1\n直接跳转\n\n\njmp *Operand\n\n\n间接跳转\n\n\nje Label\njz\nZF\n相等 &#x2F; 零\n\n\njne Label\njnz\n¬ZF\n不相等 &#x2F; 非零\n\n\njs Label\n\nSF\n负数\n\n\njns Label\n\n¬SF\n非负数\n\n\njg Label\njnle\n(SF &#x3D; OF) ∧ ¬ZF\n大于（有符号）\n\n\njge Label\njnl\nSF &#x3D; OF\n大于等于（有符号）\n\n\njl Label\njnge\nSF ≠ OF\n小于（有符号）\n\n\njle Label\njng\n(SF ≠ OF) ∨ ZF\n小于等于（有符号）\n\n\nja Label\njnbe\n¬CF ∧ ¬ZF\n超过（无符号）\n\n\njae Label\njnb\n¬CF\n超过或相等（无符号≥）\n\n\njb Label\njnae\nCF\n低于（无符号）\n\n\njbe Label\njna\nCF ∨ ZF\n低于或相等（无符号≤）\n\n\n","categories":["《深入理解计算机系统》"],"tags":["汇编语言"]},{"title":"内核socket套接字的创建","url":"/2025/05/21/%E5%86%85%E6%A0%B8socket%E5%88%9B%E5%BB%BA/","content":"内核socket创建用户程序执行syscall指令，系统会从用户态陷入内核态并根据传入的系统调用号（例如在x86架构下socket系统调用号为41）从系统调用表中找到对应的处理函数，socekt（）系统调用对应的处理函数如下所示：\nSYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)&#123; return __sys_socket(family, type, protocol);&#125;\n\n上述宏SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)经过一系列展开后其实就是系统调用表中系统调用号41对应的处理函数\n上述__sys_socket函数定义如下：\n1.sock_create（）int __sys_socket(int family, int type, int protocol)&#123; struct socket *sock; int flags; //根据传入的参数创建socket sock = __sys_socket_create(family, type,       update_socket_protocol(family, type, protocol)); if (IS_ERR(sock))  return PTR_ERR(sock); //不关心低4bit sock的type flags = type &amp; ~SOCK_TYPE_MASK; if (SOCK_NONBLOCK != O_NONBLOCK &amp;&amp; (flags &amp; SOCK_NONBLOCK))  //清位之后置位  flags = (flags &amp; ~SOCK_NONBLOCK) | O_NONBLOCK; //将socket映射一个文件描述符号 return sock_map_fd(sock, flags &amp; (O_CLOEXEC | O_NONBLOCK));&#125;\n\n该函数主要做了两个事情，创建socekt和映射描述符fd。\n上面创建socket函数__sys_socket_create定义如下：\n2.__sys_socket_create（）static struct socket *__sys_socket_create(int family, int type, int protocol)&#123; struct socket *sock; int retval; /* Check the SOCK_* constants for consistency.  */ BUILD_BUG_ON(SOCK_CLOEXEC != O_CLOEXEC); BUILD_BUG_ON((SOCK_MAX | SOCK_TYPE_MASK) != SOCK_TYPE_MASK); BUILD_BUG_ON(SOCK_CLOEXEC &amp; SOCK_TYPE_MASK); BUILD_BUG_ON(SOCK_NONBLOCK &amp; SOCK_TYPE_MASK); //用户参数合法性检查，是否有非法标志位 if ((type &amp; ~SOCK_TYPE_MASK) &amp; ~(SOCK_CLOEXEC | SOCK_NONBLOCK))  return ERR_PTR(-EINVAL); type &amp;= SOCK_TYPE_MASK; //创建套接字 retval = sock_create(family, type, protocol, &amp;sock); if (retval &lt; 0)  return ERR_PTR(retval); return sock;&#125;\n\n上述sock_create为实际创建socket的函数，它包裹了__sock_create()如下所示：\nint sock_create(int family, int type, int protocol, struct socket **res)&#123; return __sock_create(current-&gt;nsproxy-&gt;net_ns, family, type, protocol, res, 0);&#125;\n\n可以看到__sock_create多带了一个参数current-&gt;nsproxy-&gt;net_ns  这个current是一个宏，用于获取当前进程的task_struct指针\ncurrent宏定义如下:\nstatic __always_inline struct task_struct *get_current(void)&#123; //这个pcpu_hot中有一个字段就是current_task也就是task_struct //this_cpu_read_stable()就是读取per_cpu变量的一个宏 return this_cpu_read_stable(pcpu_hot.current_task);//从per-cpu变量中获取当前的task_sturct结构&#125;\n\n上述current宏其实等同于指向一个task_struct的指针，而__sock_create(current-&gt;nsproxy-&gt;net_ns, family, type, protocol, res, 0);中参数current-&gt;nsproxy-&gt;net_ns就是指向一个具体的网络命名空间，为什么要传入这个网络命令空间作为参数？举个例子，如果进程属于某个容器的网络命名空间，创建的套接字必须关联到该容器的网络栈，而非宿主机的默认命名空间， 比如在创建docker进程的时候，就会设置sproxy-&gt;net_ns所属的网络命令空间。最终的目的一定是为了流量隔离。\n接下来看一下真正创建socket的函数__sock_create\n3.__sock_create()int __sock_create(struct net *net, int family, int type, int protocol,\t\t\t struct socket **res, int kern)&#123;\tint err;\tstruct socket *sock;\tconst struct net_proto_family *pf;\t//合法性检查\tif (family &lt; 0 || family &gt;= NPROTO)\t\treturn -EAFNOSUPPORT;\tif (type &lt; 0 || type &gt;= SOCK_MAX)\t\treturn -EINVAL;\t//过时的 PF_INET + SOCK_PACKET 参数组合转换为现代支持的 PF_PACKET 协议族\tif (family == PF_INET &amp;&amp; type == SOCK_PACKET) &#123;\t\tpr_info_once(&quot;%s uses obsolete (PF_INET,SOCK_PACKET)\\n&quot;,\t\t\t     current-&gt;comm);\t\tfamily = PF_PACKET;\t&#125;\t//安全相关的钩子\terr = security_socket_create(family, type, protocol, kern);\tif (err)\t\treturn err;\t//分配并初始化一个套接字对应的 inode 和socket 结构\tsock = sock_alloc();\tif (!sock) &#123;\t\tnet_warn_ratelimited(&quot;socket: no more sockets\\n&quot;);\t\treturn -ENFILE;\t//这里的type也是用户创建socket的type\tsock-&gt;type = type;\trcu_read_lock();\t//从sock_register数组中找到一个元素pf，这个pf中有一个create()回调函数，\t//这个回调函数就是family类型(比如AF_INET)需要的create函数。\tpf = rcu_dereference(net_families[family]);\terr = -EAFNOSUPPORT;\tif (!pf)\t\tgoto out_release;\t//增加引用计数？有些family可能是以模块方式加载的？？？\tif (!try_module_get(pf-&gt;owner))\t\tgoto out_release;\t/* Now protected by module ref count */\trcu_read_unlock();\t//如果用户指定的family类型是AF_INIT,那这个函数就是调用的inet_create()\terr = pf-&gt;create(net, sock, protocol, kern);\tif (err &lt; 0)\t\tgoto out_module_put;\tif (!try_module_get(sock-&gt;ops-&gt;owner))\t\tgoto out_module_busy;\t//减引用计数\tmodule_put(pf-&gt;owner);\t//安全模块相关\terr = security_socket_post_create(sock, family, type, protocol, kern);\tif (err)\t\tgoto out_sock_release;\t*res = sock;\treturn 0;out_module_busy:\terr = -EAFNOSUPPORT;out_module_put:\tsock-&gt;ops = NULL;\tmodule_put(pf-&gt;owner);out_sock_release:\tsock_release(sock);\treturn err;out_release:\trcu_read_unlock();\tgoto out_sock_release;&#125;EXPORT_SYMBOL(__sock_create);\n\n\n上述代码中通过调用sock_alloc()分配了inode和socket结构体，并对inode结构体进行初始化，比如设置唯一的inode编号等，具体代码如下：\nstruct socket *sock_alloc(void)&#123;\tstruct inode *inode;\tstruct socket *sock;\t//调用socket文件系统的超级块的ops申请一个inode，注意：socket结构体也是在这里分配的\tinode = new_inode_pseudo(sock_mnt-&gt;mnt_sb);\tif (!inode)\t\treturn NULL;\t//通过container_of拿到socket结构体 \tsock = SOCKET_I(inode);\tinode-&gt;i_ino = get_next_ino();//分配唯一的inode编号\tinode-&gt;i_mode = S_IFSOCK | S_IRWXUGO; //文件类型\tinode-&gt;i_uid = current_fsuid();\tinode-&gt;i_gid = current_fsgid();\tinode-&gt;i_op = &amp;sockfs_inode_ops;//绑定ops\treturn sock;&#125;\n\n上述代码通过调用new_inode_pseudo()创建了inode和socket，socket的获取通过宏SOCKET_I（container_of）返回socket其中sock_mnt是一个vfsmount(可以理解为一个挂载点)结构mnt_sb为一个超级块，在sock_init()中被挂载，sock_init()在start_kernel中会最终被调用到。\npf = rcu_dereference(net_families[family]); 这一行作用是根据用户传入的不同的协议族（比如AF_INET）来选择具体的回调函数，然后会调用pf-&gt;create(net, sock, protocol, kern); 这个-&gt;create() 就取决于family的类型。对于AF_INET类型的family，就是调用inet_create()，注册的过程由sock_register()实现，该函数就是将不同的family类型，注册到一个数组中(这个数组叫net_families)。对应的函数如下：\nint sock_register(const struct net_proto_family *ops)&#123;\tint err;\tif (ops-&gt;family &gt;= NPROTO) &#123;\t\tpr_crit(&quot;protocol %d &gt;= NPROTO(%d)\\n&quot;, ops-&gt;family, NPROTO);\t\treturn -ENOBUFS;\t&#125;\tspin_lock(&amp;net_family_lock);\tif (rcu_dereference_protected(net_families[ops-&gt;family],\t\t\t\t      lockdep_is_held(&amp;net_family_lock)))\t\terr = -EEXIST;\telse &#123;\t\t//这里注册了不同family类型到net_families数组中！\t\trcu_assign_pointer(net_families[ops-&gt;family], ops);\t\terr = 0;\t&#125;\tspin_unlock(&amp;net_family_lock);\tpr_info(&quot;NET: Registered %s protocol family\\n&quot;, pf_family_names[ops-&gt;family]);\treturn err;&#125;\n\n对于AF_INET(ipv4)协议族，上述注册的函数为inet_create()，在inet_init()中被调用，同样inet_init()也是最终被start_kernel()调用到。\n也就是说err = pf-&gt;create(net, sock, protocol, kern);会根据协议族的类型调用不同的create函数，同时传入用户制定的类型(TYPE)和协议做为参数，下面默认使用ipv4协议族进行举例，待分析函数就是inet_create()函数实现如下所示：\n也就是说err = pf-&gt;create(net, sock, protocol, kern);会根据协议族的类型调用不同的create函数，同时传入用户制定的类型(TYPE)和协议做为参数，下面使用ipv4协议族进行举例，对应的函数就是inet_create()，该函数其实主要处理了三个逻辑：\n\n根据用户制定协议从inetsw找到socket和sock对应的ops\n创建sock结构，并进行一系列的初始化（例如绑定sock的ops，这里不同的协议对应不同的ops）\n调用sock的init函数，完成对具体协议的初始化inet_create函数定义如下：\n\nstatic int inet_create(struct net *net, struct socket *sock, int protocol,\t\t       int kern)&#123;\tstruct sock *sk;\tstruct inet_protosw *answer;\tstruct inet_sock *inet;\tstruct proto *answer_prot;\tunsigned char answer_flags;\tint try_loading_module = 0;\tint err;\t\t//参数合法性检查\tif (protocol &lt; 0 || protocol &gt;= IPPROTO_MAX)\t\treturn -EINVAL;\t//初始化socket的状态\tsock-&gt;state = SS_UNCONNECTED;\t/* Look for the requested type/protocol pair. */lookup_protocol:\terr = -ESOCKTNOSUPPORT;\trcu_read_lock();\t//遍历inetsw[sock-&gt;type]这个元素的链表，找到protocol相同的元素，\tlist_for_each_entry_rcu(answer, &amp;inetsw[sock-&gt;type], list) &#123;\t\terr = 0;\t\t/* Check the non-wild match. */\t\t//精确匹配，用户指定的protocol和链表中的某个元素相同。\t\tif (protocol == answer-&gt;protocol) &#123;\t\t\tif (protocol != IPPROTO_IP)\t\t\t\tbreak;\t\t&#125; else &#123;\t\t\t/* Check for the two wild cases. */\t\t\t//如果用户指定的proto是0那就走这个分支，\t\t\t//比如type是SOCK_STREAM，proto=0 那answer关联的就是TCP\t\t\tif (IPPROTO_IP == protocol) &#123;\t\t\t\tprotocol = answer-&gt;protocol;\t\t\t\tbreak;\t\t\t&#125;\t\t\tif (IPPROTO_IP == answer-&gt;protocol)\t\t\t\tbreak;\t\t&#125;\t\terr = -EPROTONOSUPPORT;\t&#125;\t//错误的处理\tif (unlikely(err)) &#123;\t\tif (try_loading_module &lt; 2) &#123;\t\t\trcu_read_unlock();\t\t\t/*\t\t\t * Be more specific, e.g. net-pf-2-proto-132-type-1\t\t\t * (net-pf-PF_INET-proto-IPPROTO_SCTP-type-SOCK_STREAM)\t\t\t */\t\t\tif (++try_loading_module == 1)\t\t\t\trequest_module(&quot;net-pf-%d-proto-%d-type-%d&quot;,\t\t\t\t\t       PF_INET, protocol, sock-&gt;type);\t\t\t/*\t\t\t * Fall back to generic, e.g. net-pf-2-proto-132\t\t\t * (net-pf-PF_INET-proto-IPPROTO_SCTP)\t\t\t */\t\t\telse\t\t\t\trequest_module(&quot;net-pf-%d-proto-%d&quot;,\t\t\t\t\t       PF_INET, protocol);\t\t\tgoto lookup_protocol;\t\t&#125; else\t\t\tgoto out_rcu_unlock;\t&#125;\terr = -EPERM;\t//用户有权限才能创建raw socket套接字\tif (sock-&gt;type == SOCK_RAW &amp;&amp; !kern &amp;&amp;\t    !ns_capable(net-&gt;user_ns, CAP_NET_RAW))\t\tgoto out_rcu_unlock;\t//将上述找到的answer-&gt;ops赋值给socket的ops\tsock-&gt;ops = answer-&gt;ops;\t//将上述找到的answer-&gt;ops赋值给answer_prot，下面创建sock结构的时候会用到\tanswer_prot = answer-&gt;prot;\tanswer_flags = answer-&gt;flags;\trcu_read_unlock();\tWARN_ON(!answer_prot-&gt;slab);\terr = -ENOMEM;\t//注意： 这里申请一个sock结构，这个sock结构可以理解为传输层协议和socket之间的一个中间层\t//对上提供socket层的结构，\t//对下与具体的协议相关\t//kern 标识这个套接字是否是内核创建的\tsk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot, kern);\tif (!sk)\t\tgoto out;\terr = 0;\t//标识端口是否可以重用 这里raw 和icmp是设置了INET_PROTOSW_REUSE 这个标志位。\tif (INET_PROTOSW_REUSE &amp; answer_flags)\t\tsk-&gt;sk_reuse = SK_CAN_REUSE;\tinet = inet_sk(sk);\t//是否是一个面向连接套接字，对于TCP是有这个标志位的\tinet_assign_bit(IS_ICSK, sk, INET_PROTOSW_ICSK &amp; answer_flags);\tinet_clear_bit(NODEFRAG, sk);\t//如果是rawsocket就指定了端口号？\tif (SOCK_RAW == sock-&gt;type) &#123;\t\tinet-&gt;inet_num = protocol;\t\tif (IPPROTO_RAW == protocol)\t\t\tinet_set_bit(HDRINCL, sk);\t&#125;\t//根据系统参数决定是否开启mtu探测\tif (READ_ONCE(net-&gt;ipv4.sysctl_ip_no_pmtu_disc))\t\tinet-&gt;pmtudisc = IP_PMTUDISC_DONT;\telse\t\tinet-&gt;pmtudisc = IP_PMTUDISC_WANT;\t\t//设置ip_id字段\tatomic_set(&amp;inet-&gt;inet_id, 0);\t//这里初始化了上面申请的sock结构体的各个字段\tsock_init_data(sock, sk);\tsk-&gt;sk_destruct\t   = inet_sock_destruct;\t//这里记录了用户指定的协议\tsk-&gt;sk_protocol\t   = protocol;\tsk-&gt;sk_backlog_rcv = sk-&gt;sk_prot-&gt;backlog_rcv;\tsk-&gt;sk_txrehash = READ_ONCE(net-&gt;core.sysctl_txrehash);\t//初始化inet_sock的一些字段 单播/多播ttl，tos，管理多播的mc_list\tinet-&gt;uc_ttl\t= -1;   \tinet_set_bit(MC_LOOP, sk);\tinet-&gt;mc_ttl\t= 1;\tinet_set_bit(MC_ALL, sk);\tinet-&gt;mc_index\t= 0;\tinet-&gt;mc_list\t= NULL;\tinet-&gt;rcv_tos\t= 0;\t//tcp或者udp 应该不会走这个逻辑，因为还没有调用bind，inet_num此时应该为0\tif (inet-&gt;inet_num) &#123;\t\tinet-&gt;inet_sport = htons(inet-&gt;inet_num);\t\t/* Add to protocol hash chains. */\t\terr = sk-&gt;sk_prot-&gt;hash(sk);\t\tif (err) &#123;\t\t\tsk_common_release(sk);\t\t\tgoto out;\t\t&#125;\t&#125;\t//这里是特定协议的初始化逻辑\tif (sk-&gt;sk_prot-&gt;init) &#123;\t\terr = sk-&gt;sk_prot-&gt;init(sk);\t\tif (err) &#123;\t\t\tsk_common_release(sk);\t\t\tgoto out;\t\t&#125;\t&#125;\tif (!kern) &#123;\t\terr = BPF_CGROUP_RUN_PROG_INET_SOCK(sk);\t\tif (err) &#123;\t\t\tsk_common_release(sk);\t\t\tgoto out;\t\t&#125;\t&#125;out:\treturn err;out_rcu_unlock:\trcu_read_unlock();\tgoto out;&#125;\n\n上述代码首先根据用户指定的type和protocol类型从inewsw[]中找到匹配的socket和sock的ops，注意这里inewsw[]是一个数组，数组中的每个元素又是一个链表，其实可以理解成一个hash表，hash表的key是type，而protocol是用来寻找某个桶中的的具体的一个元素。上述的inet_sw数组中的元素是由inetsw_array[]中填充进来的，填充的过程在inet_init()函数中实现。inetsw_array[]数组的定义和填充inetsw[]的代码如下：\n//这个数组的作用就是把数组中的元素注册到inet_sw[]中static struct inet_protosw inetsw_array[] =&#123;\t&#123;\t\t.type =       SOCK_STREAM,\t\t.protocol =   IPPROTO_TCP,\t\t.prot =       &amp;tcp_prot,\t\t.ops =        &amp;inet_stream_ops,\t\t.flags =      INET_PROTOSW_PERMANENT |\t\t\t      INET_PROTOSW_ICSK,\t&#125;,\t&#123;\t\t.type =       SOCK_DGRAM,\t\t.protocol =   IPPROTO_UDP,\t\t.prot =       &amp;udp_prot,\t\t.ops =        &amp;inet_dgram_ops,\t\t.flags =      INET_PROTOSW_PERMANENT,       &#125;,       &#123;\t\t.type =       SOCK_DGRAM,\t\t.protocol =   IPPROTO_ICMP,\t\t.prot =       &amp;ping_prot,\t\t.ops =        &amp;inet_sockraw_ops,\t\t.flags =      INET_PROTOSW_REUSE,       &#125;,       &#123;\t       .type =       SOCK_RAW,\t       .protocol =   IPPROTO_IP,\t/* wild card */\t       .prot =       &amp;raw_prot,\t       .ops =        &amp;inet_sockraw_ops,\t       .flags =      INET_PROTOSW_REUSE,       &#125;&#125;;\n\n上述代码为inetsw_array[]数组，其中prot为socket的ops，用户态不同的系统调用会调用到socket的不同ops上。prot则为具体协议的ops。也就是说ops是socket关联的回调函数，prot为sock关联的回调函数，两者其实是密切相关的，可以理解为ops是用户与内核的一个桥梁或者中间层，而prot则是具体的实现。\n注册inetsw_array到inet_sw[]数组中的代码在inet_init()中，代码如下：\n//遍历inetsw_array数组中的元素后调用inet_register_protosw函数将元素插入到inetsw[]中\tfor (q = inetsw_array; q &lt; &amp;inetsw_array[INETSW_ARRAY_LEN]; ++q)\t\tinet_register_protosw(q);void inet_register_protosw(struct inet_protosw *p)&#123;\tstruct list_head *lh;\tstruct inet_protosw *answer;\tint protocol = p-&gt;protocol;\tstruct list_head *last_perm;\tspin_lock_bh(&amp;inetsw_lock);\t//合法性检查\tif (p-&gt;type &gt;= SOCK_MAX)\t\tgoto out_illegal;\t//last_perm保存的是一个socket-&gt;type中最后一个永久协议的位置\tlast_perm = &amp;inetsw[p-&gt;type];\tlist_for_each(lh, &amp;inetsw[p-&gt;type]) &#123;\t\tanswer = list_entry(lh, struct inet_protosw, list);\t\t/* Check only the non-wild match. */\t\t//不是永久协议的情况（TCP/UDP为永久协议）\t\tif ((INET_PROTOSW_PERMANENT &amp; answer-&gt;flags) == 0)\t\t\tbreak;\t\t//和永久协议的protocol一样\t\tif (protocol == answer-&gt;protocol)\t\t\tgoto out_permanent;\t\t//走到这里给永久协议赋值\t\tlast_perm = lh;\t&#125;\t//将新的协议注册到协议之后。\tlist_add_rcu(&amp;p-&gt;list, last_perm);out:\tspin_unlock_bh(&amp;inetsw_lock);\treturn;out_permanent:\tpr_err(&quot;Attempt to override permanent protocol %d\\n&quot;, protocol);\tgoto out;out_illegal:\tpr_err(&quot;Ignoring attempt to register invalid socket type %d\\n&quot;,\t       p-&gt;type);\tgoto out;&#125;\n","categories":["网络协议栈源码学习"],"tags":["socket"]},{"title":"处理器体系结构","url":"/2025/06/05/%E5%A4%84%E7%90%86%E5%99%A8%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%80%EF%BC%89/","content":"处理器体系结构一个处理器支持的指令和指令的字节级编码称为它的指令集体系结构(Instruction-Set Architecture, ISA) \n本章将简要介绍处理器硬件的设计，学习一个硬件系统执行某种 ISA指令的方式，以更好地理解计算机是如何工作的。\n本章首先定义一个简单的指令集，作为处理器实现的运行示例。因为受 x86-64 指令集的启发，它被俗称为 “x86”, 所以我们称我们的指令集为 “Y86-64” 指令集。 与 x86-64 相比， Y86-64 指令集的数据类型、指令和寻址方式都要少一些。它的字节级编码也比较简单，虽然 Y86-64 指令集很简单，它仍然足够完整，能让我们写一些处理整数的程序。\nY86-64 的寄存器\n寄存器：Y86-64 有 15 个通用寄存器（与 x86-64 类似），如 %rax、%rsp、%rbp 等，每个存储 64 位数据。%rsp 主要用作栈指针。\n\n条件码：ZF、SF、OF 用于保存最近计算的结果信息。\n\n程序计数器（PC）：存储当前将要执行的指令地址。\n\n\nY86-64 指令\ny86-64 的 movq 指令分成了 4 个不同的指令：irmovq、rrmovq、mrmovq 和 rmmovq，分别显示地指明源和目的的格式。源可以是立即数（i）、寄存器（r）或内存（m）。指令名字的第一个字母就表明了源的类型，目前可以是寄存器（r）或内存（m）。指令名字的第二个字母指明了目的的类型。它决定如何实现数据传送时，显式地指明数据传送的路径。\n有 4 个整数操作指令，如图它们是 addq、subq、andq 和 xorq。它们只对寄存器数据进行操作，而 Y86-64 还允许对内存数据进行这些操作。这些指令会设置 3 个条件码 ZF、SF 和 OF（零、符号和溢出）。\n7 个跳转指令是 jmp、jle、jl、je、jne、jge 和 jg。根据分支指令的类型和条件代码的设置来选择分支。分支条件和 x86-64 的一样）。\n有 6 个条件传送指令：cmovle、cmovl、cmove、cmovne、cmovge 和 cmovg。这些指令的格式与寄存器-寄存器传送指令 rrmovq 一样，但是只有当条件码满足所需的要求时，才会更新目的寄存器的值。\ncall 指令将返回地址入栈，然后跳到目的地址。ret 指令从栈中调用出返回。\npushq 和 popq 指令实现了入栈和出栈，和 x86-64 中一样。\nY86-64 只有一条与主机操作相关的指令 halt。x86-64 的应用程序不允许使用这条指令，因为它会导致整个系统停止运行。对于 Y86-64 来说，执行 halt 指令会导致处理器终止，并将状态设置为 HLT。\n\n指令和对应的编码如下所示：\n\n\n\n指令\n字节 0\n字节 1\n字节 2\n字节 3\n字节 4\n字节 5\n字节 6\n字节 7\n字节 8\n字节 9\n\n\n\nhalt\n00\n\n\n\n\n\n\n\n\n\n\n\nnop\n10\n\n\n\n\n\n\n\n\n\n\n\nrrmovq rA, rB\n20\nrA rB\n\n\n\n\n\n\n\n\n\n\nirmovq V, rB\n30\nF rB\nV\nV\nV\nV\nV\nV\nV\nV\n\n\nrmmovq rA, D(rB)\n40\nrA rB\nD\nD\nD\nD\nD\nD\nD\nD\n\n\nmrmovq D(rB), rA\n50\nrA rB\nD\nD\nD\nD\nD\nD\nD\nD\n\n\nOPq rA, rB\n6 fn\nrA rB\n\n\n\n\n\n\n\n\n\n\njXX Dest\n7 fn\nDest\nDest\nDest\nDest\nDest\nDest\nDest\nDest\nDest\n\n\ncmovXX rA, rB\n2 fn\nrA rB\n\n\n\n\n\n\n\n\n\n\ncall Dest\n80\nDest\nDest\nDest\nDest\nDest\nDest\nDest\nDest\nDest\n\n\nret\n90\n\n\n\n\n\n\n\n\n\n\n\npushq rA\nA0\nrA F\n\n\n\n\n\n\n\n\n\n\npopq rA\nB0\nrA F\n\n\n\n\n\n\n\n\n\n\n说明：\n\nrA, rB 表示寄存器编号（各4位）。\nF 表示无效寄存器编号。\nfn 表示功能码&#x2F;条件码。\nV 表示立即数（8字节）。\nD 表示位移（8字节）。\nDest 表示目标地址（8字节）。\n\n有的指令只有一个字节长，而有的需要操作数的指令编码就更长一些。\nRISC 和 CISC 指令集\n\n\n特性\nCISC\n早期的 RISC\n\n\n\n指令数量\n指令数量很多。Intel 描述全套指令的文档有 1200 多页。\n指令数量少得多，通常少于 100 个。\n\n\n延迟执行的指令\n有些指令的延迟很长，如从内存复制块、复杂地址传递等。\n没有较长延迟的指令，有些早期 RISC 甚至没有整数乘除法指令，需用其他方式实现。\n\n\n指令长度\n编码是可变长度的，x86-64 的指令长度可以是 1~15 字节。**\n编码是固定长度的，通常所有指令都是 4 字节。\n\n\n地址操作方式\n地址操作方式丰富，支持多种寻址模式，如偏移+基址+变址寄存器+缩放因子组合。\n简单寻址方式，通常只有基址加偏移寻址。\n\n\n内存和寄存器操作\n支持对内存和寄存器数据进行算术和逻辑运算。\n通常只能对寄存器数据进行算术和逻辑运算。与内存交互需使用 load&#x2F;store 指令，属于 load&#x2F;store 架构。\n\n\n程序级细节可见性\n对机器级程序来说实现细节不可见，ISA 抽象了程序之间执行顺序的细节。\n机器级程序实现细节可见，有些 RISC 禁止特定指令序列，必须满足约束条件后才可执行。\n\n\n条件码\n有条件码作为副产品（如 ZF、SF、OF），用于条件分支检测。\n没有条件码，需要通过测试指令将结果放入普通寄存器，再基于此判断。\n\n\n过程链接\n使用栈传参和返回地址，栈被用来存储过程参数和返回地址。\n使用大量寄存器进行参数传递与返回，避免对内存引用。通常有 32 个以上寄存器。\n\n\nRISC 和 CISC 和核心区别如下:设计理念不同：\n\nRISC 追求“简单指令 + 快速流水线”\nCISC 追求“复杂指令 + 高表达能力 + 更节省代码空间”\n\n指令执行粒度不同：\n\nRISC 一条指令只做一件事（例如：加法只能寄存器之间）\nCISC 一条指令可以做多件事（例如：内存中的数+寄存器的数→再写回内存）\n\n是否 Load&#x2F;Store 架构：\n\nRISC：运算只能在寄存器之间，访问内存要用专门的 load/store\nCISC：可以直接对内存进行加减乘除等操作（如 add [eax], ebx）\n\n举个例子对比：\nRISC 指令：\nLDR r0, [r1]LDR r2, [r3]ADD r4, r0, r2STR r4, [r5]\n\n需要 4 条指令把内存中两个值相加后写回。\n\nCISC 指令（x86）：\nADD [eax], ebx\n\n一条指令就可以把内存中一个值和寄存器相加，并写回内存。\n\n\n指令处理**在处理一条指令时，可以将其操作组织为一系列固定的处理阶段，**每个阶段执行特定功能，有助于形成统一的执行顺序，充分利用硬件资源。各阶段及其简要说明如下：\n取指\n使用 PC（程序计数器）作为地址，从内存中读取指令字节。\n提取出指令的：\n操作码（icode）\n功能码（ifun）\n目标寄存器编号（rA, rB，如果指令需要）\n立即数（valC，如果指令包含立即数）\n\n\n\n译码根据 rA、rB 字段，从寄存器堆中读取源操作数：\n\nvalA = R[rA]\nvalB = R[rB]\n\n上面的valA 和valB是ALU的两个输入\n执行执行算术&#x2F;逻辑操作：\n\n加法、减法、位移、比较……\n\n计算地址（如内存访问地址）：\n\nvalE = valB + valC（例如内存偏移地址）\n\n计算条件分支是否成立（对条件跳转指令）\n访存\n如果是读（如 mrmovq，popq）：\nvalM = Mem[valE]\n\n\n如果是写（如 rmmovq，pushq）：\nMem[valE] = valA\n\n\n\n写回将结果写入目标寄存器\n根据指令类型，将 valE 或 valM 写入 rA 或 rB：\n\n如：R[rB] = valE 或 R[rA] = valM\n\n更新 PC更新程序计数器 PC，为下一条指令准备\n举例：\naddq %rax, %rbx 的执行流程如下：\n\n\n\n阶段\n操作说明\n\n\n\n取指\n从 PC 位置取出 addq 指令\n\n\n译码\n读出 rax → valA，rbx → valB\n\n\n执行\nALU 执行 valE &#x3D; valA + valB\n\n\n访存\n无操作\n\n\n写回\n将结果 valE 写入 rbx（目的寄存器）\n\n\n更新 PC\nPC &#x3D; valP\n\n\nmrmovq 8(%rbp), %rax 的执行流程如下\n\n\n\n阶段\n动作说明\n\n\n\n取指（Fetch）\n从内存中读取当前指令的字节内容：包括 icode（为 mrmovq）、ifun、rB（即 rbp）、rA（即 rax）和立即数 valC = 8，并计算下一条指令地址 valP = PC + 指令长度。\n\n\n译码（Decode）\n从寄存器文件中读出： - valB = R[rbp]（基地址） - valA 不使用（但可能会读取）\n\n\n执行（Execute）\n用 ALU 计算内存地址： - valE = valB + valC = R[rbp] + 8\n\n\n访存（Memory）\n从内存地址 valE 读取 8 字节数据： - valM = Mem[valE]\n\n\n写回（Write Back）\n将 valM 写入目的寄存器： - R[rax] = valM\n\n\n更新 PC（PC Update）\nPC = valP，准备执行下一条指令。\n\n\n流水线流水线（Pipeline）**是一种**提高指令吞吐率（吞吐量）*的技术，核心思想就是将指令的执行过程划分成若干个阶段，让*多条指令同时在不同阶段执行，从而**并行化处理流程，类似工业生产线。\n常见的流水线阶段（以经典的五级流水线为例）：\n\n\n\n阶段缩写\n阶段名称\n英文全称\n功能说明\n\n\n\nIF\n取指阶段\nInstruction Fetch\n从内存中读取指令\n\n\nID\n译码阶段\nInstruction Decode\n分析指令含义，读取寄存器操作数\n\n\nEX\n执行阶段\nExecute\n运算、地址计算或条件判断\n\n\nMEM\n访存阶段\nMemory Access\n对数据内存进行读&#x2F;写操作（如load&#x2F;store）\n\n\nWB\n写回阶段\nWrite Back\n将计算结果写回寄存器\n\n\n假设我们有三条指令：I1、I2、I3，它们的流水线执行如下：\n\n\n\n周期\nI1\nI2\nI3\n\n\n\n1\nIF\n\n\n\n\n2\nID\nIF\n\n\n\n3\nEX\nID\nIF\n\n\n4\nMEM\nEX\nID\n\n\n5\nWB\nMEM\nEX\n\n\n6\n\nWB\nMEM\n\n\n7\n\n\nWB\n\n\n流水线的局限性不一致划分：\n流水线各阶段所需时间不同，导致不能统一设定短时钟周期的问题。\n举例：\n设想设计一个 5 阶段流水线，划分如下：\n\n\n\n阶段\n功能\n延迟（ps）\n\n\n\nIF\n取指\n80\n\n\nID\n译码\n60\n\n\nEX\n执行（ALU）\n180\n\n\nMEM\n访存\n100\n\n\nWB\n写回\n40\n\n\n\n由于 EX 最慢，占用了 180ps；\n所以整个流水线的最短时钟周期 &#x3D; 180ps（必须以最慢阶段为限）；\n然而其他阶段（例如 WB 只要 40ps）却浪费了大量时间 → 这就是“不一致划分”。\n\n流水线太长，性能反而下降：\n分支惩罚严重扩大（Branch Penalty ↑）分支预测失败时，需要清空整个流水线，流水线越深，浪费越多。\n\n比如：\n5级流水线：预测失败，丢掉5条指令\n20级流水线：预测失败，丢掉20条指令\n\n\n\n带反馈的流水线系统在流水线系统中，指令从头到尾的执行并不总是严格按照顺序完成\n虽然每条指令在流水线中依次经过取指、译码、执行等阶段，但由于依赖关系或跳转，某些指令会被暂停、重排或撤销，不一定严格顺序完成\n比如：\n\n分支指令：需要执行阶段判断是否跳转 → 才知道下一条指令取哪里\n数据依赖：当前指令需要前一条指令的计算结果 → 必须等它先完成\n\n举例：\n1: add r1, r2, r3     ; r1 = r2 + r32: sub r4, r1, r5     ; r4 = r1 - r5\n\n\n第二条指令 依赖第一条指令的执行结果 r1；\n如果你流水线太快地推进，就可能在 r1 还没写回时，sub 就读到了旧值或错值；\n所以需要一种 机制把 r1 的新值反馈给第二条指令 → 这就是数据前递的一种反馈。\n\n\n\n\n周期\n指令1（add）\n指令2（sub）\n问题说明\n\n\n\n1\nIF\n\n\n\n\n2\nID\nIF\n\n\n\n3\nEX\nID\n❗r1 尚未计算出来，sub 读不到新值\n\n\n4\nMEM\nEX\n❌ sub 用错值执行\n\n\n5\nWB\nMEM\n\n\n\n带反馈的方法：\n\n在 add 的 EX 阶段算出 r1 的值；\n直接从 EX 阶段前递到 sub 的 EX 阶段使用，绕过寄存器文件；\n不用等到 WB 阶段再写回 → 提前“反馈”给下条指令。\n\n\n\n\n周期\n指令1（add）\n指令2（sub）\n说明\n\n\n\n1\nIF\n\n\n\n\n2\nID\nIF\n\n\n\n3\nEX\nID\n\n\n\n4\nMEM\nEX (用前递)\n✅ 从 add 的 EX 阶段取值\n\n\n5\nWB\nMEM\n\n\n\n流水线冒险*当多条指令在流水线中*同时执行时，由于它们之间存在某些冲突或依赖关系**，导致流水线无法按原计划推进，甚至必须暂停、插入空周期或重新执行。\n\n\n\n类型\n全称\n说明\n\n\n\n1️⃣ 数据冒险（Data Hazard）\n指令之间有数据依赖\n后面的指令使用前面指令尚未计算出的结果\n\n\n2️⃣ 控制冒险（Control Hazard）\n与跳转&#x2F;分支指令相关\n分支结果未确定时，无法决定取哪条指令\n\n\n\n\n\n\n\n数据冒险：\nadd r1, r2, r3    ; r1 = r2 + r3sub r4, r1, r5    ; r4 = r1 - r5  ← 依赖上面 r1 的结果\n\n\n如果 sub 提前执行，会用到错误的 r1 → 数据冒险\n解决方法：数据前递（Forwarding）、暂停（Stall）（好像就是插入气泡）\n\n控制冒险（Control Hazard）asm复制编辑beq r1, r2, label  ; 如果相等则跳转add r3, r4, r5      ; 跳 or 不跳？影响下一条指令是否执行\n\n\n如果跳转条件尚未判断完成 → 无法确定下一条指令 → 控制冒险\n解决方法：分支预测、分支延迟槽、清空流水线\n\n","categories":["《深入理解计算机系统》"],"tags":["处理器体系结构"]},{"title":"socket I/O","url":"/2025/06/03/%E5%A5%97%E6%8E%A5%E5%AD%97IO/","content":"socket I&#x2F;O输出系统调用send、sendto 和 sendmsg 都是用于发送数据的系统调用，面向连接的协议通常使用send(例如tcp或者已经connect的udp)，而无连接的协议通常使用send_to而sendmsg支持发送时设置多个缓存区，也就是说一次调用可以发送多个数据包。\nsend系统调用用户态调用send的函数原型\n#include &lt;sys/socket.h&gt;ssize_t send(int sockfd, const void *buf, size_t len, int flags);\n上述参数解释如下：\n\n\n\n参数名\n类型\n描述\n注意事项\n\n\n\nsockfd\nint\n已连接的 socket 文件描述符\n必须是通过 connect() 建立连接的 TCP socket 或已连接的 UDP socket\n\n\nbuf\nconst void *\n待发送数据的缓冲区地址\n用户态指针，指向要发送的数据（如字符串、二进制数据等）\n\n\nlen\nsize_t\n要发送的数据长度（字节数）\n实际发送的数据量可能小于此值（需检查返回值）\n\n\nflags\nint\n控制发送行为的标志位\n多个标志可通过按位或 | 组合（如 MSG_DONTWAIT | MSG_NOSIGNAL）\n\n\nsend系统调用内核实现\nSYSCALL_DEFINE4(send, int, fd, void __user *, buff, size_t, len,\t\tunsigned int, flags)&#123;\treturn __sys_sendto(fd, buff, len, flags, NULL, 0);&#125;\n__sys_sendto实现如下所示：\nint __sys_sendto(int fd, void __user *buff, size_t len, unsigned int flags,\t\t struct sockaddr __user *addr,  int addr_len)&#123;\tstruct socket *sock;\tstruct sockaddr_storage address;//这个叫通用地址容器，目的就是兼容用户态传进来的不同结构，例如sock_addrin或者ll\tint err;\tstruct msghdr msg;\tstruct iovec iov;\tint fput_needed;\t//这里吧用户态的地址和长度记录到msg.msg_iter这个结构体中，后续内核会操作这个结构体，ITER_SOURCE 表示写\terr = import_single_range(ITER_SOURCE, buff, len, &amp;iov, &amp;msg.msg_iter);\tif (unlikely(err))\t\treturn err;\t//根据fd找到socekt结构体 从fdt中找到file从file的私有字段找到socket\tsock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed);\tif (!sock)\t\tgoto out;\t//设置msg的其他字段\tmsg.msg_name = NULL;\tmsg.msg_control = NULL;\tmsg.msg_controllen = 0;\tmsg.msg_namelen = 0;\tmsg.msg_ubuf = NULL;\t//用户调用sendto时才走这个分支，因为addr不为空\tif (addr) &#123;\t\terr = move_addr_to_kernel(addr, addr_len, &amp;address);\t\tif (err &lt; 0)\t\t\tgoto out_put;\t\tmsg.msg_name = (struct sockaddr *)&amp;address;\t\tmsg.msg_namelen = addr_len;\t&#125;\tflags &amp;= ~MSG_INTERNAL_SENDMSG_FLAGS;\t//如果需要设置非阻塞\tif (sock-&gt;file-&gt;f_flags &amp; O_NONBLOCK)\t\tflags |= MSG_DONTWAIT;\tmsg.msg_flags = flags;\t//调用对应socekt的发函数\terr = __sock_sendmsg(sock, &amp;msg);out_put:\tfput_light(sock-&gt;file, fput_needed);out:\treturn err;&#125;\n\n\n上述代码主要工作就是申请了一个msg变量，并把用户态传进来的数据的地址和长度记录到msg变量的迭代器msg_iter中，并根据fd找到对应的socket套接字，然后调用__sock_sendmsg发送数据。import_single_range里面调用到iov_iter_ubuf来填充迭代器这个结构，填充这个结构的目的就是内核接下来会操作这个结构进行数据的拷贝。\nstatic inline void iov_iter_ubuf(struct iov_iter *i, unsigned int direction,\t\t\tvoid __user *buf, size_t count)&#123;\tWARN_ON(direction &amp; ~(READ | WRITE));\t*i = (struct iov_iter) &#123;\t\t.iter_type = ITER_UBUF, //标记为用户缓冲区类型\t\t.copy_mc = false,\t\t.user_backed = true, //标记为来自用户空间\t\t.data_source = direction,  //记录数据流向\t\t.ubuf = buf, //用户缓冲区指针\t\t.count = count, //数据长度\t\t.nr_segs = 1  //单段缓冲区，非聚合分散\t&#125;;&#125;\n\n__sock_sendmsg中的逻辑就是调用socket的ops函数，对于AF_INE类型的socket调用的send就是inet_sendmsg具体代码如下：\nstatic int __sock_sendmsg(struct socket *sock, struct msghdr *msg)&#123;\tint err = security_socket_sendmsg(sock, msg,\t\t\t\t\t  msg_data_left(msg));\treturn err ?: sock_sendmsg_nosec(sock, msg);&#125;//sock_sendmsg_nosec定义如下static inline int sock_sendmsg_nosec(struct socket *sock, struct msghdr *msg)&#123;\tint ret = INDIRECT_CALL_INET(READ_ONCE(sock-&gt;ops)-&gt;sendmsg, inet6_sendmsg,\t\t\t\t     inet_sendmsg, sock, msg,\t\t\t\t     msg_data_left(msg));\tBUG_ON(ret == -EIOCBQUEUED);\tif (trace_sock_send_length_enabled())\t\tcall_trace_sock_send_length(sock-&gt;sk, ret, 0);\treturn ret;&#125;\n在inet_sendmsg 中则会进一步调用socket所关联的sock结果的sendmsg，注意这里socket与sock的关联是在创建socket的时候根据协议和类型确定的，对于tcp调用的是tcp_sendmsg对于udp调用的是udp_sendmsg。\nsendto系统调用实现可以看到sendto系统调用与上面的send系统调用最终调用的都是__sys_sendto，区别就是sendto中的addr字段传入的不为空，在__sys_sendto会有由move_addr_to_kernel处理。\nSYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,\t\tunsigned int, flags, struct sockaddr __user *, addr,\t\tint, addr_len)&#123;\treturn __sys_sendto(fd, buff, len, flags, addr, addr_len);&#125;\n\n//传入了用户态的地址空间，和地址长度int move_addr_to_kernel(void __user *uaddr, int ulen, struct sockaddr_storage *kaddr)&#123;\tif (ulen &lt; 0 || ulen &gt; sizeof(struct sockaddr_storage))\t\treturn -EINVAL;\tif (ulen == 0)\t\treturn 0;\t//将用户态的结构体copy到内核，这里的kaddr可以兼容所有的sockaddr的结构体128字节\tif (copy_from_user(kaddr, uaddr, ulen))\t\treturn -EFAULT;\t//安全相关\treturn audit_sockaddr(ulen, kaddr);&#125;\n\nsendmsg系统调用实现sendmsg送复杂网络消息​​的系统调用，支持多块数据（iovec）、目标地址、控制信息（如 cmsg）等高级功能，对应内核部分代码如下：\nSYSCALL_DEFINE3(sendmsg, int, fd, struct user_msghdr __user *, msg, unsigned int, flags)&#123;\treturn __sys_sendmsg(fd, msg, flags, true);&#125;long __sys_sendmsg(int fd, struct user_msghdr __user *msg, unsigned int flags,\t\t   bool forbid_cmsg_compat)&#123;\tint fput_needed, err;\tstruct msghdr msg_sys;\tstruct socket *sock;\tif (forbid_cmsg_compat &amp;&amp; (flags &amp; MSG_CMSG_COMPAT))\t\treturn -EINVAL;\t//根据fd查找对应的socket\tsock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed);\tif (!sock)\t\tgoto out;\t//传入用户指定的msg和内核的msg\terr = ___sys_sendmsg(sock, msg, &amp;msg_sys, flags, NULL, 0);\tfput_light(sock-&gt;file, fput_needed);out:\treturn err;&#125;\n与sendto和send不同，sendmsg首先查找对应的socket，然后创建内核msg结构和传入用户的设置的msg结构，调用___sys_sendmsg发送：___sys_sendmsg函数中，主要就是把用户指定的msg信息，存到内核创建的msg_sys中\nstatic int ___sys_sendmsg(struct socket *sock, struct user_msghdr __user *msg,\t\t\t struct msghdr *msg_sys, unsigned int flags,\t\t\t struct used_address *used_address,\t\t\t unsigned int allowed_msghdr_flags)&#123;\tstruct sockaddr_storage address;\tstruct iovec iovstack[UIO_FASTIOV], *iov = iovstack;\tssize_t err;\tmsg_sys-&gt;msg_name = &amp;address;\t//将用户态的msg拷贝到内核态msg\terr = sendmsg_copy_msghdr(msg_sys, msg, flags, &amp;iov);\tif (err &lt; 0)\t\treturn err;\t//调用协议栈发送\terr = ____sys_sendmsg(sock, msg_sys, flags, used_address,\t\t\t\tallowed_msghdr_flags);\tkfree(iov);\treturn err;&#125;\n上述____sys_sendmsg中的逻辑为将用户设置的控制信息并保存到内核msg中，\nstatic int ____sys_sendmsg(struct socket *sock, struct msghdr *msg_sys,\t\t\t   unsigned int flags, struct used_address *used_address,\t\t\t   unsigned int allowed_msghdr_flags)&#123;\tunsigned char ctl[sizeof(struct cmsghdr) + 20]\t\t\t\t__aligned(sizeof(__kernel_size_t));\t/* 20 is size of ipv6_pktinfo */\tunsigned char *ctl_buf = ctl;\tint ctl_len;\tssize_t err;\terr = -ENOBUFS;\t//msg_controllen为用户msghdr控制字段的总长度\tif (msg_sys-&gt;msg_controllen &gt; INT_MAX)\t\tgoto out;\tflags |= (msg_sys-&gt;msg_flags &amp; allowed_msghdr_flags);\tctl_len = msg_sys-&gt;msg_controllen;\t//32位系统的处理逻辑\tif ((MSG_CMSG_COMPAT &amp; flags) &amp;&amp; ctl_len) &#123;\t\terr =\t\t    cmsghdr_from_user_compat_to_kern(msg_sys, sock-&gt;sk, ctl,\t\t\t\t\t\t     sizeof(ctl));\t\tif (err)\t\t\tgoto out;\t\tctl_buf = msg_sys-&gt;msg_control;\t\tctl_len = msg_sys-&gt;msg_controllen;\t&#125; else if (ctl_len) &#123;\t//64位的处理逻辑\t\tBUILD_BUG_ON(sizeof(struct cmsghdr) !=\t\t\t     CMSG_ALIGN(sizeof(struct cmsghdr)));\t\tif (ctl_len &gt; sizeof(ctl)) &#123;\t\t\tctl_buf = sock_kmalloc(sock-&gt;sk, ctl_len, GFP_KERNEL);\t\t\tif (ctl_buf == NULL)\t\t\t\tgoto out;\t\t&#125;\t\terr = -EFAULT;\t\t//这里的msg_control_user是用户态的指针所以用copy_from_user\t\tif (copy_from_user(ctl_buf, msg_sys-&gt;msg_control_user, ctl_len))\t\t\tgoto out_freectl;\t\tmsg_sys-&gt;msg_control = ctl_buf;\t\tmsg_sys-&gt;msg_control_is_user = false;\t&#125;\tflags &amp;= ~MSG_INTERNAL_SENDMSG_FLAGS;\tmsg_sys-&gt;msg_flags = flags;\t//设置非阻塞\tif (sock-&gt;file-&gt;f_flags &amp; O_NONBLOCK)\t\tmsg_sys-&gt;msg_flags |= MSG_DONTWAIT;\t/*\t * If this is sendmmsg() and current destination address is same as\t * previously succeeded address, omit asking LSM&#x27;s decision.\t * used_address-&gt;name_len is initialized to UINT_MAX so that the first\t * destination address never matches.\t */\tif (used_address &amp;&amp; msg_sys-&gt;msg_name &amp;&amp;\t    used_address-&gt;name_len == msg_sys-&gt;msg_namelen &amp;&amp;\t    !memcmp(&amp;used_address-&gt;name, msg_sys-&gt;msg_name,\t\t    used_address-&gt;name_len)) &#123;\t\terr = sock_sendmsg_nosec(sock, msg_sys);\t\tgoto out_freectl;\t&#125;\terr = __sock_sendmsg(sock, msg_sys);\t/*\t * If this is sendmmsg() and sending to current destination address was\t * successful, remember it.\t */\tif (used_address &amp;&amp; err &gt;= 0) &#123;\t\tused_address-&gt;name_len = msg_sys-&gt;msg_namelen;\t\tif (msg_sys-&gt;msg_name)\t\t\tmemcpy(&amp;used_address-&gt;name, msg_sys-&gt;msg_name,\t\t\t       used_address-&gt;name_len);\t&#125;out_freectl:\tif (ctl_buf != ctl)\t\tsock_kfree_s(sock-&gt;sk, ctl_buf, ctl_len);out:\treturn err;&#125;\n\n接收系统调用recv,recvfrom,recvmsg三个接收系统调用与发送系统调用类似，不同的地方就是数据流向是相反的。\n\n\n\n系统调用\n适用场景\n关键功能\n典型用途\n\n\n\nrecv\n已连接的套接字（如 TCP）\n- 从已建立连接的套接字接收数据- 不支持获取发送方地址\nTCP 数据接收\n\n\nrecvfrom\n无连接套接字（如 UDP）\n- 接收数据包- 可获取发送方地址（struct sockaddr）\nUDP 数据接收\n\n\nrecvmsg\n所有套接字（最通用）\n- 支持多缓冲区（struct iovec）- 支持控制信息（cmsg）- 可获取发送方地址\n高级场景：- 文件描述符传递- 接收 TTL&#x2F;接口信息\n\n\nrecv系统调用实现SYSCALL_DEFINE4(recv, int, fd, void __user *, ubuf, size_t, size,\t\tunsigned int, flags)&#123;\treturn __sys_recvfrom(fd, ubuf, size, flags, NULL, NULL);&#125;\nint __sys_recvfrom(int fd, void __user *ubuf, size_t size, unsigned int flags,\t\t   struct sockaddr __user *addr, int __user *addr_len)&#123;\tstruct sockaddr_storage address;\tstruct msghdr msg = &#123;\t\t/* Save some cycles and don&#x27;t copy the address if not needed */\t\t.msg_name = addr ? (struct sockaddr *)&amp;address : NULL,\t&#125;;\tstruct socket *sock;\tstruct iovec iov;\tint err, err2;\tint fput_needed;\t//将用户态指向的缓冲区地址，存到msg中\terr = import_single_range(ITER_DEST, ubuf, size, &amp;iov, &amp;msg.msg_iter);\tif (unlikely(err))\t\treturn err;\t//查找对应的socket\tsock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed);\tif (!sock)\t\tgoto out;\tif (sock-&gt;file-&gt;f_flags &amp; O_NONBLOCK)\t\tflags |= MSG_DONTWAIT;\t//调用套接字的ops收包\terr = sock_recvmsg(sock, &amp;msg, flags);\t//recv系统调用传入的addr为空,如果不为空，会记录源数据包的源ip地址\tif (err &gt;= 0 &amp;&amp; addr != NULL) &#123;\t\terr2 = move_addr_to_user(&amp;address,\t\t\t\t\t msg.msg_namelen, addr, addr_len);\t\tif (err2 &lt; 0)\t\t\terr = err2;\t&#125;\tfput_light(sock-&gt;file, fput_needed);out:\treturn err;&#125;\nrecvfrom系统调用实现可获取发送方的 ​​IP 地址和端口​​，适用于 UDP 等无连接协议（每个数据包可能来自不同发送方）\nSYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,\t\tunsigned int, flags, struct sockaddr __user *, addr,\t\tint __user *, addr_len)&#123;\treturn __sys_recvfrom(fd, ubuf, size, flags, addr, addr_len);&#125;\n与recv一样最终都调用到了__sys_recvfrom，不同的地方在于传入的add不为空，用户可以拿到数据包的源ip地址\nrecvmsg系统调用实现SYSCALL_DEFINE3(recvmsg, int, fd, struct user_msghdr __user *, msg,\t\tunsigned int, flags)&#123;\treturn __sys_recvmsg(fd, msg, flags, true);&#125;\n\n上述__sys_recvmsg与发送系统调用sendmsg类似，主要工作就是将数据包拷贝拷贝到用户的指定的缓冲区，以及数据包的源ip地址，和数据包的控制信息。\nstatic int ____sys_recvmsg(struct socket *sock, struct msghdr *msg_sys,\t\t\t   struct user_msghdr __user *msg,\t\t\t   struct sockaddr __user *uaddr,\t\t\t   unsigned int flags, int nosec)&#123;\tstruct compat_msghdr __user *msg_compat =\t\t\t\t\t(struct compat_msghdr __user *) msg;\tint __user *uaddr_len = COMPAT_NAMELEN(msg);\tstruct sockaddr_storage addr;\tunsigned long cmsg_ptr;\tint len;\tssize_t err;\tmsg_sys-&gt;msg_name = &amp;addr;\tcmsg_ptr = (unsigned long)msg_sys-&gt;msg_control;\tmsg_sys-&gt;msg_flags = flags &amp; (MSG_CMSG_CLOEXEC|MSG_CMSG_COMPAT);\t/* We assume all kernel code knows the size of sockaddr_storage */\tmsg_sys-&gt;msg_namelen = 0;\tif (sock-&gt;file-&gt;f_flags &amp; O_NONBLOCK)\t\tflags |= MSG_DONTWAIT;\tif (unlikely(nosec))\t\terr = sock_recvmsg_nosec(sock, msg_sys, flags);\telse\t//收包逻辑，调用收包函数\t\terr = sock_recvmsg(sock, msg_sys, flags);\tif (err &lt; 0)\t\tgoto out;\t//这里是接收数据包的长度\tlen = err;\t//拷贝源ip地址到用户态的uaddr(把下面的addr拷贝到了uaddr)\tif (uaddr != NULL) &#123;\t\terr = move_addr_to_user(&amp;addr,\t\t\t\t\tmsg_sys-&gt;msg_namelen, uaddr,\t\t\t\t\tuaddr_len);\t\tif (err &lt; 0)\t\t\tgoto out;\t&#125;\t//更新用户态的标志位，比如消息截断等标志（收一部分）\terr = __put_user((msg_sys-&gt;msg_flags &amp; ~MSG_CMSG_COMPAT),\t\t\t COMPAT_FLAGS(msg));\tif (err)\t\tgoto out;\tif (MSG_CMSG_COMPAT &amp; flags)\t\terr = __put_user((unsigned long)msg_sys-&gt;msg_control - cmsg_ptr,\t\t\t\t &amp;msg_compat-&gt;msg_controllen);\telse\t\terr = __put_user((unsigned long)msg_sys-&gt;msg_control - cmsg_ptr,\t\t\t\t &amp;msg-&gt;msg_controllen);\tif (err)\t\tgoto out;\terr = len;out:\treturn err;&#125;","categories":["网络协议栈源码学习"],"tags":["socket"]},{"title":"TCP 延迟确认定时器","url":"/2025/08/07/%E5%BB%B6%E8%BF%9Fack%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"TCP延迟确认定时器（Delayed ACK Timer）延迟确认定时器是TCP协议栈用来推迟发送ACK报文的一种机制。 它的目标是：减少纯ACK包数量，提升网络效率，尤其适用于交互式和小包频繁的应用场景。\n延迟定时器的初始化tcp的延迟定时器在初始化sock的时候完成，代码如下：\nvoid tcp_init_xmit_timers(struct sock *sk)&#123;    ...\tinet_csk_init_xmit_timers(sk, &amp;tcp_write_timer, &amp;tcp_delack_timer,\t\t\t\t  &amp;tcp_keepalive_timer);\t...&#125;\n\n第二个参数为延迟确认定时器的回调函数，tcp_delack_timer如果用户没有持有sock，则会调用tcp_delack_timer_handler处理重传逻辑，具体的逻辑为，如果存在压缩ack则跳转到压缩ack的逻辑中。否则进入正常的处理流程，首先会判断是否位pingPong模式，如果不是pingpong模式，则继续增加下次的超时时间，因为属于单向发送数据所以可以延迟的更久，如果不是pingpong模式，则会退出pingpong模式，因为pingpong模式是不会走到这定时器逻辑中的，因为pingpong模式都是捎带ack，所以说这里要退出pingpong模式吧。\n无论是上述那种情况，最终都会刷新时间戳，调用tcp_send_ack发送数据包。\n/* Called with BH disabled */void tcp_delack_timer_handler(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\t//不需要处理的状态，直接返回\tif ((1 &lt;&lt; sk-&gt;sk_state) &amp; (TCPF_CLOSE | TCPF_LISTEN))\t\treturn;\t/* Handling the sack compression case */\t//处理压缩ack 如果收到了乱序数据包就会增加，\tif (tp-&gt;compressed_ack) &#123;\t\t//跟新tp的时间戳字段\t\ttcp_mstamp_refresh(tp);\t\t//处理压缩ack\t\ttcp_sack_compress_send_ack(sk);\t\treturn;\t&#125;\t//没有这个标志，直接返回， 这个标志是在tcp_send_delayed_ack（接收路径上）设置的，表示已经启动的定时器\tif (!(icsk-&gt;icsk_ack.pending &amp; ICSK_ACK_TIMER))\t\treturn;\t//如果没到时间就重新设置一下\tif (time_after(icsk-&gt;icsk_ack.timeout, jiffies)) &#123;\t\tsk_reset_timer(sk, &amp;icsk-&gt;icsk_delack_timer, icsk-&gt;icsk_ack.timeout);\t\treturn;\t&#125;\t//清楚标志位\ticsk-&gt;icsk_ack.pending &amp;= ~ICSK_ACK_TIMER;\t//判断是否有ack要发送，这个是在收包的时候inet_csk_schedule_ack设置的,判断pending和上面的区别是已经有ack了\tif (inet_csk_ack_scheduled(sk)) &#123;\t\t//是否是pingpong模式。pingpong模式就是交互模式，通过时间判断是否是进入pingpong，在发包的时候判断的\t\tif (!inet_csk_in_pingpong_mode(sk)) &#123;\t\t\t/* Delayed ACK missed: inflate ATO. */\t\t\t//不是pingbong模式，就是批量数据流，这里左移让下次更久！合理， 但是不能超过rto\t\t\ticsk-&gt;icsk_ack.ato = min(icsk-&gt;icsk_ack.ato &lt;&lt; 1, icsk-&gt;icsk_rto);\t\t&#125; else &#123;\t\t\t/* Delayed ACK missed: leave pingpong mode and\t\t\t * deflate ATO.\t\t\t */\t\t\t//退出pingpong模式，因为是交互式场景，应该随着数据发ack？\t\t\tinet_csk_exit_pingpong_mode(sk);\t\t\t//40ms\t\t\ticsk-&gt;icsk_ack.ato      = TCP_ATO_MIN;\t\t&#125;\t\t//刷新时间戳\t\ttcp_mstamp_refresh(tp);\t\t//真正发送ack\t\ttcp_send_ack(sk);\t\t//增加统计计数\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKS);\t&#125;&#125;\n\n上述处理压缩ack的代码逻辑如下：\nvoid tcp_sack_compress_send_ack(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//直接返回\tif (!tp-&gt;compressed_ack)\t\treturn;\t//取消压缩ack的定时器\tif (hrtimer_try_to_cancel(&amp;tp-&gt;compressed_ack_timer) == 1)\t\t__sock_put(sk);\t/* Since we have to send one ack finally,\t * substract one from tp-&gt;compressed_ack to keep\t * LINUX_MIB_TCPACKCOMPRESSED accurate.\t */\t//更新统计计数\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPACKCOMPRESSED,\t\t      tp-&gt;compressed_ack - 1);\t//清零\ttp-&gt;compressed_ack = 0;\t//立即发送ack\ttcp_send_ack(sk);&#125;\n\n只有设置了icsk_ack.pending的ICSK_ACK_TIMER和ICSK_ACK_SCHED标志才能真正的发送一个数据包\n在inet_csk_schedule_ack会设置ICSK_ACK_SCHED，在收包的多个地方会调用这个函数。\n在tcp_send_delayed_ack中（也就是激活定时器的函数）会同时设置这两个标志位，接收端不存在乱序数据包的会调用这个函数。\n//不存在乱序的时候void tcp_send_delayed_ack(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tint ato = icsk-&gt;icsk_ack.ato;\tunsigned long timeout;\t//当前的ato 大于最小值\tif (ato &gt; TCP_DELACK_MIN) &#123;\t\tconst struct tcp_sock *tp = tcp_sk(sk);\t\tint max_ato = HZ / 2;\t\t//交互模式，或者 在push模式下就要增大ato？ 因为有要push的包所以延迟发送ack\t\tif (inet_csk_in_pingpong_mode(sk) ||\t\t    (icsk-&gt;icsk_ack.pending &amp; ICSK_ACK_PUSHED))\t\t\tmax_ato = TCP_DELACK_MAX;\t\t/* Slow path, intersegment interval is &quot;high&quot;. */\t\t/* If some rtt estimate is known, use it to bound delayed ack.\t\t * Do not use inet_csk(sk)-&gt;icsk_rto here, use results of rtt measurements\t\t * directly.\t\t */\t\t//拿到rtt\t\tif (tp-&gt;srtt_us) &#123;\t\t\tint rtt = max_t(int, usecs_to_jiffies(tp-&gt;srtt_us &gt;&gt; 3),\t\t\t\t\tTCP_DELACK_MIN);\t\t\t//最大ato 肯定不能大于rtt\t\t\tif (rtt &lt; max_ato)\t\t\t\tmax_ato = rtt;\t\t&#125;\t\t//更新ato\t\tato = min(ato, max_ato);\t&#125;\t//更新ato\tato = min_t(u32, ato, inet_csk(sk)-&gt;icsk_delack_max);\t/* Stay within the limit we were given */\t//定时器触发的时间\ttimeout = jiffies + ato;\t/* Use new timeout only if there wasn&#x27;t a older one earlier. */\t//如果已经启动了\tif (icsk-&gt;icsk_ack.pending &amp; ICSK_ACK_TIMER) &#123;\t\t/* If delack timer is about to expire, send ACK now. */\t\t//马上到定时器的触发时间了 直接发送ack\t\tif (time_before_eq(icsk-&gt;icsk_ack.timeout, jiffies + (ato &gt;&gt; 2))) &#123;\t\t\ttcp_send_ack(sk);\t\t\treturn;\t\t&#125;\t\t//如果还有一段时间到期，但是当前在之前的后面，设置为原来的\t\tif (!time_before(timeout, icsk-&gt;icsk_ack.timeout))\t\t\ttimeout = icsk-&gt;icsk_ack.timeout;\t&#125;\t//设置标志位，启动定时器。\ticsk-&gt;icsk_ack.pending |= ICSK_ACK_SCHED | ICSK_ACK_TIMER;\ticsk-&gt;icsk_ack.timeout = timeout;\tsk_reset_timer(sk, &amp;icsk-&gt;icsk_delack_timer, timeout);&#125;\n\n发送延迟ack的接口为tcp_send_ack ，主要逻辑为申请一个没有数据的skb之后直接调用__tcp_transmit_skb发送数据包。具体逻辑如下：\n/* This routine sends an ack and also updates the window. */void __tcp_send_ack(struct sock *sk, u32 rcv_nxt)&#123;\tstruct sk_buff *buff;\t/* If we have been reset, we may not send again. */\tif (sk-&gt;sk_state == TCP_CLOSE)\t\treturn;\t/* We are not putting this on the write queue, so\t * tcp_transmit_skb() will set the ownership to this\t * sock.\t */\t//这里size的大小是320 GFP_ATOMIC追求快速\tbuff = alloc_skb(MAX_TCP_HEADER,\t\t\t sk_gfp_mask(sk, GFP_ATOMIC | __GFP_NOWARN));\t//小概率分配失败\tif (unlikely(!buff)) &#123;\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\t\tunsigned long delay;\t\t//下次触发时间，相当于指数退避\t\tdelay = TCP_DELACK_MAX &lt;&lt; icsk-&gt;icsk_ack.retry;\t\tif (delay &lt; TCP_RTO_MAX)//相当于没超过容许的最大值\t\t\ticsk-&gt;icsk_ack.retry++;\t\tinet_csk_schedule_ack(sk); //设置了pending位再次调度\t\ticsk-&gt;icsk_ack.ato = TCP_ATO_MIN;//重置了ato 防止下次时间翻倍\t\t//复位\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK, delay, TCP_RTO_MAX);\t\treturn; //直接返回\t&#125;\t/* Reserve space for headers and prepare control bits. */\t//修改data和tailz指针\tskb_reserve(buff, MAX_TCP_HEADER);\ttcp_init_nondata_skb(buff, tcp_acceptable_seq(sk), TCPHDR_ACK);\t/* We do not want pure acks influencing TCP Small Queues or fq/pacing\t * too much.\t * SKB_TRUESIZE(max(1 .. 66, MAX_TCP_HEADER)) is unfortunately ~784\t */\t//设置ture_size 为2 表示为纯ack ？？？？\tskb_set_tcp_pure_ack(buff);\t/* Send it off, this clears delayed acks for us. */\t//调用发包函数\t__tcp_transmit_skb(sk, buff, 0, (__force gfp_t)0, rcv_nxt);&#125;\n\n延迟确认定时器的启动在tcp_send_delayed_ack中会启动延迟确认定时器，该函数在不存在乱序的时候会被调用\n在tcp_ack 中分配数据包失败的时候会启动延迟确认ack\n在发送syn包后可能会会启动延迟ack\nstatic int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,\t\t\t\t\t const struct tcphdr *th)&#123;    ...\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,TCP_DELACK_MAX, TCP_RTO_MAX);\t...&#125;\n\n\n\n延迟确认定时器的关闭在tcp_event_ack_sent会关闭延迟ack，该函数在__tcp_transmit_skb中被调用\nstatic int __tcp_transmit_skb(struct sock *sk, struct sk_buff *skb,\t\t\t      int clone_it, gfp_t gfp_mask, u32 rcv_nxt)&#123;\t...\tif (likely(tcb-&gt;tcp_flags &amp; TCPHDR_ACK))\t\ttcp_event_ack_sent(sk, rcv_nxt);\t...&#125;\n\n/* Account for an ACK we sent. */static inline void tcp_event_ack_sent(struct sock *sk, u32 rcv_nxt)&#123;\t...\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_DACK);\t...&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP定时器","TCP"]},{"title":"网卡硬件各组件","url":"/2025/05/23/%E7%BD%91%E5%8D%A1%E7%A1%AC%E4%BB%B6%E6%A8%A1%E5%9D%97/","content":"网卡核心硬件组成现代有线以太网卡中，MAC、PHY、DMA、PCIe 是网卡最核心的硬件模块，这些模块构成了数据通信的基础框架。以下是它们的详细分工和协作关系，具体的架构图如下所示：\nPHYPHY 层属于 OSI 物理层（Layer 1），主要负责 数字信号 ↔ 模拟信号 的转换，具体包括：\n\n链路管理\n\n\n自动协商​​：与对端设备协商速率（如10&#x2F;100&#x2F;1000 Mbps）和双工模式。\n链路检测​​：监测连接状态（如网线是否插入）。\n\n\n信号转换​\n\n\n数模转换​​：将MAC层生成的数字信号转换为适合线缆（如双绞线、光纤）传输的模拟信号（如电信号或光信号）。\n模数转换​​：将接收到的模拟信号还原为数字信号供上层处理。\n\n\n物理介质适配​\n\n支持不同介质标准（如以太网的RJ-45接口、光纤接口），适应电压、阻抗等物理特性,例如：100BASE-TX（双绞线）、1000BASE-SX（光纤）等。\n\n编码与解码\n\n使用特定编码方案（如曼彻斯特编码、PAM4）以提高抗干扰能力，确保信号完整性。\nMAC\n发送数据时​​，计算 ​​CRC（循环冗余校验）​​，确保数据完整性。\n接收数据时​​：\n从 PHY 层接收原始比特流，解析成以太网帧。\n检查​目标MAC地址​​（仅接收发给本机、广播或组播的帧）。\n校验 ​​FCS​​，丢弃损坏的帧。\n\n\n流量控制：使用 ​​PAUSE 帧（IEEE 802.3x）​​ 通知对端设备暂停发送，防止缓冲区溢出。\n\nDMA数据直接传输，网卡通过DMA引擎直接读写主机内存\nRSSRSS 是一种由 网卡硬件实现 的多队列技术，主要用于 提升多核 CPU 的网络数据包处理性能。网卡硬件将流量分散到多个接收队列（RX Queues），每个队列绑定不同CPU核心。\nTSOTSO 是一种由 网卡硬件实现 的优化技术，旨在 将TCP数据包的分片（Segmentation）任务从CPU转移到网卡，从而大幅降低CPU负载并提升网络吞吐量。\nPCIe\n提供网卡与CPU&#x2F;内存的物理通道，决定最大带宽（如100G需PCIe 4.0 x8）。\n支持DMA、MSI-X中断，优化响应速度\n\n","categories":["其他"],"tags":["网卡"]},{"title":"虚拟内存（一）","url":"/2025/06/28/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98(%E4%B8%80)/","content":"虚拟内存概念：为了更有效地管理内存并减少出错，现代系统提供了一种对主存的抽象，叫做 虚拟内存（VM）。  虚拟内存是硬件异常、地址翻译、主存、磁盘文件和内核软件的共同作用，为每个进程提供了一个大的一致且私有的地址空间。\n虚拟内存提供三大核心能力：\n\n高效利用主存将主存视为磁盘地址空间的高速缓存，仅保存活跃区域，按需在磁盘和主存之间调页，从而高效使用物理内存。\n\n一致的地址空间给每个进程提供一致的虚拟地址空间，简化内存管理和程序设计。\n\n地址空间隔离与保护防止进程间互相干扰，保障系统稳定性和安全性。\n\n\n物理寻址与虚拟寻址物理寻址（Physical Addressing） 主存由 M 个连续的字节单元组成，每个字节有唯一的物理地址（如 0, 1, 2, …）。 早期 PC、一些数字信号处理器、嵌入式微控制器和 Cray 超级计算机等直接使用物理地址，CPU 访问内存时直接生成物理地址，通过内存总线取出数据。\n虚拟寻址（Virtual Addressing） 现代处理器一般使用虚拟寻址。CPU 生成的地址是虚拟地址（VA），需要通过**地址翻译（Address Translation）转换成对应的物理地址（PA）。 地址翻译由 CPU 的内存管理单元（MMU）**和操作系统配合完成。MMU 使用操作系统维护的查询表（如页表），动态把虚拟地址映射到物理地址，实现灵活的内存管理和保护。\n地址空间就是一个有序的、非负整数地址的集合，用来给数据单元（如字节）编号。\n本质：就是一组连续的数字 0, 1, 2, ..., N-1。\n线性地址空间：如果是连续的整数集合，就叫做线性地址空间，现代 CPU 都是线性的。\n大小：由地址能表示的最大值决定，比如：\n\n32 位虚拟地址空间：最多表示 2^32 个不同的地址。\n64 位虚拟地址空间：最多表示 2^64 个不同的地址。\n\n虚拟内存作为缓存的工具在概念上，虚拟内存可以看作是一个由 N 个连续字节组成的大数组，这个数组存在磁盘上，每个字节都有唯一的虚拟地址作为索引。这个大数组里的数据不是全部都放在主存中，而是像其他缓存一样，把活跃部分缓存在主存里。 磁盘是存储层次结构中的较低层，主存是较高层，两者之间的数据传输是按块进行的。为此，虚拟内存系统把整个虚拟地址空间分割成固定大小的块，称为 虚拟页（Virtual Page, VP），每个页大小是 P 字节。同样，物理内存也被分成相同大小的 物理页（Physical Page, PP），也叫做 页帧（Page Frame）。\n在任何时刻，所有的虚拟页可以分为三个互不重叠的类别：\n\n未分配页：还没有被使用或创建的虚拟页，没有实际数据，对应的磁盘空间也不存在。\n已分配且已缓存页：已经分配，并且当前缓存在物理内存中的页。\n已分配但未缓存页：已经分配，但目前没有缓存在物理内存中的页，数据保存在磁盘上。\n\n\nDRAM 缓存的组织结构在存储层次结构里：\n\nSRAM 缓存 指的是 CPU 内部的 L1&#x2F;L2&#x2F;L3 缓存，用来缓存 DRAM。\nDRAM 缓存（虚拟内存缓存）指的是主存用来缓存磁盘上的虚拟页。\n\n位置决定开销差异\n\nDRAM 比 SRAM 慢大约 10 倍。\n磁盘比 DRAM 慢 100,000 倍以上。\n所以 DRAM 缓存的未命中（需要从磁盘换页） 代价远远比 SRAM 缓存的未命中高。\n\n映射方式：全相联\n\nDRAM 缓存是全相联的，任何虚拟页可以放到任何物理页帧。\n\n写策略：总是写回\n\n因为磁盘访问太慢，虚拟内存的写操作不会立刻写回磁盘。\n写会先落在物理内存页帧里，等到换出时才写回（写回，write-back），而不是像某些 SRAM 缓存可选直写（write-through）。\n\n主存（DRAM 缓存）是磁盘（虚拟页）的缓存层。\n当进程修改了某个虚拟页：\n\n改动先只写到物理内存里的页帧（Page Frame）\n页帧会被标记为 脏页（Dirty Page）\n\n等到这个物理页帧要被换出（被别的页替换掉）时，操作系统才把脏页写回磁盘。\n这样能减少对磁盘的频繁写操作，提高性能。\n页表同任何缓存一样，虚拟内存系统必须有某种方法来判定一个虚拟页是否缓存在DRAM 中的某个地方。 如果是，系统还必须确定这个虚拟页存放在哪个物理页中。如果不命中，系统必须判断这个虚拟页存放在磁盘的哪个位置，在物理内存中选择一个牺牲页，并将虚拟页从磁盘复制到 DRAM 中，替换这个牺牲页。 这些功能是由软硬件联合提供的，包括操作系统软件、 MMU(内存管理单元）中的地址翻译硬件和一个存放在物理内存中叫做页表(page table)的数据结构，页表将虚拟页映射到物理页。每次地址翻译硬件将一个虚拟地址转换为物理地址时，都会读取页表。 操作系统负责维护页表的内容，以及在磁盘与 DRAM之间来回传送页。\n页表（Page Table）\n\n本质是一个页表条目（Page Table Entry，PTE） 的数组。\n虚拟地址空间里的每个虚拟页都在页表里有且只有一个对应的 PTE，位置是固定的（根据虚拟页号直接找到）。\n\nPTE（页表条目）的内容\n\n每个 PTE 里至少包含：\n有效位（Valid Bit）：表明这个虚拟页是否当前缓存在物理内存（DRAM）里。\n地址字段：如果有效位是 1，表示这个字段保存了该虚拟页对应的物理页帧的起始位置。\n\n\n\n如下图所示：\n\n缺页缺页就是CPU 访问了一个虚拟地址，对应的页不在物理内存里。\n在虚拟内存的习惯说法中， DRAM 缓存不命中称为缺页(page fault)。 下图展示了在缺页之前页表的状态。 CPU 引用了 VP 3 中的一个字， VP 3 并未缓存在 DRAM 中。mmu从内存中读取 PTE 3, 从有效位推断出 VP 3 未被缓存，并且触发一个缺页异常。缺页异常调用内核中的缺页异常处理程序，该程序会选择一个牺牲页（这里不一定会发生换入换出吧？如果物理内存里有空闲页帧，就直接放进去，不需要换出）， 在此例中就是存放在 pp 3 中的 VP 4。\n\n接下来，内核从磁盘复制 VP 3 到内存中的 pp 3, 更新 PTE 3, 随后返回。当异常 理程序返回时**，它会重新启动导致缺页的指令**，该指令会把导致缺页的虚拟地址重发送到 地址翻译硬件。但是现在， VP 3 已经缓存在主存中了，那么页命中也能由地址翻译硬件正常处理了。\n\n","categories":["《深入理解计算机系统》"],"tags":["虚拟内存"]},{"title":"虚拟内存（二）","url":"/2025/06/29/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98(%E4%BA%8C)/","content":"虚拟内存简要总结虚拟内存的核心作用\n\n每个进程都有独立的虚拟地址空间和页表，实现地址空间隔离和内存保护。\n操作系统通过页表把进程的虚拟地址映射到物理内存，实现内存管理的灵活性。\n\n虚拟内存带来的好处\n\n简化链接：不同进程的代码和数据在虚拟空间中布局一致（比如代码总从同一个虚拟地址开始），链接器生成的可执行文件不依赖物理内存实际布局，简化了实现。\n简化加载：加载器只需分配虚拟页并做映射，实际数据只有在访问时才按需从磁盘加载到内存（即按需调页、懒加载）。\n简化共享：同一份物理内存（如操作系统内核、C标准库）可以被多个进程的虚拟空间同时映射，实现高效代码和数据共享。\n简化内存分配：进程申请新内存时，操作系统只需分配若干虚拟页，并将它们映射到任意空闲的物理页，不必保证物理上连续。\n\n共享机制\n\n默认情况下，每个进程的虚拟空间互不影响，各自私有。\n需要共享的内容（如内核代码、共享库）可被多个进程的虚拟空间指向同一份物理内存。\n\n内存映射 (memory mapping)\n\n操作系统允许把一组虚拟页映射到文件的任意位置，实现高效的文件访问与共享（如 mmap 系统调用）。\n\n虚拟内存实现内存保护的原理\n内存保护的目标\n防止用户进程修改只读的代码区。\n防止用户进程访问内核空间（内核代码和数据）。\n防止进程间相互访问对方的私有内存。\n防止进程随意修改共享内存，除非所有共享方都明确允许。\n\n\n独立地址空间与访问控制\n每个进程有自己的虚拟地址空间，彼此隔离，天然实现了基本的内存保护。\n\n\n页表权限位（PTE 许可位）\n在每个页表项（PTE）中增加了权限控制位（如 SUP、READ、WRITE）：\nSUP（超级用户&#x2F;内核模式）：此页是否只能被内核模式访问，用户模式下不允许访问。\nREAD：此页是否允许读。\nWRITE：此页是否允许写。\n\n\n通过这些权限位，操作系统可以精确控制每个页面的访问权限。\n比如：用户进程不能读写内核空间，只能访问本进程被允许的区域。\n\n\n\n\n访问权限检查的实现方式\n每次CPU访问内存，都会通过页表查找虚拟地址对应的物理页，同时检查权限位。\n如果当前进程没有足够权限（比如用户进程试图写只读区、读内核区），CPU会触发异常，即“段错误（segmentation fault）”，操作系统捕获并处理，比如终止该进程。\n\n\n\n\n地址翻译虚拟地址空间（VAS）和物理地址空间（PAS）是两个不同的地址集合。\n地址映射就是虚拟地址空间的每个元素映射到物理地址空间的元素，或者无效（不存在于物理内存中，产生缺页）\n虚拟地址结构\n虚拟地址由**虚拟页号（VPN）和虚拟页内偏移量（VPO）**两部分组成。\n假设虚拟地址是n位，其中低p位是VPO，高(n-p)位是VPN。\n例：32位地址空间，4KB页（12位偏移），则VPN占20位，VPO占12位。\n\n\n\n页表的作用\n页表基址寄存器（PTBR）：指向当前进程的页表起始地址。\n页表用来存储虚拟页号到物理页号（PPN）的映射关系。\n每个页表项包含：\n有效位：表示该虚拟页是否在物理内存中（1&#x3D;在，0&#x3D;不在）。\n物理页号（PPN）：虚拟页号实际对应的物理页号。\n\n\n\n\n虚拟内存系统中页面命中和缺页的流程如下图所示：\n页面命中：\n\nCPU生成虚拟地址（VA）\n程序执行时（如读写数据或取指令），CPU发出一个虚拟地址，传递给MMU。\n\n\nMMU生成页表项地址（PTEA）\nMMU用虚拟地址的页号部分和页表基址，算出页表项的物理地址（PTEA）。\n\n\n高速缓存&#x2F;主存返回PTE\nMMU访问cache或主存，取得这个PTE内容（主要是物理页号、有效位等）。\n\n\nMMU构造物理地址（PA）\nMMU把PTE中的物理页号（PPN）和虚拟地址的页内偏移拼起来，形成物理地址（PA），传递给cache&#x2F;主存。\n\n\n高速缓存&#x2F;主存返回数据\ncache&#x2F;主存用物理地址去存取数据，把结果送回CPU，继续指令执行\n\n\n\n\n缺页处理：\nCPU生成虚拟地址（VA）\nMMU生成PTEA，cache&#x2F;内存未找到有效PTE（有效位&#x3D;0）\n\n发生异常（缺页异常，step 4）\n\nCPU&#x2F;操作系统进入缺页异常处理程序\n\n操作系统查明缺页原因（比如页面还在磁盘上）。\n通过缺页异常处理程序，把所需页面调入内存。\n\n从磁盘读取页面\n\n需要的数据页面从磁盘调入物理内存（新页）。\n\n更新页表\n\n操作系统把新页面的物理页号写回页表，并把有效位置1。\n\n重新发起原指令\n\n程序恢复，被中断的指令重新执行，流程回到页面命中那一套。\n\nSRAM与虚拟内存在实际工作中，CPU访问内存时，首先会用虚拟地址经过MMU翻译成物理地址，然后用物理地址去查找Cache。如果Cache命中就直接返回数据，否则再去主存中查找。需要注意的是，页表项（PTE）本身也可以像普通数据一样被缓存在Cache中，以提升访问效率。\n\nTLBTLB就像是“地址翻译的加速器”，直接缓存了最近的页表项，大部分虚拟地址转换都能在MMU内部1步完成，从而让虚拟内存的性能几乎和物理内存一样高效。\n为什么需要TLB？\n问题：每次虚拟地址到物理地址的转换都要查页表（查PTE），\n最慢的时候要访问一次主存（几十到几百CPU周期）。\n如果PTE在L1 cache里，也要1-2个周期。\n\n\n优化目标：连这1-2个周期都想省掉，让地址翻译像查寄存器一样快。\n\nTLB是什么？\nTLB就是MMU内部的一个超快的小型Cache，专门缓存最近用过的一组“虚拟页号VPN ➔ 页表项PTE”的映射。\nTLB很小（几十到几百项）但速度极快，一般为全相联或组相联。\n\nTLB的工作机制TLB命中:\n\nCPU产生虚拟地址（VA）\nMMU用VA里的虚拟页号（VPN）查TLB\nTLB里通过索引和标记直接查到PTE（页表项）\n\n\nMMU用PTE快速翻译出物理地址（PA）\n用物理地址访问Cache&#x2F;主存，返回数据给CPU\n\nTLB未命中:\n\nTLB没找到对应的VPN\nMMU只能去查更慢的L1 Cache&#x2F;主存获取PTE\n查到PTE后，把它塞进TLB（可能覆盖掉最久未用的条目）\n继续用新查到的PTE翻译虚拟地址为物理地址，访问内存\n\n\n多级页表单级页表的缺点以32位系统为例，虚拟地址空间有4GB。\n每个页面大小4KB。\n那么虚拟地址空间里一共需要 4GB &#x2F; 4KB &#x3D; 1M（&#x3D;1048576）个页。\n每个PTE（页表项）用4字节，总共页表需要 4 * 1M &#x3D; 4MB。\n问题： 只要进程一启动，就需要给它分配4MB的页表，不管它实际只用了多少内存（哪怕你只用了1页，也要有4MB的页表）。\n多级页表\n思想： 把页表分成多级（比如两级），只为实际用到的部分分配内存。\n比如：两级页表，一级页表每一项指向一个二级页表（实际存放PTE），一级页表本身只有1024项（对应1024个“4MB片段”）。\n如果某个4MB范围内一个页面都没用过，那一级页表对应的那一项就设为NULL（不分配二级页表）。\n只有用到内存的那部分，才分配对应的二级页表。\n这样大部分没用到的虚拟地址空间都不用分配实际的页表存储空间，节省内存。\n\n\nK级页表\n虚拟地址被分为K段VPN（虚拟页号）和1段VPO（页内偏移）。\n每一级页表的VPN用于索引对应级别的页表，逐级查找下一级页表地址。\n最终第K级页表的PTE中，保存的是目标物理页号（PPN）或磁盘地址。\nMMU翻译一个虚拟地址时，要访问K个PTE（比如2级页表，要先查一级表，再查二级表）。\n对于最简单的只有一级页表结构，页内偏移（PPO）和VPO相同。\n\n\n","categories":["《深入理解计算机系统》"],"tags":["虚拟内存"]},{"title":"虚拟内存（五）","url":"/2025/07/06/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98(%E4%BA%94)/","content":"显示空闲链表隐式空闲链表查找效率低，适合块数很少的情况；显式空闲链表把空闲块用前后指针连成链表，比如用双向链表，可以大大加快查找速度（只跟空闲块数量有关），如果用后进先出插入，释放块和合并可以是常数时间，但如果按地址排序虽然释放块需要找位置（慢一些），但内存利用率更高；缺点是显式链表需要额外的前后指针，块最小尺寸增大，增加了内部碎片。\n\n与内存有关的错误间接引用坏指针\n间接引用就是用指针去访问内存。\n坏指针就是：\n指向了不该访问的区域（比如没分配过的虚拟内存洞）\n或者指向了只读区域（但你却想写它）\n或者根本就是乱的值（未初始化、被覆盖、拼错……）\n\n\n\n当你去解引用（访问 *p）一个坏指针时，操作系统会触发保护机制（段错误或保护错误），直接把程序干掉，防止你破坏别的内存。\n例子：\nint val;scanf(&quot;%d&quot;, &amp;val);  // 正确：传的是变量 val 的地址scanf(&quot;%d&quot;, val);  // 错误：把 val 的值当成地址传了\n\n可能的后果\n\n最好 的情况：那个“地址”根本是无效区域，操作系统直接让程序崩溃（段错误）。\n最坏 的情况：那个“值”刚好是你程序里有效的、可写的某块内存地址，于是 scanf 会往这块内存写数据 —— 但是你本意并不想动这里！ 这就等于是把脏数据写到你完全没打算改的地方。 当时可能没事，程序继续跑，等之后用到那块内存时，就会莫名出现奇怪的错误，甚至难以定位\n\n读未初始化的内存当 malloc 一块堆内存，或者定义一个局部变量（比如 int x;），编译器&#x2F;操作系统不会保证把它自动清零。\n这意味着，这块内存里之前残留的比特值还在，内容是随机的（别的程序或者以前的数据遗留）。\n栈缓冲区溢出栈（stack）：程序运行时，用来存放局部变量、函数调用时的返回地址、保存的寄存器等等。\n缓冲区（buffer）：在 C 里通常就是 char arr[100] 这种数组，用来临时存放数据（比如字符串）。\n溢出（overflow）：如果往数组里写的内容超过了它原本分配的大小，就会覆盖后面的内存。\n指针大小混淆错误int **makeArray1(int n, int m) &#123;    int i;    int **A = (int **)Malloc(n * sizeof(int));  // 错误    for (i = 0; i &lt; n; i++)        A[i] = (int *)Malloc(m * sizeof(int));    return A;&#125;\n\n意图：\n\n创建一个指针数组 A，包含 n 个指针（int *），\n每个指针再指向一块 m 个 int 的内存。\n\n也就是想要构造一个 n x m 的二维数组。\nint **A = (int **)Malloc(n * sizeof(int));\n\n这里的意思是：我要申请 n 个指针的空间，但是 sizeof 写错了。\n\nA 是 int ** 类型，对应是一个 int * 指针数组。\n\n越界写内存可能正好没引发崩溃，因为你写到的可能是 别的分配块的头尾，比如堆块的「脚部」信息（用于记录块大小和空闲状态）。\n当你以后释放内存时，分配器会用这些脚部标记做块合并，结果发现结构被破坏，合并逻辑就会崩溃。\n引用不存在的变量(悬空指针)int *stackref()&#123;    \tint val;    \t    \treturn &amp;val;&#125;\n\nint val; 是在函数 stackref 的 栈帧 上分配的局部变量。\nreturn &amp;val; 把 val 的地址返回给外面。\n但一旦 stackref 返回，这个函数的栈帧就会被弹出，局部变量 val 不复存在。\n外面的指针 p 仍然保存着 val 原来的地址，这个地址现在指向的内存已经 不再属于 val，而是留给以后别的函数调用重用。\n所以 p 成了一个 悬空指针，它指向一块已经失效的栈内存。\n访问或使用已释放内存先用 malloc 分配了 x，它指向一块有效的堆内存。\n然后调用 free(x) 把这块内存释放了，这块内存在分配器眼里已经不再有效，已经放回了空闲块。\n接着又访问 x：\n\n从语法上看 x 还是一个合法的指针，但它指向的内存已经不属于你了。\n可能这块内存被分配器用来给别的 malloc 复用了，数据可能已经被覆盖。\n\n后果：\n会读到脏数据（可能是别的对象的数据）。\n写的话会破坏别的对象或分配器内部结构，导致堆损坏（Heap Corruption）。\n内存泄露","categories":["《深入理解计算机系统》"],"tags":["虚拟内存"]},{"title":"虚拟内存（三）","url":"/2025/07/01/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%EF%BC%88%E4%B8%89%EF%BC%89/","content":"地址翻译以Core i7为例：如下图所示：\n\n**什么叫N路组相联? **8路组相联Cache例子如下：\nCache总大小：16KB\n块大小：64B\n8路组相联\n地址空间：32位\n\n计算组数和分段\n\n总块数 &#x3D; 16KB &#x2F; 64B &#x3D; 16384B &#x2F; 64B &#x3D; 256块\n组数 &#x3D; 256块 &#x2F; 8 &#x3D; 32组\n\n地址分解\n\n块内偏移：64B，需要6位（2^6&#x3D;64）\n组索引：32组，需要5位（2^5&#x3D;32）\nTag：剩下高21位（32-6-5&#x3D;21）\n\n举一个32位实际地址\n假设地址为0xCAFEBABE 二进制：1100 1010 1111 1110 1011 1010 1011 1110\n分段如下：\n\nTag: 21位，高位\n组索引: 接下来的5位\n块内偏移: 低6位\n\nTag &#x3D; 110010101111111010110 &#x3D; 0x657F56\n组索引 &#x3D; 10101 &#x3D; 21（十进制）\n块内偏移 &#x3D; 011110 &#x3D; 30（十进制\n查找流程\n\n先用组索引\n组索引21，直接定位到第21组。\n\n\n组内查找（8路）\n组内有8个块（可能存着不同Tag的数据）。\n查看这8个块，有没有Tag是0x657F56并且有效。\n如果有，命中！直接返回数据（取块内第30字节）。\n如果没有，发生缺失，需要把主存相应块装进来，替换组内某个块。\n\n\n\n页表项的结构页表项（Entry）格式：比如一级、二级、三级页表，每一项64位，内容包括“物理基地址”和一系列属性位（控制位）。\n物理基地址：页表项的高40位，用来指向下一级页表的物理地址（4KB对齐）。\n低12位：各种控制标志（如P、RW、US等\n\nlinux虚拟内存系统每个进程都有独立的虚拟地址空间\n用户空间（进程虚拟内存）：每个进程看到的都是“独立”的虚拟空间，包括：\n代码段（.text）：可执行指令\n数据段（.data&#x2F;.bss）：全局变量\n堆（heap）：由brk&#x2F;mmap增长，动态分配\n共享库：动态链接库映射区域\n栈（stack）：通常从高地址向低地址生长\n\n\n每个进程的这些区域虚拟地址相同，但对应的物理页面不同，互不影响。\n\n内核虚拟内存空间\n位于用户空间之上（高地址区，通常x86-64为0xffff800000000000及以上）。\n所有进程都映射同一份内核代码和全局数据，即物理页是共享的。\n包含：\n内核代码和数据（共享）\n物理内存映射区：连续虚拟页映射到所有物理内存页，为内核直接访问物理内存提供便利（如DMA、页表管理等）\n每个进程独有的数据结构：如页表、task_struct、mm_struct、内核栈（每个进程单独分配）\n\n\n\n地址翻译关系\n用户空间虚拟地址 → 多级页表翻译 → 进程自己的物理页面\n内核空间虚拟地址 → 所有进程页表都映射到同一物理内存区域\n\n\nLinux 虚拟内存区域Linux 将虚拟内存组织成一些区域（也叫做段）的集合。 一个区域(area)就是已经存在着的（已分配的）虚拟内存的连续片(chunk), 这些页是以某种方式相关联的。例如，代码 段、数据段、堆、共享库段，以及用户栈都是不同的区域。每个存在的虚拟页面都保存在某个区域中，而不属千某个区域的虚拟页是不存在的，并且不能被进程引用。 区域的概念很重要，因为它允许虚拟地址空间有间隙。内核不用记录那些不存在的虚拟页，而这样的页也不占用内存、磁盘或者内核本身中的任何额外资源。\nlinux组织虚拟内存的方式如下所示：\n\n触发缺页的场景缺页处理器的三个主要步骤1. 检查虚拟地址是否合法\n查找vm_area_struct链表&#x2F;树，判断A是否落在某个VMA区域内（vm_start &lt;= A &lt; vm_end）。\n不合法：直接发送段错误（SIGSEGV），终止进程（比如访问未分配内存）。\n优化：实际Linux用红黑树加速VMA查找，而不仅仅是链表。\n\n\n\n2. 检查访问权限是否合法\n检查对A的访问操作（读&#x2F;写&#x2F;执行）是否在当前VMA的权限（vm_prot&#x2F;vm_flags）内。\n比如只读区域不能写，用户进程不能访问内核空间。\n不合法：保护异常（如写只读、越权），终止进程。\n\n\n\n3. 进行缺页调度\n到这步说明是合法访问，但物理页不在内存。\n换入页面：如在swap&#x2F;磁盘，读入内存。\n分配新页：如第一次写入匿名页、堆&#x2F;栈自动扩展等。\n页替换：如果物理内存已满，选择牺牲页（如LRU），若被修改过先写回磁盘。\n更新页表：将新的物理页和虚拟地址A建立映射，置有效位。\n恢复指令：异常处理返回后，CPU重新执行导致缺页的那条指令，这次就能翻译成功。\n\n\n\n","categories":["《深入理解计算机系统》"],"tags":["虚拟内存"]},{"title":"虚拟内存（四）","url":"/2025/07/03/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"malloc()malloc如何管理一个堆的，如下所示：\n\na) p1 &#x3D; malloc(4 * sizeof(int))\n\n申请4个int的空间，malloc从最前面切一块给p1，前4格变深色。\np1指向新分配的这块内存起始处。\n\n\nb) p2 &#x3D; malloc(5 * sizeof(int))\n\n申请5个int的空间，为了对齐，实际分配6格，p2指向新分配块的起始处。\np2后面多出1格（深蓝），是为“对齐”而加的（比如要8字节对齐，补了1字节）。\n\n\nc) p3 &#x3D; malloc(6 * sizeof(int))\n\n申请6个int的空间，从剩余空闲区的头部分出去6格，p3指向新块。\n\n\nd) free(p2)\n\n释放p2指向的那一块（包括对齐的多余空间），这一块变成“空闲”状态（浅色），但p2指针本身没变，只是p2指向的内存已经不能再用。\n\n\ne) p4 &#x3D; malloc(2 * sizeof(int))\n\n再次申请2个int的空间，malloc会优先复用刚刚释放的空闲块的前2格，p4指向这里。\n剩下4格还是空闲，以后还可以分配。\n\n碎片碎片：就是虽然堆上还有空闲内存，但分配请求还是失败了（因为空闲块无法直接满足请求），这就导致了内存浪费，降低了利用率。\n碎片分为两种：\n内部碎\n指的是“已分配的内存块”比实际需要的要大，导致部分空间浪费在“块内部”。\n产生原因：\n有些分配器规定最小分配单位（比如你只要 2 字节，分配器必须分 8 字节）。\n为了对齐，比如分配 5 个字节，实际给你 8 个字节，后面空着不用。\n\n\n图 b 就是典型：malloc(5*sizeof(int))，但分配了6个int空间，多出来的1个就是“内部碎片”。\n内部碎片的量&#x3D;每个分配块的“总大小” - “实际需要大小”，全堆累加。\n\n外部碎片\n指的是：堆上总的空闲空间足够，但没有一块连续空闲块能满足分配请求。\n举例：图 9e 里，假如此时需要分配 6 个 int 空间，但空闲区被分成了两个小块（每块&lt;6），就没法直接分配出一块6的，必须要向系统再要新内存。\n外部碎片的根本问题：内存空闲块“被切得很碎”，不能凑出一块大的。\n外部碎片的难点是，它依赖于将来的分配请求：\n如果后续请求都很小（比如≤4字节），这些小空闲块可以用完，也就没有外部碎片。\n如果有大请求（比如要8字节），就没法直接满足了，外部碎片就出现了。\n\n\n\n分配器的实现难点：真实世界的分配器（如glibc中的malloc），既要快（高吞吐），又要省（高利用率），所以必须要解决很多实际问题，主要包括：\n（1）空闲块如何组织？\n释放后的内存如何记录下来（比如用链表、树，或者分段等方法管理）？\n\n（2）放置策略\n每次分配新内存时，从所有空闲块里怎么选一块？\n是选第一个合适的？最大&#x2F;最小的？还是最靠前的？\n\n\n\n（3）分割策略\n如果空闲块比请求的大，如何把剩下的部分保留下来？（比如分割成两块）\n\n（4）合并策略\n当有块被释放时，如果相邻块也是空闲的，是否应该合并成更大的空闲块？（防止出现太多小碎片）\n\n隐式空闲链表\n\n头部（Header）\n\n\n每个内存块的开头有一个“头部”（通常4字节或8字节），保存块的大小和分配状态。\n图中用“块大小”字段的低3位来记录状态：\n00a：a表示分配标志（allocated），如果a=1表示已分配，a=0表示空闲。\n因为块总是按8字节对齐，最低3位平时都为0，可以拿出来用。\n\n\n\n\n有效载荷（Payload）\n\n\n这是用户调用malloc后实际可以用的那块空间（也是malloc返回的指针指向的起始地址）。\n只包括用户申请到的那部分。\n\n\n填充（Padding，可选）\n\n\n为了保证对齐，如果用户申请的空间不是对齐大小，会在末尾加一些无用的填充字节，把整个块补齐到8字节的倍数。\n这个填充区是“可选的”，只在需要时加。\n\n分配器通过顺序扫描每块头部信息，来判断块边界、大小和分配状态\n\n内存分配时，怎么找“空闲块”主要有三种策略\n首次适配（First Fit）\n\n做法：每次都从链表头开始遍历，找到第一个大小足够的空闲块就用。\n优点：实现简单，通常速度比较快。\n缺点：容易在链表前面留下很多“小碎片”，这些碎片很难再利用，后面分配大块时就需要一直往后找，效率下降。\n\n下一次适配（Next Fit）\n\n做法：和首次适配类似，但每次搜索不是从头开始，而是从上一次分配结束的地方开始搜索。\n优点：如果前面已经被分成很多碎片，跳过它们会快一些。\n缺点：长期运行后，内存利用率往往比首次适配更低（因为大块可能被“跳过”，碎片问题更严重）。\n\n最佳适配（Best Fit）\n\n做法：遍历所有空闲块，找到“刚好能放下所需空间”的最小空闲块。\n优点：理论上能最大化内存利用率，减少碎片。\n缺点：要遍历所有空闲块，速度慢，效率低。\n\n分割空闲块 “用多少”\n当分配器找到一个足够大的空闲块时，如果这个块比需要分配的还要大，分配器要决定要不要把它分成两块——一块刚好满足用户的请求，剩下的留作空闲块。\n\n\n如果不分割，直接把整个空闲块都分配了，虽然简单，但会浪费空间，产生“内部碎片”（即：用户用不到的空间也被分出去了）。\n\n如果分割，就能更高效利用内存，把多出来的部分留作下次分配用。\n\n\n\n找不到合适的空闲块怎么办？\n当程序调用 malloc 申请内存时，分配器会遍历所有空闲块，想找到一个足够大的来满足请求。\n如果没有任何一个空闲块能满足分配请求（比如所有空闲块都太小，或根本没有空闲块）：\n\n第一步：尝试合并相邻空闲块（合并算法）\n\n如果有些空闲块在物理内存上是相邻的，可以把它们合并成一个更大的空闲块（这样有可能凑出足够大的空间）。\n合并的方法叫“块合并&#x2F;块归并（coalescing）”。\n\n第二步：还是不够怎么办？\n\n如果合并之后还是没有足够大的空闲块，或者本来就没什么空闲块了，怎么办？\n\n第三步：向内核要新内存\n\n分配器会调用操作系统的系统调用（ sbrk 或 mmap），\n让操作系统分配一块新的堆内存（把进程的堆往上扩大一块）。\n分配器把新获得的这块大内存当作一个新的“大空闲块”，放入自己的空闲块链表中。\n然后在这块新内存里，分出一块满足请求的空间，剩下的还作为空闲块继续管理\n\n合并空闲块\n合并就是把相邻的空闲块合成一个更大的空闲块，这样可以减少碎片。\n什么时候合并，有两种策略：\n\na) 立即合并（Immediate Coalescing）\n\n每次释放内存时，立刻检查相邻块，如果它们是空闲的，就直接合并。\n优点：简单，效率高，始终保持碎片最少。\n缺点：有时会导致“合并-分割-合并-分割”来回抖动（比如一直分配&#x2F;释放相邻的小块时）。\n\nb) 推迟合并（Deferred Coalescing）\n\n释放内存时不立即合并，而是等到某些条件触发时（比如分配失败时），再把所有空闲块扫描并合并一次。\n优点：减少不必要的合并&#x2F;分割操作，提高某些场景下的性能。\n缺点：在推迟期间会出现假碎片，直到触发合并操作。\n\n带边界标记的合井\n当前块释放时，可以直接看下一个块的头部，判断它是不是空闲。\n\n如果是，就可以把下一个块的大小合并到当前块，这一步是常数时间 O(1) 的。\n\n如果要合并前一个空闲块，问题在于：你不知道前一个块的起始位置（因为你的指针是指向当前块的）。\n\n如果用“隐式空闲链表”（即每个块只有头部，没有其他指针），那就只能从堆头顺着头部一直找，直到找到前一个块——这就是线性时间 O(n)，很慢。\n\n\n边界标记法（boundary tag）\n\n每个块除了头部外，还在结尾加一个“脚部”（footer，也叫边界标记）。\n脚部就是头部的一个副本，记录块的大小和是否已分配。\n这样你拿到当前块后，可以直接看当前块前面“一个字”位置的脚部，得到前一个块的信息（它的大小和起始地址）。\n所以，释放当前块时，可以在常数时间找到前一个块，并判断要不要合并。\n\n","categories":["《深入理解计算机系统》"],"tags":["虚拟内存"]},{"title":"链接(一)","url":"/2025/06/23/%E9%93%BE%E6%8E%A5(%E4%B8%80)/","content":"链接链接（Linking）是一个将多个代码和数据片段（来自编译器生成的目标文件）收集并组合为一个单一文件的过程，这个文件可以被加载到内存中执行。\n 链接的目标文件可以是：\n\n可执行文件\n共享库（动态库）\n或中间的可重定位文件\n\n链接可以发生在不同的阶段：\n\n\n\n链接阶段\n说明\n\n\n\n链接时（Link-time）\n最常见的，指用 ld 或 gcc 把多个 .o、.a、.so 链接为一个可执行文件或库。\n\n\n加载时（Load-time）\n程序被操作系统加载到内存时，动态链接器（如 /lib64/ld-linux.so.2）把 .so 文件装载并绑定符号。\n\n\n运行时（Run-time）\n程序运行过程中自行调用 dlopen() &#x2F; dlsym() 等函数动态加载库并解析符号。\n\n\n静态链接静态链接是指链接器将所有需要的代码、数据和库函数一次性合并到一个最终的可执行文件中，使这个文件在运行时不再依赖外部库\n举例：\n// main.cint add(int a, int b);int main() &#123;    return add(2, 3);&#125;\n\n// add.cint add(int a, int b) &#123;    return a + b;&#125;\n\ngcc -c main.c -o main.ogcc -c add.c -o add.ogcc -static main.o add.o -o app\n\n静态链接的两个核心任务：符号解析（Symbol Resolution）\n每个 .o 文件里都可能定义或引用符号（比如变量名、函数名）。\n链接器要搞清楚：main.o 中调用的 add 函数，在 add.o 中定义。\n它的任务就是：把所有引用的符号，找到它真正定义的位置。\n\n重定位（Relocation）\n编译器和汇编器把代码和变量的位置都写成从地址 0 开始，也就是未定地址。\n链接器决定每个函数&#x2F;变量在最终程序里的实际位置，并：\n修改机器码中的跳转地址\n修改数据引用的偏移地址\n\n\n\n这一步就叫做 “重定位” —— 把所有相对&#x2F;未知地址“补丁”打上真实的地址。\n目标文件保存了指令、数据和符号等信息，通常使用如 ELF等平台特定格式进行组织。\n三种目标文件类型\n\n\n\n类型\n英文名称\n可否执行\n是否可重定位\n生成者\n说明\n\n\n\n可重定位目标文件\nRelocatable Object File\n否\n是\n编译器 &#x2F; 汇编器\n.o 文件，可参与链接\n\n\n可执行目标文件\nExecutable Object File\n是\n否\n链接器\n.out、a.out、无扩展名，已可运行\n\n\n共享目标文件\nShared Object File\n否（被调用）\n是（动态）\n编译器（加 -shared）\n.so 文件，运行时加载\n\n\n可以用如下命令来查看目标文件的类型和格式\n~/workspace$ file main.o main.o: ELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), not stripped\n\n可重定位目标文件可重定位目标文件是由多个**节（section）**组成的结构化文件，包含编译后的机器指令、数据、符号表和重定位信息，用于链接器进一步生成可执行文件或共享库。\n┌─────────────┐│ ELF Header  │ ← 描述整个文件的结构、类型、偏移等├─────────────┤│ .text       │ ← 机器码指令（函数体）│ .rodata     │ ← 只读数据（字符串常量、跳转表等）│ .data       │ ← 初始化全局/静态变量│ .bss        │ ← 未初始化全局/静态变量（只记录大小，不占磁盘空间）│ .symtab     │ ← 符号表（函数名、变量名等）│ .rel.text   │ ← 标记 `.text` 中需要重定位的位置│ .rel.data   │ ← 标记 `.data` 中需要重定位的位置│ .debug      │ ← 调试信息（如变量、类型、源码行号等）│ .line       │ ← 行号映射信息（源码行 ↔ 指令地址）│ .strtab     │ ← 字符串表（`.symtab` 和 `.debug` 的名称信息）├─────────────┤│ 节头部表     │ ← 描述上面每个 section 的偏移、大小、名字等└─────────────┘\n\n具体例子如下：\n~/workspace$ readelf -S main.oThere are 14 section headers, starting at offset 0x208:\n\nSection Headers:  [Nr] Name              Type             Address           Offset       Size              EntSize          Flags  Link  Info  Align  [ 0]                   NULL             0000000000000000  00000000       0000000000000000  0000000000000000           0     0     0  [ 1] .text             PROGBITS         0000000000000000  00000040       0000000000000000  0000000000000000  AX       0     0     1  [ 2] .data             PROGBITS         0000000000000000  00000040       0000000000000000  0000000000000000  WA       0     0     1  [ 3] .bss              NOBITS           0000000000000000  00000040       0000000000000000  0000000000000000  WA       0     0     1  [ 4] .text.startup     PROGBITS         0000000000000000  00000040       000000000000000f  0000000000000000  AX       0     0     16  [ 5] .rela.text.s[...] RELA             0000000000000000  00000158       0000000000000018  0000000000000018   I      11     4     8  [ 6] .comment          PROGBITS         0000000000000000  0000004f       0000000000000013  0000000000000001  MS       0     0     1  [ 7] .note.GNU-stack   PROGBITS         0000000000000000  00000062       0000000000000000  0000000000000000           0     0     1  [ 8] .note.gnu.pr[...] NOTE             0000000000000000  00000068       0000000000000030  0000000000000000   A       0     0     8  [ 9] .eh_frame         PROGBITS         0000000000000000  00000098       0000000000000030  0000000000000000   A       0     0     8  [10] .rela.eh_frame    RELA             0000000000000000  00000170       0000000000000018  0000000000000018   I      11     9     8  [11] .symtab           SYMTAB           0000000000000000  000000c8       0000000000000078  0000000000000018          12     3     8  [12] .strtab           STRTAB           0000000000000000  00000140       0000000000000011  0000000000000000           0     0     1  [13] .shstrtab         STRTAB           0000000000000000  00000188       000000000000007a  0000000000000000           0     0     1\n\n| 节编号 | 节名                   | 类型       | 描述                                                           || --- | -------------------- | -------- | ------------------------------------------------------------ || 1   | `.text`              | PROGBITS | 空（Size = 0），可能没用上                                            || 2   | `.data`              | PROGBITS | 空，表示没有初始化的全局变量                                               || 3   | `.bss`               | NOBITS   | 空，表示没有未初始化的全局变量                                              || 4   | `.text.startup`      | PROGBITS | 真正的代码所在，包含 `main()` 函数，大小 0x0f（15 字节）                      || 5   | `.rela.text.startup` | RELA     | 重定位表，告诉链接器 `.text.startup` 中有符号（如 `puts()`）未解析，需要打补丁       || 6   | `.comment`           | PROGBITS | 编译器信息，如 `GCC: (Ubuntu 13.2.0...)`                            || 7   | `.note.GNU-stack`    | PROGBITS | 标记栈是否可执行（通常为空），现代系统中用于安全                                     || 8   | `.note.gnu.*`        | NOTE     | GNU 工具链信息，比如构建器版本                                            || 9   | `.eh_frame`          | PROGBITS | 栈展开信息（异常处理支持，main 里 `call` 会用到）                              || 10  | `.rela.eh_frame`     | RELA     | `.eh_frame` 的重定位表（地址待定）                                      || 11  | `.symtab`            | SYMTAB   | 符号表，记录了 `main` 函数、字符串 `.LC0`、外部符号 `puts` 等                 || 12  | `.strtab`            | STRTAB   | 字符串表，供 `.symtab` 使用，存储符号名字（如 `main`, `puts`, `.LC0`）       || 13  | `.shstrtab`          | STRTAB   | 节名字字符串表，供 section header 本身使用，存 `.text`, `.data`, `.bss` 等名字 |\n\n符号和符号表在可重定位目标文件中，每个模块都有符号表，符号分为三类：\n\n\n\n符号类型\n描述\n\n\n\n全局符号\n模块中定义，可以被其他模块引用（如非 static 的全局变量&#x2F;函数）\n\n\n外部符号\n模块中引用，但定义在其他模块中（如调用外部函数）\n\n\n局部符号\n只在当前模块中定义和引用（如 static 修饰的变量&#x2F;函数）\n\n\n局部程序变量（非 static 的局部变量）不会出现在符号表中，因为它们由运行时栈管理，不需要链接器关心。\n带 static 的局部变量 会被分配到 .bss 或 .data 段，并出现在符号表中，拥有唯一的内部名称（如 x.1, x.2）用于区分\n通过 readelf -s可以 看到的符号表条目\n举例：\n// main.cint add(int a, int b);int main() &#123; return add(2, 3); &#125;int f() &#123;  static int x = 0;  return x;&#125;\n\n对应的符号表如下：\nSymbol table &#x27;.symtab&#x27; contains 8 entries:   Num:    Value          Size Type    Bind   Vis      Ndx Name     0: 0000000000000000     0 NOTYPE  LOCAL  DEFAULT  UND      1: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS main.c     2: 0000000000000000     0 SECTION LOCAL  DEFAULT    1 .text     3: 0000000000000000     0 SECTION LOCAL  DEFAULT    4 .bss     4: 0000000000000000     4 OBJECT  LOCAL  DEFAULT    4 x.0     5: 0000000000000000    21 FUNC    GLOBAL DEFAULT    1 main     6: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  UND add     7: 0000000000000015    12 FUNC    GLOBAL DEFAULT    1 f\n\n注意：2\t0x0\t0\tSECTION\tLOCAL\tDEFAULT\t1\t.text 是符号表中的一个符号，类型为 SECTION。不是函数、变量等程序员写的“用户符号”，而是由汇编器自动为每个 section 添加的“段标识符”。\n上述段符号的用途主要是：\n\n供链接器使用，在链接阶段帮助识别每个节的位置；\n在汇编器内部和调试工具中用于定位节的起始地址。\n\n你可以把它当作是“代表一个 section 的名字和索引的特殊符号”，用于支持定位和重定位。\n哪些不会成为符号？非 static 的局部变量（比如 int y = 0; 在函数内部定义的）不会出现在符号表中，因为它们是栈上的临时变量，链接器不需要知道。\n","categories":["《深入理解计算机系统》"],"tags":["链接"]},{"title":"链接(二)","url":"/2025/06/25/%E9%93%BE%E6%8E%A5(%E4%BA%8C)/","content":"符号解析符号解析就是链接器将“引用的符号”与“定义的符号”进行匹配的过程。\n链接器如何解析多重定义的全局符号链接器处理多个 可重定位目标文件（.o 文件）。\n每个模块中可能会定义同名的全局符号（如变量、函数）。\n符号分为两类：\n\n强符号（strong）：函数、已初始化的全局变量（如 int x = 1;）\n弱符号（weak）：未初始化的全局变量（如 int x;）\n\n\n\n\n规则编号\n描述\n\n\n\n规则 1\n多个强符号同名 → 报错，链接失败（冲突无法解决）。\n\n\n规则 2\n强符号 vs 弱符号同名 → 选择强符号，忽略弱的。\n\n\n规则 3\n多个弱符号同名 → 任意选择一个（通常选择第一个出现的）。\n\n\n静态库静态库（static library）：是由多个可重定位目标文件（.o）打包成的一个 .a 文件，供链接器使用。\n链接器在使用静态库时：只提取被程序实际引用的目标文件，不会整个复制，从而节省磁盘和内存空间。\n优点：\n\n将函数各自编译为 .o 文件后，用工具打包成 .a 文件。\n编译程序时只需写出库文件名即可（如 -lm -lc），简洁方便。\n链接器会自动挑选需要的模块，避免冗余。\n.a 文件是一种存档格式，带有成员信息索引（如文件大小、位置等）。\n\n举例：\ngcc main.o -L. -lmylib -o myapp\n\n\n-L. 指定当前目录搜索库；\n-lmylib 会让链接器查找 libmylib.a（静态库）；\n链接器会从中挑出 main.o 需要用到的 .o 文件，做静态链接，生成 myapp。\n\n静态库与静态链接\n\n\n\n项目\n描述\n\n\n\n静态库\n存放多个 .o 文件的归档文件（.a）\n\n\n静态链接\n使用链接器把 .o 文件（含来自 .a 的）合并为一个最终的可执行文件\n\n\n二者关系\n静态链接 可能使用 静态库作为输入源之一\n\n\n链接器如何使用静态库来解析引用链接器符号解析的核心流程\n初始状态：\nE：已选中的目标文件集合（最终会链接成可执行文件）\nU：未解析的符号集合（只引用但还没找到定义）\nD：已定义的符号集合（已经找到定义的符号）\n初始时这三个集合是空的。\n\n\n处理目标文件（.o）：\n把 .o 文件加入到 E。\n收集该文件中定义的符号加到 D。\n把引用的、未定义的符号加到 U。\n\n\n处理静态库（.a）：\n对 .a 中的每个成员目标文件：\n如果它定义了 U 中某个符号，就把它加入到 E 中。\n然后更新 U 和 D。\n\n\n重复这个过程直到 U、D 不再变化。\n没有用到的库成员就丢弃（不会加入最终可执行程序中）。\n\n\n最终检查：\n如果还有未解析的符号（U 非空），就报错并终止。\n\n\n\n链接器是按命令行从左到右顺序处理的，这会导致以下情况\n举例：\ngcc -static ./libvector.a main2.c\n\n\n先处理 libvector.a，这时 U 是空（没有要解析的符号），所以库的任何成员都不会被加载；\n后处理 main2.c，其中调用了 addvec()，这会把 addvec 加入 U；\n但已经错过了加载库的机会，链接器不会回头重新看 libvector.a；\n所以 addvec 没找到 → 报错 undefined reference to &#39;addvec&#39;。\n\n如果 libx.a 和 liby.a 之间存在循环引用（互相调用），就需要重复指定，否则会有未解析符号\ngcc foo.o libx.a liby.a libx.a\n\n重定位重定位就是链接器将多个 .o 文件合并时，为每个代码和数据分配实际内存地址，并修正代码中对符号的引用地址，使它们能在程序运行时正确访问。\n重定位的两步骤：\n重定位节和符号定义（合并和分配地址）\n链接器会将多个 .o 文件中相同类型的节（比如 .data, .text）合并成一个大的节。\n然后：\n为这些合并后的节分配内存地址。\n为每个符号（函数、全局变量等）确定最终运行时地址。\n\n\n\n重定位节中的符号引用（修正引用地址）\n接下来，链接器会在代码和数据中找到所有对符号的引用（比如调用 printf 的指令）。\n使用 .o 文件中记录的**重定位条目（relocation entry）**来：\n替换或修改那些引用地址，让它们指向真正的地址。\n\n\n\n重定位条目什么是重定位条目：\n举例：\n当编译器&#x2F;汇编器生成一个目标文件（.o 文件）时，它还不知道变量或函数在内存里的最终地址比如：\nextern int x;void foo() &#123;    x = 42;&#125;\n\nx 是在别的文件里定义的，当前 .o 文件根本不知道 x 的地址。 所以，汇编器会留下一个“记号”告诉链接器：\n每一个重定位条目描述了：\n\n\n\n字段\n含义\n\n\n\noffset\n要修改的地方（代码或数据中的偏移）\n\n\nsymbol\n当前访问的是哪个符号（变量或函数）\n\n\ntype\n用哪种方式修改（是绝对地址还是相对地址）\n\n\naddend\n可选的补偿偏移（比如增加一个偏移量）\n\n\n这些信息一般存在 .rela.text（代码里的引用）或 .rela.data（数据段引用）段中。\n可执行目标文件：可执行文件是编译器和链接器将.o 文件转换成的最终程序二进制文件，操作系统可以直接把它加载到内存并运行\n可执行 ELF 文件的结构：\n+----------------+| ELF Header     | &lt;== 文件的元数据，如魔数、架构类型、入口点等+----------------+| 程序头表       | &lt;== OS 用来加载程序的结构（段的偏移、权限等）+----------------+| .init          | &lt;== 程序启动初始化函数（调用 _init）| .text          | &lt;== 代码段（程序指令）| .rodata        | &lt;== 只读数据（如字符串常量）| .data          | &lt;== 已初始化的全局变量| .bss           | &lt;== 未初始化的全局变量（运行时置 0）| .symtab        | &lt;== 符号表（调试信息用）| .debug         | &lt;== 调试信息| .line          | &lt;== 源码行号表| .strtab        | &lt;== 字符串表+----------------+| 节头部表       | &lt;== 每个节的信息（调试器用）+----------------+\n\n可执行目标文件的结构与可重定位目标文件类似，整体格式由 ELF 头描述。其中包含了程序的入口地址（entry point），即程序运行时将执行的第一条指令的位置。\n其中的 .text、.rodata 和 .data 等节与可重定位文件中的同名节基本相同，只是它们已经被重定位到程序运行时的实际内存地址。\n此外，可执行文件中的 .init 节中定义了一个初始化函数 _init，程序启动时会自动调用它。由于可执行文件已经是完全链接好的，不再需要用于重定位的 .rel 节。\n程序头部表（Program Header Table） 是 ELF（Executable and Linkable Format）可执行文件中的一个重要结构，用于指导操作系统的程序加载器如何将可执行文件映射到内存中运行。它和节头部表（Section Header Table）不同，节头部表更多是给链接器用的，而程序头部表是给操作系统加载程序用的。\n动态库动态库是一种编译好但不嵌入可执行程序本体的代码库，它提供了函数和变量，供多个程序在运行时动态加载并使用\n优点：\n磁盘空间共享：一个 .so 文件可以被多个程序共用，不像静态库那样每个程序都复制一份。\n内存空间共享：多个进程可以共享 .so 的 .text（代码段），减少了内存占用。\n动态链接过程\n编译器使用 -fPIC（生成位置无关代码）和 -shared 生成共享库文件：\n\ngcc -shared -fPIC -o libvector.so addvec.c multvec.c\n\n\n编译主程序并链接共享库\n\ngcc -o prog21 main2.c ./libvector.so\n\n\n运行时加载流程\n\n\n加载器（loader）启动 prog21。\n它发现程序有一个 .interp 段，里面指定了动态链接器（如 /lib64/ld-linux-x86-64.so.2）。\n动态链接器负责：\n加载 libvector.so 和 libc.so\n重定位 prog21 中对 printf、addvec 等函数的调用地址\n\n\n动态链接完成后，跳转到主程序入口地址开始执行\n\n位置无关代码背景\n共享库（.so 文件）常常被多个进程同时加载使用，为了节省内存，系统希望这些进程可以共享这段库的代码段（text segment）。\n但是每个进程的虚拟地址空间不同，无法保证这个共享库总是被加载到相同的地址。\n解决方法：使用位置无关代码（PIC）\nPIC 为了解决“全局变量地址不确定”的问题，引入了 GOT 表。可以把 GOT 理解成一个中间跳板表：\n程序访问变量不是直接访问变量地址，而是：\n\n先从 GOT 表中找到变量的地址\n然后跳转或读写这个地址\n\n注意：“GOT 表属于动态库本身，位于动态库的 .data 段的开始部分”。\n每个进程加载 .so 时，都会为 .so 分配一段自己的私有地址空间副本（包括 GOT 表）。所以：\n\nGOT 表内容每个进程是独立的\n写进去的地址是针对这个进程自己的虚拟地址\n\n库拦截通过替换或重定义动态库中的函数，以拦截或修改函数行为。\n运行时拦截：\n在程序启动时，通过设置环境变量（如 LD_PRELOAD），优先加载你自己的动态库，从而覆盖掉原本的库函数。\n 原理：\n动态链接器（如 Linux 上的 ld-linux.so）在程序启动时会加载所有依赖的 .so 文件，它会按照顺序查找函数符号。 如果你用 LD_PRELOAD 指定了一个动态库，这个库里的符号会优先被解析，就能**“拦截”掉原始函数**了。\n举例：\n\n写一个库，重定义 malloc\n\n// mymalloc.c#define _GNU_SOURCE#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;dlfcn.h&gt;void* malloc(size_t size) &#123;    void* (*real_malloc)(size_t) = dlsym(RTLD_NEXT, &quot;malloc&quot;);  // 调用原始 malloc    printf(&quot;malloc(%zu) called\\n&quot;, size);    return real_malloc(size);&#125;\n\n\n编译成共享库\n\ngcc -shared -fPIC -o libmymalloc.so mymalloc.c -ldl\n\n\n运行目标程序时使用 LD_PRELOAD\n\nLD_PRELOAD=./libmymalloc.so ./your_program\n\n","categories":["《深入理解计算机系统》"],"tags":["链接"]},{"title":"TCP 零窗口探测定时器","url":"/2025/08/11/%E9%9B%B6%E7%AA%97%E5%8F%A3%E6%8E%A2%E6%B5%8B%E5%AE%9A%E6%97%B6%E5%99%A8/","content":"零窗口探测定时器TCP 零窗口探测定时器是在对端通告窗口为 0 时启动的，用来周期性发送探测包，检查对端窗口是否重新打开。 定时器到期时会发一个零长度或过时序号的 ACK 段，逼对端回 ACK，从而获知最新窗口大小。\n零窗口探测定时器和超时重传使用一个定时器，根据不同的pending来区分具体走哪个逻辑，具体如下所示：\nvoid tcp_write_timer_handler(struct sock *sk)&#123;\t...\ttcp_mstamp_refresh(tcp_sk(sk));\tevent = icsk-&gt;icsk_pending;\tswitch (event) &#123;\tcase ICSK_TIME_RETRANS:\t\ticsk-&gt;icsk_pending = 0;\t\ttcp_retransmit_timer(sk);\t\tbreak;\tcase ICSK_TIME_PROBE0:\t\ticsk-&gt;icsk_pending = 0;\t\t//零窗口探测\t\ttcp_probe_timer(sk);\t\tbreak;\t&#125;&#125;\n\ntcp_probe_timer 为零窗口探测到期的处理函数，主要逻辑为首先怕判断发送队列是否为空，或者没有未确认的数据包，这时候根本也不需要零窗口探测，因此直接返回，然后获取最大的探测次数，这里用的是retry2 默认 15次，如果超过次数了，直接err 否则的话调用\ntcp_send_probe0发送零窗口探测报文。\nstatic void tcp_probe_timer(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct sk_buff *skb = tcp_send_head(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tint max_probes;\t//如果发送队列为空，或者没有在途中的数据包，直接返回\tif (tp-&gt;packets_out || !skb) &#123;\t\ticsk-&gt;icsk_probes_out = 0;\t\ticsk-&gt;icsk_probes_tstamp = 0;\t\treturn;\t&#125;\t/* RFC 1122 4.2.2.17 requires the sender to stay open indefinitely as\t * long as the receiver continues to respond probes. We support this by\t * default and reset icsk_probes_out with incoming ACKs. But if the\t * socket is orphaned or the user specifies TCP_USER_TIMEOUT, we\t * kill the socket when the retry count and the time exceeds the\t * corresponding system limit. We also implement similar policy when\t * we use RTO to probe window in tcp_retransmit_timer().\t */\tif (!icsk-&gt;icsk_probes_tstamp) &#123;\t\ticsk-&gt;icsk_probes_tstamp = tcp_jiffies32; //第一次探测的时间戳\t&#125; else &#123;\t\tu32 user_timeout = READ_ONCE(icsk-&gt;icsk_user_timeout); //set_sock_opt 用户设置的超时时间\t\t//用户设置的超时时间大于当前时间减去第一个0窗口探测的时间，直接返回err 因为超时了\t\tif (user_timeout &amp;&amp;\t\t    (s32)(tcp_jiffies32 - icsk-&gt;icsk_probes_tstamp) &gt;=\t\t     msecs_to_jiffies(user_timeout))\t\tgoto abort;\t&#125;\tmax_probes = READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_retries2);\t//在零窗口探测中走这个分支的概率很小吧！先忽略\tif (sock_flag(sk, SOCK_DEAD)) &#123;\t\t//判断是否存活，为啥要用inet_csk_rto_backoff 这个呢？\t\tconst bool alive = inet_csk_rto_backoff(icsk, TCP_RTO_MAX) &lt; TCP_RTO_MAX;\t\t//更具是否存活返回一个最大探测次数 如果已经死了， 则大概率返0\t\tmax_probes = tcp_orphan_retries(sk, alive);\t\tif (!alive &amp;&amp; icsk-&gt;icsk_backoff &gt;= max_probes)\t\t\tgoto abort; //直接返回err\t\t//检查是否系统资源不足\t\tif (tcp_out_of_resources(sk, true))\t\t\treturn;\t&#125;\t//探测的次数超过retry2了 直接返回err\tif (icsk-&gt;icsk_probes_out &gt;= max_probes) &#123;abort:\t\ttcp_write_err(sk);\t&#125; else &#123;\t\t/* Only send another probe if we didn&#x27;t close things up. */\t\t//发送零窗口探测报文\t\ttcp_send_probe0(sk);\t&#125;&#125;\n\n上述逻辑最终调用tcp_send_probe0进一步处理零窗口探测，在tcp_send_probe0中，会调用tcp_write_wakeup发送实际的数据包，发送之后，会对重传次数，下一次的超时时间进行重新计算，并设置下一次零窗口探测的时间，具体代码逻辑如下：\n/* A window probe timeout has occurred.  If window is not closed send * a partial packet else a zero probe. */void tcp_send_probe0(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct net *net = sock_net(sk);\tunsigned long timeout;\tint err;\t//发送0窗口探测报文\terr = tcp_write_wakeup(sk, LINUX_MIB_TCPWINPROBE);\t//发送出去的数据包的未确认数量为0,且发送队列为空，认为不需要探测了\tif (tp-&gt;packets_out || tcp_write_queue_empty(sk)) &#123;\t\t/* Cancel probe timer, if it is not required. */\t\ticsk-&gt;icsk_probes_out = 0;\t\ticsk-&gt;icsk_backoff = 0;\t\ticsk-&gt;icsk_probes_tstamp = 0;//复位第一次的时间戳\t\treturn;//终止探测了！\t&#125;\t//增加0窗口探测次数\ticsk-&gt;icsk_probes_out++;\t//如果发送失败了\tif (err &lt;= 0) &#123;\t\tif (icsk-&gt;icsk_backoff &lt; READ_ONCE(net-&gt;ipv4.sysctl_tcp_retries2))//小于retry2\t\t\ticsk-&gt;icsk_backoff++; //增加退避次数\t\t//根据上面的退避次数计算一个超时时间\t\ttimeout = tcp_probe0_when(sk, TCP_RTO_MAX);\t&#125; else &#123;\t\t/* If packet was not sent due to local congestion,\t\t * Let senders fight for local resources conservatively.\t\t */\t\t//发送成功了，设置正常的间隔\t\ttimeout = TCP_RESOURCE_PROBE_INTERVAL;\t&#125;\ttimeout = tcp_clamp_probe0_to_user_timeout(sk, timeout);\t//设置下一次零窗口探测时间\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0, timeout, TCP_RTO_MAX);&#125;\n\ntcp_write_wakeup是实际调用发送数据包的函数，这里分为两种情况，第一种情况：发送队列数据包不为空，且序列号在右边界内，则首先尝试从发送队列中peek一个skb，然后根据mss等大小对数据包进行处理后直接发送。\n第二种情况，大概率走这个分支（因为都已经零窗口了那数据包的序列号几乎不会在右边界范围内吧），则直接发送一个对端已经确认的序列号-1的纯ack报文\n/* Initiate keepalive or window probe from timer. *///注意`tcp_write_wakeup`在保活或者零窗口探测中都会被调用int tcp_write_wakeup(struct sock *sk, int mib)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb;\tif (sk-&gt;sk_state == TCP_CLOSE)\t\treturn -1;\t//取出一个skb\tskb = tcp_send_head(sk);\t//数据包非空，且序列号落在右边界内\tif (skb &amp;&amp; before(TCP_SKB_CB(skb)-&gt;seq, tcp_wnd_end(tp))) &#123;\t\tint err;\t\t//拿到mss\t\t\t\tunsigned int mss = tcp_current_mss(sk);\t\t//计算可以用多少字节\t\tunsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)-&gt;seq;\t\t//下一个要push的序列号如果在endseq的前面\t\tif (before(tp-&gt;pushed_seq, TCP_SKB_CB(skb)-&gt;end_seq))\t\t\ttp-&gt;pushed_seq = TCP_SKB_CB(skb)-&gt;end_seq; //更新push seq，表示这学数据要尽快交付\t\t/* We are probing the opening of a window\t\t * but the window size is != 0\t\t * must have been a result SWS avoidance ( sender )\t\t */\t\t//如果可用的空间不够了，或者skb的len大于mss,就需要分段了\t\tif (seg_size &lt; TCP_SKB_CB(skb)-&gt;end_seq - TCP_SKB_CB(skb)-&gt;seq ||\t\t    skb-&gt;len &gt; mss) &#123;\t\t\tseg_size = min(seg_size, mss);\t\t\tTCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_PSH;\t\t\tif (tcp_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\t\t\t\t\t skb, seg_size, mss, GFP_ATOMIC))\t\t\t\treturn -1;\t\t&#125; else if (!tcp_skb_pcount(skb))//如果没有设置这个skb的segs数量\t\t\t//设置segs的数量\t\t\t\t\t\ttcp_set_skb_tso_segs(skb, mss);\t\t//设置push的标志位\t\tTCP_SKB_CB(skb)-&gt;tcp_flags |= TCPHDR_PSH;\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\t\tif (!err)\t\t\ttcp_event_new_data_sent(sk, skb);\t\treturn err;\t&#125; else &#123;\t\t//大概率应该走这个分支？\t\tif (between(tp-&gt;snd_up, tp-&gt;snd_una + 1, tp-&gt;snd_una + 0xFFFF))\t\t\ttcp_xmit_probe_skb(sk, 1, mib);   //有紧急数据的情况，发送数据包的序列号为snd_una\t\treturn tcp_xmit_probe_skb(sk, 0, mib);//发送数据包的序列号为 una - 1 真正的探测包，数据是无法交付给应用层的\t&#125;&#125;\n\n注意tcp_xmit_probe_skb(sk, 0, mib)中第二参数为0，则tcp_xmit_probe_skb在计算序列号的时候会将数据包的序列号设置为 una-1，防止对方应用层收到的同时还可以逼迫对方回复ack。\nstatic int tcp_xmit_probe_skb(struct sock *sk, int urgent, int mib)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct sk_buff *skb;\t/* We don&#x27;t queue it, tcp_transmit_skb() sets ownership. */\tskb = alloc_skb(MAX_TCP_HEADER,\t\t\tsk_gfp_mask(sk, GFP_ATOMIC | __GFP_NOWARN));\tif (!skb)\t\treturn -1;\t/* Reserve space for headers and set control bits. */\t\t//调整data和tail位置\tskb_reserve(skb, MAX_TCP_HEADER);\t/* Use a previous sequence.  This should cause the other\t * end to send an ack.  Don&#x27;t queue or clone SKB, just\t * send it.\t */\t//这里会根据传进来的参数设置序号\ttcp_init_nondata_skb(skb, tp-&gt;snd_una - !urgent, TCPHDR_ACK);\tNET_INC_STATS(sock_net(sk), mib);\treturn tcp_transmit_skb(sk, skb, 0, (__force gfp_t)0);&#125;\n\n零窗口探测定时器的激活__tcp_push_pending_frames中，如果发包之后如果未确认的包数为0，同时发送队列不为空会调用tcp_check_probe_timer启动零窗口探测定时器\nvoid __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,\t\t\t       int nonagle)&#123;\t/* If we are closed, the bytes will have to remain here.\t * In time closedown will finish, we empty the write queue and\t * all will be happy.\t */\tif (unlikely(sk-&gt;sk_state == TCP_CLOSE))\t\t\t\treturn;\t//返回值：如果未确认的包数为0，同时发送队列不为空（这叫做右数据要发但是没有数据在途中，所以符合场景，合理）\tif (tcp_write_xmit(sk, cur_mss, nonagle, 0,\t\t\t   sk_gfp_mask(sk, GFP_ATOMIC)))\t\ttcp_check_probe_timer(sk);&#125;static inline void tcp_check_probe_timer(struct sock *sk)&#123;\tif (!tcp_sk(sk)-&gt;packets_out &amp;&amp; !inet_csk(sk)-&gt;icsk_pending)\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0,\t\t\t\t     tcp_probe0_base(sk), TCP_RTO_MAX);&#125;\n\ntcp_ack_probe中也会启动零窗口探测定时器，tcp_ack_probe在tcp_ack中被调用\nstatic void tcp_ack_probe(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct sk_buff *head = tcp_send_head(sk);\tconst struct tcp_sock *tp = tcp_sk(sk);\t/* Was it a usable window open? */\tif (!head)\t\treturn;\tif (!after(TCP_SKB_CB(head)-&gt;end_seq, tcp_wnd_end(tp))) &#123;\t\ticsk-&gt;icsk_backoff = 0;\t\ticsk-&gt;icsk_probes_tstamp = 0;\t\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_PROBE0);\t\t/* Socket must be waked up by subsequent tcp_data_snd_check().\t\t * This function is not for random using!\t\t */\t&#125; else &#123;\t\t//窗口不够用了\t\tunsigned long when = tcp_probe0_when(sk, TCP_RTO_MAX);\t\twhen = tcp_clamp_probe0_to_user_timeout(sk, when);\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0, when, TCP_RTO_MAX);\t&#125;&#125;/* This routine deals with incoming acks, but not outgoing ones. */static int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)&#123;\t...no_queue:\ttcp_ack_probe(sk);\t...&#125;","categories":["网络协议栈源码学习"],"tags":["TCP定时器","TCP"]},{"title":"TCP输入 快速路径（一）","url":"/2026/01/03/TCPtcp_rcv_established%E8%BE%93%E5%85%A5/","content":"前文说到tcp_v4_rcv为tcp报文接收的总入口。本篇文章则进一步介绍建立连接状态下的\tTCP报文接收的处理函数tcp_rcv_established。\ntcp_rcv_established() 负责处理 已建立连接 上收到的每一个 TCP报文，并分为fast path和 slow path两种情况进行处理，核心逻辑是完成以下工作：\n\n校验（校验和、PAWS、seq 是否可接受、RST&#x2F;SYN 防注入）\n处理 ACK（推进 snd_una，清理重传队列，拥塞控制&#x2F;速率采样等）\n处理数据（按序入队、乱序队列、合并 skb、唤醒用户态）\n决定何时发 ACK（立即&#x2F;延迟&#x2F;压缩 ACK）\n\ntcp_rcv_established具体代码如下所示：\nvoid tcp_rcv_established(struct sock *sk, struct sk_buff *skb)&#123;\tenum skb_drop_reason reason = SKB_DROP_REASON_NOT_SPECIFIED;\tconst struct tcphdr *th = (const struct tcphdr *)skb-&gt;data;\tstruct tcp_sock *tp = tcp_sk(sk);\tunsigned int len = skb-&gt;len;\t/* TCP congestion window tracking */\ttrace_tcp_probe(sk, skb);\t//更新时间戳，后面ack会用到吧\ttcp_mstamp_refresh(tp);\t//检查以下是否有关联的dst\tif (unlikely(!rcu_access_pointer(sk-&gt;sk_rx_dst)))\t\tinet_csk(sk)-&gt;icsk_af_ops-&gt;sk_rx_dst_set(sk, skb);\t/*\t *\tHeader prediction.\t *\tThe code loosely follows the one in the famous\t *\t&quot;30 instruction TCP receive&quot; Van Jacobson mail.\t *\t *\tVan&#x27;s trick is to deposit buffers into socket queue\t *\ton a device interrupt, to call tcp_recv function\t *\ton the receive process context and checksum and copy\t *\tthe buffer to user space. smart...\t *\t *\tOur current scheme is not silly either but we take the\t *\textra cost of the net_bh soft interrupt processing...\t *\tWe do checksum and copy also but from device to kernel.\t */\t//还没有时间戳选项\ttp-&gt;rx_opt.saw_tstamp = 0;\t/*\tpred_flags is 0xS?10 &lt;&lt; 16 + snd_wnd\t *\tif header_prediction is to be made\t *\t&#x27;S&#x27; will always be tp-&gt;tcp_header_len &gt;&gt; 2\t *\t&#x27;?&#x27; will be 0 for the fast path, otherwise pred_flags is 0 to\t *  turn it off\t(when there are holes in the receive\t *\t space for instance)\t *\tPSH flag is ignored.\t */\t//命中首部预测（头部长度不变吧，窗口不为零），序列号正好是下一个待接收的，不能确认到还没发送的数据！\tif ((tcp_flag_word(th) &amp; TCP_HP_BITS) == tp-&gt;pred_flags &amp;&amp;\t    TCP_SKB_CB(skb)-&gt;seq == tp-&gt;rcv_nxt &amp;&amp;\t    !after(TCP_SKB_CB(skb)-&gt;ack_seq, tp-&gt;snd_nxt)) &#123;\t\tint tcp_header_len = tp-&gt;tcp_header_len; //获取头部长度\t\t/* Timestamp header prediction: tcp_header_len\t\t * is automatically equal to th-&gt;doff*4 due to pred_flags\t\t * match.\t\t */\t\t/* Check timestamp */\t\t//只有一个时间戳选项\t\tif (tcp_header_len == sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &#123;\t\t\t/* No? Slow path! */\t\t\t//提取时间戳选项，到两个字段中\t\t\tif (!tcp_parse_aligned_timestamp(tp, th))\t\t\t\tgoto slow_path;\t\t\t/* If PAWS failed, check it more carefully in slow path */\t\t\t//paws 判断时间戳是否倒退？，新的时间戳比我们之前记录要小\t\t\tif ((s32)(tp-&gt;rx_opt.rcv_tsval - tp-&gt;rx_opt.ts_recent) &lt; 0)\t\t\t\tgoto slow_path;\t\t\t/* DO NOT update ts_recent here, if checksum fails\t\t\t * and timestamp was corrupted part, it will result\t\t\t * in a hung connection since we will drop all\t\t\t * future packets due to the PAWS test.\t\t\t */\t\t&#125;\t\t//没有负载的情况\t\tif (len &lt;= tcp_header_len) &#123;\t\t\t/* Bulk data transfer: sender */\t\t\t//纯ack包\t\t\tif (len == tcp_header_len) &#123;\t\t\t\t/* Predicted packet is in window by definition.\t\t\t\t * seq == rcv_nxt and rcv_wup &lt;= rcv_nxt.\t\t\t\t * Hence, check seq&lt;=rcv_wup reduces to:\t\t\t\t */\t\t\t\tif (tcp_header_len ==\t\t\t\t    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &amp;&amp;\t\t\t\t    tp-&gt;rcv_nxt == tp-&gt;rcv_wup)\t\t\t\t\t//这里是数据包有时间戳选项，把时间戳保存到tp字段中，注意和上面进行区分！\t\t\t\t\ttcp_store_ts_recent(tp);\t\t\t\t/* We know that such packets are checksummed\t\t\t\t * on entry.\t\t\t\t */\t\t\t\t//对ack进行处理\t\t\t\ttcp_ack(sk, skb, 0);\t\t\t\t__kfree_skb(skb);\t\t\t\t//检查是否有需要发送的数据\t\t\t\ttcp_data_snd_check(sk);\t\t\t\t/* When receiving pure ack in fast path, update\t\t\t\t * last ts ecr directly instead of calling\t\t\t\t * tcp_rcv_rtt_measure_ts()\t\t\t\t */\t\t\t\ttp-&gt;rcv_rtt_last_tsecr = tp-&gt;rx_opt.rcv_tsecr;\t\t\t\treturn;\t\t\t//非法，直接丢弃\t\t\t&#125; else &#123; /* Header too small */\t\t\t\treason = SKB_DROP_REASON_PKT_TOO_SMALL;\t\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\t\t\t\tgoto discard;\t\t\t&#125;\t\t//有负载的情况\t\t&#125; else &#123;\t\t\tint eaten = 0;\t\t\tbool fragstolen = false;\t\t\t//计算校验和\t\t\tif (tcp_checksum_complete(skb))\t\t\t\tgoto csum_error;\t\t\t//内存压力检查，这里几乎不可能把\t\t\tif ((int)skb-&gt;truesize &gt; sk-&gt;sk_forward_alloc)\t\t\t\tgoto step5;\t\t\t/* Predicted packet is in window by definition.\t\t\t * seq == rcv_nxt and rcv_wup &lt;= rcv_nxt.\t\t\t * Hence, check seq&lt;=rcv_wup reduces to:\t\t\t */\t\t\t//和上面一样跟新ts_recent\t\t\tif (tcp_header_len ==\t\t\t    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &amp;&amp;\t\t\t    tp-&gt;rcv_nxt == tp-&gt;rcv_wup)\t\t\t\ttcp_store_ts_recent(tp);\t\t\t//这里使用时间戳回显计算rtt,和tcpack里面计算的rtt的区别是什么？用途不一样？\t\t\ttcp_rcv_rtt_measure_ts(sk, skb);\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPHPHITS);\t\t\t/* Bulk data transfer: receiver */\t\t\tskb_dst_drop(skb);\t\t\t__skb_pull(skb, tcp_header_len);\t\t\t//数据包入队，这里更新了rcv_nxt\t\t\teaten = tcp_queue_rcv(sk, skb, &amp;fragstolen);\t\t\t//估计发送端mss 接收端rtt，调整接收慢启动阈值！！！\t\t\ttcp_event_data_recv(sk, skb);\t\t\t//处理ack\t\t\tif (TCP_SKB_CB(skb)-&gt;ack_seq != tp-&gt;snd_una) &#123;\t\t\t\t/* Well, only one small jumplet in fast path... */\t\t\t\ttcp_ack(sk, skb, FLAG_DATA);\t\t\t\t//是否需要发送数据\t\t\t\ttcp_data_snd_check(sk);\t\t\t\t//判断是否需要发送ack\t\t\t\tif (!inet_csk_ack_scheduled(sk))\t\t\t\t\tgoto no_ack;\t\t\t&#125; else &#123;\t\t\t//处理ack没有推进的情况\t\t\t\ttcp_update_wl(tp, TCP_SKB_CB(skb)-&gt;seq);\t\t\t&#125;\t\t\t//决定是上面时候发送ack\t\t\t__tcp_ack_snd_check(sk, 0);no_ack:\t\t\tif (eaten)\t\t\t\tkfree_skb_partial(skb, fragstolen);\t\t\t//唤醒用户态的read/recv\t\t\ttcp_data_ready(sk);\t\t\treturn;\t\t&#125;\t&#125;//快速路径slow_path:\t//计算校验和\tif (len &lt; (th-&gt;doff &lt;&lt; 2) || tcp_checksum_complete(skb))\t\tgoto csum_error;\t//合法性检查\tif (!th-&gt;ack &amp;&amp; !th-&gt;rst &amp;&amp; !th-&gt;syn) &#123;\t\treason = SKB_DROP_REASON_TCP_FLAGS;\t\tgoto discard;\t&#125;\t/*\t *\tStandard slow path.\t */\tif (!tcp_validate_incoming(sk, skb, th, 1))\t\treturn;step5:\treason = tcp_ack(sk, skb, FLAG_SLOWPATH | FLAG_UPDATE_TS_RECENT);\tif ((int)reason &lt; 0) &#123;\t\treason = -reason;\t\tgoto discard;\t&#125;\t//处理接收端rtt\ttcp_rcv_rtt_measure_ts(sk, skb);\t/* Process urgent data. */\t//处理紧急数据\ttcp_urg(sk, skb, th);\t/* step 7: process the segment text */\ttcp_data_queue(sk, skb);\t//检查是否有待发送的数据\ttcp_data_snd_check(sk);\t//决定什么时候发送ack\ttcp_ack_snd_check(sk);\treturn;csum_error:\treason = SKB_DROP_REASON_TCP_CSUM;\ttrace_tcp_bad_csum(skb);\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);discard:\ttcp_drop_reason(sk, skb, reason);&#125;\n\ntcp_rcv_established主要分为快速路径和慢速路径进行处理，快速路径下数据包会直接入队，同时唤醒用户进程（网上说百分之90都会走快速路径），首先介绍快速路径的处理逻辑：\n进入快速路径的前提是满足首部预测条件，在三次握手和tcp_ack的处理逻辑中会设置首部预测标记位，当数据包到来时，如果命中了首部预测，且数据包的序列号正是接收端所预期的，同时确认号合法，则进入快速路径的处理流程，这里注意，即使进入到快速路径的处理流程中，也可能会fall back到慢速路径中处理。\n快速路径中首先解析数据包的时间戳选项，之后判断是否能通过PAWS机制（这里就是判断时间戳是否合法），之后快速路径中针对数据包是否有负载进行处理。\n如果不存在负载，则会尝试提取记录时间戳选项，调用tcp_ack进行处理(前面有分析过)之后调用tcp_data_snd_check判断发送路径上是否有数据需要发送,，然后直接返回了！\n如果存在负载，则首先计算校验和，之后提取时间戳选项，并调用tcp_rcv_rtt_measure_ts计算接受侧rtt(注意区分这个和传统rtt的区别这个貌似时用来计算延迟ack用到的)，tcp_rcv_rtt_measure_ts代码如下所示：\nstatic inline void tcp_rcv_rtt_measure_ts(struct sock *sk,\t\t\t\t\t  const struct sk_buff *skb)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//同一个样本直接返回\tif (tp-&gt;rx_opt.rcv_tsecr == tp-&gt;rcv_rtt_last_tsecr)\t\treturn;\ttp-&gt;rcv_rtt_last_tsecr = tp-&gt;rx_opt.rcv_tsecr;\t//前提条件是两个数据包之间要差一个mss，过滤噪声？？？\tif (TCP_SKB_CB(skb)-&gt;end_seq -\t    TCP_SKB_CB(skb)-&gt;seq &gt;= inet_csk(sk)-&gt;icsk_ack.rcv_mss) &#123;\t\t//这个就是rtt\t\tu32 delta = tcp_time_stamp(tp) - tp-&gt;rx_opt.rcv_tsecr;\t\tu32 delta_us;\t\t//这个值是否合法\t\tif (likely(delta &lt; INT_MAX / (USEC_PER_SEC / TCP_TS_HZ))) &#123;\t\t\tif (!delta)\t\t\t\tdelta = 1;\t\t\tdelta_us = delta * (USEC_PER_SEC / TCP_TS_HZ);\t\t\t//这里第二参数是0貌似就是表示是基于时间戳计算出来的rtt\t\t\ttcp_rcv_rtt_update(tp, delta_us, 0);\t\t&#125;\t&#125;&#125;\n\ntcp_rcv_rtt_measure_ts中首先判断样本的合法性，如果是同一个样本直接返回，否则判断两个数据包确认的数据是否超过的估计的mss（认为超过mss才有意义用于计算rtt），之后根据当前时间减去回显的时间戳，得到rtt。然后调用tcp_rcv_rtt_update更新接收侧rtt（注意第三个参数为0表示用时间戳选项计算接收端rtt）。\n上述具体代码如下所示：\nstatic void tcp_rcv_rtt_update(struct tcp_sock *tp, u32 sample, int win_dep)&#123;\tu32 new_sample = tp-&gt;rcv_rtt_est.rtt_us;\tlong m = sample;\t//是否是第一次采样，通过肯定不是\tif (new_sample != 0) &#123;\t\t/* If we sample in larger samples in the non-timestamp\t\t * case, we could grossly overestimate the RTT especially\t\t * with chatty applications or bulk transfer apps which\t\t * are stalled on filesystem I/O.\t\t *\t\t * Also, since we are only going for a minimum in the\t\t * non-timestamp case, we do not smooth things out\t\t * else with timestamps disabled convergence takes too\t\t * long.\t\t */\t\t//这个表示有时间戳选项，因为第三个参数是0 ，新的 RTT = 87.5% 旧值 + 12.5% 新样本\t\tif (!win_dep) &#123;\t\t\tm -= (new_sample &gt;&gt; 3);\t\t\tnew_sample += m;\t\t&#125; else &#123;\t\t//没有时间戳选项，新的 RTT = min(旧 RTT, 新样本)\t\t\tm &lt;&lt;= 3;\t\t\tif (m &lt; new_sample)\t\t\t\tnew_sample = m;\t\t&#125;\t&#125; else &#123;\t\t/* No previous measure. */\t\t//第一次采样，直接把传输的参数作为rtt\t\tnew_sample = m &lt;&lt; 3;\t&#125;\ttp-&gt;rcv_rtt_est.rtt_us = new_sample;&#125;\n\n可以看到上述最终的rtt保存到了rcv_rtt_est.rtt_us中，在后续计算延迟ack的时间会使用到\n回到快速路径的处理逻辑中，计算完接收侧rtt后会调用tcp_queue_rcv将数据包放入接收队列，并更新rcv_nxt，具体代码如下所示：\nstatic int __must_check tcp_queue_rcv(struct sock *sk, struct sk_buff *skb,\t\t\t\t      bool *fragstolen)&#123;\tint eaten;\t//获取尾部的skb\tstruct sk_buff *tail = skb_peek_tail(&amp;sk-&gt;sk_receive_queue);\t//尝试把当前skb放到taii中\teaten = (tail &amp;&amp;\t\t tcp_try_coalesce(sk, tail,\t\t\t\t  skb, fragstolen)) ? 1 : 0;\t//这里肯定是要更新rcv_nxt的，合理\ttcp_rcv_nxt_update(tcp_sk(sk), TCP_SKB_CB(skb)-&gt;end_seq);\t//合并失败，直接入队\tif (!eaten) &#123;\t\t__skb_queue_tail(&amp;sk-&gt;sk_receive_queue, skb);\t\tskb_set_owner_r(skb, sk);\t&#125;\treturn eaten;&#125;\n\n数据包完成入队之后，会调用tcp_event_data_recv完成对接收rtt，发送方mss，以及延迟ack的超时时间的估算。具体代码如下所示：\nstatic void tcp_event_data_recv(struct sock *sk, struct sk_buff *skb)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tu32 now;\t//设置需要回ack标志位\tinet_csk_schedule_ack(sk);\t//接收端估计对端的mss\ttcp_measure_rcv_mss(sk, skb);\t//计算接收rtt\ttcp_rcv_rtt_measure(tp);\tnow = tcp_jiffies32;\t//还没有初始化延迟ack\tif (!icsk-&gt;icsk_ack.ato) &#123;\t\t/* The _first_ data packet received, initialize\t\t * delayed ACK engine.\t\t */\t\ttcp_incr_quickack(sk, TCP_MAX_QUICKACKS);\t\ticsk-&gt;icsk_ack.ato = TCP_ATO_MIN; //40ms\t&#125; else &#123;\t\t//当前时间减去最后一个包收上来的时间\t\tint m = now - icsk-&gt;icsk_ack.lrcvtime;\t\t//间隔很快\t\tif (m &lt;= TCP_ATO_MIN / 2) &#123;\t\t\t/* The fastest case is the first. */\t\t\t//ato变小\t\t\ticsk-&gt;icsk_ack.ato = (icsk-&gt;icsk_ack.ato &gt;&gt; 1) + TCP_ATO_MIN / 2;\t\t&#125; else if (m &lt; icsk-&gt;icsk_ack.ato) &#123; //比ato还小，表示ack还是慢了\t\t\t//继续缩小\t\t\ticsk-&gt;icsk_ack.ato = (icsk-&gt;icsk_ack.ato &gt;&gt; 1) + m;\t\t\t//不能比rto大\t\t\tif (icsk-&gt;icsk_ack.ato &gt; icsk-&gt;icsk_rto)\t\t\t\ticsk-&gt;icsk_ack.ato = icsk-&gt;icsk_rto;\t\t&#125; else if (m &gt; icsk-&gt;icsk_rto) &#123; //太久没收到包了，重置\t\t\t/* Too long gap. Apparently sender failed to\t\t\t * restart window, so that we send ACKs quickly.\t\t\t */\t\t\ttcp_incr_quickack(sk, TCP_MAX_QUICKACKS);\t\t&#125;\t&#125;\ticsk-&gt;icsk_ack.lrcvtime = now;\t//ecn处理\ttcp_ecn_check_ce(sk, skb);\t//触发窗口的自动增长\tif (skb-&gt;len &gt;= 128)\t\ttcp_grow_window(sk, skb, true);&#125;\n\ntcp_event_data_recv的作用是收到数据包调用inet_csk_schedule_ack设置需要设置ack的标志位，调用tcp_measure_rcv_mss估计对端的mss调用tcp_rcv_rtt_measure估算接收端的rtt，以及调用tcp_grow_window自动调节接收窗口大小。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输入"]},{"title":"TCP输入 快速路径（二）","url":"/2026/01/05/TCPtcp_rcv_established%E8%BE%93%E5%85%A5%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"前文在TCP输入 快速路径（一）说到快速路径的处理逻辑tcp_event_data_recv中会调用tcp_measure_rcv_mss估算发送方的mss，这里计算估算对端的mss的目的是给延迟ack的计算做准备工作（注意区分压缩ack），估算mss的具体代码如下所示：\nstatic void tcp_measure_rcv_mss(struct sock *sk, const struct sk_buff *skb)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tconst unsigned int lss = icsk-&gt;icsk_ack.last_seg_size;//上1轮的估计mss\tunsigned int len;\ticsk-&gt;icsk_ack.last_seg_size = 0;\t/* skb-&gt;len may jitter because of SACKs, even if peer\t * sends good full-sized frames.\t */\t//大包就是gsosize的大小\tlen = skb_shinfo(skb)-&gt;gso_size ? : skb-&gt;len;\tif (len &gt;= icsk-&gt;icsk_ack.rcv_mss) &#123;\t\t/* Note: divides are still a bit expensive.\t\t * For the moment, only adjust scaling_ratio\t\t * when we update icsk_ack.rcv_mss.\t\t */\t\t//len发生了变化？更新scaling_ratio，这个就是实际能用多少空间的比例把\t\tif (unlikely(len != icsk-&gt;icsk_ack.rcv_mss)) &#123;\t\t\tu64 val = (u64)skb-&gt;len &lt;&lt; TCP_RMEM_TO_WIN_SCALE;\t\t\tdo_div(val, skb-&gt;truesize);\t\t\ttcp_sk(sk)-&gt;scaling_ratio = val ? val : 1;\t\t&#125;\t\t//更新rcv_mss，被通告mss钳制\t\ticsk-&gt;icsk_ack.rcv_mss = min_t(unsigned int, len,\t\t\t\t\t       tcp_sk(sk)-&gt;advmss);\t\t/* Account for possibly-removed options */\t\t//len太大的情况\t\tif (unlikely(len &gt; icsk-&gt;icsk_ack.rcv_mss +\t\t\t\t   MAX_TCP_OPTION_SPACE))\t\t\ttcp_gro_dev_warn(sk, skb, len);\t\t/* If the skb has a len of exactly 1*MSS and has the PSH bit\t\t * set then it is likely the end of an application write. So\t\t * more data may not be arriving soon, and yet the data sender\t\t * may be waiting for an ACK if cwnd-bound or using TX zero\t\t * copy. So we set ICSK_ACK_PUSHED here so that\t\t * tcp_cleanup_rbuf() will send an ACK immediately if the app\t\t * reads all of the data and is not ping-pong. If len &gt; MSS\t\t * then this logic does not matter (and does not hurt) because\t\t * tcp_cleanup_rbuf() will always ACK immediately if the app\t\t * reads data and there is more than an MSS of unACKed data.\t\t */\t\t//如果有psh 则尽快ack\t\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_PSH)\t\t\ticsk-&gt;icsk_ack.pending |= ICSK_ACK_PUSHED;\t&#125; else &#123;\t\t//这里应该是小包的i情况\t\t/* Otherwise, we make more careful check taking into account,\t\t * that SACKs block is variable.\t\t *\t\t * &quot;len&quot; is invariant segment length, including TCP header.\t\t */\t\t//加上一个tcp的头部长度\t\tlen += skb-&gt;data - skb_transport_header(skb);\t\t//是否基本大于536？\t\tif (len &gt;= TCP_MSS_DEFAULT + sizeof(struct tcphdr) ||\t\t    /* If PSH is not set, packet should be\t\t     * full sized, provided peer TCP is not badly broken.\t\t     * This observation (if it is correct 8)) allows\t\t     * to handle super-low mtu links fairly.\t\t     */\t\t    (len &gt;= TCP_MIN_MSS + sizeof(struct tcphdr) &amp;&amp;\t\t     !(tcp_flag_word(tcp_hdr(skb)) &amp; TCP_REMNANT))) &#123;\t\t\t/* Subtract also invariant (if peer is RFC compliant),\t\t\t * tcp header plus fixed timestamp option length.\t\t\t * Resulting &quot;len&quot; is MSS free of SACK jitter.\t\t\t */\t\t\tlen -= tcp_sk(sk)-&gt;tcp_header_len;\t\t\ticsk-&gt;icsk_ack.last_seg_size = len;\t\t\t//注意：上次和这次长度一致 更新mss\t\t\tif (len == lss) &#123;\t\t\t\ticsk-&gt;icsk_ack.rcv_mss = len;\t\t\t\treturn;\t\t\t&#125;\t\t&#125;\t\tif (icsk-&gt;icsk_ack.pending &amp; ICSK_ACK_PUSHED)\t\t\ticsk-&gt;icsk_ack.pending |= ICSK_ACK_PUSHED2;\t\ticsk-&gt;icsk_ack.pending |= ICSK_ACK_PUSHED;\t&#125;&#125;\n\n上述tcp_measure_rcv_mss的核心目的就是估算对端实际在发的段大小（通过gso_size 或者数据包的长度），并把结果放到 icsk-&gt;icsk_ack.rcv_mss，后续给 后续延迟  ack使用，这里注意上述的gso_size应该是GRO逻辑种设置的。\n估算mss之后会调用tcp_rcv_rtt_measure计算接收端的rtt估计，这里注意区分传统意义上的rtt，这个接收端rtt也主要用于压缩ack的计算（注意区分延迟ack），具体代码如下所示：\nstatic inline void tcp_rcv_rtt_measure(struct tcp_sock *tp)&#123;\tu32 delta_us;\t//第一次计算\tif (tp-&gt;rcv_rtt_est.time == 0)\t\tgoto new_measure;\tif (before(tp-&gt;rcv_nxt, tp-&gt;rcv_rtt_est.seq))\t\treturn;\t//计算rtt\tdelta_us = tcp_stamp_us_delta(tp-&gt;tcp_mstamp, tp-&gt;rcv_rtt_est.time);\tif (!delta_us)\t\tdelta_us = 1;\t//这里表示不用时间戳来计算rtt\ttcp_rcv_rtt_update(tp, delta_us, 1);new_measure:    //这里是一个窗口的大小\ttp-&gt;rcv_rtt_est.seq = tp-&gt;rcv_nxt + tp-&gt;rcv_wnd;\ttp-&gt;rcv_rtt_est.time = tp-&gt;tcp_mstamp;&#125;\n\ntcp_rcv_rtt_measure() 测的是接收数据到我窗口被填满这一轮所经历的时间！！！，用于指导接收侧 delayed ACK策略，而不是端到端链路 RTT。如果支持时间戳选项，可以发现外面也会使用时间戳选项计算rtt但是和这个是一个目的，都是为了指导压缩ack\n估算对端mss和rtt后，如果没有初始化过延迟ack的超时时间或者超过rto的时间内没有收到数据包，**则会调用tcp_incr_quickack**计算应当立即回复ack多少次，具体代码如下所示：\nstatic void tcp_incr_quickack(struct sock *sk, unsigned int max_quickacks)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\t//需要多少个ack可以把接收窗口填满\tunsigned int quickacks = tcp_sk(sk)-&gt;rcv_wnd / (2 * icsk-&gt;icsk_ack.rcv_mss);\t//最小是2个\tif (quickacks == 0)\t\tquickacks = 2;\t//钳制一下\tquickacks = min(quickacks, max_quickacks);\t//更新，这里是只增不减\tif (quickacks &gt; icsk-&gt;icsk_ack.quick)\t\ticsk-&gt;icsk_ack.quick = quickacks;&#125;\n\ntcp_incr_quickack的主要目的是根据当前接收能力动态计算需要多少次立即 ACK，在连接的关键阶段主动加速 ACK 反馈，从而加快对端 cwnd 增长。\ntcp_event_data_recv处理完上述逻辑后，如果数据包足够大(&gt;128)，则会尝试调用tcp_grow_window扩大接接收窗口，具体代码如下所示：\nstatic void tcp_grow_window(struct sock *sk, const struct sk_buff *skb,\t\t\t    bool adjust)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tint room;\t//计算还有多少的剩余空间\troom = min_t(int, tp-&gt;window_clamp, tcp_space(sk)) - tp-&gt;rcv_ssthresh;\tif (room &lt;= 0)\t\treturn;\t/* Check #1 */\t//是否在内存压力之下\tif (!tcp_under_memory_pressure(sk)) &#123;\t\t//把负载去除了\t\tunsigned int truesize = truesize_adjust(adjust, skb);\t\tint incr;\t\t/* Check #2. Increase window, if skb with such overhead\t\t * will fit to rcvbuf in future.\t\t */\t\t//skb的开销是否太大，如果太大按 2 个 MSS 的量往上加\t\tif (tcp_win_from_space(sk, truesize) &lt;= skb-&gt;len)\t\t\tincr = 2 * tp-&gt;advmss;\t\telse\t\t//折半的方法增长窗口\t\t\tincr = __tcp_grow_window(sk, skb, truesize);\t\t//这个单位是字节\t\tif (incr) &#123;\t\t\tincr = max_t(int, incr, 2 * skb-&gt;len);\t\t\t//这里更新了接收窗口的慢启动阈值\t\t\ttp-&gt;rcv_ssthresh += min(room, incr);\t\t\t//应该是接收窗口变大了 所以要把快速ack通知给对端？\t\t\tinet_csk(sk)-&gt;icsk_ack.quick |= 1;\t\t&#125;\t&#125; else &#123;\t\t/* Under pressure:\t\t * Adjust rcv_ssthresh according to reserved mem\t\t */\t\t//正常计算rcv_ssthresh\t\ttcp_adjust_rcv_ssthresh(sk);\t&#125;&#125;\n\ntcp_grow_window中先获取通告窗口和实际剩余窗口的甚于内存的最小值。之后判断是否在内存压力之下，如果不在内存压力之下，则首先计算一个truesize(注意这里的truesize大概率去除了非线性部分的负载)，随后判断这个数据包是否开销太大（意味着 skb 的 truesize&#x2F;len 比例太差）如果开销过大则直接给一个保守的增长量（2个mss），否则调用__tcp_grow_window计算增长的字节数，具体代码如下所示：\nstatic int __tcp_grow_window(const struct sock *sk, const struct sk_buff *skb,\t\t\t     unsigned int skbtruesize)&#123;\tconst struct tcp_sock *tp = tcp_sk(sk);\t/* Optimize this! */\tint truesize = tcp_win_from_space(sk, skbtruesize) &gt;&gt; 1;\tint window = tcp_win_from_space(sk, READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_rmem[2])) &gt;&gt; 1;\twhile (tp-&gt;rcv_ssthresh &lt;= window) &#123;\t\t//这个目的是什么呢???\t\tif (truesize &lt;= skb-&gt;len)\t\t\treturn 2 * inet_csk(sk)-&gt;icsk_ack.rcv_mss;\t\ttruesize &gt;&gt;= 1;\t\twindow &gt;&gt;= 1;\t&#125;\treturn 0;&#125;\n\n没太理解上述计算的目的，总之是返回一个可以增长的字节数，之后将增长字节数累加到接收端慢启动阈值中，用于影响后续通告给对端窗口的大小。\n若处于内存压力之下，则调用tcp_adjust_rcv_ssthresh计算接收端慢启动阈值，如果用户没有显示配置保留内存，则取当前慢启动阈值和4个通告mss的最小值，具体代码如下所示：\nstatic inline void tcp_adjust_rcv_ssthresh(struct sock *sk)&#123;\t\t//计算剩余的内存（用户配置），通常是0\tint unused_mem = sk_unused_reserved_mem(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\t//钳制一下，最小4个mss\ttp-&gt;rcv_ssthresh = min(tp-&gt;rcv_ssthresh, 4U * tp-&gt;advmss);\tif (unused_mem)\t\t//如果由保留的内存则取一个最大值\t\ttp-&gt;rcv_ssthresh = max_t(u32, tp-&gt;rcv_ssthresh,\t\t\t\t\t tcp_win_from_space(sk, unused_mem));&#125;\n\n回到tcp_rcv_established中，tcp_event_data_recv逻辑完成后，会调用tcp_ack对数据包进行处理，随后调用tcp_data_snd_check尝试发送数据，之后**调用__tcp_ack_snd_check决定何时发送ack**具体代码如下所示：\n//现在立刻发ack还是延迟发ack  这里注意延迟ack和压缩ack不是一个东西static void __tcp_ack_snd_check(struct sock *sk, int ofo_possible)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tunsigned long rtt, delay;\t    /* More than one full frame received... */\t\t//收到的数据已经超过一个mss &amp;&amp; （quick ack mode || 有pending || ）\tif (((tp-&gt;rcv_nxt - tp-&gt;rcv_wup) &gt; inet_csk(sk)-&gt;icsk_ack.rcv_mss &amp;&amp;\t     /* ... and right edge of window advances far enough.\t      * (tcp_recvmsg() will send ACK otherwise).\t      * If application uses SO_RCVLOWAT, we want send ack now if\t      * we have not received enough bytes to satisfy the condition.\t      */\t\t//有多少数据还可以给应用\t    (tp-&gt;rcv_nxt - tp-&gt;copied_seq &lt; sk-&gt;sk_rcvlowat ||\t     __tcp_select_window(sk) &gt;= tp-&gt;rcv_wnd)) ||   //接收窗口已经大于了通告给对端的窗口大小\t    /* We ACK each frame or... */\t\t\t    tcp_in_quickack_mode(sk) ||\t\t\t\t\t\t//用户设置的？？\t    /* Protocol state mandates a one-time immediate ACK */\t    inet_csk(sk)-&gt;icsk_ack.pending &amp; ICSK_ACK_NOW) &#123;send_now:\t\t//立即发送ack\t\ttcp_send_ack(sk);\t\treturn;\t&#125;\t//没有乱序的情况，那么走延迟ack，其实就是启动定时器，定时器里面分析过\tif (!ofo_possible || RB_EMPTY_ROOT(&amp;tp-&gt;out_of_order_queue)) &#123;\t\ttcp_send_delayed_ack(sk);\t\treturn;\t&#125;\t//这里可能回走第二个条件，乱序很严重 回直接发送ack\tif (!tcp_is_sack(tp) ||\t    tp-&gt;compressed_ack &gt;= READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_comp_sack_nr))\t\tgoto send_now;\t//rcv_nxt 变了 说明接收端按序推进了（某些洞被填上了）\tif (tp-&gt;compressed_ack_rcv_nxt != tp-&gt;rcv_nxt) &#123;\t\ttp-&gt;compressed_ack_rcv_nxt = tp-&gt;rcv_nxt;\t\ttp-&gt;dup_ack_counter = 0;\t&#125;\t//必须要超过三次才能进入延迟ack模式？\tif (tp-&gt;dup_ack_counter &lt; TCP_FASTRETRANS_THRESH) &#123;\t\ttp-&gt;dup_ack_counter++;\t\tgoto send_now;\t&#125;\ttp-&gt;compressed_ack++;\tif (hrtimer_is_queued(&amp;tp-&gt;compressed_ack_timer))\t\treturn;\t/* compress ack timer : 5 % of rtt, but no more than tcp_comp_sack_delay_ns */\trtt = tp-&gt;rcv_rtt_est.rtt_us;\tif (tp-&gt;srtt_us &amp;&amp; tp-&gt;srtt_us &lt; rtt)\t\trtt = tp-&gt;srtt_us;\t//计算压缩 ACK 的延迟， 这里注意上面的的rtt使用时间戳选项计算出来的！\tdelay = min_t(unsigned long,\t\t      READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_comp_sack_delay_ns),\t\t      rtt * (NSEC_PER_USEC &gt;&gt; 3)/20);\tsock_hold(sk);\t//启动压缩ack定时器\thrtimer_start_range_ns(&amp;tp-&gt;compressed_ack_timer, ns_to_ktime(delay),\t\t\t       READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_comp_sack_slack_ns),\t\t\t       HRTIMER_MODE_REL_PINNED_SOFT);&#125;\n\n__tcp_ack_snd_check中首先判断是否需要立即响应ack，立即响应ack的条件如下所示：\n\n当前设置了需要立刻响应ack的标志位\n收到的数据包已经超过了1个mss同时用户程序没有读的字节数小于1，或者当前窗口大于可以通告给对端大小\n用户设置了快速ack\n\n如果当前乱序队列为空，则调用tcp_send_delayed_ack启动延迟ack定时器（延迟ack定时器里面分析过）\n如果当前乱序很严重超过了系统参数，则也会立即回复ack\n如果存在某些空洞被填上的情况则回增加dup_ack_counter计数并立即响应ack，只有超过TCP_FASTRETRANS_THRESH（3次）之后才会进入压缩ack模式，这里的言外之意应该是存在持续乱序情况吧，之后根据接收端rtt计算压缩ack的超时时间，并启动压缩ack定时器\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输入"]},{"title":"TCP输入 慢速路径（一）","url":"/2026/01/07/TCPtcp_rcv_established%E8%BE%93%E5%85%A5%EF%BC%88%E4%B8%89%EF%BC%89/","content":"TCP建立连接下的收包路径中，如果没有命中fastpath（例如窗口为0，头部长度变化, 序列号不是预期的情况）会进入到慢速路径的处理逻辑中。\n慢速路径中首先计算校验和，如果校验和错误或者没有任何控制标志，直接丢弃数据包，否则调用tcp_validate_incoming进一步检查报文的合法性（PAWS机制，是否在窗口内等），具体代码如下所示:\n/* Does PAWS and seqno based validation of an incoming segment, flags will * play significant role here. */static bool tcp_validate_incoming(struct sock *sk, struct sk_buff *skb,\t\t\t\t  const struct tcphdr *th, int syn_inerr)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tSKB_DR(reason);\t/* RFC1323: H1. Apply PAWS check first. */\t//paws检查 检查没过进入这个分支\tif (tcp_fast_parse_options(sock_net(sk), skb, th, tp) &amp;&amp;\t    tp-&gt;rx_opt.saw_tstamp &amp;&amp;\t    tcp_paws_discard(sk, skb)) &#123;\t\tif (!th-&gt;rst) &#123;\t\t\t//如果不是rst 是syn的化回挑战ack，正常数据包就会一个dupack\t\t\tif (unlikely(th-&gt;syn))\t\t\t\tgoto syn_challenge;\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PAWSESTABREJECTED);\t\t\tif (!tcp_oow_rate_limited(sock_net(sk), skb,\t\t\t\t\t\t  LINUX_MIB_TCPACKSKIPPEDPAWS,\t\t\t\t\t\t  &amp;tp-&gt;last_oow_ack_time))\t\t\t\ttcp_send_dupack(sk, skb);\t\t\tSKB_DR_SET(reason, TCP_RFC7323_PAWS);\t\t\tgoto discard;\t\t&#125;\t\t/* Reset is accepted even if it did not pass PAWS. */\t&#125;\t/* Step 1: check sequence number */\t//检查学列好是否在接收窗口内，不接受进入下面处理逻辑\treason = tcp_sequence(tp, TCP_SKB_CB(skb)-&gt;seq, TCP_SKB_CB(skb)-&gt;end_seq);\tif (reason) &#123;\t\t/* RFC793, page 37: &quot;In all states except SYN-SENT, all reset\t\t * (RST) segments are validated by checking their SEQ-fields.&quot;\t\t * And page 69: &quot;If an incoming segment is not acceptable,\t\t * an acknowledgment should be sent in reply (unless the RST\t\t * bit is set, if so drop the segment and return)&quot;.\t\t */\t\t//同上\t\tif (!th-&gt;rst) &#123;\t\t\tif (th-&gt;syn)\t\t\t\tgoto syn_challenge;\t\t\tif (!tcp_oow_rate_limited(sock_net(sk), skb,\t\t\t\t\t\t  LINUX_MIB_TCPACKSKIPPEDSEQ,\t\t\t\t\t\t  &amp;tp-&gt;last_oow_ack_time))\t\t\t\ttcp_send_dupack(sk, skb);\t\t&#125; else if (tcp_reset_check(sk, skb)) &#123;\t\t\tgoto reset;\t\t&#125;\t\tgoto discard;\t&#125;\t/* Step 2: check RST bit */\t//建立连接状态下收到了rst就进入这个分支把\tif (th-&gt;rst) &#123;\t\t/* RFC 5961 3.2 (extend to match against (RCV.NXT - 1) after a\t\t * FIN and SACK too if available):\t\t * If seq num matches RCV.NXT or (RCV.NXT - 1) after a FIN, or\t\t * the right-most SACK block,\t\t * then\t\t *     RESET the connection\t\t * else\t\t *     Send a challenge ACK\t\t */\t\t//一个合法的rst直接复位\t\tif (TCP_SKB_CB(skb)-&gt;seq == tp-&gt;rcv_nxt ||\t\t    tcp_reset_check(sk, skb))\t\t\tgoto reset;\t\t//如果有sack 则允许匹配最右端的sack边界\t\tif (tcp_is_sack(tp) &amp;&amp; tp-&gt;rx_opt.num_sacks &gt; 0) &#123;\t\t\tstruct tcp_sack_block *sp = &amp;tp-&gt;selective_acks[0];\t\t\tint max_sack = sp[0].end_seq;\t\t\tint this_sack;\t\t\tfor (this_sack = 1; this_sack &lt; tp-&gt;rx_opt.num_sacks;\t\t\t     ++this_sack) &#123;\t\t\t\tmax_sack = after(sp[this_sack].end_seq,\t\t\t\t\t\t max_sack) ?\t\t\t\t\tsp[this_sack].end_seq : max_sack;\t\t\t&#125;\t\t\tif (TCP_SKB_CB(skb)-&gt;seq == max_sack)\t\t\t\tgoto reset;\t\t&#125;\t\t/* Disable TFO if RST is out-of-order\t\t * and no data has been received\t\t * for current active TFO socket\t\t */\t\t//如果rst不可信，则回复挑战ack\t\tif (tp-&gt;syn_fastopen &amp;&amp; !tp-&gt;data_segs_in &amp;&amp;\t\t    sk-&gt;sk_state == TCP_ESTABLISHED)\t\t\ttcp_fastopen_active_disable(sk);\t\ttcp_send_challenge_ack(sk);\t\tSKB_DR_SET(reason, TCP_RESET);\t\tgoto discard;\t&#125;\t/* step 3: check security and precedence [ignored] */\t/* step 4: Check for a SYN\t * RFC 5961 4.2 : Send a challenge ack\t */\t//这里是建立连接状态下收到syn 发一个challenge ack\tif (th-&gt;syn) &#123;syn_challenge:\t\tif (syn_inerr)\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNCHALLENGE);\t\ttcp_send_challenge_ack(sk);\t\tSKB_DR_SET(reason, TCP_INVALID_SYN);\t\tgoto discard;\t&#125;\tbpf_skops_parse_hdr(sk, skb);\treturn true;discard:\ttcp_drop_reason(sk, skb, reason);\treturn false;reset:\ttcp_reset(sk, skb);\t__kfree_skb(skb);\treturn false;&#125;\n\ntcp_validate_incoming中首先判断是否存在时间戳选项，如果存在进行PAWS机制检查，若PAWS失败，判断是否为syn包，syn会回复challenge ack。不是syn包回复dupack，并默默丢弃，如果为rst报文则会在下面进一步判断。\n之后判断数据包的序列号是否在接收窗口内，如果不在接收窗口内和上面处理逻辑类似，如果是合法rst（是预期的）则直接复位连接。\n如果在接收窗口内，收到了合法rst也直接复位连接，如果同时携带sack则也有可能直接复位连接，如果当前rst报文不可信（序列号有问题）会回复挑战ack， 合理！\n如果在建立连接状态下收到了syn包，这里列直接回复挑战ack，合理\n回到tcp_rcv_established中，判断完慢速路径报文是否合法后，如果合法会调用tcp_ack进一步处理，之后和快速路径类似，计算接收端rtt，并调用tcp_data_queue进一步处理，之后调用tcp_data_snd_check检查是否有需要待发送的数据，最终调用tcp_ack_snd_check决定是否立即发送ack。\n上述tcp_data_queue是慢速路径处理的核心，涉及到乱序队列等逻辑的处理具体代码如下所示：\nstatic void tcp_data_queue(struct sock *sk, struct sk_buff *skb)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tenum skb_drop_reason reason;\tbool fragstolen;\tint eaten;\t/* If a subflow has been reset, the packet should not continue\t * to be processed, drop the packet.\t */\tif (sk_is_mptcp(sk) &amp;&amp; !mptcp_incoming_options(sk, skb)) &#123;\t\t__kfree_skb(skb);\t\treturn;\t&#125;\t//空数据包\tif (TCP_SKB_CB(skb)-&gt;seq == TCP_SKB_CB(skb)-&gt;end_seq) &#123;\t\t__kfree_skb(skb);\t\treturn;\t&#125;\tskb_dst_drop(skb);\t__skb_pull(skb, tcp_hdr(skb)-&gt;doff * 4);\treason = SKB_DROP_REASON_NOT_SPECIFIED;\ttp-&gt;rx_opt.dsack = 0;\t/*  Queue data for delivery to the user.\t *  Packets in sequence go to the receive queue.\t *  Out of sequence packets to the out_of_order_queue.\t */\t//数据包时顺序到达的走这里，否则走下面进入乱序队列\tif (TCP_SKB_CB(skb)-&gt;seq == tp-&gt;rcv_nxt) &#123;\t\t//如果接收窗口变成0了，这里直接丢弃数据包，快速回复一个ack告诉对端窗口为0了\t\tif (tcp_receive_window(tp) == 0) &#123;\t\t\treason = SKB_DROP_REASON_TCP_ZEROWINDOW;\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPZEROWINDOWDROP);\t\t\tgoto out_of_window;\t\t&#125;\t\t/* Ok. In sequence. In window. */queue_and_out:\t\t//这里很关键，如果缓冲区大小紧张，这里可能直接丢弃乱序队列的数据包，返回-1表示无法拯救了！\t\tif (tcp_try_rmem_schedule(sk, skb, skb-&gt;truesize)) &#123;\t\t\t/* TODO: maybe ratelimit these WIN 0 ACK ? */\t\t\t//立即发ack通告给对端\t\t\tinet_csk(sk)-&gt;icsk_ack.pending |=\t\t\t\t\t(ICSK_ACK_NOMEM | ICSK_ACK_NOW);\t\t\tinet_csk_schedule_ack(sk);\t\t\t//通知用户态尽力读数据！\t\t\tsk-&gt;sk_data_ready(sk);\t\t\t//receive_queue 不为空丢弃数据包\t\t\tif (skb_queue_len(&amp;sk-&gt;sk_receive_queue)) &#123;\t\t\t\treason = SKB_DROP_REASON_PROTO_MEM;\t\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVQDROP);\t\t\t\tgoto drop;\t\t\t&#125;\t\t\t//更新内存记账，还是有机会入队的！有可能吗\t\t\tsk_forced_mem_schedule(sk, skb-&gt;truesize);\t\t&#125;\t\t//这里是数据包入队\t\teaten = tcp_queue_rcv(sk, skb, &amp;fragstolen);\t\tif (skb-&gt;len)\t\t//计算接收端rtt mss等信息\t\t\ttcp_event_data_recv(sk, skb);\t\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN)\t\t//四次挥手fin包处理逻辑\t\t\ttcp_fin(sk);\t\t//乱序队列不为空\t\tif (!RB_EMPTY_ROOT(&amp;tp-&gt;out_of_order_queue)) &#123;\t\t\t//这里就是处理乱序队列的逻辑，尝试把乱序数据放到有序队列中\t\t\ttcp_ofo_queue(sk);\t\t\t/* RFC5681. 4.2. SHOULD send immediate ACK, when\t\t\t * gap in queue is filled.\t\t\t */\t\t\t//乱序队列为空立即回复ack\t\t\tif (RB_EMPTY_ROOT(&amp;tp-&gt;out_of_order_queue))\t\t\t\tinet_csk(sk)-&gt;icsk_ack.pending |= ICSK_ACK_NOW;\t\t&#125;\t\t//数据包携带了sack选项，因为可能有些sack需要清理了\t\tif (tp-&gt;rx_opt.num_sacks)\t\t\ttcp_sack_remove(tp);\t\t//是否能走到快速路径中\t\ttcp_fast_path_check(sk);\t\t//貌似是被合并释放部分\t\tif (eaten &gt; 0)\t\t\tkfree_skb_partial(skb, fragstolen);\t\tif (!sock_flag(sk, SOCK_DEAD))\t\t//通知用户\t\t\ttcp_data_ready(sk);\t\treturn;\t&#125;\t//完全是旧的数据\tif (!after(TCP_SKB_CB(skb)-&gt;end_seq, tp-&gt;rcv_nxt)) &#123;\t\t//虚假重传\t\ttcp_rcv_spurious_retrans(sk, skb);\t\t/* A retransmit, 2nd most common case.  Force an immediate ack. */\t\treason = SKB_DROP_REASON_TCP_OLD_DATA;\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKLOST);\t\t//告诉发送端收到了重复数据\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)-&gt;seq, TCP_SKB_CB(skb)-&gt;end_seq);out_of_window:\t\t//进入快速 ACK 模式，可能是告诉对端收到了虚假重传了\t\ttcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);\t\tinet_csk_schedule_ack(sk);drop:\t\t//直接释放数据包\t\ttcp_drop_reason(sk, skb, reason);\t\treturn;\t&#125;\t/* Out of window. F.e. zero window probe. */\t//是否超出了接收窗口，直接释放数据包\tif (!before(TCP_SKB_CB(skb)-&gt;seq,\t\t    tp-&gt;rcv_nxt + tcp_receive_window(tp))) &#123;\t\treason = SKB_DROP_REASON_TCP_OVERWINDOW;\t\tgoto out_of_window;\t&#125;\t//如果存在部分重叠的情况\tif (before(TCP_SKB_CB(skb)-&gt;seq, tp-&gt;rcv_nxt)) &#123;\t\t/* Partial packet, seq &lt; rcv_next &lt; end_seq */\t\t//设置dsack信息\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)-&gt;seq, tp-&gt;rcv_nxt);\t\t/* If window is closed, drop tail of packet. But after\t\t * remembering D-SACK for its head made in previous line.\t\t */\t\t//没有窗口可用了 直接丢弃\t\tif (!tcp_receive_window(tp)) &#123;\t\t\treason = SKB_DROP_REASON_TCP_ZEROWINDOW;\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPZEROWINDOWDROP);\t\t\tgoto out_of_window;\t\t&#125;\t\t//这里部分重叠的数据包也入队了，那怎么处理重叠的部分呢？？貌似是不处理\t\tgoto queue_and_out;\t&#125;\t//其他情况，进入乱序队列\ttcp_data_queue_ofo(sk, skb);&#125;\n\ntcp_data_queue和核心思想就是在已建连状态下把收到的数据段放进接收队列&#x2F;乱序队列，主要分为以下几种情况\n\n按照顺序到达\n完全是旧的数据\n超出接收窗口\n部分重叠\n窗口合法，但是乱序\n\n首先介绍数据包按序到达的情况，如果序号正好是预期的序号，但是接收窗口已经为0了，则直接丢弃数据包。否则调用tcp_try_rmem_schedule判断是否由于缓冲区等问题需要进一步处理，(返回0 表示不存在缓冲区压力，返回-1表示存在压力)，函数内部会涉及清空乱序队列等操作。\ntcp_try_rmem_schedule具体代码如下所示：\nstatic int tcp_try_rmem_schedule(struct sock *sk, struct sk_buff *skb,\t\t\t\t unsigned int size)&#123;\t//是否缓冲区不够了 或者内存压力之下\tif (atomic_read(&amp;sk-&gt;sk_rmem_alloc) &gt; sk-&gt;sk_rcvbuf ||\t    !sk_rmem_schedule(sk, skb, size)) &#123;\t\t//合并数据包或者增大缓冲区，这里很关键，如果还是缓冲区不足，则返回-1\t\tif (tcp_prune_queue(sk, skb) &lt; 0)\t\t\treturn -1;\t\t//这里几乎不可能吧\t\twhile (!sk_rmem_schedule(sk, skb, size)) &#123;\t\t\t//开始丢弃乱序数据包！！！！\t\t\tif (!tcp_prune_ofo_queue(sk, skb))\t\t\t\treturn -1;\t\t&#125;\t&#125;\treturn 0;&#125;\n\ntcp_try_rmem_schedule首先判断是否缓冲区大小不足，或者当前在内存压力之下，如果没有则数据包会直接入队列，否则会调用tcp_prune_queue尝试增大缓冲区大小，或者是和并数据包，甚至丢弃乱序数据包！！！tcp_prune_queue代码如下所示：\nstatic int tcp_prune_queue(struct sock *sk, const struct sk_buff *in_skb)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PRUNECALLED);\t//超过了缓冲区大小\tif (atomic_read(&amp;sk-&gt;sk_rmem_alloc) &gt;= sk-&gt;sk_rcvbuf)\t\ttcp_clamp_window(sk);\t//内存压力之下，调整慢启动接收阈值\telse if (tcp_under_memory_pressure(sk))\t\ttcp_adjust_rcv_ssthresh(sk);\t//上面可能修改过缓冲区大小这里直接返回0\tif (atomic_read(&amp;sk-&gt;sk_rmem_alloc) &lt;= sk-&gt;sk_rcvbuf)\t\treturn 0;\t//合并skb减小内存消耗\ttcp_collapse_ofo_queue(sk);\t//接收队列不为空，尝试合并接收队列\tif (!skb_queue_empty(&amp;sk-&gt;sk_receive_queue))\t\t\t\ttcp_collapse(sk, &amp;sk-&gt;sk_receive_queue, NULL,\t\t\t     skb_peek(&amp;sk-&gt;sk_receive_queue),\t\t\t     NULL,\t\t\t     tp-&gt;copied_seq, tp-&gt;rcv_nxt);\t//再次判断是否小于内存限制\tif (atomic_read(&amp;sk-&gt;sk_rmem_alloc) &lt;= sk-&gt;sk_rcvbuf)\t\treturn 0;\t/* Collapsing did not help, destructive actions follow.\t * This must not ever occur. */\t//上述操作还是无法缓解，就开始丢弃乱序数据包！！！！\t//可能存在流量攻击，或者用户态不取数据包\ttcp_prune_ofo_queue(sk, in_skb);\tif (atomic_read(&amp;sk-&gt;sk_rmem_alloc) &lt;= sk-&gt;sk_rcvbuf)\t\treturn 0;\t/* If we are really being abused, tell the caller to silently\t * drop receive data on the floor.  It will get retransmitted\t * and hopefully then we&#x27;ll have sufficient space.\t */\tNET_INC_STATS(sock_net(sk), LINUX_MIB_RCVPRUNED);\t/* Massive buffer overcommit. */\ttp-&gt;pred_flags = 0;\treturn -1;&#125;\n\ntcp_prune_queue可以总结为在接收端内存不够时候的处理逻辑，如果是因为当前套接字缓冲区不足则调用tcp_clamp_window增加缓冲区大小，如果是因为内存压力，则调用tcp_adjust_rcv_ssthresh重新计算慢启动阈值。上述tcp_clamp_window具体代码如下所示：\nstatic void tcp_clamp_window(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct net *net = sock_net(sk);\tint rmem2;\t//退出快速ack 目的是让对端发慢点！！！\ticsk-&gt;icsk_ack.quick = 0;\t//套接字的最大接收缓存大小\trmem2 = READ_ONCE(net-&gt;ipv4.sysctl_tcp_rmem[2]);\t//没到最大上限没改过缓冲区大小，不在内存压力之下，不超过系统全局内存\tif (sk-&gt;sk_rcvbuf &lt; rmem2 &amp;&amp;\t    !(sk-&gt;sk_userlocks &amp; SOCK_RCVBUF_LOCK) &amp;&amp;\t    !tcp_under_memory_pressure(sk) &amp;&amp;\t    sk_memory_allocated(sk) &lt; sk_prot_mem_limits(sk, 0)) &#123;\t\t//增加接收缓冲区大小\t\tWRITE_ONCE(sk-&gt;sk_rcvbuf,\t\t\t   min(atomic_read(&amp;sk-&gt;sk_rmem_alloc), rmem2));\t&#125;\t//重新设置接收端慢启动阈值\tif (atomic_read(&amp;sk-&gt;sk_rmem_alloc) &gt; sk-&gt;sk_rcvbuf)\t\ttp-&gt;rcv_ssthresh = min(tp-&gt;window_clamp, 2U * tp-&gt;advmss);&#125;\n\ntcp_clamp_window中如果发现当前缓冲区大小没有达到单个套接字的最大缓冲区大小，同时不在内存压力之下，会进一步扩大当前套接字接收缓冲区的大小。\n回到tcp_prune_queue中，如果经过上述操作缓冲区有足够空间，则直接返回，否则会调用tcp_collapse_ofo_queue尝试合并乱序队列的数据包，也就是把多个数据包合并成一个skb，若接收队列不为空，则也尝试合并接收队列中的数据包。\n尝试合并完成之后会再次判断缓冲区大小是否足够，不够则会调用tcp_prune_ofo_queue从乱序队列的尾部开始丢弃数据包，如果上述操作仍不无法有足够空间，外层会丢弃接收队列中的数据包。 这里涉及到了主动丢包的逻辑，需要注意！\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输入"]},{"title":"TCP输入 慢速路径（二）","url":"/2026/01/09/TCPtcp_rcv_established%E8%BE%93%E5%85%A5%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"上文TCP输入 慢速路径（一）说到TCP慢速路径收到报文后，如果数据包序列号是预期的，则首先进行内存压力检查，如果不存在内存压力或者解决了内存压力，会直接将数据包放入接收队列，随后会判断乱序队列是否为空，如果乱序队列不为空，则会调用tcp_ofo_queue尝试将乱序队列中的数据尝试放到有序接收队列中，具体代码如下所示：\nstatic void tcp_ofo_queue(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t__u32 dsack_high = tp-&gt;rcv_nxt;\tbool fin, fragstolen, eaten;\tstruct sk_buff *skb, *tail;\tstruct rb_node *p;\t//乱序队列\tp = rb_first(&amp;tp-&gt;out_of_order_queue);\twhile (p) &#123;\t\tskb = rb_to_skb(p);\t\t//需要比下一个预期接收的大直接break\t\tif (after(TCP_SKB_CB(skb)-&gt;seq, tp-&gt;rcv_nxt))\t\t\tbreak;\t\t//这里判断是否发生重叠\t\tif (before(TCP_SKB_CB(skb)-&gt;seq, dsack_high)) &#123;\t\t\t__u32 dsack = dsack_high;//保存起来\t\t\t//整个skb都在旧的部分\t\t\tif (before(TCP_SKB_CB(skb)-&gt;end_seq, dsack_high))\t\t\t\tdsack_high = TCP_SKB_CB(skb)-&gt;end_seq;\t\t\t//用于告诉发送端收到了重复的数据\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb)-&gt;seq, dsack);\t\t&#125;\t\tp = rb_next(p);\t\trb_erase(&amp;skb-&gt;rbnode, &amp;tp-&gt;out_of_order_queue);\t\t//完全是旧的数据直接 continue\t\tif (unlikely(!after(TCP_SKB_CB(skb)-&gt;end_seq, tp-&gt;rcv_nxt))) &#123;\t\t\ttcp_drop_reason(sk, skb, SKB_DROP_REASON_TCP_OFO_DROP);\t\t\tcontinue;\t\t&#125;\t\t//获取接收队列的最后一个数据包\t\ttail = skb_peek_tail(&amp;sk-&gt;sk_receive_queue);\t\t//追加新的skb，这里注意如果序列号不正正好好的连续这里直接false\t\teaten = tail &amp;&amp; tcp_try_coalesce(sk, tail, skb, &amp;fragstolen);\t\t//更新下一个待接收的序列号\t\ttcp_rcv_nxt_update(tp, TCP_SKB_CB(skb)-&gt;end_seq);\t\tfin = TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN;\t\t//没有追加上，入队列，这里有问题啊 部分重叠的数也入队了？？\t\tif (!eaten)\t\t\t__skb_queue_tail(&amp;sk-&gt;sk_receive_queue, skb);\t\telse\t\t\t\t\t\tkfree_skb_partial(skb, fragstolen);\t\t//乱序队列中有个fin 走fin包处理逻辑\t\tif (unlikely(fin)) &#123;\t\t\ttcp_fin(sk);\t\t\t/* tcp_fin() purges tp-&gt;out_of_order_queue,\t\t\t * so we must end this loop right now.\t\t\t */\t\t\tbreak;\t\t&#125;\t&#125;&#125;\n\ntcp_ofo_queue核心工作就是当接收端发现乱序队列中已有的数据已经连续可用时，就按序从 out-of-order 红黑树中取出这些 skb：一边生成 DSACK 记录重复区间，一边丢弃完全过期的数据，能与接收队列尾部合并的就合并，不能的就入接收队列，同时推进 rcv_nxt；如果遇到 FIN 就进入连接关闭处理。这里注意：貌似部分重叠的数据也直接入队了！。\n回到tcp_data_queue中处理完乱序队列后，会重新处理sack选项的信息，因为此时整理的乱序丢列，需要重新更新。之后调用tcp_fast_path_check重新设置快速路径用到的flag。最后通知用户态的进程，有数据到了。\n如果是完全旧的数据包（数据包的结束序列号在下一个待接收序列号的左侧）则认为是一次虚假的重传设置dsack并进入快速ack模式，目的是即使通知对端收到了虚假重传。\n如果数据包的序列号超出了本端的接窗口则直接丢弃书包\n如果数据包存在部分重叠的情况，则设置dsack的信息，并直接入队，这里注意：重叠的部分也直接入队了？，应用程序读取的时候会处理这种情况？\n其他情况则认为就是收到了乱序的报文，调用tcp_data_queue_ofo进行进一步处理，tcp_data_queue_ofo具体代码如下所示：\nstatic void tcp_data_queue_ofo(struct sock *sk, struct sk_buff *skb)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct rb_node **p, *parent;\tstruct sk_buff *skb1;\tu32 seq, end_seq;\tbool fragstolen;\ttcp_ecn_check_ce(sk, skb);\t//判断是否内存原因无法继续处理，和外面处理逻辑一样\tif (unlikely(tcp_try_rmem_schedule(sk, skb, skb-&gt;truesize))) &#123;\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFODROP);\t\tsk-&gt;sk_data_ready(sk);\t\ttcp_drop_reason(sk, skb, SKB_DROP_REASON_PROTO_MEM);\t\treturn;\t&#125;\t/* Disable header prediction. */\t//这里直接关掉了fastpath\ttp-&gt;pred_flags = 0;\t//设置需要回复ack 的标志\tinet_csk_schedule_ack(sk);\t//乱序包统计计数++\ttp-&gt;rcv_ooopack += max_t(u16, 1, skb_shinfo(skb)-&gt;gso_segs);\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOQUEUE); //这可以观测乱序数据包的数量\t//数据包的开始序列号\tseq = TCP_SKB_CB(skb)-&gt;seq;\t//数据包的结束序列号\tend_seq = TCP_SKB_CB(skb)-&gt;end_seq;\t//乱序队列\tp = &amp;tp-&gt;out_of_order_queue.rb_node;\t//乱序队列为空的情况\tif (RB_EMPTY_ROOT(&amp;tp-&gt;out_of_order_queue)) &#123;\t\t/* Initial out of order segment, build 1 SACK. */\t\tif (tcp_is_sack(tp)) &#123;\t\t\t//构造sack的信息\t\t\ttp-&gt;rx_opt.num_sacks = 1;\t\t\ttp-&gt;selective_acks[0].start_seq = seq;\t\t\ttp-&gt;selective_acks[0].end_seq = end_seq;\t\t&#125;\t\t//插入乱序队列\t\trb_link_node(&amp;skb-&gt;rbnode, NULL, p);\t\trb_insert_color(&amp;skb-&gt;rbnode, &amp;tp-&gt;out_of_order_queue);\t\ttp-&gt;ooo_last_skb = skb;//乱序队列中最后一个书包\t\tgoto end;\t&#125;\t/* In the typical case, we are adding an skb to the end of the list.\t * Use of ooo_last_skb avoids the O(Log(N)) rbtree lookup.\t */\t//这里是尝试将当前数据包加入到末尾skb上 注意序列号必须和最后一个连续\t//感觉通常不会走到这个逻辑吧\tif (tcp_ooo_try_coalesce(sk, tp-&gt;ooo_last_skb,\t\t\t\t skb, &amp;fragstolen)) &#123;coalesce_done:\t\t/* For non sack flows, do not grow window to force DUPACK\t\t * and trigger fast retransmit.\t\t */\t\tif (tcp_is_sack(tp))\t\t\ttcp_grow_window(sk, skb, true);\t\tkfree_skb_partial(skb, fragstolen);\t\tskb = NULL;\t\tgoto add_sack;\t&#125;\t/* Can avoid an rbtree lookup if we are adding skb after ooo_last_skb */\t//还在当前乱序数据包的最后一个的后面，那就直接插入就可以了\tif (!before(seq, TCP_SKB_CB(tp-&gt;ooo_last_skb)-&gt;end_seq)) &#123;\t\tparent = &amp;tp-&gt;ooo_last_skb-&gt;rbnode;\t\tp = &amp;parent-&gt;rb_right;\t\tgoto insert;\t&#125;\t/* Find place to insert this segment. Handle overlaps on the way. */\t//真正的核心，在红黑树中查找插入点，同时处理重叠和覆盖\tparent = NULL;\t//这里p是红黑树的根\twhile (*p) &#123;\t\tparent = *p;\t\tskb1 = rb_to_skb(parent);\t\t//一直往左左走\t\tif (before(seq, TCP_SKB_CB(skb1)-&gt;seq)) &#123;\t\t\tp = &amp;parent-&gt;rb_left;\t\t\tcontinue;\t\t&#125;\t\t//判断右边\t\tif (before(seq, TCP_SKB_CB(skb1)-&gt;end_seq)) &#123;\t\t\t//新段完全被当前段覆盖 直接丢弃了，注意和外面区分，这里是乱序队列完全是旧的\t\t\tif (!after(end_seq, TCP_SKB_CB(skb1)-&gt;end_seq)) &#123;\t\t\t\t/* All the bits are present. Drop. */\t\t\t\tNET_INC_STATS(sock_net(sk),\t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\t\t\t\ttcp_drop_reason(sk, skb,\t\t\t\t\t\tSKB_DROP_REASON_TCP_OFOMERGE);\t\t\t\tskb = NULL;\t\t\t\t//构造sack\t\t\t\ttcp_dsack_set(sk, seq, end_seq);\t\t\t\tgoto add_sack;\t\t\t&#125;\t\t\t//部分重叠的情况\t\t\tif (after(seq, TCP_SKB_CB(skb1)-&gt;seq)) &#123;\t\t\t\t/* Partial overlap. */\t\t\t\ttcp_dsack_set(sk, seq, TCP_SKB_CB(skb1)-&gt;end_seq);\t\t\t&#125; else &#123;\t\t\t\t/* skb&#x27;s seq == skb1&#x27;s seq and skb covers skb1.\t\t\t\t * Replace skb1 with skb.\t\t\t\t */\t\t\t\t//这里是一定是起点相同的情况！！！\t\t\t\trb_replace_node(&amp;skb1-&gt;rbnode, &amp;skb-&gt;rbnode,\t\t\t\t\t\t&amp;tp-&gt;out_of_order_queue);\t\t\t\t//设置dsack 这里是一个扩展，在原有的基础上？\t\t\t\ttcp_dsack_extend(sk,\t\t\t\t\t\t TCP_SKB_CB(skb1)-&gt;seq,\t\t\t\t\t\t TCP_SKB_CB(skb1)-&gt;end_seq);\t\t\t\tNET_INC_STATS(sock_net(sk),\t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\t\t\t\t\t\t\t\ttcp_drop_reason(sk, skb1,\t\t\t\t\t\tSKB_DROP_REASON_TCP_OFOMERGE);\t\t\t\t//处理右侧可能重叠的部分\t\t\t\tgoto merge_right;\t\t\t&#125;\t\t//在skb1的右侧，如果进入分支表示合并成功！\t\t&#125; else if (tcp_ooo_try_coalesce(sk, skb1,\t\t\t\t\t\tskb, &amp;fragstolen)) &#123;\t\t\tgoto coalesce_done;\t\t&#125;\t\tp = &amp;parent-&gt;rb_right;\t&#125;insert:\t/* Insert segment into RB tree. */\trb_link_node(&amp;skb-&gt;rbnode, parent, p);\trb_insert_color(&amp;skb-&gt;rbnode, &amp;tp-&gt;out_of_order_queue);merge_right:\t//把右边的重复数据段干掉!\t/* Remove other segments covered by skb. */\twhile ((skb1 = skb_rb_next(skb)) != NULL) &#123;\t\t//和右侧没有重叠的 直接break\t\tif (!after(end_seq, TCP_SKB_CB(skb1)-&gt;seq))\t\t\tbreak;\t\t//部分重叠，不处理\t\tif (before(end_seq, TCP_SKB_CB(skb1)-&gt;end_seq)) &#123;\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)-&gt;seq,\t\t\t\t\t end_seq);\t\t\tbreak;\t\t&#125;\t\t//重叠的情况，完全覆盖，从红黑树中移除\t\trb_erase(&amp;skb1-&gt;rbnode, &amp;tp-&gt;out_of_order_queue);\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)-&gt;seq,\t\t\t\t TCP_SKB_CB(skb1)-&gt;end_seq);\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOMERGE);\t\ttcp_drop_reason(sk, skb1, SKB_DROP_REASON_TCP_OFOMERGE);\t&#125;\t/* If there is no skb after us, we are the last_skb ! */\t//更新最后一个skb\tif (!skb1)\t\ttp-&gt;ooo_last_skb = skb;add_sack://更新sack 的信息\tif (tcp_is_sack(tp))\t\ttcp_sack_new_ofo_skb(sk, seq, end_seq);end://如果skb没被丢弃\tif (skb) &#123;\t\t/* For non sack flows, do not grow window to force DUPACK\t\t * and trigger fast retransmit.\t\t */\t\t//尝试增大窗口，目的是让对端发送更多数据？触发快重传？\t\tif (tcp_is_sack(tp))\t\t\ttcp_grow_window(sk, skb, false);\t\t//整理skb内存布局\t\tskb_condense(skb);\t\t//内存记账！\t\tskb_set_owner_r(skb, sk);\t&#125;&#125;\n\ntcp_data_queue_ofo核心逻辑就是将乱序到达的数据包按序插入到红黑树中，并更新sack的信息。首先也检查是否在内存压力之下，和外层处理逻辑类似，之后直接关闭快速路径，累加乱序数据包计数，之后拿到数据包的起始和结束序号。\n如果乱序队列为空，则数据包直接放入乱序队列，并更新sack的信息。\n如果当前数据包的开始序列号正好和乱序队列队列的最后一个数据包序列号上连续则直接尝试合并数据包，如果合并成功则直接构造sack信息并返回。\n如果当前数据包还在乱序队列最后一个数据包的后面则直接插入乱序队列并返回，否则进入while循环中从乱序队列的根节点开始找到合适的处理位置，插入或者处理数据包。如果当前数据包已经完全被乱序队列中的数据包覆盖则直接丢弃数据包，如果部分重叠则直接插入，后面会进一步处理。如果当前数据包和红黑树中的数据包起点相同则设置dsack后会丢弃原有的数据包，并处理右侧重叠的部分。\n如果恰好为当前待处理数据包的右侧序列号，则会尝试直接合并数据包（大概率不会吧）。\n处理右侧的逻辑：如果当前数据包和右侧数据包存在部分重叠则不处理，如果完全重叠的话则把红黑树中的书包直接移除。\n上述处理乱序的流程中如果需要处理sack则会调用tcp_sack_new_ofo_skb来填充sack信息。具体代码如下所示：\n/* Reasonable amount of sack blocks included in TCP SACK option * The max is 4, but this becomes 3 if TCP timestamps are there. * Given that SACK packets might be lost, be conservative and use 2. */#define TCP_SACK_BLOCKS_EXPECTED 2static void tcp_sack_new_ofo_skb(struct sock *sk, u32 seq, u32 end_seq)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct tcp_sack_block *sp = &amp;tp-&gt;selective_acks[0];\tint cur_sacks = tp-&gt;rx_opt.num_sacks;\tint this_sack;\t//没有，直接填充sack\tif (!cur_sacks)\t\tgoto new_sack;\t//尝试更已右的sack合并。\tfor (this_sack = 0; this_sack &lt; cur_sacks; this_sack++, sp++) &#123;\t\tif (tcp_sack_extend(sp, seq, end_seq)) &#123;\t\t\tif (this_sack &gt;= TCP_SACK_BLOCKS_EXPECTED) //段太多\t\t\t\ttcp_sack_compress_send_ack(sk);\t//取消压缩ack 立即发送sack信息\t\t\t/* Rotate this_sack to the first one. */\t\t\tfor (; this_sack &gt; 0; this_sack--, sp--)//冒泡排序\t\t\t\tswap(*sp, *(sp - 1));\t\t\tif (cur_sacks &gt; 1)\t\t\t\ttcp_sack_maybe_coalesce(tp);//有多个段，尝试把相邻的在合并一下\t\t\treturn;\t\t&#125;\t&#125;\t//取消压缩ack\tif (this_sack &gt;= TCP_SACK_BLOCKS_EXPECTED)\t\ttcp_sack_compress_send_ack(sk);\t/* Could not find an adjacent existing SACK, build a new one,\t * put it at the front, and shift everyone else down.  We\t * always know there is at least one SACK present already here.\t *\t * If the sack array is full, forget about the last one.\t */\t//丢掉最后一个sack  只能容纳4个\tif (this_sack &gt;= TCP_NUM_SACKS) &#123;\t\tthis_sack--;\t\ttp-&gt;rx_opt.num_sacks--;\t\tsp--;\t&#125;\t//体右移一格，给第 0 腾位置\tfor (; this_sack &gt; 0; this_sack--, sp--)\t\t*sp = *(sp - 1);//这里就是真正的填充sack 的信息！new_sack:\t/* Build the new head SACK, and we&#x27;re done. */\tsp-&gt;start_seq = seq;\tsp-&gt;end_seq = end_seq;\ttp-&gt;rx_opt.num_sacks++; //注意这里更新了sack的块数&#125;\n\ntcp_sack_new_ofo_skb作用就是构造sack信息，首先是一个for循环尝试跟现有的sack合并，如果合并成功同时段数很多就立即发送ack，若合并失败也会判断sack段数是否过多（超过2个），并取消压缩ack定时器并立即发送ack，紧接着判断段数是否超过4个（sack块只能容纳4个），之后给整体右移动，给下一个要填写的sack块腾出位置，并完成真正的写入操作：-）。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输入"]},{"title":"TCP输入 recvmsg实现","url":"/2026/01/14/TCP%E6%8E%A5%E6%94%B6recvmsg%E5%AE%9E%E7%8E%B0/","content":"用户态调用read或者recv等系统调用接收数据时，最终会调用到传输层（sk_prot）的回调，对应TCP的实现为tcp_recvmsg,该接口主要目的就是将sk_receive_queue（接收队列）里按序列号找数据，把 payload 拷贝到用户缓冲区，如果暂时没有合适的数据，就根据阻塞等条件决定等还是返回，最后根据这次读走的数据量决定要不要发 ACK&#x2F;更新窗口。\ntcp_recvmsg具体代码如下所示：\nint tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int flags,\t\tint *addr_len)&#123;\tint cmsg_flags = 0, ret;\tstruct scm_timestamping_internal tss;\t//用户态获取错误对立的数据\tif (unlikely(flags &amp; MSG_ERRQUEUE))\t\treturn inet_recv_error(sk, msg, len, addr_len);\tif (sk_can_busy_loop(sk) &amp;&amp;\t\t\t\t\t\t\t\t\t//用户启用了busypoll\t    skb_queue_empty_lockless(&amp;sk-&gt;sk_receive_queue) &amp;&amp; //接收对立为空\t    sk-&gt;sk_state == TCP_ESTABLISHED)\t\t\t\t\t\t//建立连接的状态\t\tsk_busy_loop(sk, flags &amp; MSG_DONTWAIT);\t\t//直接调用驱动的poll函数貌似不需要驱动支持\tlock_sock(sk);//加锁\t//返回值是拷贝给用户的字节数\tret = tcp_recvmsg_locked(sk, msg, len, flags, &amp;tss, &amp;cmsg_flags);\t//解锁\trelease_sock(sk);\t//是否有控制信息\tif ((cmsg_flags || msg-&gt;msg_get_inq) &amp;&amp; ret &gt;= 0) &#123;\t\t//接收时间戳返给用户\t\tif (cmsg_flags &amp; TCP_CMSG_TS)\t\t\ttcp_recv_timestamp(msg, sk, &amp;tss);\t\t//接收队列还有多少可读字节\t\tif (msg-&gt;msg_get_inq) &#123;\t\t\tmsg-&gt;msg_inq = tcp_inq_hint(sk);\t\t\tif (cmsg_flags &amp; TCP_CMSG_INQ)\t\t\t\t//拷贝给用户\t\t\t\tput_cmsg(msg, SOL_TCP, TCP_CM_INQ,\t\t\t\t\t sizeof(msg-&gt;msg_inq), &amp;msg-&gt;msg_inq);\t\t&#125;\t&#125;\treturn ret;&#125;\n\ntcp_recvmsg中首先判断是否需要从错误队列读取错误信息，如果读取错误信息则直接走inet_recv_error不走正常收包流程。\n如果用户启用了busy_poll，且接收队列为空的情况下，则会尝试轮询收报，这里网卡应该是感知不到的， 应该是通过DD位保证数据一致性的吧？\n之后调用tcp_recvmsg_locked将数据包真正的从内核拷贝到用户态，并返回拷贝的字节数。tcp_recvmsg_locked代码如下所示：\nstatic int tcp_recvmsg_locked(struct sock *sk, struct msghdr *msg, size_t len,\t\t\t      int flags, struct scm_timestamping_internal *tss,\t\t\t      int *cmsg_flags)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tint copied = 0;\tu32 peek_seq;\tu32 *seq;\tunsigned long used;\tint err;\tint target;\t\t/* Read at least this many bytes */\tlong timeo;\tstruct sk_buff *skb, *last;\tu32 urg_hole = 0;\terr = -ENOTCONN;\tif (sk-&gt;sk_state == TCP_LISTEN)\t\tgoto out;\t//用户选项设置，看一下接收队列还有多少数据\tif (tp-&gt;recvmsg_inq) &#123;\t\t*cmsg_flags = TCP_CMSG_INQ;\t\tmsg-&gt;msg_get_inq = 1;\t&#125;\t//获取超时时间\ttimeo = sock_rcvtimeo(sk, flags &amp; MSG_DONTWAIT);\t/* Urgent data needs to be handled specially. */\t//紧急数据的处理\tif (flags &amp; MSG_OOB)\t\tgoto recv_urg;\t//repair模式下的处理\tif (unlikely(tp-&gt;repair)) &#123;\t\terr = -EPERM;\t\tif (!(flags &amp; MSG_PEEK))\t\t\tgoto out;\t\tif (tp-&gt;repair_queue == TCP_SEND_QUEUE)\t\t\tgoto recv_sndq;\t\terr = -EINVAL;\t\tif (tp-&gt;repair_queue == TCP_NO_QUEUE)\t\t\tgoto out;\t\t/* &#x27;common&#x27; recv queue MSG_PEEK-ing */\t&#125;\t//应用程序已经读到哪个序列号的位置\tseq = &amp;tp-&gt;copied_seq;\t//是否设置了peek\tif (flags &amp; MSG_PEEK) &#123;\t\tpeek_seq = tp-&gt;copied_seq;\t\tseq = &amp;peek_seq;\t&#125;\t//至少读多少才行，默认是1\ttarget = sock_rcvlowat(sk, flags &amp; MSG_WAITALL, len);\t//不断从 sk_receive_queue 找到当前 seq 对应的 skb，拷贝数据\tdo &#123;\t\tu32 offset;\t\t/* Are we at urgent data? Stop if we have read anything or have SIGURG pending. */\t\t//处理紧急数据\t\tif (unlikely(tp-&gt;urg_data) &amp;&amp; tp-&gt;urg_seq == *seq) &#123;\t\t\tif (copied)\t\t\t\tbreak;\t\t\tif (signal_pending(current)) &#123;\t\t\t\tcopied = timeo ? sock_intr_errno(timeo) : -EAGAIN;\t\t\t\tbreak;\t\t\t&#125;\t\t&#125;\t\t/* Next get a buffer. */\t\t//获取当前队列尾部的skb\t\tlast = skb_peek_tail(&amp;sk-&gt;sk_receive_queue);\t\tskb_queue_walk(&amp;sk-&gt;sk_receive_queue, skb) &#123;\t\t\tlast = skb;\t\t\t/* Now that we have two receive queues this\t\t\t * shouldn&#x27;t happen.\t\t\t */\t\t\t//seq 不能在skb的序号之前，\t\t\tif (WARN(before(*seq, TCP_SKB_CB(skb)-&gt;seq),\t\t\t\t &quot;TCP recvmsg seq # bug: copied %X, seq %X, rcvnxt %X, fl %X\\n&quot;,\t\t\t\t *seq, TCP_SKB_CB(skb)-&gt;seq, tp-&gt;rcv_nxt,\t\t\t\t flags))\t\t\t\tbreak;\t\t\t//找到skb中的偏移\t\t\toffset = *seq - TCP_SKB_CB(skb)-&gt;seq;\t\t\t//不能有syn标志\t\t\tif (unlikely(TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_SYN)) &#123;\t\t\t\tpr_err_once(&quot;%s: found a SYN, please report !\\n&quot;, __func__);\t\t\t\toffset--;\t\t\t&#125;\t\t\t//表示序列号落在当前这个skb的范围内\t\t\tif (offset &lt; skb-&gt;len)\t\t\t\tgoto found_ok_skb;\t\t\t//数据包中存在fin标志\t\t\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN)\t\t\t\tgoto found_fin_ok;\t\t\tWARN(!(flags &amp; MSG_PEEK),\t\t\t     &quot;TCP recvmsg seq # bug 2: copied %X, seq %X, rcvnxt %X, fl %X\\n&quot;,\t\t\t     *seq, TCP_SKB_CB(skb)-&gt;seq, tp-&gt;rcv_nxt, flags);\t\t&#125;\t\t/* Well, if we have backlog, try to process it now yet. */\t\t//如果没找到skb 同时没有读够数据，且backlog为空，直接返回\t\tif (copied &gt;= target &amp;&amp; !READ_ONCE(sk-&gt;sk_backlog.tail))\t\t\tbreak;\t\t//已经读了一些数据\t\tif (copied) &#123;\t\t\tif (!timeo ||\t\t\t\t\t\t\t//非阻塞\t\t\t    sk-&gt;sk_err ||\t\t\t\t\t\t//存在错误\t\t\t    sk-&gt;sk_state == TCP_CLOSE ||\t\t//已经close\t\t\t    (sk-&gt;sk_shutdown &amp; RCV_SHUTDOWN) ||\t//被动关闭\t\t\t    signal_pending(current))\t\t\t//ctrl c？？\t\t\t\tbreak;\t\t&#125; else &#123;\t\t\tif (sock_flag(sk, SOCK_DONE))\t\t//比如说主动关闭\t\t\t\tbreak;\t\t\tif (sk-&gt;sk_err) &#123;\t\t\t\t\t\t//比如说收到rst，icmp\t\t\t\tcopied = sock_error(sk);\t\t\t\tbreak;\t\t\t&#125;\t\t\tif (sk-&gt;sk_shutdown &amp; RCV_SHUTDOWN)\t\t //被动关闭\t\t\t\tbreak;\t\t\tif (sk-&gt;sk_state == TCP_CLOSE) &#123;\t\t\t\t/* This occurs when user tries to read\t\t\t\t * from never connected socket.\t\t\t\t */\t\t\t\tcopied = -ENOTCONN;\t\t\t\tbreak;\t\t\t&#125;\t\t\tif (!timeo) &#123;//非阻塞模式，没有copy到数据\t\t\t\tcopied = -EAGAIN;\t\t\t\tbreak;\t\t\t&#125;\t\t\t//ctrl +c\t\t\tif (signal_pending(current)) &#123;\t\t\t\tcopied = sock_intr_errno(timeo);\t\t\t\tbreak;\t\t\t&#125;\t\t&#125;\t\t//已经读够了数据，处理后备队列\t\tif (copied &gt;= target) &#123;\t\t\t/* Do not sleep, just process backlog. */\t\t\t__sk_flush_backlog(sk);\t\t&#125; else &#123;\t\t\t//是否立即回ack\t\t\ttcp_cleanup_rbuf(sk, copied);\t\t\t//睡眠等待数据\t\t\terr = sk_wait_data(sk, &amp;timeo, last);\t\t\tif (err &lt; 0) &#123;\t\t\t\terr = copied ? : err;\t\t\t\tgoto out;\t\t\t&#125;\t\t&#125;\t\t//peek\t\tif ((flags &amp; MSG_PEEK) &amp;&amp;\t\t    (peek_seq - copied - urg_hole != tp-&gt;copied_seq)) &#123;\t\t\tnet_dbg_ratelimited(&quot;TCP(%s:%d): Application bug, race in MSG_PEEK\\n&quot;,\t\t\t\t\t    current-&gt;comm,\t\t\t\t\t    task_pid_nr(current));\t\t\tpeek_seq = tp-&gt;copied_seq;\t\t&#125;\t\tcontinue;found_ok_skb:\t\t/* Ok so how much can we use? */\t\t//计算当前数据包有多少可以copy\t\tused = skb-&gt;len - offset;\t\tif (len &lt; used)\t\t\tused = len;\t\t/* Do we have urgent data here? */\t\t//紧急数据处理\t\tif (unlikely(tp-&gt;urg_data)) &#123;\t\t\tu32 urg_offset = tp-&gt;urg_seq - *seq;\t\t\tif (urg_offset &lt; used) &#123;\t\t\t\tif (!urg_offset) &#123;\t\t\t\t\tif (!sock_flag(sk, SOCK_URGINLINE)) &#123;\t\t\t\t\t\tWRITE_ONCE(*seq, *seq + 1);\t\t\t\t\t\turg_hole++;\t\t\t\t\t\toffset++;\t\t\t\t\t\tused--;\t\t\t\t\t\tif (!used)\t\t\t\t\t\t\tgoto skip_copy;\t\t\t\t\t&#125;\t\t\t\t&#125; else\t\t\t\t\tused = urg_offset;\t\t\t&#125;\t\t&#125;\t\t//这里是把数据真正拷贝到用户态！！\t\tif (!(flags &amp; MSG_TRUNC)) &#123;\t\t\terr = skb_copy_datagram_msg(skb, offset, msg, used);\t\t\tif (err) &#123;\t\t\t\t/* Exception. Bailout! */\t\t\t\tif (!copied)\t\t\t\t\tcopied = -EFAULT;\t\t\t\tbreak;\t\t\t&#125;\t\t&#125;\t\t//更新seq， copy总量，用户还希望接收的字节数\t\tWRITE_ONCE(*seq, *seq + used);\t\tcopied += used;\t\tlen -= used;\t\t//自动调大tcp的接收缓冲区\t\ttcp_rcv_space_adjust(sk);skip_copy:\t\t//紧急数据处理\t\tif (unlikely(tp-&gt;urg_data) &amp;&amp; after(tp-&gt;copied_seq, tp-&gt;urg_seq)) &#123;\t\t\tWRITE_ONCE(tp-&gt;urg_data, 0);\t\t\ttcp_fast_path_check(sk);\t\t&#125;\t\t//如果设置了时间戳，提取时间戳的到tss参数中，并设置控制标志位\t\tif (TCP_SKB_CB(skb)-&gt;has_rxtstamp) &#123;\t\t\ttcp_update_recv_tstamps(skb, tss);\t\t\t*cmsg_flags |= TCP_CMSG_TS;\t\t&#125;\t\t//还没有读完这个skb的数据 继续读\t\tif (used + offset &lt; skb-&gt;len)\t\t\tcontinue;\t\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN)\t\t\tgoto found_fin_ok;\t\tif (!(flags &amp; MSG_PEEK))\t\t//从接收队列中移除skb\t\t\ttcp_eat_recv_skb(sk, skb);\t\tcontinue;found_fin_ok:\t\t/* Process the FIN. */\t\t//处理fin包\t\tWRITE_ONCE(*seq, *seq + 1);\t\tif (!(flags &amp; MSG_PEEK))\t\t\ttcp_eat_recv_skb(sk, skb);\t\tbreak;\t&#125; while (len &gt; 0);\t/* According to UNIX98, msg_name/msg_namelen are ignored\t * on connected socket. I was just happy when found this 8) --ANK\t */\t/* Clean up data we have read: This will do ACK frames. */\t//是否需要立即回复ack\ttcp_cleanup_rbuf(sk, copied);\treturn copied;out:\treturn err;recv_urg:\terr = tcp_recv_urg(sk, msg, len, flags);\tgoto out;recv_sndq:\terr = tcp_peek_sndq(sk, msg, len);\tgoto out;&#125;\n\n tcp_recvmsg_locked中首先判断用户是否启用INQ，启用时会将接收队列的剩余字节数通过put_cmsg返给用户，之后获取用户的超时时间和当前用户已经读到的序列号的位置，以及获取至少需要读多少才允许返回的阈值（默认是1）。\n接下进入大循环中，核心就是按照序列号找到对应的skb，计算offset，把数据包拷贝到用户态，如果没找到，就处理后备队列或者睡眠等待。\n首先获取当前接收队列尾部的skb之后计算当前序列号在当前skb中的偏移位置，同时进行一系列的安全检查（如果是fin包则会有专门的处理逻辑，因为fin包是会放到接收队列中的）。如果没找到skb 同时没有读够数据，且backlog为空，则直接退出循环。如果读取了部分数据，同时非阻塞|| 存在错误||已经close||被动关闭||等情况，则也直接退出循环。\n如果没有拷贝数据包到用户态，并且主动关闭||收到rst报文||被动关闭||非阻塞|| ctrl+c 等情况也会直接退出循环。\n如果读够了数据，则处理后备队列，之后继续执行大循环。如果没有读够足够的数据则调用tcp_cleanup_rbuf决定是否立即回复ack并根据超时时间决定是否睡眠，上述tcp_cleanup_rbuf由于历史原因导致名字和逻辑不太符合，具体代码如下所示：\nvoid tcp_cleanup_rbuf(struct sock *sk, int copied)&#123;\tstruct sk_buff *skb = skb_peek(&amp;sk-&gt;sk_receive_queue);\tstruct tcp_sock *tp = tcp_sk(sk);\t//copy seq必须小于skb的end seq\tWARN(skb &amp;&amp; !before(tp-&gt;copied_seq, TCP_SKB_CB(skb)-&gt;end_seq),\t     &quot;cleanup rbuf bug: copied %X seq %X rcvnxt %X\\n&quot;,\t     tp-&gt;copied_seq, TCP_SKB_CB(skb)-&gt;end_seq, tp-&gt;rcv_nxt);\t__tcp_cleanup_rbuf(sk, copied);&#125;/* Clean up the receive buffer for full frames taken by the user, * then send an ACK if necessary.  COPIED is the number of bytes * tcp_recvmsg has given to the user so far, it speeds up the * calculation of whether or not we must ACK for the sake of * a window update. */void __tcp_cleanup_rbuf(struct sock *sk, int copied)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tbool time_to_ack = false;\t//需要发送ack\tif (inet_csk_ack_scheduled(sk)) &#123;\t\tconst struct inet_connection_sock *icsk = inet_csk(sk);\t\t//收到了一个mss\t\tif (/* Once-per-two-segments ACK was not sent by tcp_input.c */\t\t    tp-&gt;rcv_nxt - tp-&gt;rcv_wup &gt; icsk-&gt;icsk_ack.rcv_mss ||\t\t    /*\t\t     * If this read emptied read buffer, we send ACK, if\t\t     * connection is not bidirectional, user drained\t\t     * receive buffer and there was a small segment\t\t     * in queue.\t\t     */\t\t    (copied &gt; 0 &amp;&amp;\t\t     ((icsk-&gt;icsk_ack.pending &amp; ICSK_ACK_PUSHED2) ||\t//没有push标志\t\t      ((icsk-&gt;icsk_ack.pending &amp; ICSK_ACK_PUSHED) &amp;&amp;\t\t       !inet_csk_in_pingpong_mode(sk))) &amp;&amp;\t//不是pingpong模式，pingpong模式倾向捎带ack\t\t      !atomic_read(&amp;sk-&gt;sk_rmem_alloc)))//没有数据了\t\t\ttime_to_ack = true;\t\t\t\t\t\t//立即发送ack\t&#125;\t/* We send an ACK if we can now advertise a non-zero window\t * which has been raised &quot;significantly&quot;.\t *\t * Even if window raised up to infinity, do not send window open ACK\t * in states, where we will not receive more. It is useless.\t */\t//如果拷贝了数据， 不是被动关闭的情况\tif (copied &gt; 0 &amp;&amp; !time_to_ack &amp;&amp; !(sk-&gt;sk_shutdown &amp; RCV_SHUTDOWN)) &#123;\t\t__u32 rcv_window_now = tcp_receive_window(tp); //计算通告窗口大小\t\t/* Optimize, __tcp_select_window() is not cheap. */\t\tif (2*rcv_window_now &lt;= tp-&gt;window_clamp) &#123;\t\t\t__u32 new_window = __tcp_select_window(sk);\t\t\t/* Send ACK now, if this read freed lots of space\t\t\t * in our buffer. Certainly, new_window is new window.\t\t\t * We can advertise it now, if it is not less than current one.\t\t\t * &quot;Lots&quot; means &quot;at least twice&quot; here.\t\t\t */\t\t\t//窗口大了很多，立即发送ack给对端\t\t\tif (new_window &amp;&amp; new_window &gt;= 2 * rcv_window_now)\t\t\t\ttime_to_ack = true;\t\t&#125;\t&#125;\t//发送ack\tif (time_to_ack)\t\ttcp_send_ack(sk);&#125;\n\n__tcp_cleanup_rbuf主要逻辑就是如果当前已经有需要待发送的ack了，同时内核已经收到了超过mss大小的一个报文则大概率会立即回复一个ack（不在pingpong下等情况）。 否则会进一步判断是否拷贝数据包到了用户态，同时没有关闭套接字，且可用的窗口大了很多的情况下则会立即发送ack（目的是及时通告给对端）。\n回到大循环中，如果找到了需要拷贝数据包的起始偏移（比如上面逻辑中），则会计算当前数据包右多少可以拷贝的数据（同时不超过用户传进来的大小），之后调用skb_copy_datagram_msg把数据包拷贝到用户态，并更新\t&amp;tp-&gt;copied_seq;同时调用tcp_rcv_space_adjust尝试扩大接收缓冲区，具体代码如下所示：\nvoid tcp_rcv_space_adjust(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\tu32 copied;\tint time;\ttrace_tcp_rcv_space_adjust(sk);\t//刷新时间戳\ttcp_mstamp_refresh(tp);\t//间隔\ttime = tcp_stamp_us_delta(tp-&gt;tcp_mstamp, tp-&gt;rcvq_space.time);\t//小于一个rtt 直接返回\tif (time &lt; (tp-&gt;rcv_rtt_est.rtt_us &gt;&gt; 3) || tp-&gt;rcv_rtt_est.rtt_us == 0)\t\treturn;\t/* Number of bytes copied to user in last RTT */\t//计算上一个rtt 读走了多少数据\tcopied = tp-&gt;copied_seq - tp-&gt;rcvq_space.seq;\t//当前这个周期没有比上次读的更快，不需要变化\tif (copied &lt;= tp-&gt;rcvq_space.space)\t\tgoto new_measure;\t/* A bit of theory :\t * copied = bytes received in previous RTT, our base window\t * To cope with packet losses, we need a 2x factor\t * To cope with slow start, and sender growing its cwin by 100 %\t * every RTT, we need a 4x factor, because the ACK we are sending\t * now is for the next RTT, not the current one :\t * &lt;prev RTT . &gt;&lt;current RTT .. &gt;&lt;next RTT .... &gt;\t */\t//系统参数容许调整缓冲区大小，用户没有显示修改过缓冲区大小\tif (READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_moderate_rcvbuf) &amp;&amp;\t    !(sk-&gt;sk_userlocks &amp; SOCK_RCVBUF_LOCK)) &#123;\t\tu64 rcvwin, grow;\t\tint rcvbuf;\t\t/* minimal window to cope with packet losses, assuming\t\t * steady state. Add some cushion because of small variations.\t\t */\t\t//先算一个比较靠谱的窗口大小\t\trcvwin = ((u64)copied &lt;&lt; 1) + 16 * tp-&gt;advmss;\t\t/* Accommodate for sender rate increase (eg. slow start) */\t\t//计算应用读取的速率是不是在上升\t\tgrow = rcvwin * (copied - tp-&gt;rcvq_space.space);\t\tdo_div(grow, tp-&gt;rcvq_space.space);\t\trcvwin += (grow &lt;&lt; 1);\t\t//换算成需要的缓存区大小，但是钳制一下\t\trcvbuf = min_t(u64, tcp_space_from_win(sk, rcvwin),\t\t\t       READ_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_rmem[2]));\t\tif (rcvbuf &gt; sk-&gt;sk_rcvbuf) &#123;\t\t\tWRITE_ONCE(sk-&gt;sk_rcvbuf, rcvbuf);\t\t\t/* Make the window clamp follow along.  */\t\t\t//更新最大窗口的上限\t\t\ttp-&gt;window_clamp = tcp_win_from_space(sk, rcvbuf);\t\t&#125;\t&#125;\ttp-&gt;rcvq_space.space = copied;new_measure:\t//更新下一次计算需要使用到的字段\ttp-&gt;rcvq_space.seq = tp-&gt;copied_seq;\ttp-&gt;rcvq_space.time = tp-&gt;tcp_mstamp;&#125;\n\ntcp_rcv_space_adjust核心思想就是计算一个rtt内读取的数据是否比上一次读取的多，同时系统参数允许修改缓存并且用户没有显示修改缓冲区的话则会扩大接收缓存区，并更新通告窗口的最大上限。\n回到循环中，如果读完了当前skb的所有数据，则会调用tcp_eat_recv_skb释放数据包。如果结束了循环（上面的break或者拷贝了足够的用户数据）则会直接返回拷贝的字节数：-）。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP输入"]},{"title":"TCP四次挥手-tcp_close（一）","url":"/2026/01/20/TCP%E8%BF%9E%E6%8E%A5%E7%9A%84%E7%BB%88%E6%AD%A2%EF%BC%88%E4%B8%80%EF%BC%89/","content":"由于TCP连接是全双工的，发送的同时也能接收数据，所以关闭连接时两个方向必须单独进行，当一端完成数据发送的任务后，应用层可以调用close发送FIN来终止该方向上的连接，当另一端收到这个FIN后，必须通知应用层已经终止了数据传输。同时对端需要确认这个FIN包，因此分手通常需要四次（有时候第二次和第三次可能合并成一个报文）。\ntcp_close时TCP的close系统调用的传输层实现，在关闭TCP套接字时由inet_release调用，无论是主动关闭还是被动关闭通常都会调用（很少有应用程序，收到对端的FIN包之后不关闭本端吧）close除了释放资源之外的核心逻辑就是决定优雅关闭（正常四次挥手）还是异常中止（发rst）。\ntcp_close具体实现如下\nvoid __tcp_close(struct sock *sk, long timeout)&#123;\tstruct sk_buff *skb;\tint data_was_unread = 0;\tint state;\t//表示这个socket不允许收发数据\tWRITE_ONCE(sk-&gt;sk_shutdown, SHUTDOWN_MASK);\t//listen sk的特殊处理，把监听套接字给关闭了\tif (sk-&gt;sk_state == TCP_LISTEN) &#123;\t\ttcp_set_state(sk, TCP_CLOSE);\t\t/* Special case. */\t\t//尝试迁移sock\t\tinet_csk_listen_stop(sk);\t\tgoto adjudge_to_death;\t&#125;\t/*  We need to flush the recv. buffs.  We do this only on the\t *  descriptor close, not protocol-sourced closes, because the\t *  reader process may not have drained the data yet!\t */\t//遍历接收队列，判断是否存在未接收的数据，记录字节数\twhile ((skb = __skb_dequeue(&amp;sk-&gt;sk_receive_queue)) != NULL) &#123;\t\tu32 len = TCP_SKB_CB(skb)-&gt;end_seq - TCP_SKB_CB(skb)-&gt;seq;\t\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN)\t\t\tlen--;\t\tdata_was_unread += len;\t\t__kfree_skb(skb);\t&#125;\t/* If socket has been already reset (e.g. in tcp_reset()) - kill it. */\t//如果已经是close状态（比如刚发了rst？或者刚收到rst）\tif (sk-&gt;sk_state == TCP_CLOSE)\t\tgoto adjudge_to_death;\t/* As outlined in RFC 2525, section 2.17, we send a RST here because\t * data was lost. To witness the awful effects of the old behavior of\t * always doing a FIN, run an older 2.1.x kernel or 2.0.x, start a bulk\t * GET in an FTP client, suspend the process, wait for the client to\t * advertise a zero window, then kill -9 the FTP client, wheee...\t * Note: timeout is always zero in such a case.\t */\t//repaire模式\tif (unlikely(tcp_sk(sk)-&gt;repair)) &#123;\t\tsk-&gt;sk_prot-&gt;disconnect(sk, 0);\t//存在未读的数据，不能让对端认为数据包已经收到了（发fin表示正常接收）\t&#125; else if (data_was_unread) &#123;\t\t/* Unread data was tossed, zap the connection. */\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONCLOSE);\t\ttcp_set_state(sk, TCP_CLOSE);\t\t//直接发rst\t\ttcp_send_active_reset(sk, sk-&gt;sk_allocation);\t//用户启用了so linger选项但是超时时间为0 大概率直接发rst\t&#125; else if (sock_flag(sk, SOCK_LINGER) &amp;&amp; !sk-&gt;sk_lingertime) &#123;\t\t/* Check zero linger _after_ checking for unread data. */\t\tsk-&gt;sk_prot-&gt;disconnect(sk, 0);\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\t//正常情况，设置为close状态发送fin包\t&#125; else if (tcp_close_state(sk)) &#123;\t\t/* We FIN if the application ate all the data before\t\t * zapping the connection.\t\t */\t\t/* RED-PEN. Formally speaking, we have broken TCP state\t\t * machine. State transitions:\t\t *\t\t * TCP_ESTABLISHED -&gt; TCP_FIN_WAIT1\t\t * TCP_SYN_RECV\t-&gt; TCP_FIN_WAIT1 (forget it, it&#x27;s impossible)\t\t * TCP_CLOSE_WAIT -&gt; TCP_LAST_ACK\t\t *\t\t * are legal only when FIN has been sent (i.e. in window),\t\t * rather than queued out of window. Purists blame.\t\t *\t\t * F.e. &quot;RFC state&quot; is ESTABLISHED,\t\t * if Linux state is FIN-WAIT-1, but FIN is still not sent.\t\t *\t\t * The visible declinations are that sometimes\t\t * we enter time-wait state, when it is not required really\t\t * (harmless), do not send active resets, when they are\t\t * required by specs (TCP_ESTABLISHED, TCP_CLOSE_WAIT, when\t\t * they look as CLOSING or LAST_ACK for Linux)\t\t * Probably, I missed some more holelets.\t\t * \t\t\t\t\t\t--ANK\t\t * XXX (TFO) - To start off we don&#x27;t support SYN+ACK+FIN\t\t * in a single packet! (May consider it later but will\t\t * probably need API support or TCP_CORK SYN-ACK until\t\t * data is written and socket is closed.)\t\t */\t\t//发送fin包\t\ttcp_send_fin(sk);\t&#125;\t//这里通常是直接返回，否则阻塞一会儿，等待数据发送完毕？\tsk_stream_wait_close(sk, timeout);adjudge_to_death:\tstate = sk-&gt;sk_state;\tsock_hold(sk);\t//相当于跟用户态断开关系了\tsock_orphan(sk);\tlocal_bh_disable();\tbh_lock_sock(sk);\t/* remove backlog if any, without releasing ownership. */\t//处理backlog\t__release_sock(sk);\tthis_cpu_inc(tcp_orphan_count);\t/* Have we already been destroyed by a softirq or backlog? */\t//在其他上下文已经被close调了，这个是很有可能的情况吧？\tif (state != TCP_CLOSE &amp;&amp; sk-&gt;sk_state == TCP_CLOSE)\t\tgoto out;\t/*\tThis is a (useful) BSD violating of the RFC. There is a\t *\tproblem with TCP as specified in that the other end could\t *\tkeep a socket open forever with no application left this end.\t *\tWe use a 1 minute timeout (about the same as BSD) then kill\t *\tour end. If they send after that then tough - BUT: long enough\t *\tthat we won&#x27;t make the old 4*rto = almost no time - whoops\t *\treset mistake.\t *\t *\tNope, it was not mistake. It is really desired behaviour\t *\tf.e. on http servers, when such sockets are useless, but\t *\tconsume significant resources. Let&#x27;s do it with special\t *\tlinger2\toption.\t\t\t\t\t--ANK\t */\t//防止对端一直不发送fin起一个定时器，注意这里和软中断收报逻辑是类似的！！！注意区分linger2和solinger！\t//通常应该不会进入吧，\tif (sk-&gt;sk_state == TCP_FIN_WAIT2) &#123;\t\tstruct tcp_sock *tp = tcp_sk(sk);\t\tif (READ_ONCE(tp-&gt;linger2) &lt; 0) &#123;\t\t\ttcp_set_state(sk, TCP_CLOSE);\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\t\t\t__NET_INC_STATS(sock_net(sk),\t\t\t\t\tLINUX_MIB_TCPABORTONLINGER);\t\t&#125; else &#123;\t\t\t//和收包逻辑处理tmo相同，默认60s或者用户配置的一个时间。\t\t\tconst int tmo = tcp_fin_time(sk);\t\t\t//如果大于60s 就起一个保活定时器，里面有队finwait2的特殊处理\t\t\t//到期后进入定时器，这里注意超时时间是用户设置的时间减去一个60s\t\t\tif (tmo &gt; TCP_TIMEWAIT_LEN) &#123;\t\t\t\tinet_csk_reset_keepalive_timer(sk,\t\t\t\t\t\ttmo - TCP_TIMEWAIT_LEN);\t\t\t//默认的情况，这创建了tw套接字，并插入了ehash中，注意这里第二个参数是表示的是finwait2状态下创建的\t\t\t//如果收到了对端的fin则会找到tw套接字并判断substate之后重新启动tw 定时器\t\t\t// 否则这个定时器到期后没收到fin，也会直接释放资源\t\t\t&#125; else &#123;\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\t\t\t\tgoto out;\t\t\t&#125;\t\t&#125;\t&#125;\t//几乎不会发送rst吧\tif (sk-&gt;sk_state != TCP_CLOSE) &#123;\t\t//判断是否孤儿套接字太多了默认16k\t\tif (tcp_check_oom(sk, 0)) &#123;\t\t\ttcp_set_state(sk, TCP_CLOSE);\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\t\t\t__NET_INC_STATS(sock_net(sk),\t\t\t\t\tLINUX_MIB_TCPABORTONMEMORY);\t\t&#125; else if (!check_net(sock_net(sk))) &#123;\t\t\t/* Not possible to send reset; just close */\t\t\ttcp_set_state(sk, TCP_CLOSE);\t\t&#125;\t&#125;\t//close状态的处理\tif (sk-&gt;sk_state == TCP_CLOSE) &#123;\t\tstruct request_sock *req;\t\treq = rcu_dereference_protected(tcp_sk(sk)-&gt;fastopen_rsk,\t\t\t\t\t\tlockdep_sock_is_held(sk));\t\t/* We could get here with a non-NULL req if the socket is\t\t * aborted (e.g., closed with unread data) before 3WHS\t\t * finishes.\t\t */\t\tif (req)\t\t\treqsk_fastopen_remove(sk, req, false);\t\t//最终释放资源\t\tinet_csk_destroy_sock(sk);\t&#125;\t/* Otherwise, socket is reprieved until protocol close. */out:\tbh_unlock_sock(sk);\tlocal_bh_enable();&#125;\n\ntcp_close首先设置 shutdown标志，表示不在允许正常收发数据，如果关闭的是监听套接字或者套接字处于监听状态，则会尝试调用\ninet_csk_listen_stop迁移三次握手成功但是还未取走的sk（肯定是小概率情况，这里的处理逻辑类似三次握手监听套接字没了的情况）具体代码如下所示:\nvoid inet_csk_listen_stop(struct sock *sk)&#123;\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct request_sock_queue *queue = &amp;icsk-&gt;icsk_accept_queue;\tstruct request_sock *next, *req;\t/* Following specs, it would be better either to send FIN\t * (and enter FIN-WAIT-1, it is normal close)\t * or to send active reset (abort).\t * Certainly, it is pretty dangerous while synflood, but it is\t * bad justification for our negligence 8)\t * To be honest, we are not able to make either\t * of the variants now.\t\t\t--ANK\t */\t//循环遍历全连接队列\twhile ((req = reqsk_queue_remove(queue, sk)) != NULL) &#123;\t\tstruct sock *child = req-&gt;sk, *nsk;\t\tstruct request_sock *nreq;\t\tlocal_bh_disable();\t\tbh_lock_sock(child);\t\tWARN_ON(sock_owned_by_user(child));\t\tsock_hold(child);\t\t//如果启用了reuseport，看看能不能把连接给其他的sk\t\tnsk = reuseport_migrate_sock(sk, child, NULL);\t\tif (nsk) &#123;\t\t\t//这里的nreq是新listener的节点 req是旧listener的节点\t\t\tnreq = inet_reqsk_clone(req, nsk);\t\t\tif (nreq) &#123;\t\t\t\trefcount_set(&amp;nreq-&gt;rsk_refcnt, 1);\t\t\t\t//加到新listen sk的接收队列中\t\t\t\tif (inet_csk_reqsk_queue_add(nsk, nreq, child)) &#123;\t\t\t\t\t__NET_INC_STATS(sock_net(nsk),\t\t\t\t\t\t\tLINUX_MIB_TCPMIGRATEREQSUCCESS);\t\t\t\t\treqsk_migrate_reset(req);\t\t\t\t&#125; else &#123;\t\t\t\t\t__NET_INC_STATS(sock_net(nsk),\t\t\t\t\t\t\tLINUX_MIB_TCPMIGRATEREQFAILURE);\t\t\t\t\treqsk_migrate_reset(nreq);\t\t\t\t\t__reqsk_free(nreq);\t\t\t\t&#125;\t\t\t\t/* inet_csk_reqsk_queue_add() has already\t\t\t\t * called inet_child_forget() on failure case.\t\t\t\t */\t\t\t\t//不论时候加入新的listensk了 都跳过下面资源释放的逻辑，因为已经旧的已经释放了\t\t\t\tgoto skip_child_forget;\t\t\t&#125;\t\t&#125;\t\t//上面根本没有启用reuseport的情况,真正的释放资源。\t\tinet_child_forget(sk, req, child);skip_child_forget:\t\treqsk_put(req);\t\tbh_unlock_sock(child);\t\tlocal_bh_enable();\t\tsock_put(child);\t\tcond_resched();\t&#125;\tif (queue-&gt;fastopenq.rskq_rst_head) &#123;\t\t/* Free all the reqs queued in rskq_rst_head. */\t\tspin_lock_bh(&amp;queue-&gt;fastopenq.lock);\t\treq = queue-&gt;fastopenq.rskq_rst_head;\t\tqueue-&gt;fastopenq.rskq_rst_head = NULL;\t\tspin_unlock_bh(&amp;queue-&gt;fastopenq.lock);\t\twhile (req != NULL) &#123;\t\t\tnext = req-&gt;dl_next;\t\t\treqsk_put(req);\t\t\treq = next;\t\t&#125;\t&#125;\tWARN_ON_ONCE(sk-&gt;sk_ack_backlog);&#125;\n\ninet_csk_listen_stop中循环遍历全连接队列，取出没有被accept的套接字**，调用reuseport_migrate_sock尝试找到可以接管当前套接字的的监听套接字，并加入到这个监听套接字的接收队列中**，同时增加统计计数。\n如果没有启用reuseport则直接调用inet_child_forget释放这个三次握手成功但是没有被用户接收的sock（也就是child），具体代码如下所示：\nstatic void inet_child_forget(struct sock *sk, struct request_sock *req,\t\t\t      struct sock *child)&#123;\t//复位字段\tsk-&gt;sk_prot-&gt;disconnect(child, O_NONBLOCK);\t//孤儿套接字\tsock_orphan(child);\t//增加统计计数\tthis_cpu_inc(*sk-&gt;sk_prot-&gt;orphan_count);\tif (sk-&gt;sk_protocol == IPPROTO_TCP &amp;&amp; tcp_rsk(req)-&gt;tfo_listener) &#123;\t\tBUG_ON(rcu_access_pointer(tcp_sk(child)-&gt;fastopen_rsk) != req);\t\tBUG_ON(sk != req-&gt;rsk_listener);\t\t/* Paranoid, to prevent race condition if\t\t * an inbound pkt destined for child is\t\t * blocked by sock lock in tcp_v4_rcv().\t\t * Also to satisfy an assertion in\t\t * tcp_v4_destroy_sock().\t\t */\t\tRCU_INIT_POINTER(tcp_sk(child)-&gt;fastopen_rsk, NULL);\t&#125;\t//释放资源\tinet_csk_destroy_sock(child);&#125;\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP断连"]},{"title":"TCP四次挥手-tcp_close（二）","url":"/2026/01/22/TCP%E8%BF%9E%E6%8E%A5%E7%9A%84%E7%BB%88%E6%AD%A2%EF%BC%88%E4%BA%8C%EF%BC%89/","content":"**inet_child_forget首先调用TCP注册的disconnect释放资源（关闭定时器，清理重传队列等,重置字段），并大概率会发送一个rst**终止连接，具体代码如下所示:\nint tcp_disconnect(struct sock *sk, int flags)&#123;\tstruct inet_sock *inet = inet_sk(sk);\tstruct inet_connection_sock *icsk = inet_csk(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tint old_state = sk-&gt;sk_state;\tu32 seq;\tif (old_state != TCP_CLOSE)\t\ttcp_set_state(sk, TCP_CLOSE);\t/* ABORT function of RFC793 */\t//listen状态的处理\tif (old_ state == TCP_LISTEN) &#123;\t\tinet_csk_listen_stop(sk);\t//repair\t&#125; else if (unlikely(tp-&gt;repair)) &#123;\t\tWRITE_ONCE(sk-&gt;sk_err, ECONNABORTED);\t//建链或者断开连接或者syn——recv 会发rst，也就是说大概率发rst\t&#125; else if (tcp_need_reset(old_state) ||\t\t   (tp-&gt;snd_nxt != tp-&gt;write_seq &amp;&amp;\t\t    (1 &lt;&lt; old_state) &amp; (TCPF_CLOSING | TCPF_LAST_ACK))) &#123;\t\t/* The last check adjusts for discrepancy of Linux wrt. RFC\t\t * states\t\t */\t\ttcp_send_active_reset(sk, gfp_any());\t\tWRITE_ONCE(sk-&gt;sk_err, ECONNRESET);\t&#125; else if (old_state == TCP_SYN_SENT)\t\tWRITE_ONCE(sk-&gt;sk_err, ECONNRESET);\t//停掉所有的定时器\ttcp_clear_xmit_timers(sk);\t//清掉接收队列\t__skb_queue_purge(&amp;sk-&gt;sk_receive_queue);\tWRITE_ONCE(tp-&gt;copied_seq, tp-&gt;rcv_nxt);\tWRITE_ONCE(tp-&gt;urg_data, 0);\t//清发送队列\ttcp_write_queue_purge(sk);\ttcp_fastopen_active_disable_ofo_check(sk);\t//清重传队列\tskb_rbtree_purge(&amp;tp-&gt;out_of_order_queue);\tinet-&gt;inet_dport = 0;\t//对用户调用bind的情况，移除bhash2后复位ip地址并重新加入\tinet_bhash2_reset_saddr(sk);\t//清除shutdown\tWRITE_ONCE(sk-&gt;sk_shutdown, 0);\tsock_reset_flag(sk, SOCK_DONE);\t//rtt相关\ttp-&gt;srtt_us = 0;\ttp-&gt;mdev_us = jiffies_to_usecs(TCP_TIMEOUT_INIT);\ttp-&gt;rcv_rtt_last_tsecr = 0;\t//让新发送的序列号离当前序列号远一点\tseq = tp-&gt;write_seq + tp-&gt;max_window + 2;\tif (!seq)\t\tseq = 1;\tWRITE_ONCE(tp-&gt;write_seq, seq);\t//rto  延迟ack\ticsk-&gt;icsk_backoff = 0;\ticsk-&gt;icsk_probes_out = 0;\ticsk-&gt;icsk_probes_tstamp = 0;\ticsk-&gt;icsk_rto = TCP_TIMEOUT_INIT;\ticsk-&gt;icsk_rto_min = TCP_RTO_MIN;\ticsk-&gt;icsk_delack_max = TCP_DELACK_MAX;\t//拥塞\ttp-&gt;snd_ssthresh = TCP_INFINITE_SSTHRESH;\ttcp_snd_cwnd_set(tp, TCP_INIT_CWND);\ttp-&gt;snd_cwnd_cnt = 0;\ttp-&gt;is_cwnd_limited = 0;\ttp-&gt;max_packets_out = 0;\ttp-&gt;window_clamp = 0;\ttp-&gt;delivered = 0;\ttp-&gt;delivered_ce = 0;\tif (icsk-&gt;icsk_ca_ops-&gt;release)\t\ticsk-&gt;icsk_ca_ops-&gt;release(sk);\tmemset(icsk-&gt;icsk_ca_priv, 0, sizeof(icsk-&gt;icsk_ca_priv));\ticsk-&gt;icsk_ca_initialized = 0;\ttcp_set_ca_state(sk, TCP_CA_Open);\ttp-&gt;is_sack_reneg = 0;\ttcp_clear_retrans(tp);\ttp-&gt;total_retrans = 0;\tinet_csk_delack_init(sk);\t/* Initialize rcv_mss to TCP_MIN_MSS to avoid division by 0\t * issue in __tcp_select_window()\t */\ticsk-&gt;icsk_ack.rcv_mss = TCP_MIN_MSS;\tmemset(&amp;tp-&gt;rx_opt, 0, sizeof(tp-&gt;rx_opt));\t//清除路由\t__sk_dst_reset(sk);\tdst_release(xchg((__force struct dst_entry **)&amp;sk-&gt;sk_rx_dst, NULL));\ttcp_saved_syn_free(tp);\t//清除统计计数\ttp-&gt;compressed_ack = 0;\ttp-&gt;segs_in = 0;\ttp-&gt;segs_out = 0;\ttp-&gt;bytes_sent = 0;\ttp-&gt;bytes_acked = 0;\ttp-&gt;bytes_received = 0;\ttp-&gt;bytes_retrans = 0;\ttp-&gt;data_segs_in = 0;\ttp-&gt;data_segs_out = 0;\ttp-&gt;duplicate_sack[0].start_seq = 0;\ttp-&gt;duplicate_sack[0].end_seq = 0;\ttp-&gt;dsack_dups = 0;\ttp-&gt;reord_seen = 0;\ttp-&gt;retrans_out = 0;\ttp-&gt;sacked_out = 0;\ttp-&gt;tlp_high_seq = 0;\ttp-&gt;last_oow_ack_time = 0;\ttp-&gt;plb_rehash = 0;\t/* There&#x27;s a bubble in the pipe until at least the first ACK. */\ttp-&gt;app_limited = ~0U;\ttp-&gt;rate_app_limited = 1;\ttp-&gt;rack.mstamp = 0;\ttp-&gt;rack.advanced = 0;\ttp-&gt;rack.reo_wnd_steps = 1;\ttp-&gt;rack.last_delivered = 0;\ttp-&gt;rack.reo_wnd_persist = 0;\ttp-&gt;rack.dsack_seen = 0;\ttp-&gt;syn_data_acked = 0;\ttp-&gt;rx_opt.saw_tstamp = 0;\ttp-&gt;rx_opt.dsack = 0;\ttp-&gt;rx_opt.num_sacks = 0;\ttp-&gt;rcv_ooopack = 0;\t/* Clean up fastopen related fields */\ttcp_free_fastopen_req(tp);\tinet_clear_bit(DEFER_CONNECT, sk);\ttp-&gt;fastopen_client_fail = 0;\tWARN_ON(inet-&gt;inet_num &amp;&amp; !icsk-&gt;icsk_bind_hash);\t//释放管理的页\tif (sk-&gt;sk_frag.page) &#123;\t\tput_page(sk-&gt;sk_frag.page);\t\tsk-&gt;sk_frag.page = NULL;\t\tsk-&gt;sk_frag.offset = 0;\t&#125;\t//上送错误\tsk_error_report(sk);\treturn 0;&#125;\n\ntcp_disconnect核心思想就是根据当前sk状态决定是否发送rst（例如ESTABLISHED，SYN_RECV四次挥手中间状态等 ）然后无论是否发包，都清理与旧连接相关的定时器，清理接收&#x2F;发送&#x2F;乱序队列，清除远端端口并重置 bhash2 中的源地址关联，重置 shutdown 标志，接着重新设置序列号避旧包污染，恢复 RTO&#x2F;RTT&#x2F;延迟 ACK 和拥塞控制到初始值，清空 SACK&#x2F;RACK&#x2F;重传与各种统计计数，释放路由缓存，最后释放临时页并向用户上报错误。\n回到tcp_close中，如果处理的不是listen状态的套接字（大概率都不是），则遍历接收队列记录未读取的字节数并释放数据包（后续会决定是否发送rst），之后判断套接字是否已经被关闭了，比如收到了或者发送了rst？之后判断是否存在未被读取的数据，如果有则不会走正常的四次挥手，而是发送rst，如果用户设置了linger选项同时linger=0则调用disconnect，表示不走四次挥手，而是直接发送RST终止连接。\n如果不满足上述条件，则走正常挥手发送fin包的流程，首先调用tcp_close_state将状态切换为FIN_WAIT1，之后调用tcp_send_fin发送FIN包，具体代码如下所示：\nvoid  tcp_send_fin(struct sock *sk)&#123;\tstruct sk_buff *skb, *tskb, *tail = tcp_write_queue_tail(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\t/* Optimization, tack on the FIN if we have one skb in write queue and\t * this skb was not yet sent, or we are under memory pressure.\t * Note: in the latter case, FIN packet will be sent after a timeout,\t * as TCP stack thinks it has already been transmitted.\t */\t//首先从发送队列尾部尝试获取，如果没有且在内存压力之下则直接\t//从重传队列中获取skb\ttskb = tail;\tif (!tskb &amp;&amp; tcp_under_memory_pressure(sk))\t\ttskb = skb_rb_last(&amp;sk-&gt;tcp_rtx_queue);\t//拿到了一个skb则加上fin标志 同时增加一个序列号\tif (tskb) &#123;\t\tTCP_SKB_CB(tskb)-&gt;tcp_flags |= TCPHDR_FIN;\t\tTCP_SKB_CB(tskb)-&gt;end_seq++;\t\ttp-&gt;write_seq++;\t\tif (!tail) &#123;\t\t\t/* This means tskb was already sent.\t\t\t * Pretend we included the FIN on previous transmit.\t\t\t * We need to set tp-&gt;snd_nxt to the value it would have\t\t\t * if FIN had been sent. This is because retransmit path\t\t\t * does not change tp-&gt;snd_nxt.\t\t\t */\t\t\tWRITE_ONCE(tp-&gt;snd_nxt, tp-&gt;snd_nxt + 1);\t\t\treturn;\t\t&#125;\t&#125; else &#123;\t//没有获取到可以用的skb则申请一个skb\t\tskb = alloc_skb_fclone(MAX_TCP_HEADER, sk-&gt;sk_allocation);\t\tif (unlikely(!skb))\t\t\treturn;\t\tINIT_LIST_HEAD(&amp;skb-&gt;tcp_tsorted_anchor);\t\t//resever一下\t\tskb_reserve(skb, MAX_TCP_HEADER);\t\tsk_forced_mem_schedule(sk, skb-&gt;truesize);\t\t/* FIN eats a sequence byte, write_seq advanced by tcp_queue_skb(). */\t\t//初始化数据\t\ttcp_init_nondata_skb(skb, tp-&gt;write_seq,\t\t\t\t     TCPHDR_ACK | TCPHDR_FIN);\t\t//数据包入队列\t\ttcp_queue_skb(sk, skb);\t&#125;\t//发送\t__tcp_push_pending_frames(sk, tcp_current_mss(sk), TCP_NAGLE_OFF);&#125;\n\n tcp_send_fin主要逻辑就是在关闭连接时发送 FIN（占用 1 个序列号字节）。首先尽量把 FIN 挂在现有的最后一个发送 skb 上，如果没有则会申请并构造一个带 FIN 标志的 skb，并发送出去。\n回到tcp_close中发送完fin包之后会调用sk_stream_wait_close睡眠（如果用户设置了需要等待的话），睡眠最大的意义应该是希望发送队列中的数据被发送完？？？ 之后调用sock_orphan将套接字设置为SOCK_DEAD（收包逻辑中会针对这个状态处理）此时套接字与进程上下文就脱离关系了。\n接下来处理后备队列中的数据包（重走收报逻辑，后面会释放资源），同时增加孤儿套接字统计计数，之后进一步判断当前套接字是否处于关闭状态了，由于在软中断上下文中可能已经关闭了套接字(感觉这种概率并不大，但是如果上面睡眠了的话是有可能的)。\n之后判断是否处于FIN_WAIT2状态，当发送的fin被对端确认后会进入该状态，（该状态下如果对端一直不发送fin包则本端会一直停留在该状态，为避免这种情况发送，所以在收到ack后会起一个定时器，或者直接回rst），如果用户设置了linger2的时间的话可能会直接发送一个rst，或者可能会创建一个tw状态的套接字，同时把原来的sk从ehash中移除，并启动一个定时器注意这里不是启动tw定时器，应该叫FIN_WAIT2定时器（复用了保活的定时器），因为如果收包路径中收到了对端的fin则会找到该tw套接字并判断substate之后启动真正的tw 定时器。\n之后进一步根据当前的状态进行判断，如果处于close状态了，但是孤儿套接字太多或者oom则会直接发送rst。\n如果处于close状态，则会调用inet_csk_destroy_sock完成真正释放资源的动作，inet_csk_destroy_sock具体代码如下所示：\nvoid inet_csk_destroy_sock(struct sock *sk)&#123;\tWARN_ON(sk-&gt;sk_state != TCP_CLOSE);\tWARN_ON(!sock_flag(sk, SOCK_DEAD));\t/* It cannot be in hash table! */\tWARN_ON(!sk_unhashed(sk));\t/* If it has not 0 inet_sk(sk)-&gt;inet_num, it must be bound */\tWARN_ON(inet_sk(sk)-&gt;inet_num &amp;&amp; !inet_csk(sk)-&gt;icsk_bind_hash);\tsk-&gt;sk_prot-&gt;destroy(sk);\t//清除接收，发送，以及err队列\tsk_stream_kill_queues(sk);\txfrm_sk_free_policy(sk);\t//释放之前先--\tthis_cpu_dec(*sk-&gt;sk_prot-&gt;orphan_count);\t//释放sk\tsock_put(sk);&#125;\n\n上述代码首先调用destroy回调关闭定时器，清除重传队列，发送队列，乱序队列，并包含处理TFO相关的逻辑，具体代码如下所示：\nvoid tcp_v4_destroy_sock(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\ttrace_tcp_destroy_sock(sk);\t//停调所有定时器\ttcp_clear_xmit_timers(sk);\t//调用拥塞算法的钩子\ttcp_cleanup_congestion_control(sk);\ttcp_cleanup_ulp(sk);\t/* Cleanup up the write buffer. */\t//清除重传队列和发送队列\ttcp_write_queue_purge(sk);\t/* Check if we want to disable active TFO */\ttcp_fastopen_active_disable_ofo_check(sk);\t/* Cleans up our, hopefully empty, out_of_order_queue. */\t//清除乱序队列\tskb_rbtree_purge(&amp;tp-&gt;out_of_order_queue);#ifdef CONFIG_TCP_MD5SIG\t/* Clean up the MD5 key list, if any */\tif (tp-&gt;md5sig_info) &#123;\t\ttcp_clear_md5_list(sk);\t\tkfree_rcu(rcu_dereference_protected(tp-&gt;md5sig_info, 1), rcu);\t\ttp-&gt;md5sig_info = NULL;\t\tstatic_branch_slow_dec_deferred(&amp;tcp_md5_needed);\t&#125;#endif\t/* Clean up a referenced TCP bind bucket. */\tif (inet_csk(sk)-&gt;icsk_bind_hash)\t\tinet_put_port(sk);\tBUG_ON(rcu_access_pointer(tp-&gt;fastopen_rsk));\t/* If socket is aborted during connect operation */\ttcp_free_fastopen_req(tp);\ttcp_fastopen_destroy_cipher(sk);\ttcp_saved_syn_free(tp);\t//更新计数\tsk_sockets_allocated_dec(sk);&#125;\n\n调用tcp_v4_destroy_sock之后会进一步清除接收队列和错误队列，最后调用sock_put真正的释放sock：-）。\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP断连"]},{"title":"TCP四次挥手-被动关闭","url":"/2026/01/24/TCP%E8%BF%9E%E6%8E%A5%E7%9A%84%E7%BB%88%E6%AD%A2%EF%BC%88%E4%B8%89%EF%BC%89/","content":"被动关闭方在处于ESTABLISHED状态时，收到的TCP段都由tcp_rcv_established来处理，若此时收到主动关闭方的FIN段无法命中快速路径（因为标志中不包含fin）所以必然走慢速路径，前文分析过慢速路径如果序列号是预期也会直接放到接收队列，否则放入乱序队列。在接收队列的处理逻辑中会判断是否携带fin标志，如果携带fin标志则会调用tcp_fin进行处理，具体代码如下所示 :\nstatic void tcp_data_queue(struct sock *sk, struct sk_buff *skb)&#123;\t...\tif (TCP_SKB_CB(skb)-&gt;seq == tp-&gt;rcv_nxt) &#123;\t...\t\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN)\t\t//四次挥手fin包处理逻辑\t\t\ttcp_fin(sk);\t...\t&#125;\t...\ttcp_data_queue_ofo(sk, skb);&#125;\n\nvoid tcp_fin(struct sock *sk)&#123;\tstruct tcp_sock *tp = tcp_sk(sk);\t//收到了fin包肯定需要确认\tinet_csk_schedule_ack(sk);\t//接收方向shutdown 因为对端关闭了,进程上下文从接收队列获取数据的时候会判断\tWRITE_ONCE(sk-&gt;sk_shutdown, sk-&gt;sk_shutdown | RCV_SHUTDOWN);\t//\tsock_set_flag(sk, SOCK_DONE);\tswitch (sk-&gt;sk_state) &#123;\tcase TCP_SYN_RECV:\tcase TCP_ESTABLISHED:\t\t/* Move to CLOSE_WAIT */\t\t//建连接或者客户端已经认为建立连接成功的情况下进入closewait状态\t\ttcp_set_state(sk, TCP_CLOSE_WAIT);\t\t//进入pingpong模式，因为四次挥手对时延要求高，所以回复ack要快\t\tinet_csk_enter_pingpong_mode(sk);\t\tbreak;\t//认为时重传的ifn不处理\tcase TCP_CLOSE_WAIT:\tcase TCP_CLOSING:\t\t/* Received a retransmission of the FIN, do\t\t * nothing.\t\t */\t\tbreak;\tcase TCP_LAST_ACK:\t\t/* RFC793: Remain in the LAST-ACK state. */\t\tbreak;\t//发送fin后收到fin表示同时关闭\tcase TCP_FIN_WAIT1:\t\t/* This case occurs when a simultaneous close\t\t * happens, we must ack the received FIN and\t\t * enter the CLOSING state.\t\t */\t\t//发送ack\t\ttcp_send_ack(sk);\t\ttcp_set_state(sk, TCP_CLOSING);\t\tbreak;\tcase TCP_FIN_WAIT2:\t\t/* Received a FIN -- send ACK and enter TIME_WAIT. */\t\t//主动关闭的一方收到了对端的fin，发送最后一个ack 并进入tw状态，合理\t\ttcp_send_ack(sk);\t\ttcp_time_wait(sk, TCP_TIME_WAIT, 0);\t\tbreak;\tdefault:\t\t/* Only TCP_LISTEN and TCP_CLOSE are left, in these\t\t * cases we should never reach this piece of code.\t\t */\t\tpr_err(&quot;%s: Impossible, sk-&gt;sk_state=%d\\n&quot;,\t\t       __func__, sk-&gt;sk_state);\t\tbreak;\t&#125;\t/* It _is_ possible, that we have something out-of-order _after_ FIN.\t * Probably, we should reset in this case. For now drop them.\t */\t//清理乱序队列，概率很小吧，都发送fin了后面还有数据&gt;\tskb_rbtree_purge(&amp;tp-&gt;out_of_order_queue);\tif (tcp_is_sack(tp))\t//复位sack的信息\t\ttcp_sack_reset(&amp;tp-&gt;rx_opt);\t//如果没死，则唤醒用户的进程\tif (!sock_flag(sk, SOCK_DEAD)) &#123;\t\tsk-&gt;sk_state_change(sk);\t\t/* Do not send POLL_HUP for half duplex close. */\t\tif (sk-&gt;sk_shutdown == SHUTDOWN_MASK ||\t\t    sk-&gt;sk_state == TCP_CLOSE)\t\t\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_HUP);//两个方向都关闭\t\telse\t\t\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN);//关闭了一个方向\t&#125;&#125;\n\n这里重点关注建连状态下的处理，tcp_fin中首先设置需要发送ack的标志位（由于收到了fin包需要确认，注意这里其实就是第二次挥手发送的ack报文），并处理了关闭流程中不同状态的迁移，之后设置 RCV_SHUTDOWN，表示发送端已经关闭，之后将状态设置为TCP_CLOSE_WAIT（应用程序程序读取数据时发现设置该标志位会立刻返回），并进入pingpong模式（快速回复ack）。最后完成乱序队列的清理，并复位sack的信息。\n上述代码可以看到，当被动关闭方收到主动关闭方发送的fin后设置了sk的状态为SOCK_DONE，并且设置了RCV_SHUTDOWN标志，表示发送端已经关闭，此时用户程序调用recv时会发现 数据包携带fin（注意fin包是会放到接收队列中的）标志位会停止继续拷贝数据，如果拷贝的长度为0，或者下一次调用会发现设置了shutdown标志位，都会返回用户0，通知用户对端已经关闭，具体代码在前文tcp_recvmsg中分析过，这里不在重复，对应处理逻辑如下所示：\nstatic int tcp_recvmsg_locked(struct sock *sk, struct msghdr *msg, size_t len,\t\t\t      int flags, struct scm_timestamping_internal *tss,\t\t\t      int *cmsg_flags)&#123;\t...\t//不断从 sk_receive_queue 找到当前 seq 对应的 skb，拷贝数据\tdo &#123;\t\t/* Next get a buffer. */\t\t//获取当前队列尾部的skb\t\tlast = skb_peek_tail(&amp;sk-&gt;sk_receive_queue);\t\tskb_queue_walk(&amp;sk-&gt;sk_receive_queue, skb) &#123;\t\t...\t\t\t//数据包中存在fin标志\t\t\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN)\t\t\t\tgoto found_fin_ok;\t\t&#125;\t\t/* Well, if we have backlog, try to process it now yet. */\t\t//如果没找到skb 同时没有读够数据，且backlog为空，直接返回\t\tif (copied &gt;= target &amp;&amp; !READ_ONCE(sk-&gt;sk_backlog.tail))\t\t\tbreak;\t\t//已经读了一些数据\t\tif (copied) &#123;\t\t\tif (!timeo ||\t\t\t\t\t\t\t//非阻塞\t\t\t    sk-&gt;sk_err ||\t\t\t\t\t\t//存在错误\t\t\t    sk-&gt;sk_state == TCP_CLOSE ||\t\t//已经close\t\t\t    (sk-&gt;sk_shutdown &amp; RCV_SHUTDOWN) ||\t//被动关闭\t\t\t    signal_pending(current))\t\t\t//ctrl c？？\t\t\t\tbreak;\t\t&#125; else &#123;\t\t\tif (sock_flag(sk, SOCK_DONE))\t\t//比如说主动关闭 或被动收到fin\t\t\t\tbreak;\t\t\tif (sk-&gt;sk_err) &#123;\t\t\t\t\t\t//比如说收到rst，icmp\t\t\t\tcopied = sock_error(sk);\t\t\t\tbreak;\t\t\t&#125;\t\t\tif (sk-&gt;sk_shutdown &amp; RCV_SHUTDOWN)\t\t //被动关闭\t\t\t\tbreak;\t...found_fin_ok:\t\t/* Process the FIN. */\t\t//处理fin包\t\tWRITE_ONCE(*seq, *seq + 1);\t\tif (!(flags &amp; MSG_PEEK))\t\t\ttcp_eat_recv_skb(sk, skb);\t\tbreak;\t&#125; while (len &gt; 0);\t...\ttcp_cleanup_rbuf(sk, copied);\treturn copied;\t...&#125;\n\n上述代码为被动关闭方收到fin包的处理逻辑，用户会通过recv返回0来感知对端已经关闭连接，此时仍是半关闭状态，此时用户程序通常会发完需要待发送的数之后调用close(如果应用程序不调用close，对端也会起一个定时器防止本端不发送fin包)，也就是前文分析的tcp_close，区别是此时调用tcp_close的状态时TCP_CLOSE_WAIT。\n**被动关闭方应用程序调用close后的行为与主动关闭方类似，也是判断是否由有未读数据，是否有linger选项，还是走正常关闭，不同的地方在于状态由CLOSE_WAIT 变成 LAST_ACK（发送FIN等待对端ack）**具体代码如下所示：\n//注意这里传入的第二个参数就是solinger设置的超时时间void __tcp_close(struct sock *sk, long timeout)&#123;\t...\t//遍历接收队列，判断是否存在未接收的数据，记录字节数\twhile ((skb = __skb_dequeue(&amp;sk-&gt;sk_receive_queue)) != NULL) &#123;\t\tu32 len = TCP_SKB_CB(skb)-&gt;end_seq - TCP_SKB_CB(skb)-&gt;seq;\t\tif (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN)\t\t\tlen--;\t\tdata_was_unread += len;\t\t__kfree_skb(skb);\t&#125;\t...\t//repaire模式\tif (unlikely(tcp_sk(sk)-&gt;repair)) &#123;\t\tsk-&gt;sk_prot-&gt;disconnect(sk, 0);\t//存在未读的数据，不能让对端认为数据包已经收到了（发fin表示正常接收）\t&#125; else if (data_was_unread) &#123;\t\t/* Unread data was tossed, zap the connection. */\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONCLOSE);\t\ttcp_set_state(sk, TCP_CLOSE);\t\t//直接发rst\t\ttcp_send_active_reset(sk, sk-&gt;sk_allocation);\t//用户启用了so linger选项但是超时时间为0 大概率直接发rst\t&#125; else if (sock_flag(sk, SOCK_LINGER) &amp;&amp; !sk-&gt;sk_lingertime) &#123;\t\t/* Check zero linger _after_ checking for unread data. */\t\tsk-&gt;sk_prot-&gt;disconnect(sk, 0);\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\t//正常情况，设置为下个状态发送fin包\t&#125; else if (tcp_close_state(sk)) &#123;\t\t//发送fin包\t\ttcp_send_fin(sk);\t&#125;\t...\t//close状态的处理\tif (sk-&gt;sk_state == TCP_CLOSE) &#123;\t\tstruct request_sock *req;\t\treq = rcu_dereference_protected(tcp_sk(sk)-&gt;fastopen_rsk,\t\t\t\t\t\tlockdep_sock_is_held(sk));\t\t/* We could get here with a non-NULL req if the socket is\t\t * aborted (e.g., closed with unread data) before 3WHS\t\t * finishes.\t\t */\t\tif (req)\t\t\treqsk_fastopen_remove(sk, req, false);\t\t//最终释放资源\t\tinet_csk_destroy_sock(sk);\t&#125;\t/* Otherwise, socket is reprieved until protocol close. */out:\tbh_unlock_sock(sk);\tlocal_bh_enable();&#125;\n\n当被动关闭方发送fin包之后进入到了LAST_ACK状态，此时需要等待主动关闭方发送最后一个ack，当收到最后一个ack后会tcp_rcv_state_process交由对应的LAST_ACK状态进行处理，具体代码如下所示：\nint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)&#123;\t...\tcase TCP_LAST_ACK:\t\t//对方ack了发送的fin，四次挥手最后一个数据包已经接收到了\t\tif (tp-&gt;snd_una == tp-&gt;write_seq) &#123;\t\t\ttcp_update_metrics(sk);//保存本次连接的一些信息，方便指导下次\t\t\ttcp_done(sk);\t\t\tgoto consume;\t\t&#125;\t\tbreak;\t&#125;\t...&#125;\n\n如果当前下一个待发送的序列号等于已发送未确认的的序列号，则表示fin包已经被确认了，首先调用tcp_update_metrics记录本次连接的信息（下次三次握手完成会用此次保存的信息做指导），具体代码如下所示：\n//TCP连接结束的时候把学到的信息保存起来，为后续做参考（三次握手会用到）void tcp_update_metrics(struct sock *sk)&#123;\tconst struct inet_connection_sock *icsk = inet_csk(sk);\tstruct dst_entry *dst = __sk_dst_get(sk);\tstruct tcp_sock *tp = tcp_sk(sk);\tstruct net *net = sock_net(sk);\tstruct tcp_metrics_block *tm;\tunsigned long rtt;\tu32 val;\tint m;\tsk_dst_confirm(sk);\t//没开启这个选项，直接返回\tif (READ_ONCE(net-&gt;ipv4.sysctl_tcp_nometrics_save) || !dst)\t\treturn;\trcu_read_lock();\t//发生过指数退避或者没有成功估计rtt\tif (icsk-&gt;icsk_backoff || !tp-&gt;srtt_us) &#123;\t\t/* This session failed to estimate rtt. Why?\t\t * Probably, no packets returned in time.  Reset our\t\t * results.\t\t */\t\t//获取对应的tm\t\ttm = tcp_get_metrics(sk, dst, false);\t\tif (tm &amp;&amp; !tcp_metric_locked(tm, TCP_METRIC_RTT))\t\t\ttcp_metric_set(tm, TCP_METRIC_RTT, 0);//设置为0\t\tgoto out_unlock;\t&#125; else\t\t//获取tm结构\t\ttm = tcp_get_metrics(sk, dst, true);\tif (!tm)\t\tgoto out_unlock;\t//获取之之前的rtt\trtt = tcp_metric_get(tm, TCP_METRIC_RTT);\t//计算两次的差值\tm = rtt - tp-&gt;srtt_us;\t/* If newly calculated rtt larger than stored one, store new\t * one. Otherwise, use EWMA. Remember, rtt overestimation is\t * always better than underestimation.\t */\t///重新设置rtt，核心思想是rtt高估要比低估好\tif (!tcp_metric_locked(tm, TCP_METRIC_RTT)) &#123;\t\tif (m &lt;= 0)\t\t\trtt = tp-&gt;srtt_us;\t\telse\t\t\trtt -= (m &gt;&gt; 3);\t\ttcp_metric_set(tm, TCP_METRIC_RTT, rtt);\t&#125;\t//更新rtt方差\tif (!tcp_metric_locked(tm, TCP_METRIC_RTTVAR)) &#123;\t\tunsigned long var;\t\tif (m &lt; 0)\t\t\tm = -m;\t\t/* Scale deviation to rttvar fixed point */\t\tm &gt;&gt;= 1;\t\tif (m &lt; tp-&gt;mdev_us)\t\t\tm = tp-&gt;mdev_us;\t\tvar = tcp_metric_get(tm, TCP_METRIC_RTTVAR);\t\tif (m &gt;= var)\t\t\tvar = m;\t\telse\t\t\tvar -= (var - m) &gt;&gt; 2;\t\ttcp_metric_set(tm, TCP_METRIC_RTTVAR, var);\t&#125;\t//还在慢启动阶段的话\tif (tcp_in_initial_slowstart(tp)) &#123;\t\t/* Slow start still did not finish. */\t\tif (!READ_ONCE(net-&gt;ipv4.sysctl_tcp_no_ssthresh_metrics_save) &amp;&amp;\t\t    !tcp_metric_locked(tm, TCP_METRIC_SSTHRESH)) &#123;\t\t\t//保存慢启动阈值，cwnd/2\t\t\tval = tcp_metric_get(tm, TCP_METRIC_SSTHRESH);\t\t\tif (val &amp;&amp; (tcp_snd_cwnd(tp) &gt;&gt; 1) &gt; val)\t\t\t\ttcp_metric_set(tm, TCP_METRIC_SSTHRESH,\t\t\t\t\t       tcp_snd_cwnd(tp) &gt;&gt; 1);\t\t&#125;\t\tif (!tcp_metric_locked(tm, TCP_METRIC_CWND)) &#123;\t\t\t//保存拥塞窗口,使用本次的\t\t\tval = tcp_metric_get(tm, TCP_METRIC_CWND);\t\t\tif (tcp_snd_cwnd(tp) &gt; val)\t\t\t\ttcp_metric_set(tm, TCP_METRIC_CWND,\t\t\t\t\t       tcp_snd_cwnd(tp));\t\t&#125;\t//拥塞避免\t&#125; else if (!tcp_in_slow_start(tp) &amp;&amp;\t\t   icsk-&gt;icsk_ca_state == TCP_CA_Open) &#123;\t\t/* Cong. avoidance phase, cwnd is reliable. */\t\tif (!READ_ONCE(net-&gt;ipv4.sysctl_tcp_no_ssthresh_metrics_save) &amp;&amp;\t\t    !tcp_metric_locked(tm, TCP_METRIC_SSTHRESH))\t\t\t//max(cwnd/2, snd_ssthresh)\t\t\ttcp_metric_set(tm, TCP_METRIC_SSTHRESH,\t\t\t\t       max(tcp_snd_cwnd(tp) &gt;&gt; 1, tp-&gt;snd_ssthresh));\t\t\t//(old + new)/2\t\tif (!tcp_metric_locked(tm, TCP_METRIC_CWND)) &#123;\t\t\tval = tcp_metric_get(tm, TCP_METRIC_CWND);\t\t\ttcp_metric_set(tm, TCP_METRIC_CWND, (val + tcp_snd_cwnd(tp)) &gt;&gt; 1);\t\t&#125;\t//丢包，等其他情况\t&#125; else &#123;\t\t/* Else slow start did not finish, cwnd is non-sense,\t\t * ssthresh may be also invalid.\t\t */\t\tif (!tcp_metric_locked(tm, TCP_METRIC_CWND)) &#123;\t\t\tval = tcp_metric_get(tm, TCP_METRIC_CWND);\t\t\t//new_cwnd_metric = (old_cwnd + tp-&gt;snd_ssthresh) &gt;&gt; 1;\t\t\ttcp_metric_set(tm, TCP_METRIC_CWND,\t\t\t\t       (val + tp-&gt;snd_ssthresh) &gt;&gt; 1);\t\t&#125;\t\t//更新 SSTHRESH：只在新值更大时更新\t\tif (!READ_ONCE(net-&gt;ipv4.sysctl_tcp_no_ssthresh_metrics_save) &amp;&amp;\t\t    !tcp_metric_locked(tm, TCP_METRIC_SSTHRESH)) &#123;\t\t\tval = tcp_metric_get(tm, TCP_METRIC_SSTHRESH);\t\t\tif (val &amp;&amp; tp-&gt;snd_ssthresh &gt; val)\t\t\t\ttcp_metric_set(tm, TCP_METRIC_SSTHRESH,\t\t\t\t\t       tp-&gt;snd_ssthresh);\t\t&#125;\t\t//更新乱序容忍\t\tif (!tcp_metric_locked(tm, TCP_METRIC_REORDERING)) &#123;\t\t\tval = tcp_metric_get(tm, TCP_METRIC_REORDERING);\t\t\tif (val &lt; tp-&gt;reordering &amp;&amp;\t\t\t    tp-&gt;reordering !=\t\t\t    READ_ONCE(net-&gt;ipv4.sysctl_tcp_reordering))\t\t\t\ttcp_metric_set(tm, TCP_METRIC_REORDERING,\t\t\t\t\t       tp-&gt;reordering);\t\t&#125;\t&#125;\tWRITE_ONCE(tm-&gt;tcpm_stamp, jiffies);out_unlock:\trcu_read_unlock();&#125;\n\ntcp_update_metrics的核心思想就是根据当前拥塞状态机的不同阶段尝试将慢启动阈值，拥塞窗口和rtt记录到metris这个全局结构中，下次同一条流在进行相同连接的时候（三次握手完成后）会使用该信息指导拥塞相关字段的设置。\n保存本次连接信息完成之后，调用tcp_done完成资源的释放具体代码如下所示：\nvoid tcp_done(struct sock *sk)&#123;\tstruct request_sock *req;\t/* We might be called with a new socket, after\t * inet_csk_prepare_forced_close() has been called\t * so we can not use lockdep_sock_is_held(sk)\t */\treq = rcu_dereference_protected(tcp_sk(sk)-&gt;fastopen_rsk, 1);\tif (sk-&gt;sk_state == TCP_SYN_SENT || sk-&gt;sk_state == TCP_SYN_RECV)\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ATTEMPTFAILS);\t//设置转状态为close\ttcp_set_state(sk, TCP_CLOSE);\t//关闭定时器\ttcp_clear_xmit_timers(sk);\tif (req)\t\treqsk_fastopen_remove(sk, req, false);\tWRITE_ONCE(sk-&gt;sk_shutdown, SHUTDOWN_MASK);\tif (!sock_flag(sk, SOCK_DEAD))\t\tsk-&gt;sk_state_change(sk);\telse\t\tinet_csk_destroy_sock(sk);//释放资源&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP断连"]},{"title":"TCP四次挥手-主动关闭（一）","url":"/2026/01/26/TCP%E8%BF%9E%E6%8E%A5%E7%9A%84%E7%BB%88%E6%AD%A2%EF%BC%88%E5%9B%9B%EF%BC%89/","content":"在TCP连接终止的过程中，主动关闭连接的一方会经历如下所示的四个状态：\nESTABLISHED    ↓ close()FIN_WAIT_1    ↓ 收到 ACK(对方确认FIN)FIN_WAIT_2    ↓ 收到 对方FINTIME_WAIT   ← 核心    ↓ 2MSLCLOSED\n\n主动关闭方发送fin段之后会进入到FIN_WAIT_1状态，如果收到了fin的确认则状态迁移到FIN_WAIT_2，并有可能创建tw套接字（也有可能不创建，取决于用户配置，也取决于系统当前tw套接字数量），如果在FIN_WAIT_2状态下收到了对端的fin则会发送ack并启动TIME_WAIT定时器。TIME_WAIT状态存在的原因其实就一个，防止旧的连接干扰新的连接。\n由于主动关闭方发送fin段之后由于已经不是建立连接状态，因此接收到的数据包都有由cp_rcv_state_process处理，这里重点分析发送fin段之后进入FIN_WAIT_1状态，等待对端确认的逻辑，具体代码如下所示：\nint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)&#123;\t...\tswitch (sk-&gt;sk_state) &#123;\t...\tcase TCP_FIN_WAIT1: &#123;\t\tint tmo;\t\tif (req)\t\t\ttcp_rcv_synrecv_state_fastopen(sk);\t\t//这如果break了，表示对端还没有收到本段发出的fin\t\tif (tp-&gt;snd_una != tp-&gt;write_seq)\t\t\tbreak;\t\t//对端确认了本端发送的fin，这里设置为fin_wait2状态\t\ttcp_set_state(sk, TCP_FIN_WAIT2);\t\tWRITE_ONCE(sk-&gt;sk_shutdown, sk-&gt;sk_shutdown | SEND_SHUTDOWN);\t\tsk_dst_confirm(sk);\t\t//如果在close中还没有设置dead 那这里就直接推出了！ 什么情况下还么没有设置dead？ 当启用linger的时候（注意区分linger2）\t\tif (!sock_flag(sk, SOCK_DEAD)) &#123;\t\t\t/* Wake up lingering close() */\t\t\tsk-&gt;sk_state_change(sk);\t\t\tbreak;\t\t&#125;\t\tif (READ_ONCE(tp-&gt;linger2) &lt; 0) &#123;\t\t\ttcp_done(sk);\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\t\t\treturn 1;\t\t&#125;\t\tif (TCP_SKB_CB(skb)-&gt;end_seq != TCP_SKB_CB(skb)-&gt;seq &amp;&amp;\t\t    after(TCP_SKB_CB(skb)-&gt;end_seq - th-&gt;fin, tp-&gt;rcv_nxt)) &#123;\t\t\t/* Receive out of order FIN after close() */\t\t\tif (tp-&gt;syn_fastopen &amp;&amp; th-&gt;fin)\t\t\t\ttcp_fastopen_active_disable(sk);\t\t\ttcp_done(sk);\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\t\t\treturn 1;\t\t&#125;\t\t/* 这里的代码逻辑如下：首先是围绕这个tmo来处理，这个tmo的值简单可以概括成用户是否配置，\t\t如果配置了那就要看这个值大还是小，如果没有配置，那就是\t\t走正常1分钟超时逻辑。如果用户配置了linger2 且 大于1分钟的话，\t\t那就先启动fin_wait2d定时器，注意这个定时器到期的时间时配置的时间减去一分钟，\t\t然后在这样就会先在 FIN_WAIT2 等 tmo - TIMEWAIT_LEN，\t\t到点再进 TIME_WAIT*/\t\t//用户配置的时间，或者是默认的60s\t\ttmo = tcp_fin_time(sk);\t\t//tmo超过60s的情况,这里启动fin_wait2定时器，当定时到期还没有收到对方的fin包的话当定时器到期的时候tcp_time_wait，\t\t// 里面会启动一个定时器，如果此时收到了fin包则会找到tw套接字重新经过60秒的时间\t\t// 如果还超时了则直接释放资源。\t\t// 如果fin_wait2定时器没有到期的时候收到了fin包，则会在下tcp_fin中调用tcp_time_wait\t\tif (tmo &gt; TCP_TIMEWAIT_LEN) &#123;\t\t\t//fin_wait2定时器\t\t\tinet_csk_reset_keepalive_timer(sk, tmo - TCP_TIMEWAIT_LEN);\t\t//这里是正常抓包三次挥手时候走的逻辑，如果收到fin 或者 用户锁住了sock 就会进入这个分支\t\t// 问题是收到fin包为什么会启动一个定时器呢？不应该直接进入timewwwai吗？？\t\t// 这里的逻辑如果linger2小于60秒 则启动一个时常为linger2的定时器，如果这里右fin标志后面回直接tcp_fin的处理中回直接进入timewait状态\t\t//如果是由于用户持有锁进入这个分支则启动一个60s的fin_wati2定时器当定时器到期还没有收到fin 的话就rst，如果期间收到了fin的话则tcp_fin会处理\t\t&#125; else if (th-&gt;fin || sock_owned_by_user(sk)) &#123;\t\t\t/* Bad case. We could lose such FIN otherwise.\t\t\t * It is not a big problem, but it looks confusing\t\t\t * and not so rare event. We still can lose it now,\t\t\t * if it spins in bh_lock_sock(), but it is really\t\t\t * marginal case.\t\t\t */\t\t\t////对端的数据包带fin或者用户持有sock\t\t\t//fin_wait2定时器 tcp_keepalive_timer\t\t\tinet_csk_reset_keepalive_timer(sk, tmo);\t\t&#125; else &#123;\t\t//正常四次挥手的逻辑，没有fin 标志位，这里启动一个60s的fin_wait2定时器\t\t//注意这里直接创建了tw套接字，进入了tw状态，如果60s每有收到fin则直接释放资源，如果收到了fin则在\t\t//外层中会直接找到这个tw套接字，并重新开始计时！！！！，这个和上面linger2大于60 有点类似 本质上是防止对端不发fin 我方不释放资源\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\t\t\tgoto consume;\t\t&#125;\t\tbreak;\t&#125;\t...\t&#125;\t/* step 6: check the URG bit */\ttcp_urg(sk, skb, th);\t/* step 7: process the segment text */\tswitch (sk-&gt;sk_state) &#123;\tcase TCP_CLOSE_WAIT:\tcase TCP_CLOSING:\tcase TCP_LAST_ACK:\t\tif (!before(TCP_SKB_CB(skb)-&gt;seq, tp-&gt;rcv_nxt)) &#123;\t\t\t/* If a subflow has been reset, the packet should not\t\t\t * continue to be processed, drop the packet.\t\t\t */\t\t\tif (sk_is_mptcp(sk) &amp;&amp; !mptcp_incoming_options(sk, skb))\t\t\t\tgoto discard;\t\t\tbreak;\t\t&#125;\t\tfallthrough;\tcase TCP_FIN_WAIT1:\tcase TCP_FIN_WAIT2:\t\t/* RFC 793 says to queue data in these states,\t\t * RFC 1122 says we MUST send a reset.\t\t * BSD 4.4 also does reset.\t\t */\t\t//收到了对端的fin\t\tif (sk-&gt;sk_shutdown &amp; RCV_SHUTDOWN) &#123;\t\t\t//如果数据包携带数据，发rst复位连接\t\t\tif (TCP_SKB_CB(skb)-&gt;end_seq != TCP_SKB_CB(skb)-&gt;seq &amp;&amp;\t\t\t    after(TCP_SKB_CB(skb)-&gt;end_seq - th-&gt;fin, tp-&gt;rcv_nxt)) &#123;\t\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\t\t\t\ttcp_reset(sk, skb);\t\t\t\treturn 1;\t\t\t&#125;\t\t&#125;\t\tfallthrough;//注意这里如果是处于finwai2状态，则直接继续执行，fin包交给tcp_data_queuec处理\tcase TCP_ESTABLISHED:\t\ttcp_data_queue(sk, skb);\t\tqueued = 1;\t\tbreak;\t&#125;\t...&#125;\n\n当主动关闭方发送fin包之后，后续收到的报文都会交由这个处理逻辑来处理，首先判断发出去的fin是否已经被确认过了，如果没有确认则直接break，如果确认了则将状态设置为TCP_FIN_WAIT2，接下来判断是否优雅的关闭（linger2 &lt;0）或者收到了fin但携带额外数据，则调用tcp_done直接关闭，不经历完整的四次挥手。\n接下来的处理则根据tmo的值走不同的处理逻辑，tmo的计算逻辑如下所示：\nstatic inline int tcp_fin_time(const struct sock *sk)&#123;\t//如果设置了linger2就用用户配置的，否则用系统的\tint fin_timeout = tcp_sk(sk)-&gt;linger2 ? :\t\tREAD_ONCE(sock_net(sk)-&gt;ipv4.sysctl_tcp_fin_timeout);\t//在拿到rto\tconst int rto = inet_csk(sk)-&gt;icsk_rto;\t//如果基础超时时间 fin_timeout小于3.5 * RTO 则设置为3.5个rto \t// 大概率不会进来吧 \tif (fin_timeout &lt; (rto &lt;&lt; 2) - (rto &gt;&gt; 1))\t\tfin_timeout = (rto &lt;&lt; 2) - (rto &gt;&gt; 1);\treturn fin_timeout;&#125;\n\n如果用户配置了大于60s的tmo则启动一个finwait2定时器，如果这个定时器到期了则会创建一个TCP_FIN_WAIT2状态的tw套接字。如果没到期的情况下收到了fin包则会被下面的tcp_fin处理，启动正常tw状态的定时器。\n如果没有配置tmo，则进一步判断当前数据包是否已经携带了fin（四次握手合并成了三次的情况）或者sk当前被用户持有则会启动fin_wait2定时器，防止对端不发送fin（用户被锁住的情况），如果携带了fin则会在下面的处理逻辑中同样交由tcp_fin处理（注意法fall through），里面会启动tw状态的定时器，注意这里和第一种启动定时器的类似，都是复用保活定时器(保活定时器中增加了对fin_wait2状态的特殊处理)具体代码如下所示：\nstatic void tcp_keepalive_timer (struct timer_list *t)&#123;\t...\ttcp_mstamp_refresh(tp);\t//  这个应该是fin_wait2 定时器，处于fin_wait2状态，且sock 为dead(这里的dead是tcp close中设置的)\t//\t只有两个地方激活定时器后才会走到这个分支，一个是tcp_close 另一个是tcp_rcv_state_process()\tif (sk-&gt;sk_state == TCP_FIN_WAIT2 &amp;&amp; sock_flag(sk, SOCK_DEAD)) &#123;\t\t//用户是否设置linger2\t\tif (READ_ONCE(tp-&gt;linger2) &gt;= 0) &#123;\t\t\t//这里减去了timewait时间为什么？ 没太理解 因为linger2 是fin_wait2到结束的时间？？？ \t\t\tconst int tmo = tcp_fin_time(sk) - TCP_TIMEWAIT_LEN;\t\t\tif (tmo &gt; 0) &#123;\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\t\t\t\tgoto out;\t\t\t&#125;\t\t&#125;\t\t//如果没有设置linger2，因为定时器到期了，这里直接发rst\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\t\tgoto death;\t&#125;\t...&#125;\n\n如果当前数据包没有fin标志位（正常四次挥手逻辑），则会调用tcp_time_wait创建一个TCP_FIN_WAIT2类型的tw，用于防止对端不发送fin，注意这里已经把原来的sk从ehash中删除了转而用tw sock取代。如果60s没有有收到fin则直接释放资源，否则在外层中会直接找到这个tw套接字，并重新开始计时。\ntcp_time_wait实现如下所示：\n * Move a socket to time-wait or dead fin-wait-2 state. */void tcp_time_wait(struct sock *sk, int state, int timeo)&#123;\tconst struct inet_connection_sock *icsk = inet_csk(sk);\tconst struct tcp_sock *tp = tcp_sk(sk);\tstruct net *net = sock_net(sk);\tstruct inet_timewait_sock *tw;\t//申请一个tw，并挂一个tw的定时器，这里注意是有可能直接创建失败的。\ttw = inet_twsk_alloc(sk, &amp;net-&gt;ipv4.tcp_death_row, state);\t//把必要的字段复制到tw中\tif (tw) &#123;\t\tstruct tcp_timewait_sock *tcptw = tcp_twsk((struct sock *)tw);\t\tconst int rto = (icsk-&gt;icsk_rto &lt;&lt; 2) - (icsk-&gt;icsk_rto &gt;&gt; 1);\t\ttw-&gt;tw_transparent\t= inet_test_bit(TRANSPARENT, sk);\t\ttw-&gt;tw_mark\t\t= sk-&gt;sk_mark;\t\ttw-&gt;tw_priority\t\t= sk-&gt;sk_priority;\t\ttw-&gt;tw_rcv_wscale\t= tp-&gt;rx_opt.rcv_wscale;\t\ttcptw-&gt;tw_rcv_nxt\t= tp-&gt;rcv_nxt;\t\ttcptw-&gt;tw_snd_nxt\t= tp-&gt;snd_nxt;\t\ttcptw-&gt;tw_rcv_wnd\t= tcp_receive_window(tp);\t\ttcptw-&gt;tw_ts_recent\t= tp-&gt;rx_opt.ts_recent;\t\ttcptw-&gt;tw_ts_recent_stamp = tp-&gt;rx_opt.ts_recent_stamp;\t\ttcptw-&gt;tw_ts_offset\t= tp-&gt;tsoffset;\t\ttcptw-&gt;tw_last_oow_ack_time = 0;\t\ttcptw-&gt;tw_tx_delay\t= tp-&gt;tcp_tx_delay;\t\ttw-&gt;tw_txhash\t\t= sk-&gt;sk_txhash;#if IS_ENABLED(CONFIG_IPV6)\t\tif (tw-&gt;tw_family == PF_INET6) &#123;\t\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\t\t\ttw-&gt;tw_v6_daddr = sk-&gt;sk_v6_daddr;\t\t\ttw-&gt;tw_v6_rcv_saddr = sk-&gt;sk_v6_rcv_saddr;\t\t\ttw-&gt;tw_tclass = np-&gt;tclass;\t\t\ttw-&gt;tw_flowlabel = be32_to_cpu(np-&gt;flow_label &amp; IPV6_FLOWLABEL_MASK);\t\t\ttw-&gt;tw_ipv6only = sk-&gt;sk_ipv6only;\t\t&#125;#endif\t\ttcp_time_wait_init(sk, tcptw);\t\t/* Get the TIME_WAIT timeout firing. */\t\t//确保比rto时间要长\t\tif (timeo &lt; rto)\t\t\ttimeo = rto;\t\tif (state == TCP_TIME_WAIT)\t\t\ttimeo = TCP_TIMEWAIT_LEN;\t\t/* tw_timer is pinned, so we need to make sure BH are disabled\t\t * in following section, otherwise timer handler could run before\t\t * we complete the initialization.\t\t */\t\tlocal_bh_disable();\t\t//启动定时器\t\tinet_twsk_schedule(tw, timeo);\t\t/* Linkage updates.\t\t * Note that access to tw after this point is illegal.\t\t */\t\t//hashdance，把完全的sk从ehash中移除 换成tw sock插入进去\t\tinet_twsk_hashdance(tw, sk, net-&gt;ipv4.tcp_death_row.hashinfo);\t\tlocal_bh_enable();\t&#125; else &#123;\t\t/* Sorry, if we&#x27;re out of memory, just CLOSE this\t\t * socket up.  We&#x27;ve got bigger problems than\t\t * non-graceful socket closings.\t\t */\t\tNET_INC_STATS(net, LINUX_MIB_TCPTIMEWAITOVERFLOW);\t&#125;\t//保存本次连接的拥塞信息，指导下次同一条流的三次握手\ttcp_update_metrics(sk);\ttcp_done(sk);&#125;\n\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP断连"]},{"title":"TCP四次挥手-主动关闭（二）","url":"/2026/01/28/TCP%E8%BF%9E%E6%8E%A5%E7%9A%84%E7%BB%88%E6%AD%A2%EF%BC%88%E4%BA%94%EF%BC%89/","content":"tcp_time_wait主要做了四件事，创建tw套接字，拷贝sk必要的字段到tw sock中，插入ehash中（完成hashdance），释放sk资源。\n上述创建tw套接字的过程中设置了tw定时器的回调(注意并没有激活定时器)，具体代码如下所示：\nstruct inet_timewait_sock *inet_twsk_alloc(const struct sock *sk,\t\t\t\t\t   struct inet_timewait_death_row *dr,\t\t\t\t\t   const int state)&#123;\tstruct inet_timewait_sock *tw;\t//是否超过了最大的tw套接字的数量\tif (refcount_read(&amp;dr-&gt;tw_refcount) - 1 &gt;=\t    READ_ONCE(dr-&gt;sysctl_max_tw_buckets))\t\treturn NULL;\t//申请tw套接字\ttw = kmem_cache_alloc(sk-&gt;sk_prot_creator-&gt;twsk_prot-&gt;twsk_slab,\t\t\t      GFP_ATOMIC);\t//copy必要的字段\tif (tw) &#123;\t\tconst struct inet_sock *inet = inet_sk(sk);\t\ttw-&gt;tw_dr\t    = dr;\t\t/* Give us an identity. */\t\ttw-&gt;tw_daddr\t    = inet-&gt;inet_daddr;\t\ttw-&gt;tw_rcv_saddr    = inet-&gt;inet_rcv_saddr;\t\ttw-&gt;tw_bound_dev_if = sk-&gt;sk_bound_dev_if;\t\ttw-&gt;tw_tos\t    = inet-&gt;tos;\t\ttw-&gt;tw_num\t    = inet-&gt;inet_num;\t\ttw-&gt;tw_state\t    = TCP_TIME_WAIT;\t\ttw-&gt;tw_substate\t    = state; //这个状态很关键，用于区别是什么状态下收到的套接字\t\ttw-&gt;tw_sport\t    = inet-&gt;inet_sport;\t\ttw-&gt;tw_dport\t    = inet-&gt;inet_dport;\t\ttw-&gt;tw_family\t    = sk-&gt;sk_family;\t\ttw-&gt;tw_reuse\t    = sk-&gt;sk_reuse;\t\ttw-&gt;tw_reuseport    = sk-&gt;sk_reuseport;\t\ttw-&gt;tw_hash\t    = sk-&gt;sk_hash;\t\ttw-&gt;tw_ipv6only\t    = 0;\t\ttw-&gt;tw_transparent  = inet_test_bit(TRANSPARENT, sk);\t\ttw-&gt;tw_prot\t    = sk-&gt;sk_prot_creator;\t\tatomic64_set(&amp;tw-&gt;tw_cookie, atomic64_read(&amp;sk-&gt;sk_cookie));\t\ttwsk_net_set(tw, sock_net(sk));\t\t//这里设置tw 定时器的回调\t\ttimer_setup(&amp;tw-&gt;tw_timer, tw_timer_handler, TIMER_PINNED);\t\t/*\t\t * Because we use RCU lookups, we should not set tw_refcnt\t\t * to a non null value before everything is setup for this\t\t * timewait socket.\t\t */\t\trefcount_set(&amp;tw-&gt;tw_refcnt, 0);\t\t__module_get(tw-&gt;tw_prot-&gt;owner);\t&#125;\treturn tw;&#125;\n\n重点关注tw-&gt;tw_substate状态的处理，这里区分了当前tw sock 是finwait2状态创建的还是timewati状态下创建的。当创建tw状态套接字插入到ehash中之后，后续收到的数据包都找到的是tw状态的sk并跳转到tw状态进行处理，具体代码逻辑如下：\nint tcp_v4_rcv(struct sk_buff *skb)&#123;\t...\tsk = __inet_lookup_skb(net-&gt;ipv4.tcp_death_row.hashinfo,\t\t\t       skb, __tcp_hdrlen(th), th-&gt;source,\t\t\t       th-&gt;dest, sdif, &amp;refcounted);\tif (!sk)\t\tgoto no_tcp_socket;process:\tif (sk-&gt;sk_state == TCP_TIME_WAIT)\t\tgoto do_time_wait;\t...//tw状态的处理do_time_wait:\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) &#123;\t\tdrop_reason = SKB_DROP_REASON_XFRM_POLICY;\t\tinet_twsk_put(inet_twsk(sk));\t\tgoto discard_it;\t&#125;\ttcp_v4_fill_cb(skb, iph, th);\t//计算校验和\tif (tcp_checksum_complete(skb)) &#123;\t\tinet_twsk_put(inet_twsk(sk));\t\tgoto csum_error;\t&#125;\t//决定怎么处理这个包\tswitch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) &#123;\t//允许重新开始三次握手\tcase TCP_TW_SYN: &#123;\t\t//找到监听套接字\t\tstruct sock *sk2 = inet_lookup_listener(net,\t\t\t\t\t\t\tnet-&gt;ipv4.tcp_death_row.hashinfo,\t\t\t\t\t\t\tskb, __tcp_hdrlen(th),\t\t\t\t\t\t\tiph-&gt;saddr, th-&gt;source,\t\t\t\t\t\t\tiph-&gt;daddr, th-&gt;dest,\t\t\t\t\t\t\tinet_iif(skb),\t\t\t\t\t\t\tsdif);\t\tif (sk2) &#123;\t\t\t//将tw套接字直接释放\t\t\tinet_twsk_deschedule_put(inet_twsk(sk));\t\t\tsk = sk2;\t\t\ttcp_v4_restore_cb(skb);\t\t\trefcounted = false;\t\t\tgoto process;//注意这里回到三次握手的处理中\t\t&#125;\t&#125;\t\t/* to ACK */\t\tfallthrough;\tcase TCP_TW_ACK:\t\t//对端重传fin，或者发了窗口外的数据包这里在回复一个ack，或者最后一个ack\t\ttcp_v4_timewait_ack(sk, skb);\t\tbreak;\tcase TCP_TW_RST:\t\t//比如在finwait2阶段收到了syn包\t\ttcp_v4_send_reset(sk, skb);\t\t//真正释放资源\t\tinet_twsk_deschedule_put(inet_twsk(sk));\t\tgoto discard_it;\tcase TCP_TW_SUCCESS:;\t&#125;\tgoto discard_it;&#125;\n\ngoto到timewait处理逻辑后首先调用tcp_timewait_state_process根据返回值决定怎么处理这个数据包，tcp_timewait_state_process具体实现如下所示：\nenum tcp_tw_statustcp_timewait_state_process(struct inet_timewait_sock *tw, struct sk_buff *skb,\t\t\t   const struct tcphdr *th)&#123;\tstruct tcp_options_received tmp_opt;\tstruct tcp_timewait_sock *tcptw = tcp_twsk((struct sock *)tw);\tbool paws_reject = false;\ttmp_opt.saw_tstamp = 0;\t//如果存在时间戳，再通过PAWS机制判断是否合法\tif (th-&gt;doff &gt; (sizeof(*th) &gt;&gt; 2) &amp;&amp; tcptw-&gt;tw_ts_recent_stamp) &#123;\t\ttcp_parse_options(twsk_net(tw), skb, &amp;tmp_opt, 0, NULL);\t\tif (tmp_opt.saw_tstamp) &#123;\t\t\tif (tmp_opt.rcv_tsecr)\t\t\t\ttmp_opt.rcv_tsecr -= tcptw-&gt;tw_ts_offset;\t\t\ttmp_opt.ts_recent\t= tcptw-&gt;tw_ts_recent;\t\t\ttmp_opt.ts_recent_stamp\t= tcptw-&gt;tw_ts_recent_stamp;\t\t\tpaws_reject = tcp_paws_reject(&amp;tmp_opt, th-&gt;rst);\t\t&#125;\t&#125;\t//如果是finwait2状态下创建的套接字\tif (tw-&gt;tw_substate == TCP_FIN_WAIT2) &#123;\t\t/* Just repeat all the checks of tcp_rcv_state_process() */\t\t/* Out of window, send ACK */\t\t//如果没有通过paws机制检查，或者超出了窗口则会回一个ack\t\tif (paws_reject ||\t\t    !tcp_in_window(TCP_SKB_CB(skb)-&gt;seq, TCP_SKB_CB(skb)-&gt;end_seq,\t\t\t\t   tcptw-&gt;tw_rcv_nxt,\t\t\t\t   tcptw-&gt;tw_rcv_nxt + tcptw-&gt;tw_rcv_wnd))\t\t\treturn tcp_timewait_check_oow_rate_limit(\t\t\t\ttw, skb, LINUX_MIB_TCPACKSKIPPEDFINWAIT2);\t\t//如果通过了检查携带rst，直接清除tw套接字释放资源\t\tif (th-&gt;rst)\t\t\tgoto kill;\t\t//TCP_FIN_WAIT2状态下收到syn包 同时syn包大于下一个待接收的序列号，则直接回rst\t\tif (th-&gt;syn &amp;&amp; !before(TCP_SKB_CB(skb)-&gt;seq, tcptw-&gt;tw_rcv_nxt))\t\t\treturn TCP_TW_RST;\t\t/* Dup ACK? */\t\t//没有ack标志位，或者是重复的ack\t\tif (!th-&gt;ack ||\t\t    !after(TCP_SKB_CB(skb)-&gt;end_seq, tcptw-&gt;tw_rcv_nxt) ||\t\t    TCP_SKB_CB(skb)-&gt;end_seq == TCP_SKB_CB(skb)-&gt;seq) &#123;\t\t\tinet_twsk_put(tw);\t\t\treturn TCP_TW_SUCCESS; //外面默默丢弃了\t\t&#125;\t\t/* New data or FIN. If new data arrive after half-duplex close,\t\t * reset.\t\t */\t\t//不是fin或者是新数据，直接回rst，这里是意味着finwait2就不能接对端继续发呆数据包吗？\t\t//注意可能是因为这里是tw套接字承载着finwait2\t\tif (!th-&gt;fin ||\t\t    TCP_SKB_CB(skb)-&gt;end_seq != tcptw-&gt;tw_rcv_nxt + 1)\t\t\treturn TCP_TW_RST;\t\t/* FIN arrived, enter true time-wait state. */\t\t//走到这里表示收到了合法的fin包\t\ttw-&gt;tw_substate\t  = TCP_TIME_WAIT;\t\t//更新下一个待接收的序号\t\ttcptw-&gt;tw_rcv_nxt = TCP_SKB_CB(skb)-&gt;end_seq;\t\tif (tmp_opt.saw_tstamp) &#123;\t\t\ttcptw-&gt;tw_ts_recent_stamp = ktime_get_seconds();\t\t\ttcptw-&gt;tw_ts_recent\t  = tmp_opt.rcv_tsval;\t\t&#125;\t\t//注意这里是重新开始tw 计时\t\tinet_twsk_reschedule(tw, TCP_TIMEWAIT_LEN);\t\treturn TCP_TW_ACK;\t&#125;\t/*\t *\tNow real TIME-WAIT state.\t *\t *\tRFC 1122:\t *\t&quot;When a connection is [...] on TIME-WAIT state [...]\t *\t[a TCP] MAY accept a new SYN from the remote TCP to\t *\treopen the connection directly, if it:\t *\t *\t(1)  assigns its initial sequence number for the new\t *\tconnection to be larger than the largest sequence\t *\tnumber it used on the previous connection incarnation,\t *\tand\t *\t *\t(2)  returns to TIME-WAIT state if the SYN turns out\t *\tto be an old duplicate&quot;.\t */\t//正常timewait状态的处理，通过了检查如果是纯ack或者是rst\tif (!paws_reject &amp;&amp;\t    (TCP_SKB_CB(skb)-&gt;seq == tcptw-&gt;tw_rcv_nxt &amp;&amp;\t     (TCP_SKB_CB(skb)-&gt;seq == TCP_SKB_CB(skb)-&gt;end_seq || th-&gt;rst))) &#123;\t\t/* In window segment, it may be only reset or bare ack. */\t\t//如果收到了rst 直接释放资源\t\tif (th-&gt;rst) &#123;\t\t\t/* This is TIME_WAIT assassination, in two flavors.\t\t\t * Oh well... nobody has a sufficient solution to this\t\t\t * protocol bug yet.\t\t\t */\t\t\tif (!READ_ONCE(twsk_net(tw)-&gt;ipv4.sysctl_tcp_rfc1337)) &#123;kill:\t\t\t\tinet_twsk_deschedule_put(tw);\t\t\t\treturn TCP_TW_SUCCESS;\t\t\t&#125;\t\t&#125; else &#123;\t\t\t//重新开始定时器，因为可能是ack丢失了\t\t\tinet_twsk_reschedule(tw, TCP_TIMEWAIT_LEN);\t\t&#125;\t\tif (tmp_opt.saw_tstamp) &#123;\t\t\ttcptw-&gt;tw_ts_recent\t  = tmp_opt.rcv_tsval;\t\t\ttcptw-&gt;tw_ts_recent_stamp = ktime_get_seconds();\t\t&#125;\t\tinet_twsk_put(tw);\t\treturn TCP_TW_SUCCESS;\t&#125;\t/* Out of window segment.\t   All the segments are ACKed immediately.\t   The only exception is new SYN. We accept it, if it is\t   not old duplicate and we are not in danger to be killed\t   by delayed old duplicates. RFC check is that it has\t   newer sequence number works at rates &lt;40Mbit/sec.\t   However, if paws works, it is reliable AND even more,\t   we even may relax silly seq space cutoff.\t   RED-PEN: we violate main RFC requirement, if this SYN will appear\t   old duplicate (i.e. we receive RST in reply to SYN-ACK),\t   we must return socket to time-wait state. It is not good,\t   but not fatal yet.\t */\t//窗口外的数据包，如果是纯syn包，注意这里会重新在外面找listne套接字\tif (th-&gt;syn &amp;&amp; !th-&gt;rst &amp;&amp; !th-&gt;ack &amp;&amp; !paws_reject &amp;&amp;\t    (after(TCP_SKB_CB(skb)-&gt;seq, tcptw-&gt;tw_rcv_nxt) ||//序列号大于下一个待接收的\t     (tmp_opt.saw_tstamp &amp;&amp;\t      (s32)(tcptw-&gt;tw_ts_recent - tmp_opt.rcv_tsval) &lt; 0))) &#123;//时间戳合理\t\tu32 isn = tcptw-&gt;tw_snd_nxt + 65535 + 2;//必须大于旧连接的序列号\t\tif (isn == 0)\t\t\tisn++;\t\tTCP_SKB_CB(skb)-&gt;tcp_tw_isn = isn;\t\treturn TCP_TW_SYN;\t&#125;\t//由于PAWS拒绝\tif (paws_reject)\t\t__NET_INC_STATS(twsk_net(tw), LINUX_MIB_PAWSESTABREJECTED);\t//没有携带rst，但是由于序列号被拒绝 会ack，但是限速\tif (!th-&gt;rst) &#123;\t\t/* In this case we must reset the TIMEWAIT timer.\t\t *\t\t * If it is ACKless SYN it may be both old duplicate\t\t * and new good SYN with random sequence number &lt;rcv_nxt.\t\t * Do not reschedule in the last case.\t\t */\t\tif (paws_reject || th-&gt;ack)\t\t\tinet_twsk_reschedule(tw, TCP_TIMEWAIT_LEN);\t\treturn tcp_timewait_check_oow_rate_limit(\t\t\ttw, skb, LINUX_MIB_TCPACKSKIPPEDTIMEWAIT);\t&#125;\tinet_twsk_put(tw);\treturn TCP_TW_SUCCESS;&#125;\n\ntcp_timewait_state_process首先PAWS机制检查数据包是否合法，\n如果当前的tw套接字是fin_wait2状态下创建的，具体处理情况如下所示：\n\n窗口外 &#x2F; PAWS 失 回 ACK（限速）\n收到 RST 直接 kill tw_sock\n收到 SYN 且 seq 合法 → 回 RST\n防止旧连接干扰新连接建立。\n纯 ACK &#x2F; 重复 ACK  丢弃报文\n如果不是 FIN，或者 FIN 位置不对 回RST\n收到了和发fin真正进入timewait状态\n\n如果当前的tw套接字是time_wait状态下创建的则具体处理逻辑如下所示：\n\n窗口内收到纯 ACK 重置 TIME_WAIT\n窗口内收到 RST 什么也不做？\n窗口外默认回 ACK（限速）\n窗口外新 SYN 可能复活连接（返回 TCP_TW_SYN）\n\n回到外层timewait的处理逻辑，这里根据不同的返回值执行具体的动作，具体处理逻辑如下：\nTCP_TW_SYN： 去查是否有对应的监听 socket，有的话：直接销毁 timewait sock，把这个包交回正常三次握手流程（相当于 TIME_WAIT 让路，复活四元组）\nTCP_TW_ACK：通常是对端重传 FIN 或发了窗口外数据，回一个 ACK，维持协议正确性（同时不让对方继续重传）\nTCP_TW_RST：这个包在当前语义下是非法的（比如 FIN_WAIT2 阶段收到 SYN）回 RST，并把 timewait sock 彻底释放\nTCP_TW_SUCCESS：什么都不用做说明这个包已经被判定为可以默默丢弃\n","categories":["网络协议栈源码学习"],"tags":["TCP","TCP断连"]}]